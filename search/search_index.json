{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindocr","title":"MindOCR","text":"<p>English | \u4e2d\u6587</p> <p>Introduction | Installation | Quick Start | Model List | Notes</p>"},{"location":"#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore. It helps users to train and apply the best text detection and recognition models, such as DBNet/DBNet++ and CRNN/SVTR, to fulfill image-text understanding needs.</p>  Major Features   - **Modulation design**: We decouple the OCR task into several configurable modules. Users can set up the training and evaluation pipeline easily for customized data and models with a few lines of modification. - **High-performance**: MindOCR provides pretrained weights and the used training recipes that reach competitive performance on OCR tasks. - **Low-cost-to-apply**: We provide easy-to-use inference tools to perform text detection and recognition tasks."},{"location":"#installation","title":"Installation","text":""},{"location":"#dependency","title":"Dependency","text":"<p>To install the dependency, please run <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Additionally, please install MindSpore(&gt;=1.9) following the official installation instructions for the best fit of your machine.</p> <p>For distributed training, please install openmpi 4.0.3.</p> Environment Version MindSpore &gt;=1.9 Python &gt;=3.7 <p>Notes: - If you use ACL for Inference, the version of Python should be 3.9. - If scikit_image cannot be imported, you can use the following command line to set environment variable <code>$LD_PRELOAD</code> referring to here. Change <code>path/to</code> to your directory.   <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"#install-with-pypi","title":"Install with PyPI","text":"<p>Coming soon</p>"},{"location":"#install-from-source","title":"Install from Source","text":"<p>The latest version of MindOCR can be installed as follows: <pre><code>pip install git+https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>Notes: MindOCR is only tested on MindSpore&gt;=1.9, Linux on GPU/Ascend devices currently.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-model-training-and-evaluation","title":"1. Model Training and Evaluation","text":""},{"location":"#11-text-detection","title":"1.1 Text Detection","text":"<p>We will take DBNet model and ICDAR2015 dataset as an example to illustrate how to configure the training process with a few lines of modification on the yaml file.</p> <p>Please refer to DBNet Readme for detailed instructions.</p>"},{"location":"#12-text-recognition","title":"1.2 Text Recognition","text":"<p>We will take CRNN model and LMDB dataset as an illustration on how to configure and launch the training process easily.</p> <p>Detailed instructions can be viewed in CRNN Readme.</p> <p>Note: The training pipeline is fully extendable. To train other text detection/recognition models on a new dataset, please configure the model architecture (backbone, neck, head) and data pipeline in the yaml file and launch the training script with <code>python tools/train.py -c /path/to/yaml_config</code>.</p>"},{"location":"#2-inference-and-deployment","title":"2. Inference and Deployment","text":""},{"location":"#21-inference-with-mindspore-lite-and-acl-on-ascend-310","title":"2.1 Inference with MindSpore Lite and ACL on Ascend 310","text":"<p>MindOCR supports OCR model inference with MindSpore Lite and ACL (Ascend Computation Language)  backends. It integrates efficient text detection, classification and recognition inference pipeline for deployment.</p> <p>Please refer to MindOCR Inference on Ascend 310 for detailed illustrations.</p>"},{"location":"#22-inference-with-native-mindspore-on-ascend910gpucpu","title":"2.2 Inference with native MindSpore on Ascend910/GPU/CPU","text":"<p>MindOCR provides easy-to-use text detection and recognition inference tools supporting CPU/GPU/Ascend910 devices, based on the MindOCR-trained models.</p> <p>Please refer to MindOCR Online Inference for details.</p>"},{"location":"#model-list","title":"Model List","text":"Text Detection  - [x] [DBNet](../../configs/det/dbnet/README.md) (AAAI'2020) - [x] [DBNet++](../../configs/det/dbnet/README.md) (TPAMI'2022) - [x] [PSENet](../../configs/det/psenet/README.md) (CVPR'2019) - [x] [EAST](../../configs/det/east/README.md)(CVPR'2017) - [ ] [FCENet](https://arxiv.org/abs/2104.10442) (CVPR'2021) [coming soon]   Text Recognition  - [x] [CRNN](../../configs/rec/crnn/README.md) (TPAMI'2016) - [x] [CRNN-Seq2Seq/RARE](../../configs/rec/rare/README.md) (CVPR'2016) - [x] [SVTR](../../configs/rec/svtr/README.md) (IJCAI'2022) - [ ] [ABINet](https://arxiv.org/abs/2103.06495) (CVPR'2021) [coming soon]   For the detailed performance of the trained models, please refer to [configs](../../configs).  For detailed support for MindSpore Lite and ACL inference models, please refer to [MindOCR Models Support List](inference/models_list_en.md) and [Third-Party Models Support List](inference/models_list_thirdparty_en.md).  ## Datasets  ### Download  We give instructions on how to download the following datasets.   Text Detection  - [x] ICDAR2015 [paper](https://rrc.cvc.uab.es/files/short_rrc_2015.pdf) [homepage](https://rrc.cvc.uab.es/?ch=4) [download instruction](datasets/icdar2015.md)  - [x] Total-Text [paper](https://arxiv.org/abs/1710.10400) [homepage](https://github.com/cs-chan/Total-Text-Dataset/tree/master/Dataset) [download instruction](datasets/totaltext.md)  - [x] Syntext150k [paper](https://arxiv.org/abs/2002.10200) [homepage](https://github.com/aim-uofa/AdelaiDet) [download instruction](datasets/syntext150k.md)  - [x] MLT2017 [paper](https://ieeexplore.ieee.org/abstract/document/8270168) [homepage](https://rrc.cvc.uab.es/?ch=8&amp;com=introduction) [download instruction](datasets/mlt2017.md)  - [x] MSRA-TD500 [paper](https://ieeexplore.ieee.org/abstract/document/6247787) [homepage](http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500)) [download instruction](datasets/td500.md)  - [x] SCUT-CTW1500 [paper](https://www.sciencedirect.com/science/article/pii/S0031320319300664) [homepage](https://github.com/Yuliang-Liu/Curve-Text-Detector) [download instruction](datasets/ctw1500.md)    ### Conversion  After downloading these datasets in the `DATASETS_DIR` folder, you can run `bash tools/convert_datasets.sh` to convert all downloaded datasets into the target format. [Here](../../tools/dataset_converters/README.md) is an example of icdar2015 dataset converting.  ## Notes  ### Change Log  - 2023/06/07 1. Add new trained models     - [PSENet](../../configs/det/psenet) for text detection     - [EAST](../../configs/det/east) for text detection     - [SVTR](../../configs/rec/svtr) for text recognition 2. Add more benchmark datasets and their results     - [totaltext](datasets/totaltext.md)     - [mlt2017](datasets/mlt2017.md)     - [chinese_text_recognition](datasets/chinese_text_recognition.md) 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the `resume` parameter under the `model` field in the yaml config, e.g.,`resume: True`, load and resume training from {ckpt_save_dir}/train_resume.ckpt or `resume: /path/to/train_resume.ckpt`, load and resume training from the given path.  - 2023/05/15 1. Add new trained models     - [DBNet++](../../configs/det/dbnet) for text detection     - [CRNN-Seq2Seq](../../configs/rec/rare) for text recognition     - DBNet pretrained on SynthText is now available: [checkpoint url](https://download.mindspore.cn/toolkits/mindocr/dbnet/dbnet_resnet50_synthtext-40655acb.ckpt) 2. Add more benchmark datasets and their results     - [SynthText](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c), [MSRA-TD500](datasets/td500.md), [CTW1500](datasets/ctw1500.md)     - More benchmark results for DBNet are reported [here](../../configs/det/dbnet/README.md). 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refractored. 5. Bug fix: use meter to average loss for large datasets, disable `pred_cast_fp32` for ctcloss in AMP training, fix error when invalid polygons exist.  - 2023/05/04 1. Support loading self-defined pretrained checkpoints via setting `model-pretrained` with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting `train-ema` (default: False) and `train-ema_decay` in the yaml config. 4. Arg parameter changed\uff1a`num_columns_to_net` -&gt; `net_input_column_index`: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a`num_columns_of_labels` -&gt; `label_column_index`: change the column number corresponds to the label to the column index.  - 2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add `grouping_strategy` argument in yaml config to select a predefined grouping strategy, or use `no_weight_decay_params` argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in `configs/rec/crnn/crnn_icdar15.yaml` 2. Add gradient accumulation to support large batch size training. Usage: add `gradient_accumulation_steps` in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in `configs/rec/crnn/crnn_icdar15.yaml` 3. Add gradient clip to support training stablization. Enable it by setting `grad_clip` as True in yaml config.  - 2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set `type` of `loss_scale` as `dynamic`. A YAML example can be viewed in `configs/rec/crnn/crnn_icdar15.yaml`  - 2023/03/20 1. Arg names changed: `output_keys` -&gt; `output_columns`, `num_keys_to_net` -&gt; `num_columns_to_net` 2. Data pipeline updated  - 2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:   <pre><code>  i)   Create a new training task on the openi cloud platform.\n  ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n  iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n  iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n  v)   Fill in other blanks and launch.\n</code></pre>  - 2023/03/08 1. Add evaluation script with  arg `ckpt_load_path` 2. Arg `ckpt_save_dir` is moved from `system` to `train` in yaml. 3. Add drop_overflow_update control  ### How to Contribute  We appreciate all kinds of contributions including issues and PRs to make MindOCR better.  Please refer to [CONTRIBUTING.md](../../CONTRIBUTING.md) for the contributing guideline. Please follow the [Model Template and Guideline](../../mindocr/models/README.md) for contributing a model that fits the overall interface :)  ### License  This project follows the [Apache License 2.0](../../LICENSE) open-source license.  ### Citation  If you find this project useful in your research, please consider citing:  <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"index_mainpage/","title":"Index mainpage.md","text":""},{"location":"index_mainpage/#mindocr","title":"MindOCR","text":""},{"location":"index_mainpage/#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore. It helps users to train and apply the best text detection and recognition models, such as DBNet/DBNet++ and CRNN/SVTR, to fulfill image-text understanding needs.</p>  Major Features   - **Modulation design**: We decouple the OCR task into several configurable modules. Users can set up the training and evaluation pipeline easily for customized data and models with a few lines of modification. - **High-performance**: MindOCR provides pretrained weights and the used training recipes that reach competitive performance on OCR tasks. - **Low-cost-to-apply**: We provide easy-to-use inference tools to perform text detection and recognition tasks."},{"location":"index_mainpage/#installation","title":"Installation","text":""},{"location":"index_mainpage/#dependency","title":"Dependency","text":"<p>To install the dependency, please run <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Additionally, please install MindSpore(&gt;=1.9) following the official installation instructions for the best fit of your machine.</p> <p>For distributed training, please install openmpi 4.0.3.</p> Environment Version MindSpore &gt;=1.9 Python &gt;=3.7 <p>Notes: - If you use ACL for Inference (refer to 2.1 inference with mindspore lite and acl), the version of Python should be 3.9. - If scikit_image cannot be imported, you can use the following command line to set environment variable <code>$LD_PRELOAD</code> referring to here. Change <code>path/to</code> to your directory.   <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"index_mainpage/#install-with-pypi","title":"Install with PyPI","text":"<p>Coming soon</p>"},{"location":"index_mainpage/#install-from-source","title":"Install from Source","text":"<p>The latest version of MindOCR can be installed as follows: <pre><code>pip install git+https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>Notes: MindOCR is only tested on MindSpore&gt;=1.9, Linux on GPU/Ascend devices currently.</p>"},{"location":"index_mainpage/#quick-start","title":"Quick Start","text":""},{"location":"index_mainpage/#1-model-training-and-evaluation","title":"1. Model Training and Evaluation","text":""},{"location":"index_mainpage/#11-text-detection","title":"1.1 Text Detection","text":"<p>We will take DBNet model and ICDAR2015 dataset as an example to illustrate how to configure the training process with a few lines of modification on the yaml file.</p> <p>Please refer to <code>configs/det/dbnet/README.md</code> for detailed instructions.</p>"},{"location":"index_mainpage/#12-text-recognition","title":"1.2 Text Recognition","text":"<p>We will take CRNN model and LMDB dataset as an illustration on how to configure and launch the training process easily.</p> <p>Detailed instructions can be viewed in <code>configs/rec/crnn/README.md</code>.</p> <p>Note: The training pipeline is fully extendable. To train other text detection/recognition models on a new dataset, please configure the model architecture (backbone, neck, head) and data pipeline in the yaml file and launch the training script with <code>python tools/train.py -c /path/to/yaml_config</code>.</p>"},{"location":"index_mainpage/#2-inference-and-deployment","title":"2. Inference and Deployment","text":""},{"location":"index_mainpage/#21-inference-with-mindspore-lite-and-acl-on-ascend-310","title":"2.1 Inference with MindSpore Lite and ACL on Ascend 310","text":"<p>MindOCR supports OCR model inference with MindSpore Lite and ACL (Ascend Computation Language)  backends. It integrates efficient text detection, classification and recognition inference pipeline for deployment.</p> <p>Please refer to MindOCR Inference on Ascend 310 for detailed illustrations.</p>"},{"location":"index_mainpage/#22-inference-with-native-mindspore-on-ascend910gpucpu","title":"2.2 Inference with native MindSpore on Ascend910/GPU/CPU","text":"<p>MindOCR provides easy-to-use text detection and recognition inference tools supporting CPU/GPU/Ascend910 devices, based on the MindOCR-trained models.</p> <p>Please refer to MindOCR Online Inference for details.</p>"},{"location":"index_mainpage/#model-list","title":"Model List","text":"Text Detection  - [x] DBNet (AAAI'2020) - [x] DBNet++ (TPAMI'2022) - [x] PSENet (CVPR'2019) - [x] EAST (CVPR'2017) - [ ] FCENet (CVPR'2021) [coming soon]   Text Recognition  - [x] CRNN (TPAMI'2016) - [x] CRNN-Seq2Seq/RARE (CVPR'2016) - [x] SVTR (IJCAI'2022) - [ ] ABINet (CVPR'2021) [coming soon]   For the detailed performance of the trained models, please refer to `configs/` directory.  For detailed support for MindSpore Lite and ACL inference models, please refer to [MindOCR Models Support List](mkdocs/inference_models_list.md) and [Third-Party Models Support List](mkdocs/inference_models_list_thirdparty.md).  ## Datasets  ### Download  We give instructions on how to download the following datasets.   Text Detection  - [x] ICDAR2015 [paper](https://rrc.cvc.uab.es/files/short_rrc_2015.pdf) [homepage](https://rrc.cvc.uab.es/?ch=4)  - [x] Total-Text [paper](https://arxiv.org/abs/1710.10400) [homepage](https://github.com/cs-chan/Total-Text-Dataset/tree/master/Dataset)  - [x] Syntext150k [paper](https://arxiv.org/abs/2002.10200) [homepage](https://github.com/aim-uofa/AdelaiDet)  - [x] MLT2017 [paper](https://ieeexplore.ieee.org/abstract/document/8270168) [homepage](https://rrc.cvc.uab.es/?ch=8&amp;com=introduction)  - [x] MSRA-TD500 [paper](https://ieeexplore.ieee.org/abstract/document/6247787) [homepage](http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500))  - [x] SCUT-CTW1500 [paper](https://www.sciencedirect.com/science/article/pii/S0031320319300664) [homepage](https://github.com/Yuliang-Liu/Curve-Text-Detector)    ### Conversion  After downloading these datasets in the `DATASETS_DIR` folder, you can run `bash tools/convert_datasets.sh` to convert all downloaded datasets into the target format. [Here](mkdocs/dataset_converters.md) is an example of icdar2015 dataset converting.  ## Notes  ### Change Log  - 2023/06/07 1. Add new trained models     - PSENet for text detection     - EAST for text detection     - SVTR for text recognition 2. Add more benchmark datasets and their results     - totaltext     - mlt2017     - chinese_text_recognition 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the `resume` parameter under the `model` field in the yaml config, e.g.,`resume: True`, load and resume training from {ckpt_save_dir}/train_resume.ckpt or `resume: /path/to/train_resume.ckpt`, load and resume training from the given path.  - 2023/05/15 1. Add new trained models     - DBNet++ for text detection     - CRNN-Seq2Seq for text recognition     - DBNet pretrained on SynthText is now available: [checkpoint url](https://download.mindspore.cn/toolkits/mindocr/dbnet/dbnet_resnet50_synthtext-40655acb.ckpt) 2. Add more benchmark datasets and their results     - [SynthText](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c), MSRA-TD500, CTW1500     - More benchmark results for DBNet are reported in `configs/det/dbnet/README.md`. 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refractored. 5. Bug fix: use meter to average loss for large datasets, disable `pred_cast_fp32` for ctcloss in AMP training, fix error when invalid polygons exist.  - 2023/05/04 1. Support loading self-defined pretrained checkpoints via setting `model-pretrained` with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting `train-ema` (default: False) and `train-ema_decay` in the yaml config. 4. Arg parameter changed\uff1a`num_columns_to_net` -&gt; `net_input_column_index`: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a`num_columns_of_labels` -&gt; `label_column_index`: change the column number corresponds to the label to the column index.  - 2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add `grouping_strategy` argument in yaml config to select a predefined grouping strategy, or use `no_weight_decay_params` argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in `configs/rec/crnn/crnn_icdar15.yaml` 2. Add gradient accumulation to support large batch size training. Usage: add `gradient_accumulation_steps` in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in `configs/rec/crnn/crnn_icdar15.yaml` 3. Add gradient clip to support training stablization. Enable it by setting `grad_clip` as True in yaml config.  - 2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set `type` of `loss_scale` as `dynamic`. A YAML example can be viewed in `configs/rec/crnn/crnn_icdar15.yaml`  - 2023/03/20 1. Arg names changed: `output_keys` -&gt; `output_columns`, `num_keys_to_net` -&gt; `num_columns_to_net` 2. Data pipeline updated  - 2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:   <pre><code>  i)   Create a new training task on the openi cloud platform.\n  ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n  iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n  iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n  v)   Fill in other blanks and launch.\n</code></pre>  - 2023/03/08 1. Add evaluation script with  arg `ckpt_load_path` 2. Arg `ckpt_save_dir` is moved from `system` to `train` in yaml. 3. Add drop_overflow_update control  ### How to Contribute  We appreciate all kinds of contributions including issues and PRs to make MindOCR better.  Please refer to [CONTRIBUTING.md](mkdocs/contributing.md) for the contributing guideline. Please follow the [Model Template and Guideline](mkdocs/customize_model.md) for contributing a model that fits the overall interface :)  ### License  This project follows the [Apache License 2.0](mkdocs/license.md) open-source license.  ### Citation  If you find this project useful in your research, please consider citing:  <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"datasets/chinese_text_recognition/","title":"Chinese text recognition.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/chinese_text_recognition/#chinese-text-recognition","title":"Chinese-Text-Recognition","text":"<p>This document introduce the dataset preparation for Chinese Text Recognition.</p>"},{"location":"datasets/chinese_text_recognition/#data-downloading","title":"Data Downloading","text":"<p>Following the setup in Benchmarking-Chinese-Text-Recognition, we use the same training, validation and evaliation data as described in Datasets.</p> <p>Please download the following LMDB files as introduced in Downloads:</p> <ul> <li>scene datasets: The union dataset contains RCTW, ReCTS, LSVT, ArT, CTW</li> <li>web: MTWI</li> <li>document: generated with Text Render</li> <li>handwriting dataset: SCUT-HCCDoc</li> </ul>"},{"location":"datasets/chinese_text_recognition/#data-structure","title":"Data Structure","text":"<p>After downloading the files, please put all training files under the same folder <code>training</code>, all validation data under <code>validation</code> folder, and all evaluation data under <code>evaluation</code>.</p> <p>The data structure should be like:</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#data-configuration","title":"Data Configuration","text":"<p>To use the datasets, you can specify the datasets as follow in configuration file.</p>"},{"location":"datasets/chinese_text_recognition/#model-training","title":"Model Training","text":"<pre><code>...\ntrain:\n...\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\ndata_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\ndata_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n...\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#model-evaluation","title":"Model Evaluation","text":"<pre><code>...\ntrain:\n# NO NEED TO CHANGE ANYTHING IN TRAIN SINCE IT IS NOT USED\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\ndata_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n...\n</code></pre> <p>Back to README</p>"},{"location":"datasets/ctw1500/","title":"Ctw1500.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/ctw1500/#scut-ctw1500-datasets","title":"SCUT-CTW1500 Datasets","text":""},{"location":"datasets/ctw1500/#data-downloading","title":"Data Downloading","text":"<p>SCUT-CTW1500 Datasets official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/ctw1500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw1500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/train_images/ \\\n--label_dir path/to/ctw1500/ctw_1500_train_labels \\\n--output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/test_images/ \\\n--label_dir path/to/ctw1500/gt_ctw_1500 \\\n--output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>ctw1500/</code>.</p> <p>Back to README</p>"},{"location":"datasets/icdar2015/","title":"Icdar2015.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/icdar2015/#data-downloading","title":"Data Downloading","text":"<p>ICDAR 2015 paper</p> <p>download source: one must register an account to download the dataset.</p> Where to Download ICDAR 2015  ICDAR 2015 Challenge has three tasks. Task 1 is Text Localization. Task 3 is Word Recognition. Task 4 is End-to-end Text Spotting. Task 2 Text Segmentation is not available.  ### Text Localization  The four files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 1 are <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre>  ### Word Recognition  The three files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 3 are <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre> The three files are only needed for training word recognition models. Training text detection models does not require the three files.      ### E2E  The nine files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 4 are the union of the four files in the text localization task (task 1) and five vocabulary files <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre> If you download a file named `Challenge4_Test_Task4_GT.zip`, please note that it is the same file as `Challenge4_Test_Task1_GT.zip`, except for its name. In this repository, we will use `Challenge4_Test_Task4_GT.zip` for ICDAR2015 dataset.   <p>After downloading the icdar2015 dataset, place all the files under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>Back to README</p>"},{"location":"datasets/mlt2017/","title":"Mlt2017.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/mlt2017/#data-downloading","title":"Data Downloading","text":"<p>MLT (Multi-Lingual) 2017 paper</p> <p>download source. One need to register an account to download this dataset.</p> Where to Download MLT 2017  MLT 2017 dataset consists of two tasks. Task 1 is Text detection (Multi-Language Script) and Task 2 is Word Recognition.  ### Text Detection(Multi-script)  The 11 files downloaded from [web](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads) for task 1 are  <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre>  No need to download the Test Set.   ### Word Identification  The 6 files downloaded from [web](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads) for task 2 are <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n\n&lt;/details&gt;\n\nAfter downloading the files, place them under `[path-to-data-dir]` folder:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip  ```  [Back to README](../../../tools/dataset_converters/README.md)"},{"location":"datasets/svt/","title":"Svt.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/svt/#the-street-view-text-dataset-svt","title":"The Street View Text Dataset (SVT)","text":""},{"location":"datasets/svt/#data-downloading","title":"Data Downloading","text":"<p>The Street View Text Dataset (SVT) official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"datasets/svt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/svt/#for-recognition-task","title":"For Recognition task","text":"<p>To prepare the data for text recognition, you can run the following command:</p> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name  svt --task rec \\\n--image_dir path/to/svt1/ \\\n--label_dir path/to/svt1/train.xml \\\n--output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>Then you can have a folder <code>cropped_images/</code> and an annotation file <code>rec_train_gt.txt</code> under the folder <code>svt1/</code>.</p> <p>Back to README</p>"},{"location":"datasets/syntext150k/","title":"Syntext150k.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/syntext150k/#data-downloading","title":"Data Downloading","text":"<p>SynText150k paper</p> <p>Download Syntext-150k (Part1: 54,327 [imgs][annos]. Part2: 94,723 [imgs][annos].)</p> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>Back to README</p>"},{"location":"datasets/synthtext/","title":"Synthtext.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/synthtext/#data-downloading","title":"Data Downloading","text":"<p>SynthText is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout. Paper | Download SynthText</p> <p>Download the <code>SynthText.zip</code> file and unzip in <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: Additionally, It is strongly recommended to pre-process the <code>SynthText</code> dataset before using it as it contains some faulty data: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat\n</code></pre> This operation will generate a filtered output in the same format as the original <code>SynthText</code>.</p> <p>Back to README</p>"},{"location":"datasets/td500/","title":"Td500.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/td500/#msra-text-detection-500-database-msra-td500","title":"MSRA Text Detection 500 Database (MSRA-TD500)","text":""},{"location":"datasets/td500/#data-downloading","title":"Data Downloading","text":"<p>MSRA Text Detection 500 Database\uff08MSRA-TD500\uff09official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/td500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/td500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/train/ \\\n--label_dir path/to/MSRA-TD500/train \\\n--output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/test/ \\\n--label_dir path/to/MSRA-TD500/test \\\n--output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>MSRA-TD500/</code>.</p> <p>Back to README</p>"},{"location":"datasets/totaltext/","title":"Totaltext.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"datasets/totaltext/#data-downloading","title":"Data Downloading","text":"<p>Total-Text paper</p> <p>Download Total-Text Images from source  (size = 441Mb).</p> How to Download Total-Text Images     The Total-Text dataset can be downloaded at [this https URL](https://drive.google.com/file/d/1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2/view?usp=sharing) (size = 441Mb).   <p>Download Total-Text Ground Truth from source.</p> How to Download Total-Text Ground Truth     The groundtruth of the Total-Text dataset can be downloaded through  [this https URL](https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view?usp=sharing) for text file format('.txt').   <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>Back to README</p>"},{"location":"inference/convert_dynamic_en/","title":"Convert dynamic en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/convert_dynamic_en/#inference-model-shape-scaling","title":"Inference - Model Shape Scaling","text":""},{"location":"inference/convert_dynamic_en/#1-introduction","title":"1 Introduction","text":"<p>According to the provided dataset, the distribution range of <code>height</code> and <code>width</code> is statistically counted, and the <code>batch size</code>, <code>height</code>, and <code>width</code> combination is selected discretely to achieve auto-scaling, and then ATC or MindSpore Lite is used for model conversion.</p>"},{"location":"inference/convert_dynamic_en/#2-environment","title":"2 Environment","text":"Environment Version Device Ascend310/310P Python &gt;= 3.7"},{"location":"inference/convert_dynamic_en/#3-example-model","title":"3 Example Model","text":"<p>E.g. You need to download the inference model first( detection , recognition , classification ), then use paddle2onnx to get following ONNX models.</p> Model Type Model Name Input Shape detection ch_PP-OCRv3_det_infer.onnx -1,3,-1,-1 recognition ch_PP-OCRv3_rec_infer.onnx -1,3,48,-1 classification ch_ppocr_mobile_v2.0_cls_infer.onnx -1,3,48,192"},{"location":"inference/convert_dynamic_en/#4-example-dataset","title":"4 Example Dataset","text":"<p>E.g. Dataset of ICDAR 2015: <code>Text Localization</code> , you need to register an account first.</p> <p>Dataset preparation refer to format conversion script tools/dataset_converters/converter.py, and execute the script according to the README<code>Text Detection/Spotting Annotation</code> section. Finally, you get the images and corresponding annotation file.</p>"},{"location":"inference/convert_dynamic_en/#5-auto-scaling-tool","title":"5 Auto-Scaling Tool","text":"<p>(1) Example of auto-scaling</p> <p>Refer to <code>deploy/models_utils/auto_scaling/converter.py</code> to convert the model to OM model.   <pre><code># git clone https://github.com/mindspore-lab/mindocr\n# cd mindocr/deploy/models_utils/auto_scaling\n\n# e.g 1: auto-scaling of batch size\npython converter.py --model_path=/path/to/ch_PP-OCRv3_rec_infer.onnx \\\n                    --dataset_path=/path/to/det_gt.txt\n                    --input_shape=-1,3,48,192 \\\n                    --output_path=output\n\nThe output result is an OM model: ch_PP-OCRv3_rec_infer_dynamic_bs.om.\n</code></pre> <pre><code># e.g 2: auto-scaling of height and width\npython converter.py --model_path=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n                    --dataset_path=/path/to/images \\\n                    --input_shape=1,3,-1,-1 \\\n                    --output_path=output\n\nThe output result is an OM model: ch_PP-OCRv3_det_infer_dynamic_hw.om.\n</code></pre> <pre><code># e.g 3: auto-scaling of batch size\u3001height and width\npython converter.py --model_path=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n                    --dataset_path=/path/to/images \\\n                    --input_shape=-1,3,-1,-1 \\\n                    --output_path=output\n\nThe output results are multiple OM models: ch_PP-OCRv3_det_infer_dynamic_bs1_hw.om, ch_PP-OCRv3_det_infer_dynamic_bs4_hw.om, ..., ch_PP-OCRv3_det_infer_dynamic_bs64_hw.om.\n</code></pre> <pre><code># e.g 4: no auto-scaling\npython converter.py --model_path=/path/to/ch_ppocr_mobile_v2.0_cls_infer.onnx \\\n                    --input_shape=4,3,48,192 \\\n                    --output_path=output\n\nThe output result is an OM model: ch_ppocr_mobile_v2.0_cls_infer_static.om.\n</code></pre></p> <p>You need to adapt the corresponding data and model parameters to the script:</p> Parameter description model_path Required, the path of the model file that needs to be converted. data_path Not required, the detection model is the image path of the images, the recognition model is the path of the annotation file, and the default data will be used if not pass this parameter. input_name Not required, model input variable name, default: x. input_shape Required, model input shape: NCHW, N\u3001H\u3001W support auto-scaling. backend Not required, converter backend: atc or lite, default: atc. output_path Not required, output model save path, default: ./output. soc_version Not required, Ascend310P3 or Ascend310P, default: Ascend310P3. <p>(2) <code>ATC</code> or <code>MindSpore Lite</code> use examples</p> <p>Several examples of individual calls to <code>ATC</code> or <code>MindSpore Lite</code> conversion are given under <code>deploy/models_utils/auto_scaling/example</code>.</p> <p><pre><code># ATC\natc --model=/path/to/ch_ppocr_mobile_v2.0_cls_infer.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:-1,3,48,192\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"1;4;8;16;32\" \\\n    --soc_version=Ascend310P3 \\\n    --output=output \\\n    --log=error\n</code></pre>   The output result is an OM model: output.om. More examples refer to: ATC examples</p> <p><pre><code># MindSpore Lite\nconverter_lite  --modelFile=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n    --fmk=ONNX \\\n    --configFile=lite_config.txt \\\n    --saveType=MINDIR \\\n    --NoFusion=false \\\n    --device=Ascend \\\n    --outputFile=output\n</code></pre>   The output result is an OM model: output.om. More examples refer to: MindSpore Lite examples</p> <p>Note: <code>MindSpore Lite</code> conversion need a <code>lite_config.txt</code> file, as follows:   <pre><code>[ascend_context]\ninput_format = NCHW\ninput_shape = x:[1,3,-1,-1]\ndynamic_dims = [1248,640],[1248,672],...,[1280,768],[1280,800]\n</code></pre></p> <p>(3) Introduction to config file</p> <p><code>limit_side_len</code>: The width and height size limits of the original input data, which are compressed proportionally if out of range, can adjust the degree of discreteness of the data.</p> <p><code>strategy</code>: Data statistics algorithm strategy, support mean_std and max_min, default: mean_std.</p> <pre><code>suppose that data mean: mean, standard deviation: sigma, data max: max, data min: min.\n\nmean_std: calculation formula: [mean - n_std * sigma, mean + n_std * sigma], n_std: 3.\n\nmax_min: calculation formula: [min - (max - min)*expand_ratio/2, max + (max - min)*expand_ratio/2], expand_ratio: 0.2.\n</code></pre> <p><code>width_range/height_range</code>: The width/height size limit after discrete statistics will be filtered if exceeded.</p> <p><code>interval</code>: Auto-scaling interval size.</p> <p><code>max_scaling_num</code>: Auto-scaling combination num limit.</p> <p><code>batch_choices</code>: The default batch size range.</p> <p><code>default_scaling</code>: Dataset parameter not exists, will provide default auto-scaling data.</p> <p>(4) Auto-scaling code structure <pre><code>auto_scaling\n\u251c\u2500\u2500 configs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 auto_scaling.yaml\n\u251c\u2500\u2500 converter.py\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 atc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_dynamic_bs.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_dynamic_hw.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 atc_static.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mindspore_lite\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_bs.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_bs.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_hw.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_hw.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_static.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 lite_static.txt\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 auto_scaling.py\n    \u251c\u2500\u2500 backend\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_converter.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 lite_converter.py\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 scale_analyzer\n        \u251c\u2500\u2500 dataset_analyzer.py\n        \u2514\u2500\u2500 __init__.py\n</code></pre></p>"},{"location":"inference/convert_tutorial_en/","title":"Convert tutorial en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/convert_tutorial_en/#inference-model-conversion-tutorial","title":"Inference - Model Conversion Tutorial","text":""},{"location":"inference/convert_tutorial_en/#1-mindocr-models","title":"1. MindOCR models","text":"<p>The inference of MindOCR models supports MindSpore Lite backend.</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(offline conversion)\"| o[MindIR];\n</code></pre>"},{"location":"inference/convert_tutorial_en/#11-model-export","title":"1.1 Model Export","text":"<p>Before inference, it is necessary to export the trained ckpt to a MindIR file, which stores the model structure and weight parameters.</p> <p>Some models provide download links for MIndIR export files, as shown in Model List. You can jump to the corresponding model introduction page for download.</p>"},{"location":"inference/convert_tutorial_en/#12-model-conversion","title":"1.2 Model Conversion","text":"<p>You need to use the <code>converter_lite</code> tool to convert the above exported MindIR file offline so that it can be used for MindSpore Lite inference.</p> <p>The tutorial for the <code>converter_lite</code> command can be referred to Offline Conversion of Inference Models.</p> <p>Assuming the input model is input.mindir and the output model after <code>converter_lite</code> conversion is output.mindir, the conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>Among them, <code>config.txt</code> can be used to set the shape and inference precision of the conversion model.</p>"},{"location":"inference/convert_tutorial_en/#121-model-shape-setting","title":"1.2.1 Model Shape Setting","text":"<ul> <li>Static Shape</li> </ul> <p>If the input name of the exported model is <code>x</code>, and the input shape is <code>(1,3,736,1280)</code>, then the <code>config.txt</code> is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>The generated output.mindir is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number and size of targets is not fixed resulting. If each inference is computed at the maximum Batch Size or maximum Image Size, it will result in wasted computational resources.</p> <p>Assuming the exported model input shape is (-1, 3, -1, -1), and the NHW axes are dynamic. Therefore, some optional values can be set during model conversion to adapt to input images of various size during inference.</p> <p><code>converter_lite</code> achieves this by setting the <code>dynamic_dims</code> parameter in <code>[ascend_context]</code> through <code>--configFile</code>. Please refer to the Dynamic Shape Configuration for details. We will refer to it as Model Shape Scaling for short.</p> <p>So, there are two options for conversion, by setting different config.txt:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial_en/#122-model-precision-mode-setting","title":"1.2.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>converter_lite</code> when converting the model. Please refer to the Ascend Conversion Tool Description, the usage of <code>precision_mode</code> parameter is described in the table of the configuration file, you can choose <code>enforce_fp16</code>, <code>enforce_fp32</code>, <code>preferred_fp32</code> and <code>enforce_origin</code> etc. So, you can add the <code>precision_mode</code> parameter in the <code>[Ascend_context]</code> of the above config.txt file to set the precision mode:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>If not set, defaults to <code>enforce_fp16</code>.</p>"},{"location":"inference/convert_tutorial_en/#2-paddleocr-models","title":"2. PaddleOCR models","text":"<p>The PaddleOCR models support two inference backends: ACL and MindSpore Lite, corresponding to the OM model and MindIR model, respectively.</p> <pre><code>graph LR;\n    in1[trained model] -- export --&gt; in2[inference model] -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"inference/convert_tutorial_en/#21-trained-inference-model","title":"2.1 Trained -&gt; Inference model","text":"<p>In the download link of PaddleOCR model, there are two formats: trained model and inference model. If a training model is provided, it needs to be converted to the format of inference model.</p> <p>On the original PaddleOCR introduction page of each trained model, there are usually conversion script samples that only need to input the config file, model file, and save path of the trained model. The example is as follows:</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"inference/convert_tutorial_en/#22-inference-model-onnx","title":"2.2 Inference model -&gt; ONNX","text":"<p>Install model conversion tool paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>For detailed usage tutorials, please refer to Paddle2ONNX model transformation and prediction\u3002</p> <p>Run the conversion command to generate the onnx model:</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>The <code>input_shape_dict</code> in the parameter can generally be viewed by opening the inference model using the Netron, or found in the code in tools/export_model. py above.</p>"},{"location":"inference/convert_tutorial_en/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>The ONNX model can be converted into an OM model by ATC tools.</p> <p>Ascend Tensor Compiler (ATC) is a model conversion tool built upon the heterogeneous computing architecture CANN. It is designed to convert models of open-source frameworks into .om offline models supported by Ascend AI Processor. A detailed tutorial on the tool can be found in ATC Instructions.</p>"},{"location":"inference/convert_tutorial_en/#231-model-shape-setting","title":"2.3.1 Model Shape Setting","text":"<p>The exported ONNX in the example has an input shape of (-1, 3, -1, -1).</p> <ul> <li>Static Shape</li> </ul> <p>It can be converted to a static shape version by fixed values for NHW, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>The generated file is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>The ATC tool also supports Model Shape Scaling by parameter dynamic_dims, and some optional values can be set during model conversion to adapt to input images of various shape during inference.</p> <p>So, there are two options for conversion, by setting different command line parameters:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial_en/#232-model-precision-mode-setting","title":"2.3.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>atc</code> when converting the model. Please refer to the Command-Line Options. Optional values include <code>force_fp16</code>, <code>force_fp32</code>, <code>allow_fp32_to_fp16</code>, <code>must_keep_origin_dtype</code>, <code>allow_mix_precision</code>, etc. So, you can add the <code>precision_mode</code> parameter in the <code>atc</code> command line to set the precision:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>If not set, defaults to <code>force_fp16</code>.</p>"},{"location":"inference/convert_tutorial_en/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>The <code>converter_lite</code> can be used to convert the ONNX into a MindIR. For detailed usage tutorials, please refer to Offline Conversion of Inference Models\u3002</p> <p>The conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>The conversion process is completely the same as the MindOCR models, except that <code>--fmk</code> needs to specify that the input is the ONNX, which will not be repeated here.</p>"},{"location":"inference/convert_tutorial_en/#3-mmocr-models","title":"3. MMOCR models","text":"<p>MMOCR uses Pytorch, and its model files typically have a pth format suffix. You need to first export it to ONNX format and then convert to an OM/MindIR format file supported by ACL/MindSpore Lite.</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"inference/convert_tutorial_en/#31-mmocr-model-onnx","title":"3.1 MMOCR model -&gt; ONNX","text":"<p>MMDeploy provides the command to export MMOCR models to ONNX. For detailed tutorials, please refer to How to convert model.</p> <p>For parameter <code>deploy_cfg</code>, you need to select the <code>*_onnxruntime_dynamic.py</code> file in directory mmdeploy/configs/mmocr to export as a dynamic shape ONNX model.</p>"},{"location":"inference/convert_tutorial_en/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>Please refer to ONNX -&gt; OM in the PaddleOCR section above.</p>"},{"location":"inference/convert_tutorial_en/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>Please refer to ONNX -&gt; MIndIR in the PaddleOCR section above.</p>"},{"location":"inference/environment_en/","title":"Environment en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/environment_en/#inference-environment-installation","title":"Inference - Environment Installation","text":"<p>MindOCR supports inference for Ascend310/Ascend310P device.</p> <p>Please make sure that the Ascend AI processor software package is correctly installed on your system. If it is not installed, please refer to the section Installing Ascend AI processor software package to install it.</p> <p>The MindOCR backend supports two types of inference: ACL and MindSpore Lite. Before inference using ACL mode, you need to use ATC tool to convert the model to om format, or to use converter_lite tool to convert the model to MindIR format, the specific differences are as follows:</p> ACL Mindspore Lite Conversion Tool ATC converter_lite Inference Model Format om MindIR"},{"location":"inference/environment_en/#1-acl-inference","title":"1. ACL inference","text":"<p>For the ACL inference of MindOCR, it currently relies on the Python API interface by MindX, which currently only supports Python 3.9.</p> package version Python 3.9 MindX 3.0.0 <p>On the basis of the Python 3.9 environment, download the mxVision SDK installation package for MindX and refer to the tutorial for installation. The main steps are as follows:</p> <pre><code># add executable permissions\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# execute the installation command\n# if prompted to specify the path to CANN, add parameters such as: --cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# set environment variable\nsource mxVision/set_env.sh\n</code></pre> <p>If use python interface, after installation, test whether mindx can be imported normally\uff1a<code>python -c \"import mindx\"</code></p> <p>If prompted that mindx cannot be found, go to the mxVision/Python directory and install the corresponding Whl package:</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> If use C++ interface, the above steps are not necessary.</p>"},{"location":"inference/environment_en/#2-mindspore-lite-inference","title":"2. MindSpore Lite inference","text":"<p>For the MindSpore Lite inference of MindOCR, It requires the version 2.0.0-rc1 or higher of the MindSpore Lite cloud-side inference toolkit.</p> <p>Download the Ascend version of the cloud-side inference toolkit tar.gz file, as well as the Python interface Wheel package.</p> <p>The download address provides the Python package for version 3.7. If you need other versions, please refer to the compilation tutorial.</p> <p>Just decompress the inference toolkit, and set environment variables:</p> <pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib::$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> <p>If using python interface, install the required .whl package using pip:</p> <p><pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre> The installation is not necessary if using the C++ interface.</p>"},{"location":"inference/inference_tutorial_en/","title":"Inference tutorial en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/inference_tutorial_en/#inference-tutorial","title":"Inference - Tutorial","text":""},{"location":"inference/inference_tutorial_en/#1-introduction","title":"1. Introduction","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend, integrates text detection, angle classification, and text recognition, implements end-to-end OCR inference process, and optimizes inference performance using pipeline parallelism.</p>"},{"location":"inference/inference_tutorial_en/#2-environment","title":"2. Environment","text":"<p>Please refer to the environment installation to configure the inference runtime environment for MindOCR, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"inference/inference_tutorial_en/#3-model-conversion","title":"3. Model conversion","text":"<p>MindOCR inference not only supports exported models from trained ckpt file, but also supports the third-party models, as listed in the MindOCR Models Support List and Third-Party Models Support List.</p> <p>Please refer to the Conversion Tutorial, to convert it into a model format supported by MindOCR inference.</p>"},{"location":"inference/inference_tutorial_en/#4-inference-python","title":"4. Inference (Python)","text":"<p>Enter the inference directory\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"inference/inference_tutorial_en/#41-command-example","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--res_save_dir=det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>recognition</li> </ul> <p>Run text recognition alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=rec\n</code></pre> <p>The results will be saved in rec/rec_results.txt, with the following format:</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"inference/inference_tutorial_en/#42-detail-of-inference-parameter","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Directory containing multiple images or path of single image. device str Ascend Device type, support Ascend device_id int 0 Device id backend str lite Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism precision_mode str None Precision mode, only supports setting by Model Conversion currently, and it takes no effect here <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results vis_det_save_dir str None Saving dir for images of with detection boxes vis_pipeline_save_dir str None Saving dir for images of with detection boxes and text vis_font_path str None Font path for drawing text crop_save_dir str None Saving path for cropped images after detection show_log bool False Whether show log when inferring save_log_dir str None Log saving dir <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection det_model_name str None Model name for text detection det_config_path str None Config file for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification cls_model_name str None Model name for text angle classification cls_config_path str None Config file for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_model_name str None Model name for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase <p>Notes\uff1a</p> <ol> <li>For the adapted model, <code>*_model_path</code>,<code>*_model_name</code> and <code>*_config_path</code> are bounded together, please refer to    MindOCR Models Support List and    Third-Party Models Support List. Both <code>*_model_name</code> and <code>*_config_path</code> are used    to determine pre/post processing parameters, and you can choose which one to use;</li> <li>If you need to adapt to your own model, <code>*_config_path</code> can simply pass in your own yaml file. Please refer to the    configs of the MindOCR model or the configs of    third-party models for file format.</li> </ol>"},{"location":"inference/inference_tutorial_en/#5-inference-c","title":"5. Inference (C++)","text":"<p>Currently, only the Chinese DBNet, CRNN, and SVTR models in the PP-OCR series are supported.</p> <p>Enter the inference directory\uff1a<code>cd deploy/cpp_infer</code>,then execute the compilation script 'bash build.sh'. Once the build process is complete, an executable file named 'infer' will be generated in the 'dist' directory located in the current path.</p>"},{"location":"inference/inference_tutorial_en/#41-command-example_1","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"inference/inference_tutorial_en/#42-detail-of-inference-parameter_1","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Directory containing multiple images or path of single image. device str Ascend Device type, support Ascend device_id int 0 Device id backend str acl Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase"},{"location":"inference/model_evaluation_en/","title":"Model evaluation en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/model_evaluation_en/#model-inference-evaluation","title":"Model Inference Evaluation","text":""},{"location":"inference/model_evaluation_en/#1-text-detection","title":"1. Text detection","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_det.py \\\n--gt_path=/path/to/det_gt.txt \\\n--pred_path=/path/to/prediction/det_results.txt\n</code></pre>"},{"location":"inference/model_evaluation_en/#2-text-recognition","title":"2. Text recognition","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n--gt_path=/path/to/rec_gt.txt \\\n--pred_path=/path/to/prediction/rec_results.txt \\\n--character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>Please note that character_dict_path is an optional parameter, and the default dictionary only supports numbers and English lowercase.</p> <p>When evaluating the PaddleOCR or MMOCR series models, please refer to Third-Party Model Support List to use the corresponding dictionary.</p>"},{"location":"inference/model_perf_thirdparty_en/","title":"Model perf thirdparty en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/model_perf_thirdparty_en/#third-party-model-inference-performance-table","title":"Third-party Model Inference Performance Table","text":"<p>This document will give the performance of the third-party inference model using the MindIR format after performing model conversion.</p>"},{"location":"inference/model_perf_thirdparty_en/#1-text-detection","title":"1. Text detection","text":"name model backbone test data recall precision f-score source ch_pp_server_det_v2.0 DB ResNet18_vd MLT17 0.3637 0.6340 0.4622 PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 MLT17 0.2557 0.5021 0.3389 PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 MLT17 0.3258 0.6318 0.4299 PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 MLT17 0.2346 0.4868 0.3166 PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 MLT17 0.2403 0.4597 0.3156 PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 IC15 0.3866 0.4630 0.4214 PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 MLT17 0.5992 0.7348 0.6601 PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 0.7463 0.9043 0.8177 PaddleOCR en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 0.8387 0.7900 0.8136 MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 0.8681 0.8074 0.8367 MMOCR"},{"location":"inference/model_perf_thirdparty_en/#2-text-recognition","title":"2. Text recognition","text":"name model backbone test data accuracy norm edit distance source ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (only Chinese) 0.4991 0.7411 PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only Chinese) 0.4991 0.7535 PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (only Chinese) 0.4459 0.7036 PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (only Chinese) 0.2459 0.4878 PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only English) 0.7964 0.8854 PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 MLT17 (only English) 0.0164 0.0657 PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 MLT17 (only English) 0.4304 0.5944 PaddleOCR <p>Please note that the above models use model shape scaling, so the performance here only represents the performance under certain input shapes.</p>"},{"location":"inference/model_perf_thirdparty_en/#3-evaluation-method","title":"3. Evaluation method","text":"<p>Please refer to Model Inference Evaluation document.</p>"},{"location":"inference/models_list_en/","title":"Models list en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/models_list_en/#inference-mindocr-models-support-list","title":"Inference - MindOCR Models Support List","text":"<p>MindOCR inference supports exported models from trained ckpt file, and this document displays a list of adapted models.</p> <p>Please export or download the pre-exported MindIR file, and refer to the model conversion tutorial before inference.</p>"},{"location":"inference/models_list_en/#1-text-detection","title":"1. Text detection","text":"model backbone language config DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"inference/models_list_en/#2-text-recognition","title":"2. Text recognition","text":"model backbone dict file language config CRNN VGG7 Default en crnn_vgg7.yaml ResNet34_vd Default en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"inference/models_list_thirdparty_en/","title":"Models list thirdparty en.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"inference/models_list_thirdparty_en/#inference-third-party-models-support-list","title":"Inference - Third-Party Models Support List","text":"<p>MindOCR can support the inference of third-party models, and this document displays a list of adapted models.</p> <p>After downloading the model file, it needs to be converted to a model file supported by ACL/MindSpore Lite inference (OM or MindIR). Please refer to the model conversion tutorial.</p> <p>The original model files involved are as follows:</p> type format description pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle trained model, which can store information such as model structure, weights, optimizer status, etc pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle inference model, which can be derived from its trained model, saving the network structure and weights. pth .pth Pytorch model, which can store information such as model structure, weights, optimizer status, etc"},{"location":"inference/models_list_thirdparty_en/#1-text-detection","title":"1. Text detection","text":"name model backbone config download reference source ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR"},{"location":"inference/models_list_thirdparty_en/#2-text-recognition","title":"2. Text recognition","text":"name model backbone dict file config download reference source ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR"},{"location":"inference/models_list_thirdparty_en/#3-text-angle-classification","title":"3. Text angle classification","text":"name model config download reference source ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"inference/models_list_thirdparty_en/#4-third-party-model-inference-performance","title":"4. Third-party model inference performance","text":"<p>Please refer to the third-party model inference test performance table.</p>"},{"location":"mkdocs/contributing/","title":"Contributing","text":""},{"location":"mkdocs/contributing/#mindocr-contributing-guidelines","title":"MindOCR Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little helps, and credit will always be given.</p>"},{"location":"mkdocs/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindOCR community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"mkdocs/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"mkdocs/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"mkdocs/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"mkdocs/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to fix it.</p>"},{"location":"mkdocs/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindOCR could always use more documentation, whether as part of the official MindOCR docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"mkdocs/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"mkdocs/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindocr</code> for local development.</p> <ol> <li>Fork the <code>mindocr</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindocr.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindocr\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindocr python=3.8\nconda activate mindocr\ncd mindocr\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you finish making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"mkdocs/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindocr/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"mkdocs/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"mkdocs/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed. Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"mkdocs/convert_tutorial/","title":"Convert tutorial.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"mkdocs/convert_tutorial/#inference-model-conversion-tutorial","title":"Inference - Model Conversion Tutorial","text":""},{"location":"mkdocs/convert_tutorial/#1-mindocr-models","title":"1. MindOCR models","text":"<p>The inference of MindOCR models supports MindSpore Lite backend.</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(offline conversion)\"| o[MindIR];\n</code></pre>"},{"location":"mkdocs/convert_tutorial/#11-model-export","title":"1.1 Model Export","text":"<p>Before inference, it is necessary to export the trained ckpt to a MindIR file, which stores the model structure and weight parameters.</p> <p>Some models provide download links for MIndIR export files, as shown in Model List. You can jump to the corresponding model introduction page for download.</p>"},{"location":"mkdocs/convert_tutorial/#12-model-conversion","title":"1.2 Model Conversion","text":"<p>You need to use the <code>converter_lite</code> tool to convert the above exported MindIR file offline so that it can be used for MindSpore Lite inference.</p> <p>The tutorial for the <code>converter_lite</code> command can be referred to Offline Conversion of Inference Models.</p> <p>Assuming the input model is input.mindir and the output model after <code>converter_lite</code> conversion is output.mindir, the conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>Among them, <code>config.txt</code> can be used to set the shape and inference precision of the conversion model.</p>"},{"location":"mkdocs/convert_tutorial/#121-model-shape-setting","title":"1.2.1 Model Shape Setting","text":"<ul> <li>Static Shape</li> </ul> <p>If the input name of the exported model is <code>x</code>, and the input shape is <code>(1,3,736,1280)</code>, then the <code>config.txt</code> is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>The generated output.mindir is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number and size of targets is not fixed resulting. If each inference is computed at the maximum Batch Size or maximum Image Size, it will result in wasted computational resources.</p> <p>Assuming the exported model input shape is (-1, 3, -1, -1), and the NHW axes are dynamic. Therefore, some optional values can be set during model conversion to adapt to input images of various size during inference.</p> <p><code>converter_lite</code> achieves this by setting the <code>dynamic_dims</code> parameter in <code>[ascend_context]</code> through <code>--configFile</code>. Please refer to the Dynamic Shape Configuration for details. We will refer to it as Model Shape Scaling for short.</p> <p>So, there are two options for conversion, by setting different config.txt:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"mkdocs/convert_tutorial/#122-model-precision-mode-setting","title":"1.2.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>converter_lite</code> when converting the model. Please refer to the Ascend Conversion Tool Description, the usage of <code>precision_mode</code> parameter is described in the table of the configuration file, you can choose <code>enforce_fp16</code>, <code>enforce_fp32</code>, <code>preferred_fp32</code> and <code>enforce_origin</code> etc. So, you can add the <code>precision_mode</code> parameter in the <code>[Ascend_context]</code> of the above config.txt file to set the precision mode:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>If not set, defaults to <code>enforce_fp16</code>.</p>"},{"location":"mkdocs/convert_tutorial/#2-paddleocr-models","title":"2. PaddleOCR models","text":"<p>The PaddleOCR models support two inference backends: ACL and MindSpore Lite, corresponding to the OM model and MindIR model, respectively.</p> <pre><code>graph LR;\n    in1[trained model] -- export --&gt; in2[inference model] -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"mkdocs/convert_tutorial/#21-trained-inference-model","title":"2.1 Trained -&gt; Inference model","text":"<p>In the download link of PaddleOCR model, there are two formats: trained model and inference model. If a training model is provided, it needs to be converted to the format of inference model.</p> <p>On the original PaddleOCR introduction page of each trained model, there are usually conversion script samples that only need to input the config file, model file, and save path of the trained model. The example is as follows:</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"mkdocs/convert_tutorial/#22-inference-model-onnx","title":"2.2 Inference model -&gt; ONNX","text":"<p>Install model conversion tool paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>For detailed usage tutorials, please refer to Paddle2ONNX model transformation and prediction\u3002</p> <p>Run the conversion command to generate the onnx model:</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>The <code>input_shape_dict</code> in the parameter can generally be viewed by opening the inference model using the Netron, or found in the code in tools/export_model. py above.</p>"},{"location":"mkdocs/convert_tutorial/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>The ONNX model can be converted into an OM model by ATC tools.</p> <p>Ascend Tensor Compiler (ATC) is a model conversion tool built upon the heterogeneous computing architecture CANN. It is designed to convert models of open-source frameworks into .om offline models supported by Ascend AI Processor. A detailed tutorial on the tool can be found in ATC Instructions.</p>"},{"location":"mkdocs/convert_tutorial/#231-model-shape-setting","title":"2.3.1 Model Shape Setting","text":"<p>The exported ONNX in the example has an input shape of (-1, 3, -1, -1).</p> <ul> <li>Static Shape</li> </ul> <p>It can be converted to a static shape version by fixed values for NHW, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>The generated file is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>The ATC tool also supports Model Shape Scaling by parameter dynamic_dims, and some optional values can be set during model conversion to adapt to input images of various shape during inference.</p> <p>So, there are two options for conversion, by setting different command line parameters:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"mkdocs/convert_tutorial/#232-model-precision-mode-setting","title":"2.3.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>atc</code> when converting the model. Please refer to the Command-Line Options. Optional values include <code>force_fp16</code>, <code>force_fp32</code>, <code>allow_fp32_to_fp16</code>, <code>must_keep_origin_dtype</code>, <code>allow_mix_precision</code>, etc. So, you can add the <code>precision_mode</code> parameter in the <code>atc</code> command line to set the precision:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>If not set, defaults to <code>force_fp16</code>.</p>"},{"location":"mkdocs/convert_tutorial/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>The <code>converter_lite</code> can be used to convert the ONNX into a MindIR. For detailed usage tutorials, please refer to Offline Conversion of Inference Models\u3002</p> <p>The conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>The conversion process is completely the same as the MindOCR models, except that <code>--fmk</code> needs to specify that the input is the ONNX, which will not be repeated here.</p>"},{"location":"mkdocs/convert_tutorial/#3-mmocr-models","title":"3. MMOCR models","text":"<p>MMOCR uses Pytorch, and its model files typically have a pth format suffix. You need to first export it to ONNX format and then convert to an OM/MindIR format file supported by ACL/MindSpore Lite.</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"mkdocs/convert_tutorial/#31-mmocr-model-onnx","title":"3.1 MMOCR model -&gt; ONNX","text":"<p>MMDeploy provides the command to export MMOCR models to ONNX. For detailed tutorials, please refer to How to convert model.</p> <p>For parameter <code>deploy_cfg</code>, you need to select the <code>*_onnxruntime_dynamic.py</code> file in directory mmdeploy/configs/mmocr to export as a dynamic shape ONNX model.</p>"},{"location":"mkdocs/convert_tutorial/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>Please refer to ONNX -&gt; OM in the PaddleOCR section above.</p>"},{"location":"mkdocs/convert_tutorial/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>Please refer to ONNX -&gt; MIndIR in the PaddleOCR section above.</p>"},{"location":"mkdocs/customize_data_transform/","title":"Customize Data Transformation","text":""},{"location":"mkdocs/customize_data_transform/#guideline-for-developing-your-transformation","title":"Guideline for Developing Your Transformation","text":""},{"location":"mkdocs/customize_data_transform/#writing-guideline","title":"Writing Guideline","text":"<ol> <li> <p>Each transformation is a class with a callable function. An example is shown below.</p> </li> <li> <p>The input to the transformation function is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>Please write comments for the call function to clarify the required/modified/added keys in the data dict.</p> </li> <li> <p>Add kwargs in the class init function for extension, which is used to parse global config, such as is_train.</p> </li> </ol> <pre><code>class ToCHWImage(object):\n\"\"\" convert hwc image to chw image\n    \"\"\"\n\n    def __init__(self, channel, **kwargs):\n        self.is_train = kwargs.get('is_train', True)\n\n    def __call__(self, data: dict):\n'''\n        required keys:\n            - image\n        modified keys:\n            - image\n        '''\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre>"},{"location":"mkdocs/customize_data_transform/#add-unit-test-and-visualization","title":"Add Unit Test and Visualization","text":"<p>Please add unit test in <code>tests/ut/transforms</code> for the written transformation and try to cover different cases (inputs and settings).</p> <p>Please visually check the correctness of the transformation on image and annotation using the jupyter notebook. See <code>transform_tutorial.ipynb</code>.</p>"},{"location":"mkdocs/customize_data_transform/#important-notes","title":"Important Notes","text":"<ol> <li>For spatial transformation operaions that will be used in text detection inference or evaluation (e.g. determinstic resize, scale), please record the space transformation information in <code>shape_list</code>. Otherwise, the postprocessing method won't be able to map the results back to the orignal image space. On how to record <code>shape_list</code>, please refer to DetResize.</li> </ol>"},{"location":"mkdocs/customize_dataset/","title":"Customize Dataset","text":""},{"location":"mkdocs/customize_dataset/#guideline-for-data-module","title":"Guideline for Data Module","text":""},{"location":"mkdocs/customize_dataset/#code-structure","title":"Code Structure","text":"<pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base_dataset.py                 # base dataset class with __getitem__\n\u251c\u2500\u2500 builder.py                  # API for create dataset and loader\n\u251c\u2500\u2500 det_dataset.py              # general text detection dataset class\n\u251c\u2500\u2500 rec_dataset.py              # general rec detection dataset class\n\u251c\u2500\u2500 rec_lmdb_dataset.py             # LMDB dataset class (To be impl.)\n\u2514\u2500\u2500 transforms\n    \u251c\u2500\u2500 det_transforms.py           # processing and augmentation ops (callabel classes) especially for detection tasks\n    \u251c\u2500\u2500 general_transforms.py           # general processing and augmentation ops (callabel classes)\n    \u251c\u2500\u2500 modelzoo_transforms.py          # transformations adopted from modelzoo\n    \u251c\u2500\u2500 rec_transforms.py           # processing and augmentation ops (callabel classes) especially for recognition tasks\n    \u2514\u2500\u2500 transforms_factory.py           # API for create and run transforms\n</code></pre>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-dataset-class","title":"How to add your own dataset class","text":"<ol> <li> <p>Inherit from BaseDataset class</p> </li> <li> <p>Rewrite the following file and annotation parsing functions in BaseDataset.</p> <p>def load_data_list(self, label_file: Union[str, List[str]], sample_ratio: Union[float, List] = 1.0,  shuffle: bool = False, **kwargs) -&gt; List[dict]</p> <p>def _parse_annotation(self, data_line: str) -&gt; Union[dict, List[dict]]</p> </li> </ol>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-data-transformation","title":"How to add your own data transformation","text":"<p>Please refer to Guideline for Developing Your Transformation</p>"},{"location":"mkdocs/customize_model/","title":"Customize a New Model","text":""},{"location":"mkdocs/customize_model/#guideline-for-model-module","title":"Guideline for Model Module","text":""},{"location":"mkdocs/customize_model/#how-to-add-a-new-model-in-mindocr","title":"How to Add a New Model in MindOCR","text":"<ol> <li> <p>Decompose the model into 3 (or 2) modules: backbone, (neck,) head. Neck is usually not involved in recognition tasks.</p> </li> <li> <p>For each module:</p> <p>a. if it is implemented in MindOCR, skip since you can get the module by the <code>build_{module}</code> function .</p> <p>b. if not, please implement it and follow the module format guideline</p> </li> <li> <p>Define your model in two ways</p> <p>a. Write a model py file, which includes the model class and specification functions. Please follow the model format guideline. It is to allows users to invoke a pre-defined model easily, such as <code>model = build_model('dbnet_resnet50', pretrained=True)</code>  .</p> <p>b. Config the architecture in a yaml file. Please follow the yaml format guideline . It is to allows users to modify a base architecture quickly in yaml file.</p> </li> <li> <p>To verify the correctness of the written model, please add your yaml config file path in <code>test_models.py</code>, modify the main function to build the desired model, and then run <code>test_models.py</code></p> </li> </ol> <pre><code>python tests/ut/test_models.py --config /path/to/yaml_config_file\n</code></pre>"},{"location":"mkdocs/customize_model/#format-guideline-for-writing-a-new-module","title":"Format Guideline for Writing a New Module","text":""},{"location":"mkdocs/customize_model/#backbone","title":"Backbone","text":"<ul> <li>File naming format: <code>models/backbones/{task}_{backbone}.py</code>, e.g, <code>det_resnet.py</code>   (since the same backbone for det and rec may differ, the task prefix is necessary)</li> <li>Class naming format: {Task}{BackboneName}{Variant} e.g. <code>class DetResNet</code></li> <li>Class <code>__init__</code> args: no limitation, define by model need.</li> <li>Class attributes: MUST contain <code>out_channels</code> (List), to describe channels of each output features. e.g. <code>self.out_channels=[256, 512, 1024, 2048]</code></li> <li>Class <code>construct</code> args: x (Tensor)</li> <li>Class <code>construct</code> return: features (List[Tensor]) for features extracted from different layers in the backbone, feature dim order <code>[bs, channels, \u2026]</code>. Expect shape of each feature: <code>[bs, channels, H, W]</code></li> </ul>"},{"location":"mkdocs/customize_model/#neck","title":"Neck","text":"<ul> <li>File naming format: <code>models/necks/{neck_name}.py</code>, e.g, <code>fpn.py</code></li> <li>Class naming format: {NeckName} e.g. <code>class FPN</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=256, **kwargs)</code>.</li> <li>Class attributes: MUST contain <code>out_channels</code> attribute, to describe channel of the output feature. e.g. <code>self.out_channels=256</code></li> <li>Class <code>construct</code> args: features (List(Tensor))</li> <li>Class <code>construct</code> return: feature (Tensor) for output feature, feature dim order <code>[bs, channels, \u2026]</code></li> </ul>"},{"location":"mkdocs/customize_model/#head","title":"Head","text":"<ul> <li>File naming: <code>models/heads/{head_name}.py</code>, e.g., <code>dbhead.py</code></li> <li>Class naming: {HeadName} e.g. <code>class DBHead</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=2, **kwargs)</code>.</li> <li>Class <code>construct</code> args: feature (Tensor), extra_input (Optional[Tensor]). The extra_input tensor is only applicable for head that needs recurrent input (e.g., Attention head), or heads with multiple inputs.</li> <li>Class <code>construct</code> return: prediction (Union(Tensor, Tuple[Tensor])). If there is only one output, return Tensor. If there are multiple outputs, return Tuple of Tensor, e.g., <code>return output1, output2, output_N</code>. Note that the order should match the loss function or the postprocess function.</li> </ul> <p>Note: if there is no neck in the model architecture like crnn, you can skip writing for neck. <code>BaseModel</code> will select the last feature of the features (List(Tensor)) output by Backbone, and forward it to Head module.</p>"},{"location":"mkdocs/customize_model/#format-guideline-for-model-py-file","title":"Format Guideline for Model Py File","text":"<ul> <li>File naming: <code>models/{task}_{model_class_name}.py</code>, e.g., <code>det_dbnet.py</code></li> <li>Class naming: {ModelName}, e.g., <code>class DBNet</code></li> <li>Class MUST inherent from <code>BaseModel</code>, e.g., <code>class DBNet(BaseModel)</code></li> <li>Spec. function naming: <code>{model_class_name}_{specifiation}.py</code>, e.g. <code>def dbnet_resnet50()</code> (Note: no need to add task prefix assuming no one model can solve any two tasks)</li> <li>Spec. function args: (pretrained=False, **kwargs), e.g. <code>def dbnet_resnet50(pretrained=False, **kwargs)</code>.</li> <li>Spec. function return: model (nn.Cell), which is the model instance</li> <li>Spec. function decorator: MUST add @register_model decorator, and import model file in <code>mindocr/models/__init__.py</code>, which is to register the model to the supported model list.</li> </ul> <p>After writing and registration, model can be created via the <code>build_model</code> func.  ``` python</p>"},{"location":"mkdocs/customize_model/#in-a-python-script","title":"in a python script","text":"<p>model = build_model('dbnet_resnet50', pretrained=False) <pre><code>## Format Guideline for Yaml File\n\nTo define/config the model architecture in yaml file, you should follow the keys in the following examples.\n\n\n- For models with a neck.\n\n``` python\nmodel:              # R\n  type: det\n  backbone:             # R\n    name: det_resnet50      # R, backbone specification function name\n    pretrained: False\n  neck:             # R\n    name: FPN           # R, neck class name\n    out_channels: 256       # D, neck class __init__ arg\n    #use_asf: True\n  head:             # R, head class name\n    name: ConvHead      # D, head class __init__ arg\n    out_channels: 2\n    k: 50\n</code></pre></p> <ul> <li>For models without a neck <pre><code>model:              # R\n  type: rec\n  backbone:         # R\n    name: resnet50      # R\n    pretrained: False\n  head:             # R\n    name: ConvHead      # R\n    out_channels: 30        # D\n</code></pre></li> </ul> <p>(R - Required. D - Depends on model)</p>"},{"location":"mkdocs/customize_postprocess/","title":"Customize Postprocessing Method","text":""},{"location":"mkdocs/customize_postprocess/#guideline-for-postprocessing-module","title":"Guideline for Postprocessing Module","text":""},{"location":"mkdocs/customize_postprocess/#common-protocols","title":"Common Protocols","text":"<ol> <li>Each postprocessing module is a class with a callable function.</li> <li>The input to the postprocessing function is network prediction and additional data information if needed.</li> <li>The output of the postprocessing function is a alwasy a dict, where the key is a field name, such as 'polys' for polygons in text detection, 'text' for text detection.</li> </ol>"},{"location":"mkdocs/customize_postprocess/#detection-postprocessing-api-protocols","title":"Detection Postprocessing API Protocols","text":"<ol> <li> <p>class naming: Det{Method}Postprocess</p> </li> <li> <p>class  <code>__init__()</code> args:</p> <ul> <li><code>box_type</code> (string): options are [\"quad', 'polys\"] for quadriateral and polygon text representation.</li> <li><code>rescale_fields</code> (List[str]='polys'): indicates which fields in the output dict will be rescaled to the original image space. Field name: \"polys\" for polygons</li> </ul> </li> <li> <p><code>__call__()</code> method: If inherit from <code>DetBasePostprocess</code>DetBasePostprocess<code>`, you don't need to implement this method in your Postproc. class.     Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get text boxes (by</code>self._postprocess()<code>function) and then rescale them back to the original image space (by</code>self.rescale()` function).</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>shape_list</code> (Union[List, np.ndarray, ms.Tensor]): shape and scale info for each image in the batch, shape [batch_size, 4]. Each internal array of length 4 is [src_h, src_w, scale_h, scale_w], where src_h and src_w are height and width of the original image, and scale_h and scale_w are their scale ratio after image resizing respectively.</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: detection result as a dictionary with the following keys</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons mapped on the original image space, shape [batch_size, num_polygons, num_points, 2]. If <code>box_type</code> is 'quad', num_points=4, and the internal np.ndarray is of shape [4, 2]</li> <li><code>scores</code> (List[float]): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> </ul> </li> <li> <p><code>_postprocess()</code> method: Implement your postprocessing method here if inherit from <code>DetBasePostprocess</code>     Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to original image space in call function)</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: postprocessing result as a dict with keys:</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons on the transformed (i.e. resized normally) image space, of shape (batch_size, num_polygons, num_points, 2). If <code>box_type</code> is 'quad', num_points=4.</li> <li><code>scores</code> (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Please cast <code>pred</code> to the type you need in your implementation. Some postprocesssing steps use ops from mindspore.nn and prefer Tensor type, while some steps prefer np.ndarray type required in other libraries.</li> <li><code>_postprocess()</code> should NOT round the text box <code>polys</code> to integer in return, because they will be recaled and then rounded in the end. Rounding early will cause larger error in polygon rescaling and results in evaluation performance degradation, especially on small datasets.</li> </ul> </li> </ul> </li> <li> <p>About rescaling the polygons back to the original image spcae</p> <ul> <li>The rescaling step is necessary for a fair evaluation and is needed in cropping text regions from the orginal image in inference.</li> <li>To enable rescaling for evaluation<ol> <li>add \"shape_list\" to the <code>eval.dataset.output_columns</code> in the YAML config file of the model.</li> <li>make sure <code>rescale_fields</code> is not None (default is [\"polys\"])</li> </ol> </li> <li>To enable rescaling in inference:<ol> <li>directly parse <code>shape_list</code> (which is got from data[\"shape_list\"] after data loading) to the postprocessing function.  It works with <code>rescale_fields</code> to decide whether to do rescaling and which fields are to be rescaled.</li> </ol> </li> <li><code>shape_list</code> is originally recorded in image resize transformation, such as <code>DetResize</code>.</li> </ul> </li> </ol> <p>Example Code: DetBasePostprocess and DetDBPostprocess</p>"},{"location":"mkdocs/customize_postprocess/#recognition-postprocessing-api-protocols","title":"Recognition Postprocessing API Protocols","text":"<ol> <li> <p>class  <code>__init__()</code> should support the follow args:         - character_dict_path         - use_space_char         - blank_at_last         - lower     Please see the API docs in RecCTCLabelDecode for argument illustration.</p> </li> <li> <p><code>__call__()</code> method:</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: det_res as a dictionary with the following keys</p> <ul> <li><code>texts</code> (List[str]): list of preditected text string</li> <li><code>confs</code> (List[float]): confidence of each prediction</li> </ul> </li> </ul> </li> </ol> <p>Example code: RecCTCLabelDecode</p>"},{"location":"mkdocs/dataset_converters/","title":"Dataset Preparation","text":"<p>English | \u4e2d\u6587</p> <p>This document shows how to convert ocr annotation to the general format (not including LMDB) for model training.</p> <p>You may also refer to <code>convert_datasets.sh</code> which is a quick solution for converting annotation files of all datasets under a given directory.</p> <p>To download and convert OCR datasets to the required data format, please refer to the following instructions: Chinese text recognition, CTW1500, ICDAR2015, MLT2017, SVT, Syntext 150k, TD500, Total Text, SynthText.</p>"},{"location":"mkdocs/dataset_converters/#text-detectionspotting-annotation","title":"Text Detection/Spotting Annotation","text":"<p>The format of the converted annotation file should follow: <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>Taking ICDAR2015 (ic15) dataset as an example, to convert the ic15 dataset to the required format, please run</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/train/ch4_training_images \\\n--label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n--output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/test/ch4_test_images \\\n--label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n--output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"mkdocs/dataset_converters/#text-recognition-annotation","title":"Text Recognition Annotation","text":"<p>The annotation format for text recognition dataset follows <pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> Note that image name and text label are seperated by \\t.</p> <p>To convert, please run: <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"mkdocs/inference_models_list/","title":"Inference models list.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"mkdocs/inference_models_list/#inference-mindocr-models-support-list","title":"Inference - MindOCR Models Support List","text":"<p>MindOCR inference supports exported models from trained ckpt file, and this document displays a list of adapted models.</p> <p>Please export or download the pre-exported MindIR file, and refer to the model conversion tutorial before inference.</p>"},{"location":"mkdocs/inference_models_list/#1-text-detection","title":"1. Text detection","text":"model backbone language config DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"mkdocs/inference_models_list/#2-text-recognition","title":"2. Text recognition","text":"model backbone dict file language config CRNN VGG7 Default en crnn_vgg7.yaml ResNet34_vd Default en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"mkdocs/inference_models_list_thirdparty/","title":"Inference models list thirdparty.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"mkdocs/inference_models_list_thirdparty/#inference-third-party-models-support-list","title":"Inference - Third-Party Models Support List","text":"<p>MindOCR can support the inference of third-party models, and this document displays a list of adapted models.</p> <p>After downloading the model file, it needs to be converted to a model file supported by ACL/MindSpore Lite inference (OM or MindIR). Please refer to the model conversion tutorial.</p> <p>The original model files involved are as follows:</p> type format description pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle trained model, which can store information such as model structure, weights, optimizer status, etc pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle inference model, which can be derived from its trained model, saving the network structure and weights. pth .pth Pytorch model, which can store information such as model structure, weights, optimizer status, etc"},{"location":"mkdocs/inference_models_list_thirdparty/#1-text-detection","title":"1. Text detection","text":"name model backbone config download reference source ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR"},{"location":"mkdocs/inference_models_list_thirdparty/#2-text-recognition","title":"2. Text recognition","text":"name model backbone dict file config download reference source ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR"},{"location":"mkdocs/inference_models_list_thirdparty/#3-text-angle-classification","title":"3. Text angle classification","text":"name model config download reference source ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"mkdocs/inference_models_list_thirdparty/#4-third-party-model-inference-performance","title":"4. Third-party model inference performance","text":"<p>Please refer to the third-party model inference test performance table.</p>"},{"location":"mkdocs/inference_tutorial/","title":"Python/C++ Inference on Ascend 310","text":"<p>English | \u4e2d\u6587</p>"},{"location":"mkdocs/inference_tutorial/#inference-tutorial","title":"Inference - Tutorial","text":""},{"location":"mkdocs/inference_tutorial/#1-introduction","title":"1. Introduction","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend, integrates text detection, angle classification, and text recognition, implements end-to-end OCR inference process, and optimizes inference performance using pipeline parallelism.</p>"},{"location":"mkdocs/inference_tutorial/#2-environment","title":"2. Environment","text":"<p>Please refer to the environment installation to configure the inference runtime environment for MindOCR, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"mkdocs/inference_tutorial/#3-model-conversion","title":"3. Model conversion","text":"<p>MindOCR inference not only supports exported models from trained ckpt file, but also supports the third-party models, as listed in the MindOCR Models Support List and Third-Party Models Support List.</p> <p>Please refer to the Conversion Tutorial, to convert it into a model format supported by MindOCR inference.</p>"},{"location":"mkdocs/inference_tutorial/#4-inference-python","title":"4. Inference (Python)","text":"<p>Enter the inference directory\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"mkdocs/inference_tutorial/#41-command-example","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--res_save_dir=det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>recognition</li> </ul> <p>Run text recognition alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=rec\n</code></pre> <p>The results will be saved in rec/rec_results.txt, with the following format:</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"mkdocs/inference_tutorial/#42-detail-of-inference-parameter","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Directory containing multiple images or path of single image. device str Ascend Device type, support Ascend device_id int 0 Device id backend str lite Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism precision_mode str None Precision mode, only supports setting by Model Conversion currently, and it takes no effect here <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results vis_det_save_dir str None Saving dir for images of with detection boxes vis_pipeline_save_dir str None Saving dir for images of with detection boxes and text vis_font_path str None Font path for drawing text crop_save_dir str None Saving path for cropped images after detection show_log bool False Whether show log when inferring save_log_dir str None Log saving dir <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection det_model_name str None Model name for text detection det_config_path str None Config file for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification cls_model_name str None Model name for text angle classification cls_config_path str None Config file for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_model_name str None Model name for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase <p>Notes\uff1a</p> <ol> <li>For the adapted model, <code>*_model_path</code>,<code>*_model_name</code> and <code>*_config_path</code> are bounded together, please refer to    MindOCR Models Support List and    Third-Party Models Support List. Both <code>*_model_name</code> and <code>*_config_path</code> are used    to determine pre/post processing parameters, and you can choose which one to use;</li> <li>If you need to adapt to your own model, <code>*_config_path</code> can simply pass in your own yaml file. Please refer to the    configs of the MindOCR model or the configs of    third-party models for file format.</li> </ol>"},{"location":"mkdocs/inference_tutorial/#5-inference-c","title":"5. Inference (C++)","text":"<p>Currently, only the Chinese DBNet, CRNN, and SVTR models in the PP-OCR series are supported.</p> <p>Enter the inference directory\uff1a<code>cd deploy/cpp_infer</code>,then execute the compilation script 'bash build.sh'. Once the build process is complete, an executable file named 'infer' will be generated in the 'dist' directory located in the current path.</p>"},{"location":"mkdocs/inference_tutorial/#41-command-example_1","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"mkdocs/inference_tutorial/#42-detail-of-inference-parameter_1","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Directory containing multiple images or path of single image. device str Ascend Device type, support Ascend device_id int 0 Device id backend str acl Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase"},{"location":"mkdocs/license/","title":"License.md","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"mkdocs/modelzoo/","title":"Model Zoo","text":"model type dataset fscore(detection)/accuracy(recognition) mindocr recipe vanilla mindspore dbnet_mobilenetv3 detection icdar2015 77.28 config dbnet_resnet18 detection icdar2015 83.71 config dbnet_resnet50 detection icdar2015 84.99 config link dbnet_resnet50 detection msra-td500 85.03 config dbnet++_resnet50 detection icdar2015 86.60 config psenet_resnet152 detection icdar2015 82.06 config link east_resnet50 detection icdar2015 84.87 config link svtr_tiny recognition IC03,13,15,IIIT,etc 89.02 config crnn_vgg7 recognition IC03,13,15,IIIT,etc 82.03 config link crnn_resnet34_vd recognition IC03,13,15,IIIT,etc 84.45 config rare_resnet34_vd recognition IC03,13,15,IIIT,etc 85.19 config"},{"location":"mkdocs/online_inference/","title":"Python Online Inference","text":""},{"location":"mkdocs/online_inference/#mindocr-online-inference","title":"MindOCR Online Inference","text":"<p>About Online Inference: Online inference is to infer based on the native MindSpore framework by loading the model checkpoint file then running prediction with MindSpore APIs.</p> <p>Compared to offline inference (which is implemented in <code>deploy/py_infer</code> in MindOCR), online inferece does not require model conversion for target platforms and can run directly on the training devices (e.g. Ascend 910). But it requires installing the heavy AI framework and the model is not optimized for deployment.</p> <p>Thus, online inference is more suitable for demonstration and to visually evaluate model generalization ability on unseen data.</p>"},{"location":"mkdocs/online_inference/#dependency-and-installation","title":"Dependency and Installation","text":"Environment Version MindSpore &gt;=1.9 Python &gt;=3.7 <p>Supported platforms: Linux, MacOS, Windows (Not tested) Supported devices: CPU, GPU, and Ascend.</p> <p>Please clone MindOCR at first <pre><code>git clone https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>Then install the dependency by <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>For MindSpore(&gt;=1.9) installation, please follow the official installation instructions for the best fit of your machine.</p>"},{"location":"mkdocs/online_inference/#text-detection","title":"Text Detection","text":"<p>To run text detection on an input image or a directory containing multiple images, please execute</p> <pre><code>python tools/infer/text/predict_det.py  --image_dir {path_to_img or dir_to_imgs} --det_algorithm DB++\n</code></pre> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection result on img_108.jpg </p> <p>, where the saved txt file is as follows <pre><code>img_108.jpg [[[228, 440], [403, 413], [406, 433], [231, 459]], [[282, 280], [493, 252], [499, 293], [288, 321]], [[500, 253], [636, 232], [641, 269], [505, 289]], ...]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p> Visualization of text detection result on paper_sam.png </p> <p>, where the saved txt file is as follows <pre><code>paper_sam.png   [[[1161, 340], [1277, 340], [1277, 378], [1161, 378]], [[895, 335], [1152, 340], [1152, 382], [894, 378]], ...]\n</code></pre></p> <p>Notes: - For input images with high resolution, please set <code>--det_limit_side_len</code> larger, e.g., 1280. <code>--det_limit_type</code> can be set as \"min\" or \"max\", where \"min \" means limiting the image size to be at least  <code>--det_limit_side_len</code>, \"max\" means limiting the image size to be at most <code>--det_limit_side_len</code>.</p> <ul> <li> <p>For more argument illustrations and usage, please run <code>python tools/infer/text/predict_det.py -h</code> or view <code>tools/infer/text/config.py</code></p> </li> <li> <p>Currently, this script runs serially to avoid dynamic shape issue and achieve better performance.</p> </li> </ul>"},{"location":"mkdocs/online_inference/#supported-detection-algorithms-and-networks","title":"Supported Detection Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | DB  | dbnet_resnet50 | English |   | DB++ | dbnetpp_resnet50 | English |   | DB_MV3 | dbnet_mobilenetv3 | English |   | PSE | psenet_resnet152 | English |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_det.py</code>.</p>"},{"location":"mkdocs/online_inference/#text-recognition","title":"Text Recognition","text":"<p>To run text recognition on an input image or a directory containing multiple images, please execute</p> <p><pre><code>python tools/infer/text/predict_rec.py  --image_dir {path_to_img or dir_to_imgs} --rec_algorithm CRNN\n</code></pre> After running, the inference results will be saved in <code>{args.draw_img_save_dir}/rec_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <ul> <li>English text recognition</li> </ul> <p> </p> <p>  word_1216.png  </p> <p> </p> <p>  word_1217.png  </p> <p>Recognition results: <pre><code>word_1216.png   coffee\nword_1217.png   club\n</code></pre></p> <ul> <li>Chinese text recognition:</li> </ul> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>Recognition results: <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre></p> <p>Notes: - For more argument illustrations and usage, please run <code>python tools/infer/text/predict_rec.py -h</code> or view <code>tools/infer/text/config.py</code> - Both batch-wise and single-mode inference are supported. Batch mode is enabled by default for better speed. You can set the batch size via <code>--rec_batch_size</code>. You can also run in single-mode by set <code>--det_batch_mode</code> False, which may improve accuracy if the text length varies a lot.</p>"},{"location":"mkdocs/online_inference/#supported-recognition-algorithms-and-networks","title":"Supported Recognition Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | CRNN | crnn_resnet34 | English |   | RARE | rare_resnet34 | English |   | SVTR | svtr_tiny | English|   | CRNN_CH | crnn_resnet34_ch | Chinese |   | RARE_CH | rare_resnet34_ch | Chinese |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_rec.py</code></p> <p>Currently, space char recognition is not supported for the listed models. We will support it soon.</p>"},{"location":"mkdocs/online_inference/#text-detection-and-recognition-concatenation","title":"Text Detection and Recognition Concatenation","text":"<p>To run text spoting (i.e., detect all text regions then recognize each of them) on an input image or multiple images in a directory, please run:</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>,  where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection and recognition result on img_10.jpg  </p> <p>, where the saved txt file is as follows <pre><code>img_10.jpg  [{\"transcription\": \"residential\", \"points\": [[43, 88], [149, 78], [151, 101], [44, 111]]}, {\"transcription\": \"areas\", \"points\": [[152, 83], [201, 81], [202, 98], [153, 100]]}, {\"transcription\": \"when\", \"points\": [[36, 56], [101, 56], [101, 78], [36, 78]]}, {\"transcription\": \"you\", \"points\": [[99, 54], [143, 52], [144, 78], [100, 80]]}, {\"transcription\": \"pass\", \"points\": [[140, 54], [186, 50], [188, 74], [142, 78]]}, {\"transcription\": \"by\", \"points\": [[182, 52], [208, 52], [208, 75], [182, 75]]}, {\"transcription\": \"volume\", \"points\": [[199, 30], [254, 30], [254, 46], [199, 46]]}, {\"transcription\": \"your\", \"points\": [[164, 28], [203, 28], [203, 46], [164, 46]]}, {\"transcription\": \"lower\", \"points\": [[109, 25], [162, 25], [162, 46], [109, 46]]}, {\"transcription\": \"please\", \"points\": [[31, 18], [109, 20], [108, 48], [30, 46]]}]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p>  Visualization of text detection and recognition result on web_cvpr.png  </p> <p>, where the saved txt file is as follows</p> <pre><code>web_cvpr.png    [{\"transcription\": \"canada\", \"points\": [[430, 148], [540, 148], [540, 171], [430, 171]]}, {\"transcription\": \"vancouver\", \"points\": [[263, 148], [420, 148], [420, 171], [263, 171]]}, {\"transcription\": \"cvpr\", \"points\": [[32, 69], [251, 63], [254, 174], [35, 180]]}, {\"transcription\": \"2023\", \"points\": [[194, 44], [256, 45], [255, 72], [194, 70]]}, {\"transcription\": \"june\", \"points\": [[36, 45], [110, 44], [110, 70], [37, 71]]}, {\"transcription\": \"1822\", \"points\": [[114, 43], [190, 45], [190, 70], [113, 69]]}]\n</code></pre> <p>Notes: 1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_system.py -h</code> or view <code>tools/infer/text/config.py</code></p>"},{"location":"mkdocs/online_inference/#evaluation-of-the-inference-results","title":"Evaluation of the Inference Results","text":"<p>To infer on the whole ICDAR15 test set, please run: <pre><code>python tools/infer/text/predict_system.py --image_dir /path/to/icdar15/det/test_images  /\n                                          --det_algorithm {DET_ALGO}    /\n                                          --rec_algorithm {REC_ALGO}  /\n                                          --det_limit_type min  /\n                                          --det_limit_side_len 720\n</code></pre></p> <p>Note: Here we set<code>det_limit_type</code> as <code>min</code> for better performance, due to the input image in ICDAR15 is of high resolution (720x1280).</p> <p>After running, the results including image names, bounding boxes (<code>points</code>) and recognized texts (<code>transcription</code>) will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>. The format of prediction results is shown as follows.</p> <pre><code>img_1.jpg   [{\"transcription\": \"hello\", \"points\": [600, 150, 715, 157, 714, 177, 599, 170]}, {\"transcription\": \"world\", \"points\": [622, 126, 695, 129, 694, 154, 621, 151]}, ...]\nimg_2.jpg   [{\"transcription\": \"apple\", \"points\": [553, 338, 706, 318, 709, 342, 556, 362]}, ...]\n   ...\n</code></pre> <p>Prepare the ground truth file (in the same format as above), which can be obtained from the dataset conversion script in <code>tools/dataset_converters</code>, and run the following command to evaluate the prediction results.</p> <pre><code>python deploy/eval_utils/eval_pipeline.py --gt_path path/to/gt.txt --pred_path path/to/system_results.txt\n</code></pre> <p>Evaluation of the text spotting inference results on Ascend 910 with MindSpore 2.0rc1 are shown as follows.</p>   | Det. Algorithm| Rec. Algorithm |  Dataset     | Accuracy(%) | FPS (imgs/s) | |---------|----------|--------------|---------------|-------| | DBNet   | CRNN    | ICDAR15 | 57.82 | 4.86 | | PSENet  | CRNN    | ICDAR15 | 47.91 | 1.65| | PSENet (det_limit_side_len=1472 )  | CRNN    | ICDAR15 | 55.51 | 0.44 | | DBNet++   | RARE | ICDAR15 | 59.17  | 3.47 | | DBNet++   | SVTR | ICDAR15 | 64.42  | 2.49 |  <p>Notes: 1. Currently, online inference pipeline is not optimized for efficiency, thus FPS is only for comparison between models. If FPS is your highest priority, please refer to Inference on Ascend 310, which is much faster. 2. Unless extra inidication, all experiments are run with <code>--det_limit_type</code>=\"min\" and <code>--det_limit_side</code>=720. 3. SVTR is run in mixed precision mode (amp_level=O2) since it is optimized for O2.</p>"},{"location":"mkdocs/online_inference/#argument-list","title":"Argument List","text":"<p>All CLI argument definition can be viewed via <code>python tools/infer/text/predict_system.py -h</code> or reading <code>tools/infer/text/config.py</code>.</p>"},{"location":"mkdocs/online_inference/#developer-guide-how-to-add-a-new-model-for-inference","title":"Developer Guide - How to Add a New Model for Inference","text":""},{"location":"mkdocs/online_inference/#preprocessing","title":"Preprocessing","text":"<p>The optimal preprocessing strategy can vary from model to model, especially for the resize setting (keep_ratio, padding, etc). We define the preprocessing pipeline for each model in <code>tools/infer/text/preprocess.py</code> for different tasks.</p> <p>If you find the default preprocessing pipeline or hyper-params does not meet the network requirement, please extend by changing the if-else conditions or adding a new key-value pair to the <code>optimal_hparam</code> dict in <code>tools/infer/text/preprocess.py</code>, where key is the algorithm name and the value is the suitable hyper-param setting for the target network inference.</p>"},{"location":"mkdocs/online_inference/#network-inference","title":"Network Inference","text":"<p>Supported alogirhtms and their corresponding network names (which can be checked by using the <code>list_model()</code> API) are defined in the <code>algo_to_model_name</code> dict in <code>predict_det.py</code> and <code>predict_rec.py</code>.</p> <p>To add a new detection model for inference, please add a new key-value pair to <code>algo_to_model_name</code> dict, where the key is an algorithm name and the value is the corresponding network name registered in <code>mindocr/models/{your_model}.py</code>.</p> <p>By default, model weights will be loaded from the pro-defined URL in <code>mindocr/models/{your_model}.py</code>. If you want to load a local checkpoint instead, please set <code>--det_model_dir</code> or <code>--rec_model_dir</code> to the path of your local checkpoint or the directory containing a model checkpoint.</p>"},{"location":"mkdocs/online_inference/#postproprocess","title":"Postproprocess","text":"<p>Similar to preprocessing, the postprocessing method for each algorithm can vary. The postprocessing method for each algorithm is defined in <code>tools/infer/text/postprocess.py</code>.</p> <p>If you find the default postprocessing method or hyper-params does not meet the model need, please extend the if-else conditions or add a new key-value pair  to the <code>optimal_hparam</code> dict in <code>tools/infer/text/postprocess.py</code>, where the key is an algorithm name and the value is the hyper-param setting.</p>"},{"location":"reference/api_doc/","title":"api","text":""},{"location":"reference/api_doc/#mindocr","title":"<code>mindocr</code>","text":""},{"location":"reference/api_doc/#mindocr.data","title":"<code>mindocr.data</code>","text":""},{"location":"reference/api_doc/#mindocr.data.base_dataset","title":"<code>mindocr.data.base_dataset</code>","text":""},{"location":"reference/api_doc/#mindocr.data.base_dataset.BaseDataset","title":"<code>mindocr.data.base_dataset.BaseDataset</code>","text":"<p>         Bases: <code>object</code></p> <p>Base dataset to parse dataset files.</p> PARAMETER DESCRIPTION <code>-</code> <p> TYPE: <code>data_dir</code> </p> <code>-</code> <p> TYPE: <code>label_file</code> </p> <code>-</code> <p>names of elements in the output tuple of getitem</p> <p> TYPE: <code>output_columns (List(str</code> </p> ATTRIBUTE DESCRIPTION <code>data_list</code> <p>source data items (e.g., containing image path and raw annotation)</p> <p> TYPE: <code>List(Tuple</code> </p> Source code in <code>mindocr\\data\\base_dataset.py</code> <pre><code>class BaseDataset(object):\n\"\"\"\n    Base dataset to parse dataset files.\n\n    Args:\n        - data_dir:\n        - label_file:\n        - output_columns (List(str)): names of elements in the output tuple of __getitem__\n    Attributes:\n        data_list (List(Tuple)): source data items (e.g., containing image path and raw annotation)\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: Union[str, List[str]],\n        label_file: Union[str, List[str]] = None,\n        output_columns: List[str] = None,\n        **kwargs,\n    ):\n        self._index = 0\n        self.data_list = []\n\n        # check files\n        if isinstance(data_dir, str):\n            data_dir = [data_dir]\n        for f in data_dir:\n            if not os.path.exists(f):\n                raise ValueError(f\"data_dir '{f}' does not existed. Please check the yaml file for both train and eval\")\n        self.data_dir = data_dir\n\n        if label_file is not None:\n            if isinstance(label_file, str):\n                label_file = [label_file]\n            for f in label_file:\n                if not os.path.exists(f):\n                    raise ValueError(\n                        f\"label_file '{f}' does not existed. Please check the yaml file for both train and eval\"\n                    )\n        else:\n            label_file = []\n        self.label_file = label_file\n\n        # must specify output column names\n        self.output_columns = output_columns\n\n    def __getitem__(self, index):\n        # return self.data_list[index]\n        raise NotImplementedError\n\n    def set_output_columns(self, column_names: List[str]):\n        self.output_columns = column_names\n\n    def get_output_columns(self) -&gt; List[str]:\n\"\"\"\n        get the column names for the output tuple of __getitem__, required for data mapping in the next step\n        \"\"\"\n        # raise NotImplementedError\n        return self.output_columns\n\n    def __next__(self):\n        if self._index &gt;= len(self.data_list):\n            raise StopIteration\n        else:\n            item = self.__getitem__(self._index)\n            self._index += 1\n            return item\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def _load_image_bytes(self, img_path):\n\"\"\"load image bytes (prepared for decoding)\"\"\"\n        with open(img_path, \"rb\") as f:\n            image_bytes = f.read()\n        return image_bytes\n</code></pre> <code>mindocr.data.base_dataset.BaseDataset.get_output_columns()</code> \u00b6 <p>get the column names for the output tuple of getitem, required for data mapping in the next step</p> Source code in <code>mindocr\\data\\base_dataset.py</code> <pre><code>def get_output_columns(self) -&gt; List[str]:\n\"\"\"\n    get the column names for the output tuple of __getitem__, required for data mapping in the next step\n    \"\"\"\n    # raise NotImplementedError\n    return self.output_columns\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.builder","title":"<code>mindocr.data.builder</code>","text":""},{"location":"reference/api_doc/#mindocr.data.builder.build_dataset","title":"<code>mindocr.data.builder.build_dataset(dataset_config, loader_config, num_shards=None, shard_id=None, is_train=True, **kwargs)</code>","text":"<p>Build dataset for training and evaluation.</p> PARAMETER DESCRIPTION <code>dataset_config</code> <p>dataset parsing and processing configuartion containing the following keys - type (str): dataset class name, please choose from <code>supported_dataset_types</code>. - dataset_root (str): the root directory to store the (multiple) dataset(s) - data_dir (Union[str, List[str]]): directory to the data, which is a subfolder path related to   <code>dataset_root</code>. For multiple datasets, it is a list of subfolder paths. - label_file (Union[str, List[str]], optional): file path to the annotation related to the <code>dataset_root</code>.   For multiple datasets, it is a list of relative file paths. Not required if using LMDBDataset. - sample_ratio (float): the sampling ratio of dataset. - shuffle (boolean): whether to shuffle the order of data samples. - transform_pipeline (list[dict]): each element corresponds to a transform operation on image and/or label - output_columns (list[str]): list of output features for each sample. - net_input_column_index (list[int]): input indices for network forward func in output_columns</p> <p> TYPE: <code>dict</code> </p> <code>loader_config</code> <p>dataloader configuration containing keys: - batch_size (int): batch size for data loader - drop_remainder (boolean): whether to drop the data in the last batch when the total of data can not be   divided by the batch_size - num_workers (int): number of subprocesses used to fetch the dataset in parallel.</p> <p> TYPE: <code>dict</code> </p> <code>num_shards</code> <p>num of devices for distributed mode</p> <p> TYPE: <code>int, *optional*</code> DEFAULT: <code>None</code> </p> <code>shard_id</code> <p>device id</p> <p> TYPE: <code>int, *optional*</code> DEFAULT: <code>None</code> </p> <code>is_train</code> <p>whether it is in training stage</p> <p> TYPE: <code>boolean</code> DEFAULT: <code>True</code> </p> <code>**kwargs</code> <p>optional args for extension. If <code>refine_batch_size=True</code> is given in kwargs, the batch size will be refined to be divisable to avoid droping remainding data samples in graph model, typically used for precise evaluation.</p> <p> DEFAULT: <code>{}</code> </p> Return <p>data_loader (Dataset): dataloader to generate data batch</p> Notes <ul> <li>The main data process pipeline in MindSpore contains 3 parts: 1) load data files and generate source dataset,     2) perform per-data-row mapping such as image augmentation, 3) generate batch and apply batch mapping.</li> <li>Each of the three steps supports multiprocess. Detailed mechanism can be seen in     https://www.mindspore.cn/docs/zh-CN/r2.0.0-alpha/api_python/mindspore.dataset.html</li> <li>A data row is a data tuple item containing multiple elements such as (image_i, mask_i, label_i).     A data column corresponds to an element in the tuple like 'image', 'label'.</li> <li>The total number of <code>num_workers</code> used for data loading and processing should not be larger than the maximum     threads of the CPU. Otherwise, it will lead to resource competing overhead. Especially for distributed     training, <code>num_parallel_workers</code> should not be too large to avoid thread competition.</li> </ul> Example Source code in <code>mindocr\\data\\builder.py</code> <pre><code>def build_dataset(\n    dataset_config: dict,\n    loader_config: dict,\n    num_shards=None,\n    shard_id=None,\n    is_train=True,\n    **kwargs,\n):\n\"\"\"\n    Build dataset for training and evaluation.\n\n    Args:\n        dataset_config (dict): dataset parsing and processing configuartion containing the following keys\n            - type (str): dataset class name, please choose from `supported_dataset_types`.\n            - dataset_root (str): the root directory to store the (multiple) dataset(s)\n            - data_dir (Union[str, List[str]]): directory to the data, which is a subfolder path related to\n              `dataset_root`. For multiple datasets, it is a list of subfolder paths.\n            - label_file (Union[str, List[str]], *optional*): file path to the annotation related to the `dataset_root`.\n              For multiple datasets, it is a list of relative file paths. Not required if using LMDBDataset.\n            - sample_ratio (float): the sampling ratio of dataset.\n            - shuffle (boolean): whether to shuffle the order of data samples.\n            - transform_pipeline (list[dict]): each element corresponds to a transform operation on image and/or label\n            - output_columns (list[str]): list of output features for each sample.\n            - net_input_column_index (list[int]): input indices for network forward func in output_columns\n        loader_config (dict): dataloader configuration containing keys:\n            - batch_size (int): batch size for data loader\n            - drop_remainder (boolean): whether to drop the data in the last batch when the total of data can not be\n              divided by the batch_size\n            - num_workers (int): number of subprocesses used to fetch the dataset in parallel.\n        num_shards (int, *optional*): num of devices for distributed mode\n        shard_id (int, *optional*): device id\n        is_train (boolean): whether it is in training stage\n        **kwargs: optional args for extension. If `refine_batch_size=True` is given in kwargs, the batch size will be\n            refined to be divisable to avoid\n            droping remainding data samples in graph model, typically used for precise evaluation.\n\n    Return:\n        data_loader (Dataset): dataloader to generate data batch\n\n    Notes:\n        - The main data process pipeline in MindSpore contains 3 parts: 1) load data files and generate source dataset,\n            2) perform per-data-row mapping such as image augmentation, 3) generate batch and apply batch mapping.\n        - Each of the three steps supports multiprocess. Detailed mechanism can be seen in\n            https://www.mindspore.cn/docs/zh-CN/r2.0.0-alpha/api_python/mindspore.dataset.html\n        - A data row is a data tuple item containing multiple elements such as (image_i, mask_i, label_i).\n            A data column corresponds to an element in the tuple like 'image', 'label'.\n        - The total number of `num_workers` used for data loading and processing should not be larger than the maximum\n            threads of the CPU. Otherwise, it will lead to resource competing overhead. Especially for distributed\n            training, `num_parallel_workers` should not be too large to avoid thread competition.\n\n    Example:\n        &gt;&gt;&gt; # Load a DetDataset/RecDataset\n        &gt;&gt;&gt; from mindocr.data import build_dataset\n        &gt;&gt;&gt; data_config = {\n        &gt;&gt;&gt;     \"type\": \"DetDataset\",\n        &gt;&gt;&gt;     \"dataset_root\": \"path/to/datasets/\",\n        &gt;&gt;&gt;     \"data_dir\": \"ic15/det/train/ch4_test_images\",\n        &gt;&gt;&gt;     \"label_file\": \"ic15/det/train/det_gt.txt\",\n        &gt;&gt;&gt;     \"sample_ratio\": 1.0,\n        &gt;&gt;&gt;     \"shuffle\": False,\n        &gt;&gt;&gt;     \"transform_pipeline\": [\n        &gt;&gt;&gt;         {\n        &gt;&gt;&gt;             \"DecodeImage\": {\n        &gt;&gt;&gt;                 \"img_mode\": \"RGB\",\n        &gt;&gt;&gt;                 \"to_float32\": False\n        &gt;&gt;&gt;                 }\n        &gt;&gt;&gt;         },\n        &gt;&gt;&gt;         {\n        &gt;&gt;&gt;             \"DetLabelEncode\": {},\n        &gt;&gt;&gt;         },\n        &gt;&gt;&gt;     ],\n        &gt;&gt;&gt;     \"output_columns\": ['image', 'polys', 'ignore_tags'],\n        &gt;&gt;&gt;     \"net_input_column_index`\": [0]\n        &gt;&gt;&gt;     \"label_column_index\": [1, 2]\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt; loader_config = dict(shuffle=True, batch_size=16, drop_remainder=False, num_workers=1)\n        &gt;&gt;&gt; data_loader = build_dataset(data_config, loader_config, num_shards=1, shard_id=0, is_train=True)\n    \"\"\"\n    # Check dataset paths (dataset_root, data_dir, and label_file) and update to absolute format\n    dataset_config = _check_dataset_paths(dataset_config)\n\n    # Set default multiprocessing params for data pipeline\n    # num_parallel_workers: Number of subprocesses used to fetch the dataset, transform data, or load batch in parallel\n    num_devices = 1 if num_shards is None else num_shards\n    cores = multiprocessing.cpu_count()\n    NUM_WORKERS_BATCH = 2\n    NUM_WORKERS_MAP = int(\n        cores / num_devices - NUM_WORKERS_BATCH\n    )  # optimal num workers assuming all cpu cores are used in this job\n    num_workers = loader_config.get(\"num_workers\", NUM_WORKERS_MAP)\n    if num_workers &gt; int(cores / num_devices):\n        print(\n            f\"WARNING: `num_workers` is adjusted to {int(cores / num_devices)} since {num_workers}x{num_devices} \"\n            f\"exceeds the number of CPU cores {cores}\"\n        )\n        num_workers = int(cores / num_devices)\n    # prefetch_size: the length of the cache queue in the data pipeline for each worker, used to reduce waiting time.\n    # Larger value leads to more memory consumption. Default: 16\n    prefetch_size = loader_config.get(\"prefetch_size\", 16)  #\n    ms.dataset.config.set_prefetch_size(prefetch_size)\n    # max_rowsize: MB of shared memory between processes to copy data. Only used when python_multiprocessing is True.\n    max_rowsize = loader_config.get(\"max_rowsize\", 64)\n    # auto tune num_workers, prefetch. (This conflicts the profiler)\n    # ms.dataset.config.set_autotune_interval(5)\n    # ms.dataset.config.set_enable_autotune(True, \"./dataproc_autotune_out\")\n\n    # 1. create source dataset (GeneratorDataset)\n    # Invoke dataset class\n    dataset_class_name = dataset_config.pop(\"type\")\n    assert dataset_class_name in supported_dataset_types, \"Invalid dataset name\"\n    dataset_class = eval(dataset_class_name)\n    dataset_args = dict(is_train=is_train, **dataset_config)\n    dataset = dataset_class(**dataset_args)\n\n    dataset_column_names = dataset.get_output_columns()\n    # print('=&gt; Dataset output columns: \\n\\t', dataset_column_names)\n\n    # Generate source dataset (source w.r.t. the dataset.map pipeline)\n    # based on python callable numpy dataset in parallel\n    ds = ms.dataset.GeneratorDataset(\n        dataset,\n        column_names=dataset_column_names,\n        num_parallel_workers=num_workers,\n        num_shards=num_shards,\n        shard_id=shard_id,\n        python_multiprocessing=True,  # keep True to improve performace for heavy computation.\n        max_rowsize=max_rowsize,\n        shuffle=loader_config[\"shuffle\"],\n    )\n\n    # 2. data mapping using mindata C lib (optional)\n    # ds = ds.map(operations=transform_list, input_columns=['image', 'label'], num_parallel_workers=8,\n    # python_multiprocessing=True)\n\n    # 3. create loader\n    # get batch of dataset by collecting batch_size consecutive data rows and apply batch operations\n    num_samples = ds.get_dataset_size()\n    batch_size = loader_config[\"batch_size\"]\n\n    device_id = 0 if shard_id is None else shard_id\n    is_main_device = device_id == 0\n    print(\n        f\"INFO: Creating dataloader (training={is_train}) for device {device_id}. Number of data samples: {num_samples}\"\n    )\n\n    if \"refine_batch_size\" in kwargs:\n        batch_size = _check_batch_size(num_samples, batch_size, refine=kwargs[\"refine_batch_size\"])\n\n    drop_remainder = loader_config.get(\"drop_remainder\", is_train)\n    if is_train and drop_remainder is False and is_main_device:\n        print(\n            \"WARNING: `drop_remainder` should be True for training, otherwise the last batch may lead to training fail \"\n            \"in Graph mode\"\n        )\n\n    if not is_train:\n        if drop_remainder and is_main_device:\n            print(\n                \"WARNING: `drop_remainder` is forced to be False for evaluation to include the last batch for \"\n                \"accurate evaluation.\"\n            )\n            drop_remainder = False\n\n    dataloader = ds.batch(\n        batch_size,\n        drop_remainder=drop_remainder,\n        num_parallel_workers=min(\n            num_workers, 2\n        ),  # set small workers for lite computation. TODO: increase for batch-wise mapping\n        # input_columns=input_columns,\n        # output_columns=batch_column,\n        # per_batch_map=per_batch_map, # uncommet to use inner-batch transformation\n    )\n\n    return dataloader\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.builder.build_dataset--load-a-detdatasetrecdataset","title":"Load a DetDataset/RecDataset","text":"<p>from mindocr.data import build_dataset data_config = {     \"type\": \"DetDataset\",     \"dataset_root\": \"path/to/datasets/\",     \"data_dir\": \"ic15/det/train/ch4_test_images\",     \"label_file\": \"ic15/det/train/det_gt.txt\",     \"sample_ratio\": 1.0,     \"shuffle\": False,     \"transform_pipeline\": [         {             \"DecodeImage\": {                 \"img_mode\": \"RGB\",                 \"to_float32\": False                 }         },         {             \"DetLabelEncode\": {},         },     ],     \"output_columns\": ['image', 'polys', 'ignore_tags'],     \"net_input_column_index`\": [0]     \"label_column_index\": [1, 2] } loader_config = dict(shuffle=True, batch_size=16, drop_remainder=False, num_workers=1) data_loader = build_dataset(data_config, loader_config, num_shards=1, shard_id=0, is_train=True)</p>"},{"location":"reference/api_doc/#mindocr.data.constants","title":"<code>mindocr.data.constants</code>","text":"<p>Constant data enhancement parameters of Imagenet dataset</p>"},{"location":"reference/api_doc/#mindocr.data.det_dataset","title":"<code>mindocr.data.det_dataset</code>","text":""},{"location":"reference/api_doc/#mindocr.data.det_dataset.DetDataset","title":"<code>mindocr.data.det_dataset.DetDataset</code>","text":"<p>         Bases: <code>BaseDataset</code></p> <p>General dataset for text detection The annotation format should follow:</p> <p>.. code-block: none</p> <pre><code># image file name       annotation info containing text and polygon points encoded by json.dumps\nimg_61.jpg      [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> PARAMETER DESCRIPTION <code>is_train</code> <p>whether it is in training stage</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>data_dir</code> <p>directory to the image data</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>label_file</code> <p>(list of) path to the label file(s), where each line in the label fle contains the image file name and its ocr annotation.</p> <p> TYPE: <code>Union[str, List[str]]</code> DEFAULT: <code>None</code> </p> <code>sample_ratio</code> <p>sample ratios for the data items in label files</p> <p> TYPE: <code>Union[float, List[float]]</code> DEFAULT: <code>1.0</code> </p> <code>shuffle(bool)</code> <p>Optional, if not given, shuffle = is_train</p> <p> </p> <code>transform_pipeline</code> <p>list of dict, key - transform class name, value - a dict of param config.         e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]         if None, default transform pipeline for text detection will be taken.</p> <p> TYPE: <code>List[dict]</code> DEFAULT: <code>None</code> </p> <code>output_columns</code> <p>required, indicates the keys in data dict that are expected to output for dataloader.                 if None, all data keys will be used for return.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>global_config</code> <p>additional info, used in data transformation, possible keys: - character_dict_path</p> <p> </p> RETURNS DESCRIPTION <code>data</code> <p>Depending on the transform pipeline, get_item returns a tuple for the specified data item.</p> <p> TYPE: <code>tuple</code> </p> <p>You can specify the <code>output_columns</code> arg to order the output data for dataloader.</p> Notes <ol> <li>The data file structure should be like     \u251c\u2500\u2500 data_dir     \u2502     \u251c\u2500\u2500 000001.jpg     \u2502     \u251c\u2500\u2500 000002.jpg     \u2502     \u251c\u2500\u2500 {image_file_name}     \u251c\u2500\u2500 label_file.txt</li> </ol> Source code in <code>mindocr\\data\\det_dataset.py</code> <pre><code>class DetDataset(BaseDataset):\n\"\"\"\n    General dataset for text detection\n    The annotation format should follow:\n\n    .. code-block: none\n\n        # image file name\\tannotation info containing text and polygon points encoded by json.dumps\n        img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n\n    Args:\n        is_train (bool): whether it is in training stage\n        data_dir (str):  directory to the image data\n        label_file (Union[str, List[str]]): (list of) path to the label file(s),\n            where each line in the label fle contains the image file name and its ocr annotation.\n        sample_ratio (Union[float, List[float]]): sample ratios for the data items in label files\n        shuffle(bool): Optional, if not given, shuffle = is_train\n        transform_pipeline: list of dict, key - transform class name, value - a dict of param config.\n                    e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]\n                    if None, default transform pipeline for text detection will be taken.\n        output_columns (list): required, indicates the keys in data dict that are expected to output for dataloader.\n                            if None, all data keys will be used for return.\n        global_config: additional info, used in data transformation, possible keys:\n            - character_dict_path\n\n    Returns:\n        data (tuple): Depending on the transform pipeline, __get_item__ returns a tuple for the specified data item.\n        You can specify the `output_columns` arg to order the output data for dataloader.\n\n    Notes:\n        1. The data file structure should be like\n            \u251c\u2500\u2500 data_dir\n            \u2502     \u251c\u2500\u2500 000001.jpg\n            \u2502     \u251c\u2500\u2500 000002.jpg\n            \u2502     \u251c\u2500\u2500 {image_file_name}\n            \u251c\u2500\u2500 label_file.txt\n    \"\"\"\n\n    def __init__(\n        self,\n        is_train: bool = True,\n        data_dir: Union[str, List[str]] = None,\n        label_file: Union[List, str] = None,\n        sample_ratio: Union[List, float] = 1.0,\n        shuffle: bool = None,\n        transform_pipeline: List[dict] = None,\n        output_columns: List[str] = None,\n        **kwargs,\n    ):\n        super().__init__(data_dir=data_dir, label_file=label_file, output_columns=output_columns)\n\n        # check args\n        if isinstance(sample_ratio, float):\n            sample_ratio = [sample_ratio] * len(self.label_file)\n\n        shuffle = shuffle if shuffle is not None else is_train\n\n        # load date file list\n        self.data_list = self.load_data_list(self.label_file, sample_ratio, shuffle)\n\n        # create transform\n        if transform_pipeline is not None:\n            global_config = dict(is_train=is_train)\n            self.transforms = create_transforms(transform_pipeline, global_config)\n        else:\n            raise ValueError(\"No transform pipeline is specified!\")\n\n        # prefetch the data keys, to fit GeneratorDataset\n        _data = self.data_list[0].copy()  # WARNING: shallow copy. Do deep copy if necessary.\n        _data = run_transforms(_data, transforms=self.transforms)\n        _available_keys = list(_data.keys())\n\n        if output_columns is None:\n            self.output_columns = _available_keys\n        else:\n            self.output_columns = []\n            for k in output_columns:\n                if k in _data:\n                    self.output_columns.append(k)\n                else:\n                    raise ValueError(\n                        f\"Key '{k}' does not exist in data (available keys: {_data.keys()}). \"\n                        \"Please check the name or the completeness transformation pipeline.\"\n                    )\n\n    def __getitem__(self, index):\n        data = self.data_list[index].copy()  # WARNING: shallow copy. Do deep copy if necessary.\n\n        # perform transformation on data\n        try:\n            data = run_transforms(data, transforms=self.transforms)\n            output_tuple = tuple(data[k] for k in self.output_columns)\n        except Exception as e:\n            print(f\"Error occurred while processing the image: {self.data_list[index]['img_path']}\\n\", e, flush=True)\n            return self[random.randrange(len(self.data_list))]  # return another random sample instead\n\n        return output_tuple\n\n    def load_data_list(\n        self, label_file: List[str], sample_ratio: List[float], shuffle: bool = False, **kwargs\n    ) -&gt; List[dict]:\n\"\"\"Load data list from label_file which contains infomation of image paths and annotations\n        Args:\n            label_file: annotation file path(s)\n            sample_ratio sample ratio for data items in each annotation file\n            shuffle: shuffle the data list\n        Returns:\n            data (List[dict]): A list of annotation dict, which contains keys: img_path, annot...\n        \"\"\"\n\n        # parse image file path and annotation and load\n        data_list = []\n        for idx, label_fp in enumerate(label_file):\n            img_dir = self.data_dir[idx]\n            with open(label_fp, \"r\", encoding=\"utf-8\") as f:\n                lines = f.readlines()\n                if shuffle:\n                    lines = random.sample(lines, round(len(lines) * sample_ratio[idx]))\n                else:\n                    lines = lines[: round(len(lines) * sample_ratio[idx])]\n\n                for line in lines:\n                    img_name, annot_str = self._parse_annotation(line)\n                    if annot_str == \"[]\":\n                        continue\n                    img_path = os.path.join(img_dir, img_name)\n                    assert os.path.exists(img_path), \"{} does not exist!\".format(img_path)\n\n                    data = {\"img_path\": img_path, \"label\": annot_str}\n                    data_list.append(data)\n\n        return data_list\n\n    def _parse_annotation(self, data_line: str):\n        data_line_tmp = data_line.strip()\n        if \"\\t\" in data_line_tmp:\n            img_name, annot_str = data_line.strip().split(\"\\t\")\n        elif \" \" in data_line_tmp:\n            img_name, annot_str = data_line.strip().split(\" \")\n        else:\n            raise ValueError(\n                \"Incorrect label file format: the file name and the label should be separated by \" \"a space or tab\"\n            )\n\n        return img_name, annot_str\n</code></pre> <code>mindocr.data.det_dataset.DetDataset.load_data_list(label_file, sample_ratio, shuffle=False, **kwargs)</code> \u00b6 <p>Load data list from label_file which contains infomation of image paths and annotations</p> PARAMETER DESCRIPTION <code>label_file</code> <p>annotation file path(s)</p> <p> TYPE: <code>List[str]</code> </p> <code>shuffle</code> <p>shuffle the data list</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>data</code> <p>A list of annotation dict, which contains keys: img_path, annot...</p> <p> TYPE: <code>List[dict]</code> </p> Source code in <code>mindocr\\data\\det_dataset.py</code> <pre><code>def load_data_list(\n    self, label_file: List[str], sample_ratio: List[float], shuffle: bool = False, **kwargs\n) -&gt; List[dict]:\n\"\"\"Load data list from label_file which contains infomation of image paths and annotations\n    Args:\n        label_file: annotation file path(s)\n        sample_ratio sample ratio for data items in each annotation file\n        shuffle: shuffle the data list\n    Returns:\n        data (List[dict]): A list of annotation dict, which contains keys: img_path, annot...\n    \"\"\"\n\n    # parse image file path and annotation and load\n    data_list = []\n    for idx, label_fp in enumerate(label_file):\n        img_dir = self.data_dir[idx]\n        with open(label_fp, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            if shuffle:\n                lines = random.sample(lines, round(len(lines) * sample_ratio[idx]))\n            else:\n                lines = lines[: round(len(lines) * sample_ratio[idx])]\n\n            for line in lines:\n                img_name, annot_str = self._parse_annotation(line)\n                if annot_str == \"[]\":\n                    continue\n                img_path = os.path.join(img_dir, img_name)\n                assert os.path.exists(img_path), \"{} does not exist!\".format(img_path)\n\n                data = {\"img_path\": img_path, \"label\": annot_str}\n                data_list.append(data)\n\n    return data_list\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.predict_dataset","title":"<code>mindocr.data.predict_dataset</code>","text":"<p>Inference dataset class</p>"},{"location":"reference/api_doc/#mindocr.data.predict_dataset.PredictDataset","title":"<code>mindocr.data.predict_dataset.PredictDataset</code>","text":"<p>         Bases: <code>BaseDataset</code></p> <ol> <li>The data file structure should be like     \u251c\u2500\u2500 img_dir     \u2502     \u251c\u2500\u2500 000001.jpg     \u2502     \u251c\u2500\u2500 000002.jpg     \u2502     \u251c\u2500\u2500 {image_file_name}</li> </ol> Source code in <code>mindocr\\data\\predict_dataset.py</code> <pre><code>class PredictDataset(BaseDataset):\n\"\"\"\n    Notes:\n    1. The data file structure should be like\n        \u251c\u2500\u2500 img_dir\n        \u2502     \u251c\u2500\u2500 000001.jpg\n        \u2502     \u251c\u2500\u2500 000002.jpg\n        \u2502     \u251c\u2500\u2500 {image_file_name}\n    \"\"\"\n\n    def __init__(\n        self,\n        # is_train: bool = False,\n        dataset_root: str = \"\",\n        data_dir: str = \"\",\n        sample_ratio: Union[List, float] = 1.0,\n        shuffle: bool = None,\n        transform_pipeline: List[dict] = None,\n        output_columns: List[str] = None,\n        **kwargs,\n    ):\n        img_dir = os.path.join(dataset_root, data_dir)\n        super().__init__(data_dir=img_dir, label_file=None, output_columns=output_columns)\n        self.data_list = self.load_data_list(img_dir, sample_ratio, shuffle)\n\n        # create transform\n        if transform_pipeline is not None:\n            self.transforms = create_transforms(transform_pipeline)  # , global_config=global_config)\n        else:\n            raise ValueError(\"No transform pipeline is specified!\")\n\n        # prefetch the data keys, to fit GeneratorDataset\n        _data = self.data_list[0]\n        _data = run_transforms(_data, transforms=self.transforms)\n        _available_keys = list(_data.keys())\n        if output_columns is None:\n            self.output_columns = _available_keys\n        else:\n            self.output_columns = []\n            for k in output_columns:\n                if k in _data:\n                    self.output_columns.append(k)\n                else:\n                    raise ValueError(\n                        f\"Key '{k}' does not exist in data (available keys: {_data.keys()}). \"\n                        \"Please check the name or the completeness transformation pipeline.\"\n                    )\n\n    def __getitem__(self, index):\n        data = self.data_list[index]\n\n        # perform transformation on data\n        data = run_transforms(data, transforms=self.transforms)\n        output_tuple = tuple(data[k] for k in self.output_columns)\n\n        return output_tuple\n\n    def load_data_list(self, img_dir: str, sample_ratio: List[float], shuffle: bool = False, **kwargs) -&gt; List[dict]:\n        # read image file name\n        img_filenames = os.listdir(img_dir)\n        if shuffle:\n            img_filenames = random.sample(img_filenames, round(len(img_filenames) * sample_ratio))\n        else:\n            img_filenames = img_filenames[: round(len(img_filenames) * sample_ratio)]\n\n        img_paths = [{\"img_path\": os.path.join(img_dir, filename)} for filename in img_filenames]\n\n        return img_paths\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.rec_dataset","title":"<code>mindocr.data.rec_dataset</code>","text":""},{"location":"reference/api_doc/#mindocr.data.rec_dataset.RecDataset","title":"<code>mindocr.data.rec_dataset.RecDataset</code>","text":"<p>         Bases: <code>DetDataset</code></p> <p>General dataset for text recognition The annotation format should follow:</p> <p>.. code-block: none</p> <pre><code># image file name       ground truth text\nword_18.png     STAGE\nword_19.png     HarbourFront\n</code></pre> PARAMETER DESCRIPTION <code>is_train</code> <p>whether it is in training stage</p> <p> TYPE: <code>bool</code> </p> <code>data_dir</code> <p>directory to the image data</p> <p> TYPE: <code>str</code> </p> <code>label_file</code> <p>(list of) path to the label file(s), where each line in the label fle contains the image file name and its ocr annotation.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>sample_ratio</code> <p>sample ratios for the data items in label files</p> <p> TYPE: <code>Union[float, List[float]]</code> </p> <code>shuffle(bool)</code> <p>Optional, if not given, shuffle = is_train</p> <p> </p> <code>transform_pipeline</code> <p>list of dict, key - transform class name, value - a dict of param config.         e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]         if None, default transform pipeline for text detection will be taken.</p> <p> </p> <code>output_columns</code> <p>required, indicates the keys in data dict that are expected to output for dataloader.                 if None, all data keys will be used for return.</p> <p> TYPE: <code>list</code> </p> <code>global_config</code> <p>additional info, used in data transformation, possible keys: - character_dict_path</p> <p> </p> RETURNS DESCRIPTION <code>data</code> <p>Depending on the transform pipeline, get_item returns a tuple for the specified data item.</p> <p> TYPE: <code>tuple</code> </p> <p>You can specify the <code>output_columns</code> arg to order the output data for dataloader.</p> Notes <ol> <li>The data file structure should be like     \u251c\u2500\u2500 data_dir     \u2502     \u251c\u2500\u2500 000001.jpg     \u2502     \u251c\u2500\u2500 000002.jpg     \u2502     \u251c\u2500\u2500 {image_file_name}     \u251c\u2500\u2500 label_file.txt</li> </ol> Source code in <code>mindocr\\data\\rec_dataset.py</code> <pre><code>class RecDataset(DetDataset):\n\"\"\"\n    General dataset for text recognition\n    The annotation format should follow:\n\n    .. code-block: none\n\n        # image file name\\tground truth text\n        word_18.png\\tSTAGE\n        word_19.png\\tHarbourFront\n\n    Args:\n        is_train (bool): whether it is in training stage\n        data_dir (str):  directory to the image data\n        label_file (Union[str, List[str]]): (list of) path to the label file(s),\n            where each line in the label fle contains the image file name and its ocr annotation.\n        sample_ratio (Union[float, List[float]]): sample ratios for the data items in label files\n        shuffle(bool): Optional, if not given, shuffle = is_train\n        transform_pipeline: list of dict, key - transform class name, value - a dict of param config.\n                    e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]\n                    if None, default transform pipeline for text detection will be taken.\n        output_columns (list): required, indicates the keys in data dict that are expected to output for dataloader.\n                            if None, all data keys will be used for return.\n        global_config: additional info, used in data transformation, possible keys:\n            - character_dict_path\n\n    Returns:\n        data (tuple): Depending on the transform pipeline, __get_item__ returns a tuple for the specified data item.\n        You can specify the `output_columns` arg to order the output data for dataloader.\n\n    Notes:\n        1. The data file structure should be like\n            \u251c\u2500\u2500 data_dir\n            \u2502     \u251c\u2500\u2500 000001.jpg\n            \u2502     \u251c\u2500\u2500 000002.jpg\n            \u2502     \u251c\u2500\u2500 {image_file_name}\n            \u251c\u2500\u2500 label_file.txt\n    \"\"\"\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.rec_lmdb_dataset","title":"<code>mindocr.data.rec_lmdb_dataset</code>","text":""},{"location":"reference/api_doc/#mindocr.data.rec_lmdb_dataset.LMDBDataset","title":"<code>mindocr.data.rec_lmdb_dataset.LMDBDataset</code>","text":"<p>         Bases: <code>BaseDataset</code></p> <p>Data iterator for ocr datasets including ICDAR15 dataset. The annotaiton format is required to aligned to paddle, which can be done using the <code>converter.py</code> script.</p> PARAMETER DESCRIPTION <code>is_train</code> <p>whether the dataset is for training</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>data_dir</code> <p>data root directory for lmdb dataset(s)</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>shuffle</code> <p>Optional, if not given, shuffle = is_train</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>transform_pipeline</code> <p>list of dict, key - transform class name, value - a dict of param config.         e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}] -       if None, default transform pipeline for text detection will be taken.</p> <p> TYPE: <code>Optional[List[dict]]</code> DEFAULT: <code>None</code> </p> <code>output_columns</code> <p>optional, indicates the keys in data dict that are expected to output for dataloader. if None, all data keys will be used for return.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>filter_max_len</code> <p>Filter the records where the label is longer than the <code>max_text_len</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_text_len</code> <p>The maximum text length the dataloader expected.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>data</code> <p>Depending on the transform pipeline, get_item returns a tuple for the specified data item.</p> <p> TYPE: <code>tuple</code> </p> <p>You can specify the <code>output_columns</code> arg to order the output data for dataloader.</p> Notes <ol> <li>Dataset file structure should follow:     data_dir     \u251c\u2500\u2500 dataset01         \u251c\u2500\u2500 data.mdb         \u251c\u2500\u2500 lock.mdb     \u251c\u2500\u2500 dataset02         \u251c\u2500\u2500 data.mdb         \u251c\u2500\u2500 lock.mdb     \u251c\u2500\u2500 ...</li> </ol> Source code in <code>mindocr\\data\\rec_lmdb_dataset.py</code> <pre><code>class LMDBDataset(BaseDataset):\n\"\"\"Data iterator for ocr datasets including ICDAR15 dataset.\n    The annotaiton format is required to aligned to paddle, which can be done using the `converter.py` script.\n\n    Args:\n        is_train: whether the dataset is for training\n        data_dir: data root directory for lmdb dataset(s)\n        shuffle: Optional, if not given, shuffle = is_train\n        transform_pipeline: list of dict, key - transform class name, value - a dict of param config.\n                    e.g., [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]\n            -       if None, default transform pipeline for text detection will be taken.\n        output_columns (list): optional, indicates the keys in data dict that are expected to output for dataloader.\n            if None, all data keys will be used for return.\n        filter_max_len (bool): Filter the records where the label is longer than the `max_text_len`.\n        max_text_len (int): The maximum text length the dataloader expected.\n\n    Returns:\n        data (tuple): Depending on the transform pipeline, __get_item__ returns a tuple for the specified data item.\n        You can specify the `output_columns` arg to order the output data for dataloader.\n\n    Notes:\n        1. Dataset file structure should follow:\n            data_dir\n            \u251c\u2500\u2500 dataset01\n                \u251c\u2500\u2500 data.mdb\n                \u251c\u2500\u2500 lock.mdb\n            \u251c\u2500\u2500 dataset02\n                \u251c\u2500\u2500 data.mdb\n                \u251c\u2500\u2500 lock.mdb\n            \u251c\u2500\u2500 ...\n    \"\"\"\n\n    def __init__(\n        self,\n        is_train: bool = True,\n        data_dir: str = \"\",\n        sample_ratio: float = 1.0,\n        shuffle: Optional[bool] = None,\n        transform_pipeline: Optional[List[dict]] = None,\n        output_columns: Optional[List[str]] = None,\n        filter_max_len: bool = False,\n        max_text_len: Optional[int] = None,\n        **kwargs: Any,\n    ):\n        self.data_dir = data_dir\n        self.filter_max_len = filter_max_len\n        self.max_text_len = max_text_len\n\n        shuffle = shuffle if shuffle is not None else is_train\n\n        self.lmdb_sets = self.load_list_of_hierarchical_lmdb_dataset(data_dir)\n        if len(self.lmdb_sets) == 0:\n            raise ValueError(f\"Cannot find any lmdb dataset under `{data_dir}`. Please check the data path is correct.\")\n        self.data_idx_order_list = self.get_dataset_idx_orders(sample_ratio, shuffle)\n\n        # filter the max length\n        if filter_max_len:\n            if max_text_len is None:\n                raise ValueError(\"`max_text_len` must be provided when `filter_max_len` is True.\")\n            self.data_idx_order_list = self.filter_idx_list(self.data_idx_order_list)\n\n        # create transform\n        if transform_pipeline is not None:\n            self.transforms = create_transforms(transform_pipeline)\n        else:\n            raise ValueError(\"No transform pipeline is specified!\")\n\n        self.prefetch(output_columns)\n\n    def prefetch(self, output_columns):\n        # prefetch the data keys, to fit GeneratorDataset\n        _data = self.data_idx_order_list[0]\n        lmdb_idx, file_idx = self.data_idx_order_list[0]\n        lmdb_idx = int(lmdb_idx)\n        file_idx = int(file_idx)\n        sample_info = self.get_lmdb_sample_info(self.lmdb_sets[lmdb_idx][\"txn\"], file_idx)\n        _data = {\"img_lmdb\": sample_info[0], \"label\": sample_info[1]}\n        _data = run_transforms(_data, transforms=self.transforms)\n        _available_keys = list(_data.keys())\n\n        if output_columns is None:\n            self.output_columns = _available_keys\n        else:\n            self.output_columns = []\n            for k in output_columns:\n                if k in _data:\n                    self.output_columns.append(k)\n                else:\n                    raise ValueError(\n                        f\"Key {k} does not exist in data (available keys: {_data.keys()}). \"\n                        \"Please check the name or the completeness transformation pipeline.\"\n                    )\n\n    def filter_idx_list(self, idx_list: np.ndarray) -&gt; np.ndarray:\n        print(\"Start filtering the idx list...\")\n        new_idx_list = list()\n        for lmdb_idx, file_idx in idx_list:\n            label = self.get_lmdb_sample_info(self.lmdb_sets[int(lmdb_idx)][\"txn\"], int(file_idx), label_only=True)\n            if len(label) &gt; self.max_text_len:\n                print(\n                    f\"WARNING: skip the label with length ({len(label)}), \"\n                    f\"which is longer than than max length ({self.max_text_len}).\"\n                )\n                continue\n            new_idx_list.append((lmdb_idx, file_idx))\n        new_idx_list = np.array(new_idx_list)\n        return new_idx_list\n\n    def load_list_of_hierarchical_lmdb_dataset(self, data_dir):\n        if isinstance(data_dir, str):\n            results = self.load_hierarchical_lmdb_dataset(data_dir)\n        elif isinstance(data_dir, list):\n            results = {}\n            for sub_data_dir in data_dir:\n                start_idx = len(results)\n                lmdb_sets = self.load_hierarchical_lmdb_dataset(sub_data_dir, start_idx)\n                results.update(lmdb_sets)\n        else:\n            results = {}\n\n        return results\n\n    def load_hierarchical_lmdb_dataset(self, data_dir, start_idx=0):\n        lmdb_sets = {}\n        dataset_idx = start_idx\n        for rootdir, dirs, _ in os.walk(data_dir + \"/\"):\n            if not dirs:\n                env = lmdb.open(rootdir, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n                txn = env.begin(write=False)\n                data_size = int(txn.get(\"num-samples\".encode()))\n                lmdb_sets[dataset_idx] = {\"rootdir\": rootdir, \"env\": env, \"txn\": txn, \"data_size\": data_size}\n                dataset_idx += 1\n        return lmdb_sets\n\n    def get_dataset_idx_orders(self, sample_ratio, shuffle):\n        n_lmdbs = len(self.lmdb_sets)\n        total_sample_num = 0\n        for idx in range(n_lmdbs):\n            total_sample_num += self.lmdb_sets[idx][\"data_size\"]\n        data_idx_order_list = np.zeros((total_sample_num, 2))\n        beg_idx = 0\n        for idx in range(n_lmdbs):\n            tmp_sample_num = self.lmdb_sets[idx][\"data_size\"]\n            end_idx = beg_idx + tmp_sample_num\n            data_idx_order_list[beg_idx:end_idx, 0] = idx\n            data_idx_order_list[beg_idx:end_idx, 1] = list(range(tmp_sample_num))\n            data_idx_order_list[beg_idx:end_idx, 1] += 1\n            beg_idx = beg_idx + tmp_sample_num\n\n        if shuffle:\n            np.random.shuffle(data_idx_order_list)\n\n        data_idx_order_list = data_idx_order_list[: round(len(data_idx_order_list) * sample_ratio)]\n\n        return data_idx_order_list\n\n    def get_lmdb_sample_info(self, txn, idx, label_only=False):\n        label_key = \"label-%09d\".encode() % idx\n        label = txn.get(label_key)\n        if label is None:\n            raise ValueError(f\"Cannot find key {label_key}\")\n        label = label.decode(\"utf-8\")\n\n        if label_only:\n            return label\n\n        img_key = \"image-%09d\".encode() % idx\n        imgbuf = txn.get(img_key)\n        return imgbuf, label\n\n    def __getitem__(self, idx):\n        lmdb_idx, file_idx = self.data_idx_order_list[idx]\n        sample_info = self.get_lmdb_sample_info(self.lmdb_sets[int(lmdb_idx)][\"txn\"], int(file_idx))\n\n        data = {\"img_lmdb\": sample_info[0], \"label\": sample_info[1]}\n\n        # perform transformation on data\n        data = run_transforms(data, transforms=self.transforms)\n        output_tuple = tuple(data[k] for k in self.output_columns)\n\n        return output_tuple\n\n    def __len__(self):\n        return self.data_idx_order_list.shape[0]\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms","title":"<code>mindocr.data.transforms</code>","text":"<p>transforms init</p>"},{"location":"reference/api_doc/#mindocr.data.transforms.det_east_transforms","title":"<code>mindocr.data.transforms.det_east_transforms</code>","text":"<code>mindocr.data.transforms.det_east_transforms.EASTProcessTrain</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\det_east_transforms.py</code> <pre><code>class EASTProcessTrain:\n    def __init__(self, scale=0.25, length=512, **kwargs):\n        super(EASTProcessTrain, self).__init__()\n        self.scale = scale\n        self.length = length\n\n    def __call__(self, data):\n        vertices, labels = self._extract_vertices(data[\"label\"])\n        img = Image.fromarray(data[\"image\"])\n        img, vertices = self._adjust_height(img, vertices)\n        img, vertices = self._adjust_width(img, vertices)\n        if np.random.rand() &lt; 0.5:\n            img, vertices = self._rotate_img(img, vertices)\n        img, vertices = self._crop_img(img, vertices, labels, self.length)\n        score_map, geo_map, ignored_map = self._get_score_geo(img, vertices, labels, self.scale, self.length)\n        score_map = score_map.transpose(2, 0, 1)\n        ignored_map = ignored_map.transpose(2, 0, 1)\n        geo_map = geo_map.transpose(2, 0, 1)\n        if np.sum(score_map) &lt; 1:\n            score_map[0, 0, 0] = 1\n        image = np.asarray(img)\n        data[\"image\"] = image\n        data[\"score_map\"] = score_map\n        data[\"geo_map\"] = geo_map\n        data[\"training_mask\"] = ignored_map\n        return data\n\n    def _cal_distance(self, x1, y1, x2, y2):\n\"\"\"calculate the Euclidean distance\"\"\"\n        return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n\n    def _move_points(self, vertices, index1, index2, r, coef):\n\"\"\"\n        move the two points to shrink edge\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n          index1  : offset of point1\n          index2  : offset of point2\n          r       : [r1, r2, r3, r4] in paper\n          coef    : shrink ratio in paper\n        Output:\n          vertices: vertices where one edge has been shinked\n        \"\"\"\n        index1 = index1 % 4\n        index2 = index2 % 4\n        x1_index = index1 * 2 + 0\n        y1_index = index1 * 2 + 1\n        x2_index = index2 * 2 + 0\n        y2_index = index2 * 2 + 1\n\n        r1 = r[index1]\n        r2 = r[index2]\n        length_x = vertices[x1_index] - vertices[x2_index]\n        length_y = vertices[y1_index] - vertices[y2_index]\n        length = self._cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\n        if length &gt; 1:\n            ratio = (r1 * coef) / length\n            vertices[x1_index] += ratio * (-length_x)\n            vertices[y1_index] += ratio * (-length_y)\n            ratio = (r2 * coef) / length\n            vertices[x2_index] += ratio * length_x\n            vertices[y2_index] += ratio * length_y\n        return vertices\n\n    def _shrink_poly(self, vertices, coef=0.3):\n\"\"\"\n        shrink the text region\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n          coef    : shrink ratio in paper\n        Output:\n          v       : vertices of shrunk text region &lt;numpy.ndarray, (8,)&gt;\n        \"\"\"\n        x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n        r1 = min(self._cal_distance(x1, y1, x2, y2), self._cal_distance(x1, y1, x4, y4))\n        r2 = min(self._cal_distance(x2, y2, x1, y1), self._cal_distance(x2, y2, x3, y3))\n        r3 = min(self._cal_distance(x3, y3, x2, y2), self._cal_distance(x3, y3, x4, y4))\n        r4 = min(self._cal_distance(x4, y4, x1, y1), self._cal_distance(x4, y4, x3, y3))\n        r = [r1, r2, r3, r4]\n\n        # obtain offset to perform move_points() automatically\n        if self._cal_distance(x1, y1, x2, y2) + self._cal_distance(x3, y3, x4, y4) &gt; self._cal_distance(\n            x2, y2, x3, y3\n        ) + self._cal_distance(x1, y1, x4, y4):\n            offset = 0  # two longer edges are (x1y1-x2y2) &amp; (x3y3-x4y4)\n        else:\n            offset = 1  # two longer edges are (x2y2-x3y3) &amp; (x4y4-x1y1)\n\n        v = vertices.copy()\n        v = self._move_points(v, 0 + offset, 1 + offset, r, coef)\n        v = self._move_points(v, 2 + offset, 3 + offset, r, coef)\n        v = self._move_points(v, 1 + offset, 2 + offset, r, coef)\n        v = self._move_points(v, 3 + offset, 4 + offset, r, coef)\n        return v\n\n    def _get_rotate_mat(self, theta):\n\"\"\"positive theta value means rotate clockwise\"\"\"\n        return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n\n    def _rotate_vertices(self, vertices, theta, anchor=None):\n\"\"\"\n        rotate vertices around anchor\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n          theta   : angle in radian measure\n          anchor  : fixed position during rotation\n        Output:\n          rotated vertices &lt;numpy.ndarray, (8,)&gt;\n        \"\"\"\n        v = vertices.reshape((4, 2)).T\n        if anchor is None:\n            anchor = v[:, :1]\n        rotate_mat = self._get_rotate_mat(theta)\n        res = np.dot(rotate_mat, v - anchor)\n        return (res + anchor).T.reshape(-1)\n\n    def _get_boundary(self, vertices):\n\"\"\"\n        get the tight boundary around given vertices\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n        Output:\n          the boundary\n        \"\"\"\n        x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n        x_min = min(x1, x2, x3, x4)\n        x_max = max(x1, x2, x3, x4)\n        y_min = min(y1, y2, y3, y4)\n        y_max = max(y1, y2, y3, y4)\n        return x_min, x_max, y_min, y_max\n\n    def _cal_error(self, vertices):\n\"\"\"\n        default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n        calculate the difference between the vertices orientation and default orientation\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n        Output:\n          err     : difference measure\n        \"\"\"\n        x_min, x_max, y_min, y_max = self._get_boundary(vertices)\n        x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n        err = (\n            self._cal_distance(x1, y1, x_min, y_min)\n            + self._cal_distance(x2, y2, x_max, y_min)\n            + self._cal_distance(x3, y3, x_max, y_max)\n            + self._cal_distance(x4, y4, x_min, y_max)\n        )\n        return err\n\n    def _find_min_rect_angle(self, vertices):\n\"\"\"\n        find the best angle to rotate poly and obtain min rectangle\n        Input:\n          vertices: vertices of text region &lt;numpy.ndarray, (8,)&gt;\n        Output:\n          the best angle &lt;radian measure&gt;\n        \"\"\"\n        angle_interval = 1\n        angle_list = list(range(-90, 90, angle_interval))\n        area_list = []\n        for theta in angle_list:\n            rotated = self._rotate_vertices(vertices, theta / 180 * math.pi)\n            x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n            temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\n            area_list.append(temp_area)\n\n        sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k])\n        min_error = float(\"inf\")\n        best_index = -1\n        rank_num = 10\n        # find the best angle with correct orientation\n        for index in sorted_area_index[:rank_num]:\n            rotated = self._rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n            temp_error = self._cal_error(rotated)\n            if temp_error &lt; min_error:\n                min_error = temp_error\n                best_index = index\n        return angle_list[best_index] / 180 * math.pi\n\n    def _is_cross_text(self, start_loc, length, vertices):\n\"\"\"\n        check if the crop image crosses text regions\n        Input:\n          start_loc: left-top position\n          length   : length of crop image\n          vertices : vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n        Output:\n          True if crop image crosses text region\n        \"\"\"\n        if vertices.size == 0:\n            return False\n        start_w, start_h = start_loc\n        a = np.array(\n            [start_w, start_h, start_w + length, start_h, start_w + length, start_h + length, start_w, start_h + length]\n        ).reshape((4, 2))\n        p1 = Polygon(a).convex_hull\n        for vertice in vertices:\n            p2 = Polygon(vertice.reshape((4, 2))).convex_hull\n            inter = p1.intersection(p2).area\n            if 0.01 &lt;= inter / p2.area &lt;= 0.99:\n                return True\n        return False\n\n    def _crop_img(self, img, vertices, labels, length):\n\"\"\"\n        crop img patches to obtain batch and augment\n        Input:\n          img         : PIL Image\n          vertices    : vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          labels      : 1-&gt;valid, 0-&gt;ignore, &lt;numpy.ndarray, (n,)&gt;\n          length      : length of cropped image region\n        Output:\n          region      : cropped image region\n          new_vertices: new vertices in cropped region\n        \"\"\"\n        h, w = img.height, img.width\n        # confirm the shortest side of image &gt;= length\n        if h &gt;= w and w &lt; length:\n            img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n        elif h &lt; w and h &lt; length:\n            img = img.resize((int(w * length / h), length), Image.BILINEAR)\n        ratio_w = img.width / w\n        ratio_h = img.height / h\n        assert ratio_w &gt;= 1 and ratio_h &gt;= 1\n\n        new_vertices = np.zeros(vertices.shape)\n        if vertices.size &gt; 0:\n            new_vertices[:, [0, 2, 4, 6]] = vertices[:, [0, 2, 4, 6]] * ratio_w\n            new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * ratio_h\n\n        # find random position\n        remain_h = img.height - length\n        remain_w = img.width - length\n        flag = True\n        cnt = 0\n        while flag and cnt &lt; 1000:\n            cnt += 1\n            start_w = int(np.random.rand() * remain_w)\n            start_h = int(np.random.rand() * remain_h)\n            flag = self._is_cross_text([start_w, start_h], length, new_vertices[labels == 1, :])\n        box = (start_w, start_h, start_w + length, start_h + length)\n        region = img.crop(box)\n        if new_vertices.size == 0:\n            return region, new_vertices\n\n        new_vertices[:, [0, 2, 4, 6]] -= start_w\n        new_vertices[:, [1, 3, 5, 7]] -= start_h\n        return region, new_vertices\n\n    def _rotate_all_pixels(self, rotate_mat, anchor_x, anchor_y, length):\n\"\"\"\n        get rotated locations of all pixels for next stages\n        Input:\n          rotate_mat: rotatation matrix\n          anchor_x  : fixed x position\n          anchor_y  : fixed y position\n          length    : length of image\n        Output:\n          rotated_x : rotated x positions &lt;numpy.ndarray, (length,length)&gt;\n          rotated_y : rotated y positions &lt;numpy.ndarray, (length,length)&gt;\n        \"\"\"\n        x = np.arange(length)\n        y = np.arange(length)\n        x, y = np.meshgrid(x, y)\n        x_lin = x.reshape((1, x.size))\n        y_lin = y.reshape((1, x.size))\n        coord_mat = np.concatenate((x_lin, y_lin), 0)\n        rotated_coord = np.matmul(\n            rotate_mat.astype(np.float16), (coord_mat - np.array([[anchor_x], [anchor_y]])).astype(np.float16)\n        ) + np.array([[anchor_x], [anchor_y]])\n        rotated_x = rotated_coord[0, :].reshape(x.shape)\n        rotated_y = rotated_coord[1, :].reshape(y.shape)\n        return rotated_x, rotated_y\n\n    def _adjust_height(self, img, vertices, ratio=0.2):\n\"\"\"\n        adjust height of image to aug data\n        Input:\n          img         : PIL Image\n          vertices    : vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          ratio       : height changes in [0.8, 1.2]\n        Output:\n          img         : adjusted PIL Image\n          new_vertices: adjusted vertices\n        \"\"\"\n        ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\n        old_h = img.height\n        new_h = int(np.around(old_h * ratio_h))\n        img = img.resize((img.width, new_h), Image.BILINEAR)\n\n        new_vertices = vertices.copy()\n        if vertices.size &gt; 0:\n            new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * (new_h / old_h)\n        return img, new_vertices\n\n    def _adjust_width(self, img, vertices, ratio=0.2):\n\"\"\"\n        adjust width of image to aug data\n        Input:\n          img         : PIL Image\n          vertices    : vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          ratio       : height changes in [0.8, 1.2]\n        Output:\n          img         : adjusted PIL Image\n          new_vertices: adjusted vertices\n        \"\"\"\n        ratio_w = 1 + ratio * (np.random.rand() * 2 - 1)\n        old_w = img.width\n        new_w = int(np.around(old_w * ratio_w))\n        img = img.resize((new_w, img.height), Image.BILINEAR)\n\n        new_vertices = vertices.copy()\n        if vertices.size &gt; 0:\n            new_vertices[:, [0, 2, 4, 6]] = vertices[:, [0, 2, 4, 6]] * (new_w / old_w)\n        return img, new_vertices\n\n    def _rotate_img(self, img, vertices, angle_range=10):\n\"\"\"\n        rotate image [-10, 10] degree to aug data\n        Input:\n          img         : PIL Image\n          vertices    : vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          angle_range : rotate range\n        Output:\n          img         : rotated PIL Image\n          new_vertices: rotated vertices\n        \"\"\"\n        center_x = (img.width - 1) / 2\n        center_y = (img.height - 1) / 2\n        angle = angle_range * (np.random.rand() * 2 - 1)\n        img = img.rotate(angle, Image.BILINEAR)\n        new_vertices = np.zeros(vertices.shape)\n        for i, vertice in enumerate(vertices):\n            new_vertices[i, :] = self._rotate_vertices(\n                vertice, -angle / 180 * math.pi, np.array([[center_x], [center_y]])\n            )\n        return img, new_vertices\n\n    def _get_score_geo(self, img, vertices, labels, scale, length):\n\"\"\"\n        generate score gt and geometry gt\n        Input:\n          img     : PIL Image\n          vertices: vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          labels  : 1-&gt;valid, 0-&gt;ignore, &lt;numpy.ndarray, (n,)&gt;\n          scale   : feature map / image\n          length  : image length\n        Output:\n          score gt, geo gt, ignored\n        \"\"\"\n        score_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n        geo_map = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\n        ignored_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n\n        index = np.arange(0, length, int(1 / scale))\n        index_x, index_y = np.meshgrid(index, index)\n        ignored_polys = []\n        polys = []\n\n        for i, vertice in enumerate(vertices):\n            if labels[i] == 0:\n                ignored_polys.append(np.around(scale * vertice.reshape((4, 2))).astype(np.int32))\n                continue\n\n            poly = np.around(scale * self._shrink_poly(vertice).reshape((4, 2))).astype(np.int32)\n            polys.append(poly)\n            temp_mask = np.zeros(score_map.shape[:-1], np.float32)\n            cv2.fillPoly(temp_mask, [poly], 1)\n\n            theta = self._find_min_rect_angle(vertice)\n            rotate_mat = self._get_rotate_mat(theta)\n\n            rotated_vertices = self._rotate_vertices(vertice, theta)\n            x_min, x_max, y_min, y_max = self._get_boundary(rotated_vertices)\n            rotated_x, rotated_y = self._rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length)\n\n            d1 = rotated_y - y_min\n            d1[d1 &lt; 0] = 0\n            d2 = y_max - rotated_y\n            d2[d2 &lt; 0] = 0\n            d3 = rotated_x - x_min\n            d3[d3 &lt; 0] = 0\n            d4 = x_max - rotated_x\n            d4[d4 &lt; 0] = 0\n            geo_map[:, :, 0] += d1[index_y, index_x] * temp_mask\n            geo_map[:, :, 1] += d2[index_y, index_x] * temp_mask\n            geo_map[:, :, 2] += d3[index_y, index_x] * temp_mask\n            geo_map[:, :, 3] += d4[index_y, index_x] * temp_mask\n            geo_map[:, :, 4] += theta * temp_mask\n\n        cv2.fillPoly(ignored_map, ignored_polys, 1)\n        cv2.fillPoly(score_map, polys, 1)\n        return score_map, geo_map, ignored_map\n\n    def _extract_vertices(self, data_labels):\n\"\"\"\n        extract vertices info from txt lines\n        Input:\n          lines   : list of string info\n        Output:\n          vertices: vertices of text regions &lt;numpy.ndarray, (n,8)&gt;\n          labels  : 1-&gt;valid, 0-&gt;ignore, &lt;numpy.ndarray, (n,)&gt;\n        \"\"\"\n        vertices_list = []\n        labels_list = []\n        data_labels = eval(data_labels)\n        for data_label in data_labels:\n            vertices = data_label[\"points\"]\n            vertices = [item for point in vertices for item in point]\n            vertices_list.append(vertices)\n            labels = 0 if data_label[\"transcription\"] == \"###\" else 1\n            labels_list.append(labels)\n        return np.array(vertices_list), np.array(labels_list)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms.det_transforms","title":"<code>mindocr.data.transforms.det_transforms</code>","text":"<p>transforms for text detection tasks.</p> <code>mindocr.data.transforms.det_transforms.BorderMap</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class BorderMap:\n    def __init__(self, shrink_ratio=0.4, thresh_min=0.3, thresh_max=0.7, **kwargs):\n        self._thresh_min = thresh_min\n        self._thresh_max = thresh_max\n        self._dist_coef = 1 - shrink_ratio**2\n\n    def __call__(self, data):\n        border = np.zeros(data[\"image\"].shape[:2], dtype=np.float32)\n        mask = np.zeros(data[\"image\"].shape[:2], dtype=np.float32)\n\n        for i in range(len(data[\"polys\"])):\n            if not data[\"ignore_tags\"][i]:\n                self._draw_border(data[\"polys\"][i], border, mask=mask)\n        border = border * (self._thresh_max - self._thresh_min) + self._thresh_min\n\n        data[\"thresh_map\"] = border\n        data[\"thresh_mask\"] = mask\n        return data\n\n    def _draw_border(self, np_poly, border, mask):\n        # draw mask\n        poly = Polygon(np_poly)\n        distance = self._dist_coef * poly.area / poly.length\n        padded_polygon = np.array(expand_poly(np_poly, distance)[0], dtype=np.int32)\n        cv2.fillPoly(mask, [padded_polygon], 1.0)\n\n        # draw border\n        min_vals, max_vals = np.min(padded_polygon, axis=0), np.max(padded_polygon, axis=0)\n        width, height = max_vals - min_vals + 1\n        np_poly = np_poly - min_vals\n\n        xs = np.broadcast_to(np.linspace(0, width - 1, num=width).reshape(1, width), (height, width))\n        ys = np.broadcast_to(np.linspace(0, height - 1, num=height).reshape(height, 1), (height, width))\n\n        distance_map = [self._distance(xs, ys, p1, p2) for p1, p2 in zip(np_poly, np.roll(np_poly, 1, axis=0))]\n        distance_map = np.clip(np.array(distance_map, dtype=np.float32) / distance, 0, 1).min(axis=0)  # NOQA\n\n        min_valid = np.clip(min_vals, 0, np.array(border.shape[::-1]) - 1)  # shape reverse order: w, h\n        max_valid = np.clip(max_vals, 0, np.array(border.shape[::-1]) - 1)\n\n        border[min_valid[1] : max_valid[1] + 1, min_valid[0] : max_valid[0] + 1] = np.fmax(\n            1\n            - distance_map[\n                min_valid[1] - min_vals[1] : max_valid[1] - max_vals[1] + height,\n                min_valid[0] - min_vals[0] : max_valid[0] - max_vals[0] + width,\n            ],\n            border[min_valid[1] : max_valid[1] + 1, min_valid[0] : max_valid[0] + 1],\n        )\n\n    @staticmethod\n    def _distance(xs, ys, point_1, point_2):\n\"\"\"\n        compute the distance from each point to a line\n        ys: coordinates in the first axis\n        xs: coordinates in the second axis\n        point_1, point_2: (x, y), the end of the line\n        \"\"\"\n        a_sq = np.square(xs - point_1[0]) + np.square(ys - point_1[1])\n        b_sq = np.square(xs - point_2[0]) + np.square(ys - point_2[1])\n        c_sq = np.square(point_1[0] - point_2[0]) + np.square(point_1[1] - point_2[1])\n\n        cos = (a_sq + b_sq - c_sq) / (2 * np.sqrt(a_sq * b_sq))\n        sin_sq = np.nan_to_num(1 - np.square(cos))\n        result = np.sqrt(a_sq * b_sq * sin_sq / c_sq)\n\n        result[cos &gt;= 0] = np.sqrt(np.fmin(a_sq, b_sq))[cos &gt;= 0]\n        return result\n</code></pre> <code>mindocr.data.transforms.det_transforms.DetLabelEncode</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class DetLabelEncode:\n    def __init__(self, **kwargs):\n        pass\n\n    def order_points_clockwise(self, pts):\n        rect = np.zeros((4, 2), dtype=\"float32\")\n        s = pts.sum(axis=1)\n        rect[0] = pts[np.argmin(s)]\n        rect[2] = pts[np.argmax(s)]\n        tmp = np.delete(pts, (np.argmin(s), np.argmax(s)), axis=0)\n        diff = np.diff(np.array(tmp), axis=1)\n        rect[1] = tmp[np.argmin(diff)]\n        rect[3] = tmp[np.argmax(diff)]\n        return rect\n\n    def expand_points_num(self, boxes):\n        max_points_num = 0\n        for b in boxes:\n            if len(b) &gt; max_points_num:\n                max_points_num = len(b)\n        ex_boxes = []\n        for b in boxes:\n            ex_box = b + [b[-1]] * (max_points_num - len(b))\n            ex_boxes.append(ex_box)\n        return ex_boxes\n\n    def __call__(self, data):\n\"\"\"\n        required keys:\n            label (str): string containgin points and transcription in json format\n        added keys:\n            polys (np.ndarray): polygon boxes in an image, each polygon is represented by points\n                            in shape [num_polygons, num_points, 2]\n            texts (List(str)): text string\n            ignore_tags (np.ndarray[bool]): indicators for ignorable texts (e.g., '###')\n        \"\"\"\n        label = data[\"label\"]\n        label = json.loads(label)\n        nBox = len(label)\n        boxes, txts, txt_tags = [], [], []\n        for bno in range(0, nBox):\n            box = label[bno][\"points\"]\n            txt = label[bno][\"transcription\"]\n            boxes.append(box)\n            txts.append(txt)\n            if txt in [\"*\", \"###\"]:\n                txt_tags.append(True)\n            else:\n                txt_tags.append(False)\n        if len(boxes) == 0:\n            return None\n        boxes = self.expand_points_num(boxes)\n        boxes = np.array(boxes, dtype=np.float32)\n        txt_tags = np.array(txt_tags, dtype=np.bool)\n\n        data[\"polys\"] = boxes\n        data[\"texts\"] = txts\n        data[\"ignore_tags\"] = txt_tags\n        return data\n</code></pre> <code>mindocr.data.transforms.det_transforms.DetLabelEncode.__call__(data)</code> \u00b6 required keys <p>label (str): string containgin points and transcription in json format</p> added keys <p>polys (np.ndarray): polygon boxes in an image, each polygon is represented by points                 in shape [num_polygons, num_points, 2] texts (List(str)): text string ignore_tags (np.ndarray[bool]): indicators for ignorable texts (e.g., '###')</p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    required keys:\n        label (str): string containgin points and transcription in json format\n    added keys:\n        polys (np.ndarray): polygon boxes in an image, each polygon is represented by points\n                        in shape [num_polygons, num_points, 2]\n        texts (List(str)): text string\n        ignore_tags (np.ndarray[bool]): indicators for ignorable texts (e.g., '###')\n    \"\"\"\n    label = data[\"label\"]\n    label = json.loads(label)\n    nBox = len(label)\n    boxes, txts, txt_tags = [], [], []\n    for bno in range(0, nBox):\n        box = label[bno][\"points\"]\n        txt = label[bno][\"transcription\"]\n        boxes.append(box)\n        txts.append(txt)\n        if txt in [\"*\", \"###\"]:\n            txt_tags.append(True)\n        else:\n            txt_tags.append(False)\n    if len(boxes) == 0:\n        return None\n    boxes = self.expand_points_num(boxes)\n    boxes = np.array(boxes, dtype=np.float32)\n    txt_tags = np.array(txt_tags, dtype=np.bool)\n\n    data[\"polys\"] = boxes\n    data[\"texts\"] = txts\n    data[\"ignore_tags\"] = txt_tags\n    return data\n</code></pre> <code>mindocr.data.transforms.det_transforms.DetResize</code> \u00b6 <p>Resize the image and text polygons (if have) for text detection</p> PARAMETER DESCRIPTION <code>target_size</code> <p>target size [H, W] of the output image. If it is not None, <code>limit_type</code> will be forced to None and side limit-based resizng will not make effect. Default: None.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>keep_ratio</code> <p>whether to keep aspect ratio. Default: True</p> <p> DEFAULT: <code>True</code> </p> <code>padding</code> <p>whether to pad the image to the <code>target_size</code> after \"keep-ratio\" resizing. Only used when keep_ratio is True. Default False.</p> <p> DEFAULT: <code>False</code> </p> <code>limit_type</code> <p>it decides the resize method type. Option: 'min', 'max', None. Default: \"min\" - 'min': images will be resized by limiting the mininum side length to <code>limit_side_len</code>, i.e.,   any side of the image must be larger than or equal to <code>limit_side_len</code>. If the input image alreay fulfill this limitation, no scaling will performed. If not, input image will be up-scaled with the ratio of (limit_side_len / shorter side length) - 'max': images will be resized by limiting the maximum side length to <code>limit_side_len</code>, i.e.,   any side of the image must be smaller than or equal to <code>limit_side_len</code>. If the input image alreay fulfill   this limitation, no scaling will performed. If not, input image will be down-scaled with the ratio of   (limit_side_len / longer side length) -  None: No limitation. Images will be resized to <code>target_size</code> with or without <code>keep_ratio</code> and <code>padding</code></p> <p> DEFAULT: <code>'min'</code> </p> <code>limit_side_len</code> <p>side len limitation.</p> <p> DEFAULT: <code>736</code> </p> <code>force_divisable</code> <p>whether to force the image being resize to a size multiple of <code>divisor</code> (e.g. 32) in the end, which is suitable for some networks (e.g. dbnet-resnet50). Default: True.</p> <p> DEFAULT: <code>True</code> </p> <code>divisor</code> <p>divisor used when <code>force_divisable</code> enabled. The value is decided by the down-scaling path of the network backbone (e.g. resnet, feature map size is 2^5 smaller than input image size). Default is 32.</p> <p> DEFAULT: <code>32</code> </p> <code>interpoloation</code> <p>interpolation method</p> <p> </p> Note <ol> <li>The default choices limit_type=min, with large <code>limit_side_len</code> are recommended for inference in detection for better accuracy,</li> <li>If target_size set, keep_ratio=True, limit_type=null, padding=True, this transform works the same as ScalePadImage,</li> <li>If inference speed is the first priority to guarante, you can set limit_type=max with a small <code>limit_side_len</code> like 960.</li> </ol> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class DetResize:\n\"\"\"\n    Resize the image and text polygons (if have) for text detection\n\n    Args:\n        target_size: target size [H, W] of the output image. If it is not None, `limit_type` will be forced to None and\n            side limit-based resizng will not make effect. Default: None.\n        keep_ratio: whether to keep aspect ratio. Default: True\n        padding: whether to pad the image to the `target_size` after \"keep-ratio\" resizing. Only used when keep_ratio is\n            True. Default False.\n        limit_type: it decides the resize method type. Option: 'min', 'max', None. Default: \"min\"\n            - 'min': images will be resized by limiting the mininum side length to `limit_side_len`, i.e.,\n              any side of the image must be larger than or equal to `limit_side_len`. If the input image alreay fulfill\n            this limitation, no scaling will performed. If not, input image will be up-scaled with the ratio of\n            (limit_side_len / shorter side length)\n            - 'max': images will be resized by limiting the maximum side length to `limit_side_len`, i.e.,\n              any side of the image must be smaller than or equal to `limit_side_len`. If the input image alreay fulfill\n              this limitation, no scaling will performed. If not, input image will be down-scaled with the ratio of\n              (limit_side_len / longer side length)\n            -  None: No limitation. Images will be resized to `target_size` with or without `keep_ratio` and `padding`\n        limit_side_len: side len limitation.\n        force_divisable: whether to force the image being resize to a size multiple of `divisor` (e.g. 32) in the end,\n            which is suitable for some networks (e.g. dbnet-resnet50). Default: True.\n        divisor: divisor used when `force_divisable` enabled. The value is decided by the down-scaling path of\n            the network backbone (e.g. resnet, feature map size is 2^5 smaller than input image size). Default is 32.\n        interpoloation: interpolation method\n\n    Note:\n        1. The default choices limit_type=min, with large `limit_side_len` are recommended for inference in detection\n        for better accuracy,\n        2. If target_size set, keep_ratio=True, limit_type=null, padding=True, this transform works the same as\n        ScalePadImage,\n        3. If inference speed is the first priority to guarante, you can set limit_type=max with a small\n        `limit_side_len` like 960.\n    \"\"\"\n\n    def __init__(\n        self,\n        target_size: list = None,\n        keep_ratio=True,\n        padding=False,\n        limit_type=\"min\",\n        limit_side_len=736,\n        force_divisable=True,\n        divisor=32,\n        interpolation=cv2.INTER_LINEAR,\n        **kwargs,\n    ):\n        if target_size is not None:\n            limit_type = None\n\n        self.target_size = target_size\n        self.keep_ratio = keep_ratio\n        self.padding = padding\n        self.limit_side_len = limit_side_len\n        self.limit_type = limit_type\n        self.interpolation = interpolation\n        self.force_divisable = force_divisable\n        self.divisor = divisor\n\n        self.is_train = kwargs.get(\"is_train\", False)\n        assert target_size is None or limit_type is None, \"Only one of limit_type and target_size should be provided.\"\n        if limit_type in [\"min\", \"max\"]:\n            keep_ratio = True\n            padding = False\n            print(\n                f\"INFO: `limit_type` is {limit_type}. Image will be resized by limiting the {limit_type} \"\n                f\"side length to {limit_side_len}.\"\n            )\n        elif not limit_type:\n            assert target_size is not None or force_divisable is not None, (\n                \"One of `target_size` or `force_divisable` is required when limit_type is not set. \"\n                \"Please set at least one of them.\"\n            )\n            if target_size and force_divisable:\n                if (target_size[0] % divisor != 0) or (target_size[1] % divisor != 0):\n                    self.target_size = [max(round(x / self.divisor) * self.divisor, self.divisor) for x in target_size]\n                    print(\n                        f\"WARNING: `force_divisable` is enabled but the set target size {target_size} \"\n                        f\"is not divisable by {divisor}. Target size is ajusted to {self.target_size}\"\n                    )\n            if (target_size is not None) and keep_ratio and (not padding):\n                print(\"WARNING: output shape can be dynamic if keep_ratio but no padding.\")\n        else:\n            raise ValueError(f\"Unknown limit_type: {limit_type}\")\n\n    def __call__(self, data: dict):\n\"\"\"\n        required keys:\n            image: shape HWC\n            polys: shape [num_polys, num_points, 2] (optional)\n        modified keys:\n            image\n            (polys)\n        added keys:\n            shape: [src_h, src_w, scale_ratio_h, scale_ratio_w]\n        \"\"\"\n        img = data[\"image\"]\n        h, w = img.shape[:2]\n        if self.target_size:\n            tar_h, tar_w = self.target_size\n\n        scale_ratio = 1.0\n        allow_padding = False\n        if self.limit_type == \"min\":\n            if min(h, w) &lt; self.limit_side_len:  # upscale\n                scale_ratio = self.limit_side_len / float(min(h, w))\n        elif self.limit_type == \"max\":\n            if max(h, w) &gt; self.limit_side_len:  # downscale\n                scale_ratio = self.limit_side_len / float(max(h, w))\n        elif not self.limit_type:\n            if self.keep_ratio and self.target_size:\n                # scale the image until it fits in the target size at most. The left part could be filled by padding.\n                scale_ratio = min(tar_h / h, tar_w / w)\n                allow_padding = True\n\n        if (self.limit_type in [\"min\", \"max\"]) or (self.target_size and self.keep_ratio):\n            resize_w = math.ceil(w * scale_ratio)\n            resize_h = math.ceil(h * scale_ratio)\n            if self.target_size:\n                resize_w = min(resize_w, tar_w)\n                resize_h = min(resize_h, tar_h)\n        elif self.target_size:\n            resize_w = tar_w\n            resize_h = tar_h\n        else:  # both target_size and limit_type is None. resize by force_divisable\n            resize_w = w\n            resize_h = h\n\n        if self.force_divisable:\n            if not (\n                allow_padding and self.padding\n            ):  # no need to round it the image will be padded to the target size which is divisable.\n                # adjust the size slightly so that both sides of the image are divisable by divisor\n                # e.g. 32, which could be required by the network\n                resize_h = max(\n                    math.ceil(resize_h / self.divisor) * self.divisor, self.divisor\n                )  # diff from resize_image_type0 in pp which uses round()\n                resize_w = max(math.ceil(resize_w / self.divisor) * self.divisor, self.divisor)\n\n        resized_img = cv2.resize(img, (resize_w, resize_h), interpolation=self.interpolation)\n\n        if allow_padding and self.padding:\n            if self.target_size and (tar_h &gt;= resize_h and tar_w &gt;= resize_w):\n                # do padding\n                padded_img = np.zeros((tar_h, tar_w, 3), dtype=np.uint8)\n                padded_img[:resize_h, :resize_w, :] = resized_img\n                data[\"image\"] = padded_img\n            else:\n                print(\n                    f\"WARNING: Image shape after resize is ({resize_h}, {resize_w}), \"\n                    f\"which is larger than target_size {self.target_size}. Skip padding for the current image. \"\n                    f\"You may disable `force_divisable` to avoid this warning.\"\n                )\n        else:\n            data[\"image\"] = resized_img\n\n        scale_h = resize_h / h\n        scale_w = resize_w / w\n\n        # Only need to transform ground truth polygons in training for generating masks/maps.\n        # For evaluation, we should not change the GT polygons.\n        # The metric with input of GT polygons and predicted polygons must be computed in the original image space\n        # for consistent comparison.\n        if \"polys\" in data and self.is_train:\n            data[\"polys\"][:, :, 0] = data[\"polys\"][:, :, 0] * scale_w\n            data[\"polys\"][:, :, 1] = data[\"polys\"][:, :, 1] * scale_h\n\n        if \"shape_list\" not in data:\n            src_h, src_w = data.get(\"raw_img_shape\", (h, w))\n            data[\"shape_list\"] = np.array([src_h, src_w, scale_h, scale_w], dtype=np.float32)\n        else:\n            data[\"shape_list\"][2] = data[\"shape_list\"][2] * scale_h\n            data[\"shape_list\"][3] = data[\"shape_list\"][3] * scale_h\n\n        return data\n</code></pre> <code>mindocr.data.transforms.det_transforms.DetResize.__call__(data)</code> \u00b6 required keys modified keys <p>image (polys)</p> added keys Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>def __call__(self, data: dict):\n\"\"\"\n    required keys:\n        image: shape HWC\n        polys: shape [num_polys, num_points, 2] (optional)\n    modified keys:\n        image\n        (polys)\n    added keys:\n        shape: [src_h, src_w, scale_ratio_h, scale_ratio_w]\n    \"\"\"\n    img = data[\"image\"]\n    h, w = img.shape[:2]\n    if self.target_size:\n        tar_h, tar_w = self.target_size\n\n    scale_ratio = 1.0\n    allow_padding = False\n    if self.limit_type == \"min\":\n        if min(h, w) &lt; self.limit_side_len:  # upscale\n            scale_ratio = self.limit_side_len / float(min(h, w))\n    elif self.limit_type == \"max\":\n        if max(h, w) &gt; self.limit_side_len:  # downscale\n            scale_ratio = self.limit_side_len / float(max(h, w))\n    elif not self.limit_type:\n        if self.keep_ratio and self.target_size:\n            # scale the image until it fits in the target size at most. The left part could be filled by padding.\n            scale_ratio = min(tar_h / h, tar_w / w)\n            allow_padding = True\n\n    if (self.limit_type in [\"min\", \"max\"]) or (self.target_size and self.keep_ratio):\n        resize_w = math.ceil(w * scale_ratio)\n        resize_h = math.ceil(h * scale_ratio)\n        if self.target_size:\n            resize_w = min(resize_w, tar_w)\n            resize_h = min(resize_h, tar_h)\n    elif self.target_size:\n        resize_w = tar_w\n        resize_h = tar_h\n    else:  # both target_size and limit_type is None. resize by force_divisable\n        resize_w = w\n        resize_h = h\n\n    if self.force_divisable:\n        if not (\n            allow_padding and self.padding\n        ):  # no need to round it the image will be padded to the target size which is divisable.\n            # adjust the size slightly so that both sides of the image are divisable by divisor\n            # e.g. 32, which could be required by the network\n            resize_h = max(\n                math.ceil(resize_h / self.divisor) * self.divisor, self.divisor\n            )  # diff from resize_image_type0 in pp which uses round()\n            resize_w = max(math.ceil(resize_w / self.divisor) * self.divisor, self.divisor)\n\n    resized_img = cv2.resize(img, (resize_w, resize_h), interpolation=self.interpolation)\n\n    if allow_padding and self.padding:\n        if self.target_size and (tar_h &gt;= resize_h and tar_w &gt;= resize_w):\n            # do padding\n            padded_img = np.zeros((tar_h, tar_w, 3), dtype=np.uint8)\n            padded_img[:resize_h, :resize_w, :] = resized_img\n            data[\"image\"] = padded_img\n        else:\n            print(\n                f\"WARNING: Image shape after resize is ({resize_h}, {resize_w}), \"\n                f\"which is larger than target_size {self.target_size}. Skip padding for the current image. \"\n                f\"You may disable `force_divisable` to avoid this warning.\"\n            )\n    else:\n        data[\"image\"] = resized_img\n\n    scale_h = resize_h / h\n    scale_w = resize_w / w\n\n    # Only need to transform ground truth polygons in training for generating masks/maps.\n    # For evaluation, we should not change the GT polygons.\n    # The metric with input of GT polygons and predicted polygons must be computed in the original image space\n    # for consistent comparison.\n    if \"polys\" in data and self.is_train:\n        data[\"polys\"][:, :, 0] = data[\"polys\"][:, :, 0] * scale_w\n        data[\"polys\"][:, :, 1] = data[\"polys\"][:, :, 1] * scale_h\n\n    if \"shape_list\" not in data:\n        src_h, src_w = data.get(\"raw_img_shape\", (h, w))\n        data[\"shape_list\"] = np.array([src_h, src_w, scale_h, scale_w], dtype=np.float32)\n    else:\n        data[\"shape_list\"][2] = data[\"shape_list\"][2] * scale_h\n        data[\"shape_list\"][3] = data[\"shape_list\"][3] * scale_h\n\n    return data\n</code></pre> <code>mindocr.data.transforms.det_transforms.GridResize</code> \u00b6 <p>         Bases: <code>DetResize</code></p> <p>Resize image to make it divisible by a specified factor exactly. Resize polygons correspondingly, if provided.</p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class GridResize(DetResize):\n\"\"\"\n    Resize image to make it divisible by a specified factor exactly.\n    Resize polygons correspondingly, if provided.\n    \"\"\"\n\n    def __init__(self, factor: int = 32, **kwargs):\n        super().__init__(\n            target_size=None,\n            keep_ratio=False,\n            padding=False,\n            limit_type=None,\n            force_divisable=True,\n            divisor=factor,\n        )\n</code></pre> <code>mindocr.data.transforms.det_transforms.RandomCropWithBBox</code> \u00b6 <p>Randomly cuts a crop from an image along with polygons in the way that the crop doesn't intersect any polygons (i.e. any given polygon is either fully inside or fully outside the crop).</p> PARAMETER DESCRIPTION <code>max_tries</code> <p>number of attempts to try to cut a crop with a polygon in it. If fails, scales the whole image to        match the <code>crop_size</code>.</p> <p> DEFAULT: <code>10</code> </p> <code>min_crop_ratio</code> <p>minimum size of a crop in respect to an input image size.</p> <p> DEFAULT: <code>0.1</code> </p> <code>crop_size</code> <p>target size of the crop (resized and padded, if needed), preserves sides ratio.</p> <p> DEFAULT: <code>(640, 640)</code> </p> <code>p</code> <p>probability of the augmentation being applied to an image.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class RandomCropWithBBox:\n\"\"\"\n    Randomly cuts a crop from an image along with polygons in the way that the crop doesn't intersect any polygons\n    (i.e. any given polygon is either fully inside or fully outside the crop).\n\n    Args:\n        max_tries: number of attempts to try to cut a crop with a polygon in it. If fails, scales the whole image to\n                   match the `crop_size`.\n        min_crop_ratio: minimum size of a crop in respect to an input image size.\n        crop_size: target size of the crop (resized and padded, if needed), preserves sides ratio.\n        p: probability of the augmentation being applied to an image.\n    \"\"\"\n\n    def __init__(self, max_tries=10, min_crop_ratio=0.1, crop_size=(640, 640), p: float = 0.5, **kwargs):\n        self._crop_size = crop_size\n        self._ratio = min_crop_ratio\n        self._max_tries = max_tries\n        self._p = p\n\n    def __call__(self, data):\n        if random.random() &lt; self._p:  # cut a crop\n            start, end = self._find_crop(data)\n        else:  # scale and pad the whole image\n            start, end = np.array([0, 0]), np.array(data[\"image\"].shape[:2])\n\n        scale = min(self._crop_size / (end - start))\n\n        data[\"image\"] = cv2.resize(data[\"image\"][start[0] : end[0], start[1] : end[1]], None, fx=scale, fy=scale)\n        data[\"actual_size\"] = np.array(data[\"image\"].shape[:2])\n        data[\"image\"] = np.pad(\n            data[\"image\"], (*tuple((0, cs - ds) for cs, ds in zip(self._crop_size, data[\"image\"].shape[:2])), (0, 0))\n        )\n\n        data[\"polys\"] = (data[\"polys\"] - start[::-1]) * scale\n\n        return data\n\n    def _find_crop(self, data):\n        size = np.array(data[\"image\"].shape[:2])\n        polys = [poly for poly, ignore in zip(data[\"polys\"], data[\"ignore_tags\"]) if not ignore]\n\n        if polys:\n            # do not crop through polys =&gt; find available \"empty\" coordinates\n            h_array, w_array = np.zeros(size[0], dtype=np.int32), np.zeros(size[1], dtype=np.int32)\n            for poly in polys:\n                points = np.maximum(np.round(poly).astype(np.int32), 0)\n                w_array[points[:, 0].min() : points[:, 0].max() + 1] = 1\n                h_array[points[:, 1].min() : points[:, 1].max() + 1] = 1\n\n            if not h_array.all() and not w_array.all():  # if texts do not occupy full image\n                # find available coordinates that don't include text\n                h_avail = np.where(h_array == 0)[0]\n                w_avail = np.where(w_array == 0)[0]\n\n                min_size = np.ceil(size * self._ratio).astype(np.int32)\n                for _ in range(self._max_tries):\n                    y = np.sort(np.random.choice(h_avail, size=2))\n                    x = np.sort(np.random.choice(w_avail, size=2))\n                    start, end = np.array([y[0], x[0]]), np.array([y[1], x[1]])\n\n                    if ((end - start) &lt; min_size).any():  # NOQA\n                        continue\n\n                    # check that at least one polygon is within the crop\n                    for poly in polys:\n                        if (poly.max(axis=0) &gt; start[::-1]).all() and (poly.min(axis=0) &lt; end[::-1]).all():  # NOQA\n                            return start, end\n\n        # failed to generate a crop or all polys are marked as ignored\n        return np.array([0, 0]), size\n</code></pre> <code>mindocr.data.transforms.det_transforms.ScalePadImage</code> \u00b6 <p>         Bases: <code>DetResize</code></p> <p>Scale image and polys by the shorter side, then pad to the target_size. input image format: hwc</p> PARAMETER DESCRIPTION <code>target_size</code> <p>[H, W] of the output image.</p> <p> TYPE: <code>list</code> </p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class ScalePadImage(DetResize):\n\"\"\"\n    Scale image and polys by the shorter side, then pad to the target_size.\n    input image format: hwc\n\n    Args:\n        target_size: [H, W] of the output image.\n    \"\"\"\n\n    def __init__(self, target_size: list, **kwargs):\n        super().__init__(\n            target_size=target_size,\n            keep_ratio=True,\n            padding=True,\n            limit_type=None,\n            force_divisable=False,\n        )\n</code></pre> <code>mindocr.data.transforms.det_transforms.ShrinkBinaryMap</code> \u00b6 <p>Making binary mask from detection data with ICDAR format. Typically following the process of class <code>MakeICDARData</code>.</p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class ShrinkBinaryMap:\n\"\"\"\n    Making binary mask from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    \"\"\"\n\n    def __init__(self, min_text_size=8, shrink_ratio=0.4, **kwargs):\n        self._min_text_size = min_text_size\n        self._dist_coef = 1 - shrink_ratio**2\n\n    def __call__(self, data):\n        gt = np.zeros(data[\"image\"].shape[:2], dtype=np.float32)\n        mask = np.ones(data[\"image\"].shape[:2], dtype=np.float32)\n\n        if len(data[\"polys\"]):\n            for i in range(len(data[\"polys\"])):\n                min_side = min(np.max(data[\"polys\"][i], axis=0) - np.min(data[\"polys\"][i], axis=0))\n\n                if data[\"ignore_tags\"][i] or min_side &lt; self._min_text_size:\n                    cv2.fillPoly(mask, [data[\"polys\"][i].astype(np.int32)], 0)\n                    data[\"ignore_tags\"][i] = True\n                else:\n                    poly = Polygon(data[\"polys\"][i])\n                    shrunk = expand_poly(data[\"polys\"][i], distance=-self._dist_coef * poly.area / poly.length)\n\n                    if shrunk:\n                        cv2.fillPoly(gt, [np.array(shrunk[0], dtype=np.int32)], 1)\n                    else:\n                        cv2.fillPoly(mask, [data[\"polys\"][i].astype(np.int32)], 0)\n                        data[\"ignore_tags\"][i] = True\n\n        data[\"binary_map\"] = np.expand_dims(gt, axis=0)\n        data[\"mask\"] = mask\n        return data\n</code></pre> <code>mindocr.data.transforms.det_transforms.ValidatePolygons</code> \u00b6 Validate polygons by <ol> <li>filtering out polygons outside an image.</li> <li>clipping coordinates of polygons that are partially outside an image to stay within the visible region.</li> </ol> PARAMETER DESCRIPTION <code>min_area</code> <p>minimum area below which newly clipped polygons considered as ignored.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>clip_to_visible_area</code> <p>(Experimental) clip polygons to a visible area. Number of vertices in a polygon after                   clipping may change.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_vertices</code> <p>minimum number of vertices in a polygon below which newly clipped polygons considered as ignored.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> Source code in <code>mindocr\\data\\transforms\\det_transforms.py</code> <pre><code>class ValidatePolygons:\n\"\"\"\n    Validate polygons by:\n     1. filtering out polygons outside an image.\n     2. clipping coordinates of polygons that are partially outside an image to stay within the visible region.\n    Args:\n        min_area: minimum area below which newly clipped polygons considered as ignored.\n        clip_to_visible_area: (Experimental) clip polygons to a visible area. Number of vertices in a polygon after\n                              clipping may change.\n        min_vertices: minimum number of vertices in a polygon below which newly clipped polygons considered as ignored.\n    \"\"\"\n\n    def __init__(self, min_area: float = 1.0, clip_to_visible_area: bool = False, min_vertices: int = 4, **kwargs):\n        self._min_area = min_area\n        self._min_vert = min_vertices\n        self._clip = clip_to_visible_area\n\n    def __call__(self, data: dict):\n        size = data.get(\"actual_size\", np.array(data[\"image\"].shape[:2]))[::-1]  # convert to x, y coord\n        border = box(0, 0, *size)\n\n        new_polys, new_texts, new_tags = [], [], []\n        for np_poly, text, ignore in zip(data[\"polys\"], data[\"texts\"], data[\"ignore_tags\"]):\n            poly = Polygon(np_poly)\n            if poly.intersects(border):  # if the polygon is fully or partially within the image\n                poly = poly.intersection(border)\n                if poly.area &lt; self._min_area:\n                    ignore = True\n\n                if self._clip:  # Clip polygon to a visible area\n                    poly = poly.exterior.coords\n                    np_poly = np.array(poly[:-1])\n                    if len(np_poly) &lt; self._min_vert:\n                        ignore = True\n\n                new_polys.append(np_poly)\n                new_tags.append(ignore)\n                new_texts.append(text)\n\n        data[\"polys\"] = new_polys\n        data[\"texts\"] = new_texts\n        data[\"ignore_tags\"] = np.array(new_tags)\n\n        return data\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms.general_transforms","title":"<code>mindocr.data.transforms.general_transforms</code>","text":"<code>mindocr.data.transforms.general_transforms.DecodeImage</code> \u00b6 <p>img_mode (str): The channel order of the output, 'BGR' and 'RGB'. Default to 'BGR'. channel_first (bool): if True, image shpae is CHW. If False, HWC. Default to False</p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class DecodeImage:\n\"\"\"\n    img_mode (str): The channel order of the output, 'BGR' and 'RGB'. Default to 'BGR'.\n    channel_first (bool): if True, image shpae is CHW. If False, HWC. Default to False\n    \"\"\"\n\n    def __init__(\n        self, img_mode=\"BGR\", channel_first=False, to_float32=False, ignore_orientation=False, keep_ori=False, **kwargs\n    ):\n        self.img_mode = img_mode\n        self.to_float32 = to_float32\n        self.channel_first = channel_first\n        self.flag = cv2.IMREAD_IGNORE_ORIENTATION | cv2.IMREAD_COLOR if ignore_orientation else cv2.IMREAD_COLOR\n        self.keep_ori = keep_ori\n\n    def __call__(self, data):\n        if \"img_path\" in data:\n            with open(data[\"img_path\"], \"rb\") as f:\n                img = f.read()\n        elif \"img_lmdb\" in data:\n            img = data[\"img_lmdb\"]\n        img = np.frombuffer(img, dtype=\"uint8\")\n        img = cv2.imdecode(img, self.flag)\n\n        if self.img_mode == \"RGB\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if self.channel_first:\n            img = img.transpose((2, 0, 1))\n\n        if self.to_float32:\n            img = img.astype(\"float32\")\n        data[\"image\"] = img\n        # data['ori_image'] = img.copy()\n        data[\"raw_img_shape\"] = img.shape[:2]\n\n        if self.keep_ori:\n            data[\"image_ori\"] = img.copy()\n\n        return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.NormalizeImage</code> \u00b6 <p>normalize image, subtract mean, divide std input image: by default, np.uint8, [0, 255], HWC format. return image: float32 numpy array</p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class NormalizeImage:\n\"\"\"\n    normalize image, subtract mean, divide std\n    input image: by default, np.uint8, [0, 255], HWC format.\n    return image: float32 numpy array\n    \"\"\"\n\n    def __init__(\n        self,\n        mean: Union[List[float], str] = \"imagenet\",\n        std: Union[List[float], str] = \"imagenet\",\n        is_hwc=True,\n        bgr_to_rgb=False,\n        rgb_to_bgr=False,\n        **kwargs,\n    ):\n        # By default, imagnet MEAN and STD is in RGB order. inverse if input image is in BGR mode\n        self._channel_conversion = False\n        if bgr_to_rgb or rgb_to_bgr:\n            self._channel_conversion = True\n\n        # TODO: detect hwc or chw automatically\n        shape = (3, 1, 1) if not is_hwc else (1, 1, 3)\n        self.mean = np.array(self._get_value(mean, \"mean\")).reshape(shape).astype(\"float32\")\n        self.std = np.array(self._get_value(std, \"std\")).reshape(shape).astype(\"float32\")\n        self.is_hwc = is_hwc\n\n    def __call__(self, data):\n        img = data[\"image\"]\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        assert isinstance(img, np.ndarray), \"invalid input 'img' in NormalizeImage\"\n\n        if self._channel_conversion:\n            if self.is_hwc:\n                img = img[..., [2, 1, 0]]\n            else:\n                img = img[[2, 1, 0], ...]\n\n        data[\"image\"] = (img.astype(\"float32\") - self.mean) / self.std\n        return data\n\n    @staticmethod\n    def _get_value(val, name):\n        if isinstance(val, str) and val.lower() == \"imagenet\":\n            assert name in [\"mean\", \"std\"]\n            return IMAGENET_DEFAULT_MEAN if name == \"mean\" else IMAGENET_DEFAULT_STD\n        elif isinstance(val, list):\n            return val\n        else:\n            raise ValueError(f\"Wrong {name} value: {val}\")\n</code></pre> <code>mindocr.data.transforms.general_transforms.PackLoaderInputs</code> \u00b6 PARAMETER DESCRIPTION <code>output_columns</code> <p>the keys in data dict that are expected to output for dataloader</p> <p> TYPE: <code>list</code> </p> Call Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class PackLoaderInputs:\n\"\"\"\n    Args:\n        output_columns (list): the keys in data dict that are expected to output for dataloader\n\n    Call:\n        input: data dict\n        output: data tuple corresponding to the `output_columns`\n    \"\"\"\n\n    def __init__(self, output_columns: List, **kwargs):\n        self.output_columns = output_columns\n\n    def __call__(self, data):\n        out = []\n        for k in self.output_columns:\n            assert k in data, f\"key {k} does not exists in data, availabe keys are {data.keys()}\"\n            out.append(data[k])\n\n        return tuple(out)\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomColorAdjust</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class RandomColorAdjust:\n    def __init__(self, brightness=32.0 / 255, saturation=0.5, **kwargs):\n        contrast = kwargs.get(\"contrast\", (1, 1))\n        hue = kwargs.get(\"hue\", (0, 0))\n        self._jitter = MSRandomColorAdjust(brightness=brightness, saturation=saturation, contrast=contrast, hue=hue)\n        self._pil = ToPIL()\n\n    def __call__(self, data):\n\"\"\"\n        required keys: image\n        modified keys: image\n        \"\"\"\n        # there's a bug in MindSpore that requires images to be converted to the PIL format first\n        data[\"image\"] = np.array(self._jitter(self._pil(data[\"image\"])))\n        return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomColorAdjust.__call__(data)</code> \u00b6 <p>required keys: image modified keys: image</p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    required keys: image\n    modified keys: image\n    \"\"\"\n    # there's a bug in MindSpore that requires images to be converted to the PIL format first\n    data[\"image\"] = np.array(self._jitter(self._pil(data[\"image\"])))\n    return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomHorizontalFlip</code> \u00b6 <p>Random horizontal flip of an image with polygons in it (if any).</p> PARAMETER DESCRIPTION <code>p</code> <p>probability of the augmentation being applied to an image.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class RandomHorizontalFlip:\n\"\"\"\n    Random horizontal flip of an image with polygons in it (if any).\n    Args:\n        p: probability of the augmentation being applied to an image.\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, **kwargs):\n        self._p = p\n\n    def __call__(self, data: dict) -&gt; dict:\n        if random.random() &lt; self._p:\n            data[\"image\"] = cv2.flip(data[\"image\"], 1)\n\n            if \"polys\" in data:\n                mat = np.float32([[-1, 0, data[\"image\"].shape[1] - 1], [0, 1, 0]])\n                data[\"polys\"] = cv2.transform(data[\"polys\"], mat)\n                # TODO: assign a new starting point located in the top left\n                data[\"polys\"] = data[\"polys\"][:, ::-1, :]  # preserve the original order (e.g. clockwise)\n\n        return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomRotate</code> \u00b6 <p>Randomly rotate an image with polygons in it (if any).</p> PARAMETER DESCRIPTION <code>degrees</code> <p>range of angles [min, max]</p> <p> DEFAULT: <code>(-10, 10)</code> </p> <code>expand_canvas</code> <p>whether to expand canvas during rotation (the image size will be increased) or            maintain the original size (the rotated image will be cropped back to the original size).</p> <p> DEFAULT: <code>True</code> </p> <code>p</code> <p>probability of the augmentation being applied to an image.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class RandomRotate:\n\"\"\"\n    Randomly rotate an image with polygons in it (if any).\n    Args:\n        degrees: range of angles [min, max]\n        expand_canvas: whether to expand canvas during rotation (the image size will be increased) or\n                       maintain the original size (the rotated image will be cropped back to the original size).\n        p: probability of the augmentation being applied to an image.\n    \"\"\"\n\n    def __init__(self, degrees=(-10, 10), expand_canvas=True, p: float = 1.0, **kwargs):\n        self._degrees = degrees\n        self._canvas = expand_canvas\n        self._p = p\n\n    def __call__(self, data: dict) -&gt; dict:\n        if random.random() &lt; self._p:\n            angle = random.randint(self._degrees[0], self._degrees[1])\n            h, w = data[\"image\"].shape[:2]\n\n            center = w // 2, h // 2  # x, y\n            mat = cv2.getRotationMatrix2D(center, angle, 1)\n\n            if self._canvas:\n                # compute the new bounding dimensions of the image\n                cos, sin = np.abs(mat[0, 0]), np.abs(mat[0, 1])\n                w, h = int((h * sin) + (w * cos)), int((h * cos) + (w * sin))\n\n                # adjust the rotation matrix to take into account translation\n                mat[0, 2] += (w / 2) - center[0]\n                mat[1, 2] += (h / 2) - center[1]\n\n            data[\"image\"] = cv2.warpAffine(data[\"image\"], mat, (w, h))\n\n            if \"polys\" in data:\n                data[\"polys\"] = cv2.transform(data[\"polys\"], mat)\n\n        return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomScale</code> \u00b6 <p>Randomly scales an image and its polygons in a predefined scale range.</p> PARAMETER DESCRIPTION <code>scale_range</code> <p>(min, max) scale range.</p> <p> TYPE: <code>Union[tuple, list]</code> </p> <code>p</code> <p>probability of the augmentation being applied to an image.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>class RandomScale:\n\"\"\"\n    Randomly scales an image and its polygons in a predefined scale range.\n    Args:\n        scale_range: (min, max) scale range.\n        p: probability of the augmentation being applied to an image.\n    \"\"\"\n\n    def __init__(self, scale_range: Union[tuple, list], p: float = 0.5, **kwargs):\n        self._range = scale_range\n        self._p = p\n        assert kwargs.get(\"is_train\", True), ValueError(\"RandomScale augmentation must be used for training only\")\n\n    def __call__(self, data: dict):\n\"\"\"\n        required keys:\n            image, HWC\n            (polys)\n        modified keys:\n            image\n            (polys)\n        \"\"\"\n        if random.random() &lt; self._p:\n            scale = np.random.uniform(*self._range)\n            data[\"image\"] = cv2.resize(data[\"image\"], dsize=None, fx=scale, fy=scale)\n            if \"polys\" in data:\n                data[\"polys\"] *= scale\n\n        return data\n</code></pre> <code>mindocr.data.transforms.general_transforms.RandomScale.__call__(data)</code> \u00b6 required keys <p>image, HWC (polys)</p> modified keys <p>image (polys)</p> Source code in <code>mindocr\\data\\transforms\\general_transforms.py</code> <pre><code>def __call__(self, data: dict):\n\"\"\"\n    required keys:\n        image, HWC\n        (polys)\n    modified keys:\n        image\n        (polys)\n    \"\"\"\n    if random.random() &lt; self._p:\n        scale = np.random.uniform(*self._range)\n        data[\"image\"] = cv2.resize(data[\"image\"], dsize=None, fx=scale, fy=scale)\n        if \"polys\" in data:\n            data[\"polys\"] *= scale\n\n    return data\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms.rec_transforms","title":"<code>mindocr.data.transforms.rec_transforms</code>","text":"<p>transform for text recognition tasks.</p> <code>mindocr.data.transforms.rec_transforms.RecAttnLabelEncode</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>class RecAttnLabelEncode:\n    def __init__(\n        self,\n        max_text_len: int = 25,\n        character_dict_path: Optional[str] = None,\n        use_space_char: bool = False,\n        lower: bool = False,\n        **kwargs,\n    ) -&gt; None:\n\"\"\"\n        Convert text label (str) to a sequence of character indices according to the char dictionary\n\n        Args:\n            max_text_len: to pad the label text to a fixed length (max_text_len) of text for attn loss computate.\n            character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n                (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n            use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n            lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n                Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to\n                recognition both upper-case and lower-case.\n\n        Attributes:\n            go_idx: the index of the GO token\n            stop_idx: the index of the STOP token\n            num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n            num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n            so num_classes = num_valid_chars + 1\n        \"\"\"\n        self.max_text_len = max_text_len\n        self.lower = lower\n\n        # read dict\n        if character_dict_path is None:\n            char_list = list(\"0123456789abcdefghijklmnopqrstuvwxyz\")\n\n            self.lower = True\n            print(\"INFO: The character_dict_path is None, model can only recognize number and lower letters\")\n        else:\n            # parse char dictionary\n            char_list = []\n            with open(character_dict_path, \"r\") as f:\n                for line in f:\n                    c = line.rstrip(\"\\n\\r\")\n                    char_list.append(c)\n\n        # add space char if set\n        if use_space_char:\n            if \" \" not in char_list:\n                char_list.append(\" \")\n            self.space_idx = len(char_list) + 1\n        else:\n            if \" \" in char_list:\n                print(\n                    \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                    \"because the space char is coded in the dictionary file \",\n                    character_dict_path,\n                )\n\n        self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n        special_token = [\"&lt;GO&gt;\", \"&lt;STOP&gt;\"]\n        char_list = special_token + char_list\n\n        self.go_idx = 0\n        self.stop_idx = 1\n\n        self.dict = {c: idx for idx, c in enumerate(char_list)}\n\n        self.num_classes = len(self.dict)\n\n    def __call__(self, data: Dict[str, Any]) -&gt; str:\n        char_indices = str2idx(data[\"label\"], self.dict, max_text_len=self.max_text_len, lower=self.lower)\n\n        if char_indices is None:\n            char_indices = []\n        data[\"length\"] = np.array(len(char_indices), dtype=np.int32)\n\n        char_indices = (\n            [self.go_idx] + char_indices + [self.stop_idx] + [self.go_idx] * (self.max_text_len - len(char_indices))\n        )\n        data[\"text_seq\"] = np.array(char_indices, dtype=np.int32)\n\n        data[\"text_length\"] = len(data[\"label\"])\n        data[\"text_padded\"] = data[\"label\"] + \" \" * (self.max_text_len - len(data[\"label\"]))\n        return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecAttnLabelEncode.__init__(max_text_len=25, character_dict_path=None, use_space_char=False, lower=False, **kwargs)</code> \u00b6 <p>Convert text label (str) to a sequence of character indices according to the char dictionary</p> PARAMETER DESCRIPTION <code>max_text_len</code> <p>to pad the label text to a fixed length (max_text_len) of text for attn loss computate.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>character_dict_path</code> <p>path to dictionary, if None, a dictionary containing 36 chars (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>use_space_char(bool)</code> <p>if True, add space char to the dict to recognize the space in between two words</p> <p> </p> <code>lower</code> <p>if True, all upper-case chars in the label text will be converted to lower case. Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to recognition both upper-case and lower-case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> ATTRIBUTE DESCRIPTION <code>go_idx</code> <p>the index of the GO token</p> <p> </p> <code>stop_idx</code> <p>the index of the STOP token</p> <p> </p> <code>num_valid_chars</code> <p>the number of valid characters (including space char if used) in the dictionary</p> <p> </p> <code>num_classes</code> <p>the number of classes (which valid characters char and the speical token for blank padding).</p> <p> </p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def __init__(\n    self,\n    max_text_len: int = 25,\n    character_dict_path: Optional[str] = None,\n    use_space_char: bool = False,\n    lower: bool = False,\n    **kwargs,\n) -&gt; None:\n\"\"\"\n    Convert text label (str) to a sequence of character indices according to the char dictionary\n\n    Args:\n        max_text_len: to pad the label text to a fixed length (max_text_len) of text for attn loss computate.\n        character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n            (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n        use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n        lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n            Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to\n            recognition both upper-case and lower-case.\n\n    Attributes:\n        go_idx: the index of the GO token\n        stop_idx: the index of the STOP token\n        num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n        num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n        so num_classes = num_valid_chars + 1\n    \"\"\"\n    self.max_text_len = max_text_len\n    self.lower = lower\n\n    # read dict\n    if character_dict_path is None:\n        char_list = list(\"0123456789abcdefghijklmnopqrstuvwxyz\")\n\n        self.lower = True\n        print(\"INFO: The character_dict_path is None, model can only recognize number and lower letters\")\n    else:\n        # parse char dictionary\n        char_list = []\n        with open(character_dict_path, \"r\") as f:\n            for line in f:\n                c = line.rstrip(\"\\n\\r\")\n                char_list.append(c)\n\n    # add space char if set\n    if use_space_char:\n        if \" \" not in char_list:\n            char_list.append(\" \")\n        self.space_idx = len(char_list) + 1\n    else:\n        if \" \" in char_list:\n            print(\n                \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                \"because the space char is coded in the dictionary file \",\n                character_dict_path,\n            )\n\n    self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n    special_token = [\"&lt;GO&gt;\", \"&lt;STOP&gt;\"]\n    char_list = special_token + char_list\n\n    self.go_idx = 0\n    self.stop_idx = 1\n\n    self.dict = {c: idx for idx, c in enumerate(char_list)}\n\n    self.num_classes = len(self.dict)\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecCTCLabelEncode</code> \u00b6 <p>         Bases: <code>object</code></p> <p>Convert text label (str) to a sequence of character indices according to the char dictionary</p> PARAMETER DESCRIPTION <code>max_text_len</code> <p>to pad the label text to a fixed length (max_text_len) of text for ctc loss computate.</p> <p> DEFAULT: <code>23</code> </p> <code>character_dict_path</code> <p>path to dictionary, if None, a dictionary containing 36 chars (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.</p> <p> DEFAULT: <code>None</code> </p> <code>use_space_char(bool)</code> <p>if True, add space char to the dict to recognize the space in between two words</p> <p> </p> <code>blank_at_last(bool)</code> <p>padding with blank index (not the space index). If True, a blank/padding token will be appended to the end of the dictionary, so that blank_index = num_chars, where num_chars is the number of character in the dictionary including space char if used. If False, blank token will be inserted in the beginning of the dictionary, so blank_index=0.</p> <p> </p> <code>lower</code> <p>if True, all upper-case chars in the label text will be converted to lower case. Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to recognition both upper-case and lower-case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> ATTRIBUTE DESCRIPTION <code>blank_idx</code> <p>the index of the blank token for padding</p> <p> </p> <code>num_valid_chars</code> <p>the number of valid characters (including space char if used) in the dictionary</p> <p> </p> <code>num_classes</code> <p>the number of classes (which valid characters char and the speical token for blank padding). so num_classes = num_valid_chars + 1</p> <p> </p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>class RecCTCLabelEncode(object):\n\"\"\"Convert text label (str) to a sequence of character indices according to the char dictionary\n\n    Args:\n        max_text_len: to pad the label text to a fixed length (max_text_len) of text for ctc loss computate.\n        character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n            (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n        use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n        blank_at_last(bool): padding with blank index (not the space index). If True, a blank/padding token will be\n            appended to the end of the dictionary, so that blank_index = num_chars, where num_chars is the number of\n            character in the dictionary including space char if used. If False, blank token will be inserted in the\n            beginning of the dictionary, so blank_index=0.\n        lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n            Set to be True if dictionary only contains lower-case chars.\n            Set to be False if not and want to recognition both upper-case and lower-case.\n\n    Attributes:\n        blank_idx: the index of the blank token for padding\n        num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n        num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n            so num_classes = num_valid_chars + 1\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_text_len=23,\n        character_dict_path=None,\n        use_space_char=False,\n        blank_at_last=True,\n        lower=False,\n        **kwargs,\n        # start_token='&lt;BOS&gt;',\n        # end_token='&lt;EOS&gt;',\n        # unkown_token='',\n    ):\n        self.max_text_len = max_text_len\n        self.space_idx = None\n        self.lower = lower\n\n        # read dict\n        if character_dict_path is None:\n            char_list = [c for c in \"0123456789abcdefghijklmnopqrstuvwxyz\"]\n\n            self.lower = True\n            # print(\"INFO: The character_dict_path is None, model can only recognize number and lower letters\")\n        else:\n            # TODO: this is commonly used in other modules, wrap into a func or class.\n            # parse char dictionary\n            char_list = []\n            with open(character_dict_path, \"r\") as f:\n                for line in f:\n                    c = line.rstrip(\"\\n\\r\")\n                    char_list.append(c)\n        # add space char if set\n        if use_space_char:\n            if \" \" not in char_list:\n                char_list.append(\" \")\n            self.space_idx = len(char_list) - 1\n        else:\n            if \" \" in char_list:\n                print(\n                    \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                    \"because the space char is coded in the dictionary file \",\n                    character_dict_path,\n                )\n\n        self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n        # add blank token for padding\n        if blank_at_last:\n            # the index of a char in dict is [0, num_chars-1], blank index is set to num_chars\n            char_list.append(\"&lt;PAD&gt;\")\n            self.blank_idx = self.num_valid_chars\n        else:\n            char_list = [\"&lt;PAD&gt;\"] + char_list\n            self.blank_idx = 0\n\n        self.dict = {c: idx for idx, c in enumerate(char_list)}\n\n        self.num_classes = len(self.dict)\n\n    def __call__(self, data: dict):\n\"\"\"\n        required keys:\n            label -&gt; (str) text string\n        added keys:\n            text_seq-&gt; (np.ndarray, int32), sequence of character indices after  padding to max_text_len in shape\n            (sequence_len), where ood characters are skipped\n        added keys:\n            length -&gt; (np.int32) the number of valid chars in the encoded char index sequence,  where valid means\n            the char is in dictionary.\n            text_padded -&gt; (str) text label padded to fixed length, to solved the dynamic shape issue in dataloader.\n            text_length -&gt; int, the length of original text string label\n\n        \"\"\"\n        char_indices = str2idx(data[\"label\"], self.dict, max_text_len=self.max_text_len, lower=self.lower)\n\n        if char_indices is None:\n            char_indices = []\n            # return None\n        data[\"length\"] = np.array(len(char_indices), dtype=np.int32)\n        # padding with blank index\n        char_indices = char_indices + [self.blank_idx] * (self.max_text_len - len(char_indices))\n        # TODO: raname to char_indices\n        data[\"text_seq\"] = np.array(char_indices, dtype=np.int32)\n        #\n        data[\"text_length\"] = len(data[\"label\"])\n        data[\"text_padded\"] = data[\"label\"] + \" \" * (self.max_text_len - len(data[\"label\"]))\n\n        return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecCTCLabelEncode.__call__(data)</code> \u00b6 required keys <p>label -&gt; (str) text string</p> added keys <p>text_seq-&gt; (np.ndarray, int32), sequence of character indices after  padding to max_text_len in shape (sequence_len), where ood characters are skipped</p> added keys <p>length -&gt; (np.int32) the number of valid chars in the encoded char index sequence,  where valid means the char is in dictionary. text_padded -&gt; (str) text label padded to fixed length, to solved the dynamic shape issue in dataloader. text_length -&gt; int, the length of original text string label</p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def __call__(self, data: dict):\n\"\"\"\n    required keys:\n        label -&gt; (str) text string\n    added keys:\n        text_seq-&gt; (np.ndarray, int32), sequence of character indices after  padding to max_text_len in shape\n        (sequence_len), where ood characters are skipped\n    added keys:\n        length -&gt; (np.int32) the number of valid chars in the encoded char index sequence,  where valid means\n        the char is in dictionary.\n        text_padded -&gt; (str) text label padded to fixed length, to solved the dynamic shape issue in dataloader.\n        text_length -&gt; int, the length of original text string label\n\n    \"\"\"\n    char_indices = str2idx(data[\"label\"], self.dict, max_text_len=self.max_text_len, lower=self.lower)\n\n    if char_indices is None:\n        char_indices = []\n        # return None\n    data[\"length\"] = np.array(len(char_indices), dtype=np.int32)\n    # padding with blank index\n    char_indices = char_indices + [self.blank_idx] * (self.max_text_len - len(char_indices))\n    # TODO: raname to char_indices\n    data[\"text_seq\"] = np.array(char_indices, dtype=np.int32)\n    #\n    data[\"text_length\"] = len(data[\"label\"])\n    data[\"text_padded\"] = data[\"label\"] + \" \" * (self.max_text_len - len(data[\"label\"]))\n\n    return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecResizeImg</code> \u00b6 <p>         Bases: <code>object</code></p> <p>adopted from paddle resize, convert from hwc to chw, rescale pixel value to -1 to 1</p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>class RecResizeImg(object):\n\"\"\"adopted from paddle\n    resize, convert from hwc to chw, rescale pixel value to -1 to 1\n    \"\"\"\n\n    def __init__(self, image_shape, infer_mode=False, character_dict_path=None, padding=True, **kwargs):\n        self.image_shape = image_shape\n        self.infer_mode = infer_mode\n        self.character_dict_path = character_dict_path\n        self.padding = padding\n\n    def __call__(self, data):\n        img = data[\"image\"]\n        if self.infer_mode and self.character_dict_path is not None:\n            norm_img, valid_ratio = resize_norm_img_chinese(img, self.image_shape)\n        else:\n            norm_img, valid_ratio = resize_norm_img(img, self.image_shape, self.padding)\n        data[\"image\"] = norm_img\n        data[\"valid_ratio\"] = valid_ratio\n        # TODO: data['shape_list'] = ?\n        return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecResizeNormForInfer</code> \u00b6 <p>         Bases: <code>object</code></p> <p>Resize image for text recognition</p> PARAMETER DESCRIPTION <code>target_height</code> <p>target height after resize. Commonly, 32 for crnn, 48 for svtr. default is 32.</p> <p> DEFAULT: <code>32</code> </p> <code>target_width</code> <p>target width. Default is 320. If None, image width is scaled to make aspect ratio unchanged.</p> <p> DEFAULT: <code>320</code> </p> <code>keep_ratio</code> <p>keep aspect ratio. If True, resize the image with ratio=target_height / input_height (certain image height is required by recognition network). If False, simply resize to targte size (<code>target_height</code>, <code>target_width</code>)</p> <p> DEFAULT: <code>True</code> </p> <code>padding</code> <p>If True, pad the resized image to the targte size with zero RGB values. only used when <code>keep_ratio</code> is True.</p> <p> DEFAULT: <code>False</code> </p> Notes <ol> <li>The default choice (keep_ratio, not padding) is suitable for inference for better accuracy.</li> </ol> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>class RecResizeNormForInfer(object):\n\"\"\"\n    Resize image for text recognition\n\n    Args:\n        target_height: target height after resize. Commonly, 32 for crnn, 48 for svtr. default is 32.\n        target_width: target width. Default is 320. If None, image width is scaled to make aspect ratio unchanged.\n        keep_ratio: keep aspect ratio.\n            If True, resize the image with ratio=target_height / input_height (certain image height is required by\n            recognition network).\n            If False, simply resize to targte size (`target_height`, `target_width`)\n        padding: If True, pad the resized image to the targte size with zero RGB values.\n            only used when `keep_ratio` is True.\n\n    Notes:\n        1. The default choice (keep_ratio, not padding) is suitable for inference for better accuracy.\n    \"\"\"\n\n    def __init__(\n        self,\n        target_height=32,\n        target_width=320,\n        keep_ratio=True,\n        padding=False,\n        interpolation=cv2.INTER_LINEAR,\n        norm_before_pad=False,\n        mean=[127.0, 127.0, 127.0],\n        std=[127.0, 127.0, 127.0],\n        **kwargs,\n    ):\n        self.keep_ratio = keep_ratio\n        self.padding = padding\n        # self.targt_shape = target_shape\n        self.tar_h = target_height\n        self.tar_w = target_width\n        self.interpolation = interpolation\n        self.norm_before_pad = norm_before_pad\n        self.mean = np.array(mean, dtype=\"float32\")\n        self.std = np.array(std, dtype=\"float32\")\n\n    def norm(self, img):\n        return (img - self.mean) / self.std\n\n    def __call__(self, data):\n\"\"\"\n        data: image in shape [h, w, c]\n        \"\"\"\n        img = data[\"image\"]\n        h, w = img.shape[:2]\n        # tar_h, tar_w = self.targt_shape\n        resize_h = self.tar_h\n\n        max_wh_ratio = self.tar_w / float(self.tar_h)\n\n        if not self.keep_ratio:\n            assert self.tar_w is not None, \"Must specify target_width if keep_ratio is False\"\n            resize_w = self.tar_w  # if self.tar_w is not None else resized_h * self.max_wh_ratio\n        else:\n            src_wh_ratio = w / float(h)\n            resize_w = int(min(src_wh_ratio, max_wh_ratio) * resize_h)\n        # print('Rec resize: ', h, w, \"-&gt;\", resize_h, resize_w)\n\n        resized_img = cv2.resize(img, (resize_w, resize_h), interpolation=self.interpolation)\n\n        # TODO: norm before padding\n\n        data[\"shape_list\"] = np.array(\n            [h, w, resize_h / h, resize_w / w], dtype=np.float32\n        )  # TODO: reformat, currently align to det\n        if self.norm_before_pad:\n            resized_img = self.norm(resized_img)\n\n        if self.padding and self.keep_ratio:\n            padded_img = np.zeros((self.tar_h, self.tar_w, 3), dtype=resized_img.dtype)\n            padded_img[:, :resize_w, :] = resized_img\n            data[\"image\"] = padded_img\n        else:\n            data[\"image\"] = resized_img\n\n        if not self.norm_before_pad:\n            data[\"image\"] = self.norm(data[\"image\"])\n\n        return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.RecResizeNormForInfer.__call__(data)</code> \u00b6 Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    data: image in shape [h, w, c]\n    \"\"\"\n    img = data[\"image\"]\n    h, w = img.shape[:2]\n    # tar_h, tar_w = self.targt_shape\n    resize_h = self.tar_h\n\n    max_wh_ratio = self.tar_w / float(self.tar_h)\n\n    if not self.keep_ratio:\n        assert self.tar_w is not None, \"Must specify target_width if keep_ratio is False\"\n        resize_w = self.tar_w  # if self.tar_w is not None else resized_h * self.max_wh_ratio\n    else:\n        src_wh_ratio = w / float(h)\n        resize_w = int(min(src_wh_ratio, max_wh_ratio) * resize_h)\n    # print('Rec resize: ', h, w, \"-&gt;\", resize_h, resize_w)\n\n    resized_img = cv2.resize(img, (resize_w, resize_h), interpolation=self.interpolation)\n\n    # TODO: norm before padding\n\n    data[\"shape_list\"] = np.array(\n        [h, w, resize_h / h, resize_w / w], dtype=np.float32\n    )  # TODO: reformat, currently align to det\n    if self.norm_before_pad:\n        resized_img = self.norm(resized_img)\n\n    if self.padding and self.keep_ratio:\n        padded_img = np.zeros((self.tar_h, self.tar_w, 3), dtype=resized_img.dtype)\n        padded_img[:, :resize_w, :] = resized_img\n        data[\"image\"] = padded_img\n    else:\n        data[\"image\"] = resized_img\n\n    if not self.norm_before_pad:\n        data[\"image\"] = self.norm(data[\"image\"])\n\n    return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.Rotate90IfVertical</code> \u00b6 <p>Rotate the image by 90 degree when the height/width ratio is larger than the given threshold. Note: It needs to be called before image resize.</p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>class Rotate90IfVertical:\n\"\"\"Rotate the image by 90 degree when the height/width ratio is larger than the given threshold.\n    Note: It needs to be called before image resize.\"\"\"\n\n    def __init__(self, threshold: float = 1.5, direction: str = \"counterclockwise\", **kwargs):\n        self.threshold = threshold\n\n        if direction == \"counterclockwise\":\n            self.flag = cv2.ROTATE_90_COUNTERCLOCKWISE\n        elif direction == \"clockwise\":\n            self.flag = cv2.ROTATE_90_CLOCKWISE\n        else:\n            raise ValueError(\"Unsupported direction\")\n\n    def __call__(self, data):\n        img = data[\"image\"]\n\n        h, w, _ = img.shape\n        if h / w &gt; self.threshold:\n            img = cv2.rotate(img, self.flag)\n\n        data[\"image\"] = img\n        return data\n</code></pre> <code>mindocr.data.transforms.rec_transforms.resize_norm_img(img, image_shape, padding=True, interpolation=cv2.INTER_LINEAR)</code> \u00b6 <p>resize image</p> PARAMETER DESCRIPTION <code>img</code> <p>shape (H, W, C)</p> <p> </p> <code>image_shape</code> <p>image shape after resize, in (C, H, W)</p> <p> </p> <code>padding</code> <p>if Ture, resize while preserving the H/W ratio, then pad the blank.</p> <p> DEFAULT: <code>True</code> </p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def resize_norm_img(img, image_shape, padding=True, interpolation=cv2.INTER_LINEAR):\n\"\"\"\n    resize image\n    Args:\n        img: shape (H, W, C)\n        image_shape: image shape after resize, in (C, H, W)\n        padding: if Ture, resize while preserving the H/W ratio, then pad the blank.\n\n    \"\"\"\n    imgH, imgW = image_shape\n    h = img.shape[0]\n    w = img.shape[1]\n    c = img.shape[2]\n    if not padding:\n        resized_image = cv2.resize(img, (imgW, imgH), interpolation=interpolation)\n        resized_w = imgW\n    else:\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) &gt; imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        resized_image = cv2.resize(img, (resized_w, imgH))\n\"\"\"\n    resized_image = resized_image.astype('float32')\n    if image_shape[0] == 1:\n        resized_image = resized_image / 255\n        resized_image = resized_image[np.newaxis, :]\n    else:\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n    resized_image -= 0.5\n    resized_image /= 0.5\n    \"\"\"\n    padding_im = np.zeros((imgH, imgW, c), dtype=np.uint8)\n    padding_im[:, 0:resized_w, :] = resized_image\n    valid_ratio = min(1.0, float(resized_w / imgW))\n    return padding_im, valid_ratio\n</code></pre> <code>mindocr.data.transforms.rec_transforms.resize_norm_img_chinese(img, image_shape)</code> \u00b6 <p>adopted from paddle</p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def resize_norm_img_chinese(img, image_shape):\n\"\"\"adopted from paddle\"\"\"\n    imgH, imgW = image_shape\n    # todo: change to 0 and modified image shape\n    max_wh_ratio = imgW * 1.0 / imgH\n    h, w = img.shape[0], img.shape[1]\n    c = img.shape[2]\n    ratio = w * 1.0 / h\n    max_wh_ratio = min(max(max_wh_ratio, ratio), max_wh_ratio)\n    imgW = int(imgH * max_wh_ratio)\n    if math.ceil(imgH * ratio) &gt; imgW:\n        resized_w = imgW\n    else:\n        resized_w = int(math.ceil(imgH * ratio))\n    resized_image = cv2.resize(img, (resized_w, imgH))\n\"\"\"\n    resized_image = resized_image.astype('float32')\n    if image_shape[0] == 1:\n        resized_image = resized_image / 255\n        resized_image = resized_image[np.newaxis, :]\n    else:\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n    resized_image -= 0.5\n    resized_image /= 0.5\n    \"\"\"\n    # padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n    padding_im = np.zeros((imgH, imgW, c), dtype=np.uint8)\n    # padding_im[:, :, 0:resized_w] = resized_image\n    padding_im[:, 0:resized_w, :] = resized_image\n    valid_ratio = min(1.0, float(resized_w / imgW))\n    return padding_im, valid_ratio\n</code></pre> <code>mindocr.data.transforms.rec_transforms.str2idx(text, label_dict, max_text_len=23, lower=False)</code> \u00b6 <p>Encode text (string) to a squence of char indices</p> PARAMETER DESCRIPTION <code>text</code> <p>text string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>char_indices</code> <p>char index seq</p> <p> TYPE: <code>List[int]</code> </p> Source code in <code>mindocr\\data\\transforms\\rec_transforms.py</code> <pre><code>def str2idx(text: str, label_dict: Dict[str, int], max_text_len: int = 23, lower: bool = False) -&gt; List[int]:\n\"\"\"\n    Encode text (string) to a squence of char indices\n    Args:\n        text (str): text string\n    Returns:\n        char_indices (List[int]): char index seq\n    \"\"\"\n    if len(text) == 0 or len(text) &gt; max_text_len:\n        return None\n    if lower:\n        text = text.lower()\n\n    char_indices = []\n    # TODO: for char not in the dictionary, skipping may lead to None data. Use a char replacement? refer to mmocr\n    for char in text:\n        if char not in label_dict:\n            # print('WARNING: {} is not in dict'.format(char))\n            continue\n        char_indices.append(label_dict[char])\n    if len(char_indices) == 0:\n        print(\"WARNING: {} doesnot contain any valid char in the dict\".format(text))\n        return None\n\n    return char_indices\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms.svtr_transform","title":"<code>mindocr.data.transforms.svtr_transform</code>","text":"<code>mindocr.data.transforms.svtr_transform.CVRescale</code> \u00b6 <p>         Bases: <code>object</code></p> Source code in <code>mindocr\\data\\transforms\\svtr_transform.py</code> <pre><code>class CVRescale(object):\n    def __init__(self, factor=4, base_size=(128, 512)):\n\"\"\"Define image scales using gaussian pyramid and rescale image to target scale.\n\n        Args:\n            factor: the decayed factor from base size, factor=4 keeps target scale by default.\n            base_size: base size the build the bottom layer of pyramid\n        \"\"\"\n        if isinstance(factor, numbers.Number):\n            self.factor = round(sample_uniform(0, factor))\n        elif isinstance(factor, (tuple, list)) and len(factor) == 2:\n            self.factor = round(sample_uniform(factor[0], factor[1]))\n        else:\n            raise Exception(\"factor must be number or list with length 2\")\n        # assert factor is valid\n        self.base_h, self.base_w = base_size[:2]\n\n    def __call__(self, img):\n        if self.factor == 0:\n            return img\n        src_h, src_w = img.shape[:2]\n        cur_w, cur_h = self.base_w, self.base_h\n        scale_img = cv2.resize(img, (cur_w, cur_h), interpolation=get_interpolation())\n        for _ in range(self.factor):\n            scale_img = cv2.pyrDown(scale_img)\n        scale_img = cv2.resize(scale_img, (src_w, src_h), interpolation=get_interpolation())\n        return scale_img\n</code></pre> <code>mindocr.data.transforms.svtr_transform.CVRescale.__init__(factor=4, base_size=(128, 512))</code> \u00b6 <p>Define image scales using gaussian pyramid and rescale image to target scale.</p> PARAMETER DESCRIPTION <code>factor</code> <p>the decayed factor from base size, factor=4 keeps target scale by default.</p> <p> DEFAULT: <code>4</code> </p> <code>base_size</code> <p>base size the build the bottom layer of pyramid</p> <p> DEFAULT: <code>(128, 512)</code> </p> Source code in <code>mindocr\\data\\transforms\\svtr_transform.py</code> <pre><code>def __init__(self, factor=4, base_size=(128, 512)):\n\"\"\"Define image scales using gaussian pyramid and rescale image to target scale.\n\n    Args:\n        factor: the decayed factor from base size, factor=4 keeps target scale by default.\n        base_size: base size the build the bottom layer of pyramid\n    \"\"\"\n    if isinstance(factor, numbers.Number):\n        self.factor = round(sample_uniform(0, factor))\n    elif isinstance(factor, (tuple, list)) and len(factor) == 2:\n        self.factor = round(sample_uniform(factor[0], factor[1]))\n    else:\n        raise Exception(\"factor must be number or list with length 2\")\n    # assert factor is valid\n    self.base_h, self.base_w = base_size[:2]\n</code></pre>"},{"location":"reference/api_doc/#mindocr.data.transforms.transforms_factory","title":"<code>mindocr.data.transforms.transforms_factory</code>","text":"<p>Create and run transformations from a config or predefined transformation pipeline</p> <code>mindocr.data.transforms.transforms_factory.create_transforms(transform_pipeline, global_config=None)</code> \u00b6 <p>Create a squence of callable transforms.</p> PARAMETER DESCRIPTION <code>transform_pipeline</code> <p>list of callable instances or dicts where each key is a transformation class name, and its value are the args. e.g. [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]      [DecodeImage(img_mode='BGR')]</p> <p> TYPE: <code>List</code> </p> RETURNS DESCRIPTION <p>list of data transformation functions</p> Source code in <code>mindocr\\data\\transforms\\transforms_factory.py</code> <pre><code>def create_transforms(transform_pipeline: List, global_config: Dict = None):\n\"\"\"\n    Create a squence of callable transforms.\n\n    Args:\n        transform_pipeline (List): list of callable instances or dicts where each key is a transformation class name,\n            and its value are the args.\n            e.g. [{'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}}]\n                 [DecodeImage(img_mode='BGR')]\n\n    Returns:\n        list of data transformation functions\n    \"\"\"\n    assert isinstance(\n        transform_pipeline, list\n    ), f\"transform_pipeline config should be a list, but {type(transform_pipeline)} detected\"\n\n    transforms = []\n    for transform_config in transform_pipeline:\n        if isinstance(transform_config, dict):\n            assert len(transform_config) == 1, \"yaml format error in transforms\"\n            trans_name = list(transform_config.keys())[0]\n            param = {} if transform_config[trans_name] is None else transform_config[trans_name]\n            if global_config is not None:\n                param.update(global_config)\n            # TODO: assert undefined transform class\n\n            transform = eval(trans_name)(**param)\n            transforms.append(transform)\n        elif callable(transform_config):\n            transforms.append(transform_config)\n        else:\n            raise TypeError(\"transform_config must be a dict or a callable instance\")\n        # print(global_config)\n    return transforms\n</code></pre> <code>mindocr.data.transforms.transforms_factory.transforms_dbnet_icdar15(phase='train')</code> \u00b6 <p>Get pre-defined transform config for dbnet on icdar15 dataset.</p> PARAMETER DESCRIPTION <code>phase</code> <p>train, eval, infer</p> <p> DEFAULT: <code>'train'</code> </p> RETURNS DESCRIPTION <p>list of dict for data transformation pipeline, which can be convert to functions by 'create_transforms'</p> Source code in <code>mindocr\\data\\transforms\\transforms_factory.py</code> <pre><code>def transforms_dbnet_icdar15(phase=\"train\"):\n\"\"\"\n    Get pre-defined transform config for dbnet on icdar15 dataset.\n    Args:\n        phase: train, eval, infer\n    Returns:\n        list of dict for data transformation pipeline, which can be convert to functions by 'create_transforms'\n    \"\"\"\n    if phase == \"train\":\n        pipeline = [\n            {\"DecodeImage\": {\"img_mode\": \"RGB\", \"to_float32\": False}},\n            {\"DetLabelEncode\": None},\n            {\"RandomScale\": {\"scale_range\": [1.022, 3.0]}},\n            {\"IaaAugment\": {\"Affine\": {\"rotate\": [-10, 10]}, \"Fliplr\": {\"p\": 0.5}}},\n            {\"RandomCropWithBBox\": {\"max_tries\": 100, \"min_crop_ratio\": 0.1, \"crop_size\": (640, 640)}},\n            {\"ShrinkBinaryMap\": {\"min_text_size\": 8, \"shrink_ratio\": 0.4}},\n            {\n                \"BorderMap\": {\n                    \"shrink_ratio\": 0.4,\n                    \"thresh_min\": 0.3,\n                    \"thresh_max\": 0.7,\n                }\n            },\n            {\"RandomColorAdjust\": {\"brightness\": 32.0 / 255, \"saturation\": 0.5}},\n            {\n                \"NormalizeImage\": {\n                    \"bgr_to_rgb\": False,\n                    \"is_hwc\": True,\n                    \"mean\": [123.675, 116.28, 103.53],\n                    \"std\": [58.395, 57.12, 57.375],\n                }\n            },\n            {\"ToCHWImage\": None},\n        ]\n\n    elif phase == \"eval\":\n        pipeline = [\n            {\"DecodeImage\": {\"img_mode\": \"RGB\", \"to_float32\": False}},\n            {\"DetLabelEncode\": None},\n            {\"GridResize\": {\"factor\": 32}},\n            {\"ScalePadImage\": {\"target_size\": [736, 1280]}},\n            {\n                \"NormalizeImage\": {\n                    \"bgr_to_rgb\": False,\n                    \"is_hwc\": True,\n                    \"mean\": [123.675, 116.28, 103.53],\n                    \"std\": [58.395, 57.12, 57.375],\n                }\n            },\n            {\"ToCHWImage\": None},\n        ]\n    else:\n        pipeline = [\n            {\"DecodeImage\": {\"img_mode\": \"RGB\", \"to_float32\": False}},\n            {\"GridResize\": {\"factor\": 32}},\n            {\"ScalePadImage\": {\"target_size\": [736, 1280]}},\n            {\n                \"NormalizeImage\": {\n                    \"bgr_to_rgb\": False,\n                    \"is_hwc\": True,\n                    \"mean\": [123.675, 116.28, 103.53],\n                    \"std\": [58.395, 57.12, 57.375],\n                }\n            },\n            {\"ToCHWImage\": None},\n        ]\n    return pipeline\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses","title":"<code>mindocr.losses</code>","text":""},{"location":"reference/api_doc/#mindocr.losses.builder","title":"<code>mindocr.losses.builder</code>","text":""},{"location":"reference/api_doc/#mindocr.losses.builder.build_loss","title":"<code>mindocr.losses.builder.build_loss(name, **kwargs)</code>","text":"<p>Create the loss function.</p> PARAMETER DESCRIPTION <code>name</code> <p>loss function name, exactly the same as one of the supported loss class names</p> <p> TYPE: <code>str</code> </p> Return <p>nn.LossBase</p> Example Source code in <code>mindocr\\losses\\builder.py</code> <pre><code>def build_loss(name, **kwargs):\n\"\"\"\n    Create the loss function.\n\n    Args:\n        name (str): loss function name, exactly the same as one of the supported loss class names\n\n    Return:\n        nn.LossBase\n\n    Example:\n        &gt;&gt;&gt; # Create a CTC Loss module\n        &gt;&gt;&gt; from mindocr.losses import build_loss\n        &gt;&gt;&gt; loss_func_name = \"CTCLoss\"\n        &gt;&gt;&gt; loss_func_config = {\"pred_seq_len\": 25, \"max_label_len\": 24, \"batch_size\": 32}\n        &gt;&gt;&gt; loss_fn = build_loss(loss_func_name, **loss_func_config)\n        &gt;&gt;&gt; loss_fn\n        CTCLoss&lt;&gt;\n    \"\"\"\n    assert name in supported_losses, f\"Invalid loss name {name}, support losses are {supported_losses}\"\n\n    loss_fn = eval(name)(**kwargs)\n\n    # print('=&gt; Loss func input args: \\n\\t', inspect.signature(loss_fn.construct))\n\n    return loss_fn\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.builder.build_loss--create-a-ctc-loss-module","title":"Create a CTC Loss module","text":"<p>from mindocr.losses import build_loss loss_func_name = \"CTCLoss\" loss_func_config = {\"pred_seq_len\": 25, \"max_label_len\": 24, \"batch_size\": 32} loss_fn = build_loss(loss_func_name, **loss_func_config) loss_fn CTCLoss&lt;&gt;</p>"},{"location":"reference/api_doc/#mindocr.losses.cls_loss","title":"<code>mindocr.losses.cls_loss</code>","text":""},{"location":"reference/api_doc/#mindocr.losses.cls_loss.CrossEntropySmooth","title":"<code>mindocr.losses.cls_loss.CrossEntropySmooth</code>","text":"<p>         Bases: <code>nn.LossBase</code></p> <p>Cross entropy loss with label smoothing. Apply softmax activation function to input <code>logits</code>, and uses the given logits to compute cross entropy between the logits and the label.</p> PARAMETER DESCRIPTION <code>smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3.  Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.</p> <p> DEFAULT: <code>'mean'</code> </p> <code>weight</code> <p>Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> Inputs <p>logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.     Tuple composed of multiple logits are supported in order (main_logits, aux_logits)     for auxiliary loss used in networks like inception_v3. labels (Tensor): Ground truth label. Shape: [N] or [N, C].     (1) Shape (N), sparse labels representing the class indices. Must be int type.     (2) Shape [N, C], dense labels representing the ground truth class probability values,     or the one-hot labels. Must be float type.</p> Source code in <code>mindocr\\losses\\cls_loss.py</code> <pre><code>class CrossEntropySmooth(nn.LossBase):\n\"\"\"\n    Cross entropy loss with label smoothing.\n    Apply softmax activation function to input `logits`, and uses the given logits to compute cross entropy\n    between the logits and the label.\n\n    Args:\n        smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor: Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3.  Default: 0.0.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.\n        weight (Tensor): Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element.\n            Data type must be float16 or float32.\n\n    Inputs:\n        logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.\n            Tuple composed of multiple logits are supported in order (main_logits, aux_logits)\n            for auxiliary loss used in networks like inception_v3.\n        labels (Tensor): Ground truth label. Shape: [N] or [N, C].\n            (1) Shape (N), sparse labels representing the class indices. Must be int type.\n            (2) Shape [N, C], dense labels representing the ground truth class probability values,\n            or the one-hot labels. Must be float type.\n    \"\"\"\n\n    def __init__(self, smoothing=0.0, aux_factor=0.0, reduction=\"mean\", weight=None):\n        super().__init__()\n        self.smoothing = smoothing\n        self.aux_factor = aux_factor\n        self.reduction = reduction\n        self.weight = weight\n\n    def construct(self, logits, labels):\n        loss_aux = 0\n\n        if isinstance(logits, tuple):\n            main_logits = logits[0]\n            for aux in logits[1:]:\n                if self.aux_factor &gt; 0:\n                    loss_aux += F.cross_entropy(\n                        aux, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n                    )\n        else:\n            main_logits = logits\n\n        loss_logits = F.cross_entropy(\n            main_logits, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n        )\n        loss = loss_logits + self.aux_factor * loss_aux\n        return loss\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.det_loss","title":"<code>mindocr.losses.det_loss</code>","text":""},{"location":"reference/api_doc/#mindocr.losses.det_loss.BalancedBCELoss","title":"<code>mindocr.losses.det_loss.BalancedBCELoss</code>","text":"<p>         Bases: <code>nn.LossBase</code></p> <p>Balanced cross entropy loss.</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>class BalancedBCELoss(nn.LossBase):\n\"\"\"Balanced cross entropy loss.\"\"\"\n\n    def __init__(self, negative_ratio=3, eps=1e-6):\n        super().__init__()\n        self._negative_ratio = negative_ratio\n        self._eps = eps\n        self._bce_loss = ops.BinaryCrossEntropy(reduction=\"none\")\n\n    def construct(self, pred, gt, mask):\n\"\"\"\n        Args:\n            pred: shape :math:`(N, 1, H, W)`, the prediction of network\n            gt: shape :math:`(N, 1, H, W)`, the target\n            mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n        \"\"\"\n        pred = pred.squeeze(axis=1)\n        gt = gt.squeeze(axis=1)\n\n        positive = gt * mask\n        negative = (1 - gt) * mask\n\n        pos_count = positive.sum(axis=(1, 2), keepdims=True).astype(ms.int32)\n        neg_count = negative.sum(axis=(1, 2), keepdims=True).astype(ms.int32)\n        neg_count = ops.minimum(neg_count, pos_count * self._negative_ratio).squeeze(axis=(1, 2))\n\n        loss = self._bce_loss(pred, gt, None)\n\n        pos_loss = loss * positive\n        neg_loss = (loss * negative).view(loss.shape[0], -1)\n\n        neg_vals, _ = ops.sort(neg_loss)\n        neg_index = ops.stack((mnp.arange(loss.shape[0]), neg_vals.shape[1] - neg_count), axis=1)\n        min_neg_score = ops.expand_dims(ops.gather_nd(neg_vals, neg_index), axis=1)\n\n        neg_loss_mask = (neg_loss &gt;= min_neg_score).astype(ms.float32)  # filter values less than top k\n        neg_loss_mask = ops.stop_gradient(neg_loss_mask)\n\n        neg_loss = neg_loss_mask * neg_loss\n\n        return (pos_loss.sum() + neg_loss.sum()) / (\n            pos_count.astype(ms.float32).sum() + neg_count.astype(ms.float32).sum() + self._eps\n        )\n</code></pre> <code>mindocr.losses.det_loss.BalancedBCELoss.construct(pred, gt, mask)</code> \u00b6 PARAMETER DESCRIPTION <code>pred</code> <p>shape :math:<code>(N, 1, H, W)</code>, the prediction of network</p> <p> </p> <code>gt</code> <p>shape :math:<code>(N, 1, H, W)</code>, the target</p> <p> </p> <code>mask</code> <p>shape :math:<code>(N, H, W)</code>, the mask indicates positive regions</p> <p> </p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def construct(self, pred, gt, mask):\n\"\"\"\n    Args:\n        pred: shape :math:`(N, 1, H, W)`, the prediction of network\n        gt: shape :math:`(N, 1, H, W)`, the target\n        mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n    \"\"\"\n    pred = pred.squeeze(axis=1)\n    gt = gt.squeeze(axis=1)\n\n    positive = gt * mask\n    negative = (1 - gt) * mask\n\n    pos_count = positive.sum(axis=(1, 2), keepdims=True).astype(ms.int32)\n    neg_count = negative.sum(axis=(1, 2), keepdims=True).astype(ms.int32)\n    neg_count = ops.minimum(neg_count, pos_count * self._negative_ratio).squeeze(axis=(1, 2))\n\n    loss = self._bce_loss(pred, gt, None)\n\n    pos_loss = loss * positive\n    neg_loss = (loss * negative).view(loss.shape[0], -1)\n\n    neg_vals, _ = ops.sort(neg_loss)\n    neg_index = ops.stack((mnp.arange(loss.shape[0]), neg_vals.shape[1] - neg_count), axis=1)\n    min_neg_score = ops.expand_dims(ops.gather_nd(neg_vals, neg_index), axis=1)\n\n    neg_loss_mask = (neg_loss &gt;= min_neg_score).astype(ms.float32)  # filter values less than top k\n    neg_loss_mask = ops.stop_gradient(neg_loss_mask)\n\n    neg_loss = neg_loss_mask * neg_loss\n\n    return (pos_loss.sum() + neg_loss.sum()) / (\n        pos_count.astype(ms.float32).sum() + neg_count.astype(ms.float32).sum() + self._eps\n    )\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.det_loss.DiceLoss","title":"<code>mindocr.losses.det_loss.DiceLoss</code>","text":"<p>         Bases: <code>nn.LossBase</code></p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>class DiceLoss(nn.LossBase):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self._eps = eps\n\n    def construct(self, pred, gt, mask):\n\"\"\"\n        pred: one or two heatmaps of shape (N, 1, H, W),\n              the losses of two heatmaps are added together.\n        gt: (N, 1, H, W)\n        mask: (N, H, W)\n        \"\"\"\n        pred = pred.squeeze(axis=1) * mask\n        gt = gt.squeeze(axis=1) * mask\n\n        intersection = (pred * gt).sum()\n        union = pred.sum() + gt.sum() + self._eps\n        return 1 - 2.0 * intersection / union\n</code></pre> <code>mindocr.losses.det_loss.DiceLoss.construct(pred, gt, mask)</code> \u00b6 one or two heatmaps of shape (N, 1, H, W), <p>the losses of two heatmaps are added together.</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def construct(self, pred, gt, mask):\n\"\"\"\n    pred: one or two heatmaps of shape (N, 1, H, W),\n          the losses of two heatmaps are added together.\n    gt: (N, 1, H, W)\n    mask: (N, H, W)\n    \"\"\"\n    pred = pred.squeeze(axis=1) * mask\n    gt = gt.squeeze(axis=1) * mask\n\n    intersection = (pred * gt).sum()\n    union = pred.sum() + gt.sum() + self._eps\n    return 1 - 2.0 * intersection / union\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.det_loss.L1BalancedCELoss","title":"<code>mindocr.losses.det_loss.L1BalancedCELoss</code>","text":"<p>         Bases: <code>nn.LossBase</code></p> <p>Balanced CrossEntropy Loss on <code>binary</code>, MaskL1Loss on <code>thresh</code>, DiceLoss on <code>thresh_binary</code>. Note: The meaning of inputs can be figured out in <code>SegDetectorLossBuilder</code>.</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>class L1BalancedCELoss(nn.LossBase):\n\"\"\"\n    Balanced CrossEntropy Loss on `binary`,\n    MaskL1Loss on `thresh`,\n    DiceLoss on `thresh_binary`.\n    Note: The meaning of inputs can be figured out in `SegDetectorLossBuilder`.\n    \"\"\"\n\n    def __init__(self, eps=1e-6, bce_scale=5, l1_scale=10, bce_replace=\"bceloss\"):\n        super().__init__()\n\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss()\n\n        if bce_replace == \"bceloss\":\n            self.bce_loss = BalancedBCELoss()\n        elif bce_replace == \"diceloss\":\n            self.bce_loss = DiceLoss()\n        else:\n            raise ValueError(f\"bce_replace should be in ['bceloss', 'diceloss'], but get {bce_replace}\")\n\n        self.l1_scale = l1_scale\n        self.bce_scale = bce_scale\n\n    def construct(\n        self, pred: Union[Tensor, Tuple[Tensor]], gt: Tensor, gt_mask: Tensor, thresh_map: Tensor, thresh_mask: Tensor\n    ):\n\"\"\"\n        Compute dbnet loss\n        Args:\n            pred (Tuple[Tensor]): network prediction consists of\n                binary: The text segmentation prediction.\n                thresh: The threshold prediction (optional)\n                thresh_binary: Value produced by `step_function(binary - thresh)`. (optional)\n            gt (Tensor): Text regions bitmap gt.\n            mask (Tensor): Ignore mask, pexels where value is 1 indicates no contribution to loss.\n            thresh_mask (Tensor): Mask indicates regions cared by thresh supervision.\n            thresh_map (Tensor): Threshold gt.\n        Return:\n            loss value (Tensor)\n        \"\"\"\n        if isinstance(pred, ms.Tensor):\n            loss = self.bce_loss(pred, gt, gt_mask)\n        else:\n            binary, thresh, thresh_binary = pred\n            bce_loss_output = self.bce_loss(binary, gt, gt_mask)\n            l1_loss = self.l1_loss(thresh, thresh_map, thresh_mask)\n            dice_loss = self.dice_loss(thresh_binary, gt, gt_mask)\n            loss = dice_loss + self.l1_scale * l1_loss + self.bce_scale * bce_loss_output\n\n\"\"\"\n        if isinstance(pred, tuple):\n            binary, thresh, thresh_binary = pred\n        else:\n            binary = pred\n\n        bce_loss_output = self.bce_loss(binary, gt, gt_mask)\n\n        if isinstance(pred, tuple):\n            l1_loss = self.l1_loss(thresh, thresh_map, thresh_mask)\n            dice_loss = self.dice_loss(thresh_binary, gt, gt_mask)\n            loss = dice_loss + self.l1_scale * l1_loss + self.bce_scale * bce_loss_output\n        else:\n            loss = bce_loss_output\n        \"\"\"\n        return loss\n</code></pre> <code>mindocr.losses.det_loss.L1BalancedCELoss.construct(pred, gt, gt_mask, thresh_map, thresh_mask)</code> \u00b6 <p>Compute dbnet loss</p> PARAMETER DESCRIPTION <code>pred</code> <p>network prediction consists of binary: The text segmentation prediction. thresh: The threshold prediction (optional) thresh_binary: Value produced by <code>step_function(binary - thresh)</code>. (optional)</p> <p> TYPE: <code>Tuple[Tensor]</code> </p> <code>gt</code> <p>Text regions bitmap gt.</p> <p> TYPE: <code>Tensor</code> </p> <code>mask</code> <p>Ignore mask, pexels where value is 1 indicates no contribution to loss.</p> <p> TYPE: <code>Tensor</code> </p> <code>thresh_mask</code> <p>Mask indicates regions cared by thresh supervision.</p> <p> TYPE: <code>Tensor</code> </p> <code>thresh_map</code> <p>Threshold gt.</p> <p> TYPE: <code>Tensor</code> </p> Return <p>loss value (Tensor)</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def construct(\n    self, pred: Union[Tensor, Tuple[Tensor]], gt: Tensor, gt_mask: Tensor, thresh_map: Tensor, thresh_mask: Tensor\n):\n\"\"\"\n    Compute dbnet loss\n    Args:\n        pred (Tuple[Tensor]): network prediction consists of\n            binary: The text segmentation prediction.\n            thresh: The threshold prediction (optional)\n            thresh_binary: Value produced by `step_function(binary - thresh)`. (optional)\n        gt (Tensor): Text regions bitmap gt.\n        mask (Tensor): Ignore mask, pexels where value is 1 indicates no contribution to loss.\n        thresh_mask (Tensor): Mask indicates regions cared by thresh supervision.\n        thresh_map (Tensor): Threshold gt.\n    Return:\n        loss value (Tensor)\n    \"\"\"\n    if isinstance(pred, ms.Tensor):\n        loss = self.bce_loss(pred, gt, gt_mask)\n    else:\n        binary, thresh, thresh_binary = pred\n        bce_loss_output = self.bce_loss(binary, gt, gt_mask)\n        l1_loss = self.l1_loss(thresh, thresh_map, thresh_mask)\n        dice_loss = self.dice_loss(thresh_binary, gt, gt_mask)\n        loss = dice_loss + self.l1_scale * l1_loss + self.bce_scale * bce_loss_output\n\n\"\"\"\n    if isinstance(pred, tuple):\n        binary, thresh, thresh_binary = pred\n    else:\n        binary = pred\n\n    bce_loss_output = self.bce_loss(binary, gt, gt_mask)\n\n    if isinstance(pred, tuple):\n        l1_loss = self.l1_loss(thresh, thresh_map, thresh_mask)\n        dice_loss = self.dice_loss(thresh_binary, gt, gt_mask)\n        loss = dice_loss + self.l1_scale * l1_loss + self.bce_scale * bce_loss_output\n    else:\n        loss = bce_loss_output\n    \"\"\"\n    return loss\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.det_loss.MaskL1Loss","title":"<code>mindocr.losses.det_loss.MaskL1Loss</code>","text":"<p>         Bases: <code>nn.LossBase</code></p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>class MaskL1Loss(nn.LossBase):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self._eps = eps\n\n    def construct(self, pred, gt, mask):\n\"\"\"\n        Args:\n            pred: shape :math:`(N, 1, H, W)`, the prediction of network\n            gt: shape :math:`(N, H, W)`, the target\n            mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n        \"\"\"\n        pred = pred.squeeze(axis=1)\n        return ((pred - gt).abs() * mask).sum() / (mask.sum() + self._eps)\n</code></pre> <code>mindocr.losses.det_loss.MaskL1Loss.construct(pred, gt, mask)</code> \u00b6 PARAMETER DESCRIPTION <code>pred</code> <p>shape :math:<code>(N, 1, H, W)</code>, the prediction of network</p> <p> </p> <code>gt</code> <p>shape :math:<code>(N, H, W)</code>, the target</p> <p> </p> <code>mask</code> <p>shape :math:<code>(N, H, W)</code>, the mask indicates positive regions</p> <p> </p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def construct(self, pred, gt, mask):\n\"\"\"\n    Args:\n        pred: shape :math:`(N, 1, H, W)`, the prediction of network\n        gt: shape :math:`(N, H, W)`, the target\n        mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n    \"\"\"\n    pred = pred.squeeze(axis=1)\n    return ((pred - gt).abs() * mask).sum() / (mask.sum() + self._eps)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.det_loss.PSEDiceLoss","title":"<code>mindocr.losses.det_loss.PSEDiceLoss</code>","text":"<p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>class PSEDiceLoss(nn.Cell):\n    def __init__(self, alpha=0.7, ohem_ratio=3):\n        super().__init__()\n        self.threshold0 = Tensor(0.5, mstype.float32)\n        self.zero_float32 = Tensor(0.0, mstype.float32)\n        self.alpha = alpha\n        self.ohem_ratio = ohem_ratio\n        self.negative_one_int32 = Tensor(-1, mstype.int32)\n        self.concat = ops.Concat()\n        self.less_equal = ops.LessEqual()\n        self.greater = ops.Greater()\n        self.reduce_sum = ops.ReduceSum()\n        self.reduce_sum_keep_dims = ops.ReduceSum(keep_dims=True)\n        self.reduce_mean = ops.ReduceMean()\n        self.reduce_min = ops.ReduceMin()\n        self.cast = ops.Cast()\n        self.minimum = ops.Minimum()\n        self.expand_dims = ops.ExpandDims()\n        self.select = ops.Select()\n        self.fill = ops.Fill()\n        self.topk = ops.TopK(sorted=True)\n        self.shape = ops.Shape()\n        self.sigmoid = ops.Sigmoid()\n        self.reshape = ops.Reshape()\n        self.slice = ops.Slice()\n        self.logical_and = ops.LogicalAnd()\n        self.logical_or = ops.LogicalOr()\n        self.equal = ops.Equal()\n        self.zeros_like = ops.ZerosLike()\n        self.add = ops.Add()\n        self.gather = ops.Gather()\n        self.upsample = nn.ResizeBilinear()\n\n    def ohem_batch(self, scores, gt_texts, training_masks):\n\"\"\"\n\n        :param scores: [N * H * W]\n        :param gt_texts:  [N * H * W]\n        :param training_masks: [N * H * W]\n        :return: [N * H * W]\n        \"\"\"\n        batch_size = scores.shape[0]\n        h, w = scores.shape[1:]\n        selected_masks = ()\n        for i in range(batch_size):\n            score = self.slice(scores, (i, 0, 0), (1, h, w))\n            score = self.reshape(score, (h, w))\n\n            gt_text = self.slice(gt_texts, (i, 0, 0), (1, h, w))\n            gt_text = self.reshape(gt_text, (h, w))\n\n            training_mask = self.slice(training_masks, (i, 0, 0), (1, h, w))\n            training_mask = self.reshape(training_mask, (h, w))\n\n            selected_mask = self.ohem_single(score, gt_text, training_mask)\n            selected_masks = selected_masks + (selected_mask,)\n\n        selected_masks = self.concat(selected_masks)\n        return selected_masks\n\n    def ohem_single(self, score, gt_text, training_mask):\n        h, w = score.shape[0:2]\n        k = int(h * w)\n        pos_num = self.logical_and(self.greater(gt_text, self.threshold0), self.greater(training_mask, self.threshold0))\n        pos_num = self.reduce_sum(self.cast(pos_num, mstype.float32))\n\n        neg_num = self.less_equal(gt_text, self.threshold0)\n        neg_num = self.reduce_sum(self.cast(neg_num, mstype.float32))\n        neg_num = self.minimum(self.ohem_ratio * pos_num, neg_num)\n        neg_num = self.cast(neg_num, mstype.int32)\n\n        neg_num = neg_num + k - 1\n        neg_mask = self.less_equal(gt_text, self.threshold0)\n        ignore_score = self.fill(mstype.float32, (h, w), -1e3)\n        neg_score = self.select(neg_mask, score, ignore_score)\n        neg_score = self.reshape(neg_score, (h * w,))\n\n        topk_values, _ = self.topk(neg_score, k)\n        threshold = self.gather(topk_values, neg_num, 0)\n\n        selected_mask = self.logical_and(\n            self.logical_or(self.greater(score, threshold), self.greater(gt_text, self.threshold0)),\n            self.greater(training_mask, self.threshold0),\n        )\n\n        selected_mask = self.cast(selected_mask, mstype.float32)\n        selected_mask = self.expand_dims(selected_mask, 0)\n\n        return selected_mask\n\n    def dice_loss(self, input_params, target, mask):\n\"\"\"\n\n        :param input: [N, H, W]\n        :param target: [N, H, W]\n        :param mask: [N, H, W]\n        :return:\n        \"\"\"\n        batch_size = input_params.shape[0]\n        input_sigmoid = self.sigmoid(input_params)\n\n        input_reshape = self.reshape(input_sigmoid, (batch_size, -1))\n        target = self.reshape(target, (batch_size, -1))\n        mask = self.reshape(mask, (batch_size, -1))\n\n        input_mask = input_reshape * mask\n        target = target * mask\n\n        a = self.reduce_sum(input_mask * target, 1)\n        b = self.reduce_sum(input_mask * input_mask, 1) + 0.001\n        c = self.reduce_sum(target * target, 1) + 0.001\n        d = (2 * a) / (b + c)\n        dice_loss = self.reduce_mean(d)\n        return 1 - dice_loss\n\n    def avg_losses(self, loss_list):\n        loss_kernel = loss_list[0]\n        for i in range(1, len(loss_list)):\n            loss_kernel += loss_list[i]\n        loss_kernel = loss_kernel / len(loss_list)\n        return loss_kernel\n\n    def construct(self, model_predict, gt_texts, gt_kernels, training_masks):\n\"\"\"\n\n        :param model_predict: [N * 7 * H * W]\n        :param gt_texts: [N * H * W]\n        :param gt_kernels:[N * 6 * H * W]\n        :param training_masks:[N * H * W]\n        :return:\n        \"\"\"\n        batch_size = model_predict.shape[0]\n        model_predict = self.upsample(model_predict, scale_factor=4)\n        h, w = model_predict.shape[2:]\n        texts = self.slice(model_predict, (0, 0, 0, 0), (batch_size, 1, h, w))\n        texts = self.reshape(texts, (batch_size, h, w))\n        selected_masks_text = self.ohem_batch(texts, gt_texts, training_masks)\n        loss_text = self.dice_loss(texts, gt_texts, selected_masks_text)\n        kernels = []\n        loss_kernels = []\n        for i in range(1, 7):\n            kernel = self.slice(model_predict, (0, i, 0, 0), (batch_size, 1, h, w))\n            kernel = self.reshape(kernel, (batch_size, h, w))\n            kernels.append(kernel)\n\n        mask0 = self.sigmoid(texts)\n        selected_masks_kernels = self.logical_and(\n            self.greater(mask0, self.threshold0), self.greater(training_masks, self.threshold0)\n        )\n        selected_masks_kernels = self.cast(selected_masks_kernels, mstype.float32)\n\n        for i in range(6):\n            gt_kernel = self.slice(gt_kernels, (0, i, 0, 0), (batch_size, 1, h, w))\n            gt_kernel = self.reshape(gt_kernel, (batch_size, h, w))\n            loss_kernel_i = self.dice_loss(kernels[i], gt_kernel, selected_masks_kernels)\n            loss_kernels.append(loss_kernel_i)\n        loss_kernel = self.avg_losses(loss_kernels)\n\n        loss = self.alpha * loss_text + (1 - self.alpha) * loss_kernel\n        return loss\n</code></pre> <code>mindocr.losses.det_loss.PSEDiceLoss.construct(model_predict, gt_texts, gt_kernels, training_masks)</code> \u00b6 <p>:param model_predict: [N * 7 * H * W] :param gt_texts: [N * H * W] :param gt_kernels:[N * 6 * H * W] :param training_masks:[N * H * W] :return:</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def construct(self, model_predict, gt_texts, gt_kernels, training_masks):\n\"\"\"\n\n    :param model_predict: [N * 7 * H * W]\n    :param gt_texts: [N * H * W]\n    :param gt_kernels:[N * 6 * H * W]\n    :param training_masks:[N * H * W]\n    :return:\n    \"\"\"\n    batch_size = model_predict.shape[0]\n    model_predict = self.upsample(model_predict, scale_factor=4)\n    h, w = model_predict.shape[2:]\n    texts = self.slice(model_predict, (0, 0, 0, 0), (batch_size, 1, h, w))\n    texts = self.reshape(texts, (batch_size, h, w))\n    selected_masks_text = self.ohem_batch(texts, gt_texts, training_masks)\n    loss_text = self.dice_loss(texts, gt_texts, selected_masks_text)\n    kernels = []\n    loss_kernels = []\n    for i in range(1, 7):\n        kernel = self.slice(model_predict, (0, i, 0, 0), (batch_size, 1, h, w))\n        kernel = self.reshape(kernel, (batch_size, h, w))\n        kernels.append(kernel)\n\n    mask0 = self.sigmoid(texts)\n    selected_masks_kernels = self.logical_and(\n        self.greater(mask0, self.threshold0), self.greater(training_masks, self.threshold0)\n    )\n    selected_masks_kernels = self.cast(selected_masks_kernels, mstype.float32)\n\n    for i in range(6):\n        gt_kernel = self.slice(gt_kernels, (0, i, 0, 0), (batch_size, 1, h, w))\n        gt_kernel = self.reshape(gt_kernel, (batch_size, h, w))\n        loss_kernel_i = self.dice_loss(kernels[i], gt_kernel, selected_masks_kernels)\n        loss_kernels.append(loss_kernel_i)\n    loss_kernel = self.avg_losses(loss_kernels)\n\n    loss = self.alpha * loss_text + (1 - self.alpha) * loss_kernel\n    return loss\n</code></pre> <code>mindocr.losses.det_loss.PSEDiceLoss.dice_loss(input_params, target, mask)</code> \u00b6 <p>:param input: [N, H, W] :param target: [N, H, W] :param mask: [N, H, W] :return:</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def dice_loss(self, input_params, target, mask):\n\"\"\"\n\n    :param input: [N, H, W]\n    :param target: [N, H, W]\n    :param mask: [N, H, W]\n    :return:\n    \"\"\"\n    batch_size = input_params.shape[0]\n    input_sigmoid = self.sigmoid(input_params)\n\n    input_reshape = self.reshape(input_sigmoid, (batch_size, -1))\n    target = self.reshape(target, (batch_size, -1))\n    mask = self.reshape(mask, (batch_size, -1))\n\n    input_mask = input_reshape * mask\n    target = target * mask\n\n    a = self.reduce_sum(input_mask * target, 1)\n    b = self.reduce_sum(input_mask * input_mask, 1) + 0.001\n    c = self.reduce_sum(target * target, 1) + 0.001\n    d = (2 * a) / (b + c)\n    dice_loss = self.reduce_mean(d)\n    return 1 - dice_loss\n</code></pre> <code>mindocr.losses.det_loss.PSEDiceLoss.ohem_batch(scores, gt_texts, training_masks)</code> \u00b6 <p>:param scores: [N * H * W] :param gt_texts:  [N * H * W] :param training_masks: [N * H * W] :return: [N * H * W]</p> Source code in <code>mindocr\\losses\\det_loss.py</code> <pre><code>def ohem_batch(self, scores, gt_texts, training_masks):\n\"\"\"\n\n    :param scores: [N * H * W]\n    :param gt_texts:  [N * H * W]\n    :param training_masks: [N * H * W]\n    :return: [N * H * W]\n    \"\"\"\n    batch_size = scores.shape[0]\n    h, w = scores.shape[1:]\n    selected_masks = ()\n    for i in range(batch_size):\n        score = self.slice(scores, (i, 0, 0), (1, h, w))\n        score = self.reshape(score, (h, w))\n\n        gt_text = self.slice(gt_texts, (i, 0, 0), (1, h, w))\n        gt_text = self.reshape(gt_text, (h, w))\n\n        training_mask = self.slice(training_masks, (i, 0, 0), (1, h, w))\n        training_mask = self.reshape(training_mask, (h, w))\n\n        selected_mask = self.ohem_single(score, gt_text, training_mask)\n        selected_masks = selected_masks + (selected_mask,)\n\n    selected_masks = self.concat(selected_masks)\n    return selected_masks\n</code></pre>"},{"location":"reference/api_doc/#mindocr.losses.rec_loss","title":"<code>mindocr.losses.rec_loss</code>","text":""},{"location":"reference/api_doc/#mindocr.losses.rec_loss.CTCLoss","title":"<code>mindocr.losses.rec_loss.CTCLoss</code>","text":"<p>         Bases: <code>LossBase</code></p> <p>CTCLoss definition</p> PARAMETER DESCRIPTION <code>pred_seq_len(int)</code> <p>the length of the predicted character sequence. For text images, this value equals to W - the width of feature map encoded by the visual bacbkone. This can be obtained by probing the output shape in the network. E.g., for a training image in shape (3, 32, 100), the feature map encoded by resnet34 bacbkone is in shape (512, 1, 4), W = 4, sequence len is 4.</p> <p> </p> <code>max_label_len(int)</code> <p>the maximum number of characters in a text label, i.e. max_text_len in yaml.</p> <p> </p> <code>batch_size(int)</code> <p>batch size of input logits. bs</p> <p> </p> Source code in <code>mindocr\\losses\\rec_loss.py</code> <pre><code>class CTCLoss(LossBase):\n\"\"\"\n    CTCLoss definition\n\n    Args:\n        pred_seq_len(int): the length of the predicted character sequence. For text images, this value equals to\n            W - the width of feature map encoded by the visual bacbkone.\n            This can be obtained by probing the output shape in the network.\n            E.g., for a training image in shape (3, 32, 100), the feature map encoded by resnet34 bacbkone is\n            in shape (512, 1, 4), W = 4, sequence len is 4.\n        max_label_len(int): the maximum number of characters in a text label, i.e. max_text_len in yaml.\n        batch_size(int): batch size of input logits. bs\n    \"\"\"\n\n    def __init__(\n        self, pred_seq_len: int = 26, max_label_len: int = 25, batch_size: int = 32, reduction: str = \"mean\"\n    ) -&gt; None:\n        super(CTCLoss, self).__init__(reduction=reduction)\n        assert pred_seq_len &gt; max_label_len, (\n            \"pred_seq_len is required to be larger than max_label_len for CTCLoss. Please adjust the strides in the \"\n            \"backbone, or reduce max_text_length in yaml\"\n        )\n        self.sequence_length = Tensor(np.array([pred_seq_len] * batch_size), ms.int32)\n\n        label_indices = []\n        for i in range(batch_size):\n            for j in range(max_label_len):\n                label_indices.append([i, j])\n        self.label_indices = Tensor(np.array(label_indices), ms.int64)\n        self.ctc_loss = ops.CTCLoss(ctc_merge_repeated=True)\n\n    def construct(self, pred: Tensor, label: Tensor) -&gt; Tensor:\n\"\"\"\n        Args:\n            pred (Tensor): network prediction which is a\n                logit Tensor in shape (W, BS, NC), where W - seq len, BS - batch size. NC - num of classes\n                (types of character + blank + 1)\n            label (Tensor): GT sequence of character indices in shape (BS, SL), SL - sequence length, which is padded to\n                max_text_length\n        Returns:\n            loss value (Tensor)\n        \"\"\"\n        logit = pred\n        label_values = ops.reshape(label, (-1,))\n\n        loss, _ = self.ctc_loss(logit, self.label_indices, label_values, self.sequence_length)\n        loss = self.get_loss(loss)\n        return loss\n</code></pre> <code>mindocr.losses.rec_loss.CTCLoss.construct(pred, label)</code> \u00b6 PARAMETER DESCRIPTION <code>pred</code> <p>network prediction which is a logit Tensor in shape (W, BS, NC), where W - seq len, BS - batch size. NC - num of classes (types of character + blank + 1)</p> <p> TYPE: <code>Tensor</code> </p> <code>label</code> <p>GT sequence of character indices in shape (BS, SL), SL - sequence length, which is padded to max_text_length</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>loss value (Tensor)</p> Source code in <code>mindocr\\losses\\rec_loss.py</code> <pre><code>def construct(self, pred: Tensor, label: Tensor) -&gt; Tensor:\n\"\"\"\n    Args:\n        pred (Tensor): network prediction which is a\n            logit Tensor in shape (W, BS, NC), where W - seq len, BS - batch size. NC - num of classes\n            (types of character + blank + 1)\n        label (Tensor): GT sequence of character indices in shape (BS, SL), SL - sequence length, which is padded to\n            max_text_length\n    Returns:\n        loss value (Tensor)\n    \"\"\"\n    logit = pred\n    label_values = ops.reshape(label, (-1,))\n\n    loss, _ = self.ctc_loss(logit, self.label_indices, label_values, self.sequence_length)\n    loss = self.get_loss(loss)\n    return loss\n</code></pre>"},{"location":"reference/api_doc/#mindocr.metrics","title":"<code>mindocr.metrics</code>","text":""},{"location":"reference/api_doc/#mindocr.metrics.build_metric","title":"<code>mindocr.metrics.build_metric(config, device_num=1, **kwargs)</code>","text":"<p>Create the metric function.</p> PARAMETER DESCRIPTION <code>config</code> <p>configuration for metric including metric <code>name</code> and also the kwargs specifically for each metric. - name (str): metric function name, exactly the same as one of the supported metric class names</p> <p> TYPE: <code>dict</code> </p> <code>device_num</code> <p>number of devices. If device_num &gt; 1, metric will be computed in distributed mode, i.e., aggregate intermediate variables (e.g., num_correct, TP) from all devices by <code>ops.AllReduce</code> op so as to correctly compute the metric on dispatched data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Return <p>nn.Metric</p> Example"},{"location":"reference/api_doc/#mindocr.metrics.build_metric--create-a-recmetric-module-for-text-recognition","title":"Create a RecMetric module for text recognition","text":"<p>from mindocr.metrics import build_metric metric_config = {\"name\": \"RecMetric\", \"main_indicator\": \"acc\", \"character_dict_path\": None, \"ignore_space\": True, \"print_flag\": False} metric = build_metric(metric_config) metric  Source code in <code>mindocr\\metrics\\builder.py</code> <pre><code>def build_metric(config, device_num=1, **kwargs):\n\"\"\"\n    Create the metric function.\n\n    Args:\n        config (dict): configuration for metric including metric `name` and also the kwargs specifically for\n            each metric.\n            - name (str): metric function name, exactly the same as one of the supported metric class names\n        device_num (int): number of devices. If device_num &gt; 1, metric will be computed in distributed mode,\n            i.e., aggregate intermediate variables (e.g., num_correct, TP) from all devices\n            by `ops.AllReduce` op so as to correctly\n            compute the metric on dispatched data.\n\n    Return:\n        nn.Metric\n\n    Example:\n        &gt;&gt;&gt; # Create a RecMetric module for text recognition\n        &gt;&gt;&gt; from mindocr.metrics import build_metric\n        &gt;&gt;&gt; metric_config = {\"name\": \"RecMetric\", \"main_indicator\": \"acc\", \"character_dict_path\": None,\n        \"ignore_space\": True, \"print_flag\": False}\n        &gt;&gt;&gt; metric = build_metric(metric_config)\n        &gt;&gt;&gt; metric\n        &lt;mindocr.metrics.rec_metrics.RecMetric&gt;\n    \"\"\"\n\n    mn = config.pop(\"name\")\n    if mn in supported_metrics:\n        device_num = 1 if device_num is None else device_num\n        config.update({\"device_num\": device_num})\n        metric = eval(mn)(**config)\n    else:\n        raise ValueError(f\"Invalid metric name {mn}, support metrics are {supported_metrics}\")\n\n    return metric\n</code></pre>"},{"location":"reference/api_doc/#mindocr.metrics.builder","title":"<code>mindocr.metrics.builder</code>","text":""},{"location":"reference/api_doc/#mindocr.metrics.builder.build_metric","title":"<code>mindocr.metrics.builder.build_metric(config, device_num=1, **kwargs)</code>","text":"<p>Create the metric function.</p> PARAMETER DESCRIPTION <code>config</code> <p>configuration for metric including metric <code>name</code> and also the kwargs specifically for each metric. - name (str): metric function name, exactly the same as one of the supported metric class names</p> <p> TYPE: <code>dict</code> </p> <code>device_num</code> <p>number of devices. If device_num &gt; 1, metric will be computed in distributed mode, i.e., aggregate intermediate variables (e.g., num_correct, TP) from all devices by <code>ops.AllReduce</code> op so as to correctly compute the metric on dispatched data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Return <p>nn.Metric</p> Example"},{"location":"reference/api_doc/#mindocr.metrics.builder.build_metric--create-a-recmetric-module-for-text-recognition","title":"Create a RecMetric module for text recognition","text":"<p>from mindocr.metrics import build_metric metric_config = {\"name\": \"RecMetric\", \"main_indicator\": \"acc\", \"character_dict_path\": None, \"ignore_space\": True, \"print_flag\": False} metric = build_metric(metric_config) metric  Source code in <code>mindocr\\metrics\\builder.py</code> <pre><code>def build_metric(config, device_num=1, **kwargs):\n\"\"\"\n    Create the metric function.\n\n    Args:\n        config (dict): configuration for metric including metric `name` and also the kwargs specifically for\n            each metric.\n            - name (str): metric function name, exactly the same as one of the supported metric class names\n        device_num (int): number of devices. If device_num &gt; 1, metric will be computed in distributed mode,\n            i.e., aggregate intermediate variables (e.g., num_correct, TP) from all devices\n            by `ops.AllReduce` op so as to correctly\n            compute the metric on dispatched data.\n\n    Return:\n        nn.Metric\n\n    Example:\n        &gt;&gt;&gt; # Create a RecMetric module for text recognition\n        &gt;&gt;&gt; from mindocr.metrics import build_metric\n        &gt;&gt;&gt; metric_config = {\"name\": \"RecMetric\", \"main_indicator\": \"acc\", \"character_dict_path\": None,\n        \"ignore_space\": True, \"print_flag\": False}\n        &gt;&gt;&gt; metric = build_metric(metric_config)\n        &gt;&gt;&gt; metric\n        &lt;mindocr.metrics.rec_metrics.RecMetric&gt;\n    \"\"\"\n\n    mn = config.pop(\"name\")\n    if mn in supported_metrics:\n        device_num = 1 if device_num is None else device_num\n        config.update({\"device_num\": device_num})\n        metric = eval(mn)(**config)\n    else:\n        raise ValueError(f\"Invalid metric name {mn}, support metrics are {supported_metrics}\")\n\n    return metric\n</code></pre>"},{"location":"reference/api_doc/#mindocr.metrics.cls_metrics","title":"<code>mindocr.metrics.cls_metrics</code>","text":""},{"location":"reference/api_doc/#mindocr.metrics.cls_metrics.ClsMetric","title":"<code>mindocr.metrics.cls_metrics.ClsMetric</code>","text":"<p>         Bases: <code>object</code></p> <p>Compute the text direction classification accuracy.</p> Source code in <code>mindocr\\metrics\\cls_metrics.py</code> <pre><code>class ClsMetric(object):\n\"\"\"Compute the text direction classification accuracy.\"\"\"\n\n    def __init__(self, label_list=None, **kwargs):\n\"\"\"\n        label_list: Set in yaml config file, map the gts back to original label format (angle).\n        \"\"\"\n        assert (\n            label_list is not None\n        ), \"`label_list` should not be None. Please set it in 'metric' section in yaml config file.\"\n        self.label_list = label_list\n        self.eps = 1e-5\n        self.metric_names = [\"acc\"]\n        self.clear()\n\n    def update(self, *inputs):\n        preds, gts = inputs\n        preds = preds[\"angles\"]\n        if isinstance(gts, list):\n            gts = gts[0]\n        gts = [self.label_list[i] for i in gts]\n\n        correct_num = 0\n        all_num = 0\n        for pred, target in zip(preds, gts):\n            if pred == target:\n                correct_num += 1\n            all_num += 1\n        self.correct_num += correct_num\n        self.all_num += all_num\n\n    def eval(self):\n        acc = self.correct_num / (self.all_num + self.eps)\n        self.clear()\n        return {\"acc\": acc}\n\n    def clear(self):\n        self.correct_num = 0\n        self.all_num = 0\n</code></pre> <code>mindocr.metrics.cls_metrics.ClsMetric.__init__(label_list=None, **kwargs)</code> \u00b6 Source code in <code>mindocr\\metrics\\cls_metrics.py</code> <pre><code>def __init__(self, label_list=None, **kwargs):\n\"\"\"\n    label_list: Set in yaml config file, map the gts back to original label format (angle).\n    \"\"\"\n    assert (\n        label_list is not None\n    ), \"`label_list` should not be None. Please set it in 'metric' section in yaml config file.\"\n    self.label_list = label_list\n    self.eps = 1e-5\n    self.metric_names = [\"acc\"]\n    self.clear()\n</code></pre>"},{"location":"reference/api_doc/#mindocr.metrics.det_metrics","title":"<code>mindocr.metrics.det_metrics</code>","text":""},{"location":"reference/api_doc/#mindocr.metrics.det_metrics.DetMetric","title":"<code>mindocr.metrics.det_metrics.DetMetric</code>","text":"<p>         Bases: <code>nn.Metric</code></p> Source code in <code>mindocr\\metrics\\det_metrics.py</code> <pre><code>class DetMetric(nn.Metric):\n\"\"\" \"\"\"\n\n    def __init__(self, device_num=1, **kwargs):\n        super().__init__()\n        self._evaluator = DetectionIoUEvaluator()\n        self._gt_labels, self._det_labels = [], []\n        self.device_num = device_num\n        self.all_reduce = None if device_num == 1 else ops.AllReduce()\n        self.metric_names = [\"recall\", \"precision\", \"f-score\"]\n\n    def clear(self):\n        self._gt_labels, self._det_labels = [], []\n\n    def update(self, *inputs):\n\"\"\"\n        compute metric on a batch of data\n\n        Args:\n            inputs (tuple): contain two elements preds, gt\n                    preds (dict): text detection prediction as a dictionary with keys:\n                        polys: np.ndarray of shape (N, K, 4, 2)\n                        score: np.ndarray of shape (N, K), confidence score\n                    gts (tuple): ground truth\n                        - (polygons, ignore_tags), where polygons are in shape [num_images, num_boxes, 4, 2],\n                        ignore_tags are in shape [num_images, num_boxes], which can be defined by output_columns in yaml\n        \"\"\"\n        preds, gts = inputs\n        preds = preds[\"polys\"]\n        polys, ignore = gts[0].asnumpy().astype(np.float32), gts[1].asnumpy()\n\n        for sample_id in range(len(polys)):\n            gt = [{\"polys\": poly, \"ignore\": ig} for poly, ig in zip(polys[sample_id], ignore[sample_id])]\n            gt_label, det_label = self._evaluator(gt, preds[sample_id])\n            self._gt_labels.append(gt_label)\n            self._det_labels.append(det_label)\n\n    @ms_function\n    def all_reduce_fun(self, x):\n        res = self.all_reduce(x)\n        return res\n\n    def cal_matrix(self, det_lst, gt_lst):\n        tp = np.sum((gt_lst == 1) * (det_lst == 1))\n        fn = np.sum((gt_lst == 1) * (det_lst == 0))\n        fp = np.sum((gt_lst == 0) * (det_lst == 1))\n        return tp, fp, fn\n\n    def eval(self):\n\"\"\"\n        Evaluate by aggregating results from batch update\n\n        Returns: dict, average precision, recall, f1-score of all samples\n            precision: precision,\n            recall: recall,\n            f-score: f-score\n        \"\"\"\n        # flatten predictions and labels into 1D-array\n        self._det_labels = np.array([lbl for label in self._det_labels for lbl in label])\n        self._gt_labels = np.array([lbl for label in self._gt_labels for lbl in label])\n\n        tp, fp, fn = self.cal_matrix(self._det_labels, self._gt_labels)\n        if self.all_reduce:\n            tp = float(self.all_reduce_fun(Tensor(tp, ms.float32)).asnumpy())\n            fp = float(self.all_reduce_fun(Tensor(fp, ms.float32)).asnumpy())\n            fn = float(self.all_reduce_fun(Tensor(fn, ms.float32)).asnumpy())\n\n        recall = _safe_divide(tp, (tp + fn))\n        precision = _safe_divide(tp, (tp + fp))\n        f_score = _safe_divide(2 * recall * precision, (recall + precision))\n        return {\"recall\": recall, \"precision\": precision, \"f-score\": f_score}\n</code></pre> <code>mindocr.metrics.det_metrics.DetMetric.eval()</code> \u00b6 <p>Evaluate by aggregating results from batch update</p> DICT, AVERAGE PRECISION, RECALL, F1-SCORE OF ALL SAMPLES DESCRIPTION <code>precision</code> <p>precision,</p> <code>recall</code> <p>recall,</p> Source code in <code>mindocr\\metrics\\det_metrics.py</code> <pre><code>def eval(self):\n\"\"\"\n    Evaluate by aggregating results from batch update\n\n    Returns: dict, average precision, recall, f1-score of all samples\n        precision: precision,\n        recall: recall,\n        f-score: f-score\n    \"\"\"\n    # flatten predictions and labels into 1D-array\n    self._det_labels = np.array([lbl for label in self._det_labels for lbl in label])\n    self._gt_labels = np.array([lbl for label in self._gt_labels for lbl in label])\n\n    tp, fp, fn = self.cal_matrix(self._det_labels, self._gt_labels)\n    if self.all_reduce:\n        tp = float(self.all_reduce_fun(Tensor(tp, ms.float32)).asnumpy())\n        fp = float(self.all_reduce_fun(Tensor(fp, ms.float32)).asnumpy())\n        fn = float(self.all_reduce_fun(Tensor(fn, ms.float32)).asnumpy())\n\n    recall = _safe_divide(tp, (tp + fn))\n    precision = _safe_divide(tp, (tp + fp))\n    f_score = _safe_divide(2 * recall * precision, (recall + precision))\n    return {\"recall\": recall, \"precision\": precision, \"f-score\": f_score}\n</code></pre> <code>mindocr.metrics.det_metrics.DetMetric.update(*inputs)</code> \u00b6 <p>compute metric on a batch of data</p> PARAMETER DESCRIPTION <code>inputs</code> <p>contain two elements preds, gt     preds (dict): text detection prediction as a dictionary with keys:         polys: np.ndarray of shape (N, K, 4, 2)         score: np.ndarray of shape (N, K), confidence score     gts (tuple): ground truth         - (polygons, ignore_tags), where polygons are in shape [num_images, num_boxes, 4, 2],         ignore_tags are in shape [num_images, num_boxes], which can be defined by output_columns in yaml</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> Source code in <code>mindocr\\metrics\\det_metrics.py</code> <pre><code>def update(self, *inputs):\n\"\"\"\n    compute metric on a batch of data\n\n    Args:\n        inputs (tuple): contain two elements preds, gt\n                preds (dict): text detection prediction as a dictionary with keys:\n                    polys: np.ndarray of shape (N, K, 4, 2)\n                    score: np.ndarray of shape (N, K), confidence score\n                gts (tuple): ground truth\n                    - (polygons, ignore_tags), where polygons are in shape [num_images, num_boxes, 4, 2],\n                    ignore_tags are in shape [num_images, num_boxes], which can be defined by output_columns in yaml\n    \"\"\"\n    preds, gts = inputs\n    preds = preds[\"polys\"]\n    polys, ignore = gts[0].asnumpy().astype(np.float32), gts[1].asnumpy()\n\n    for sample_id in range(len(polys)):\n        gt = [{\"polys\": poly, \"ignore\": ig} for poly, ig in zip(polys[sample_id], ignore[sample_id])]\n        gt_label, det_label = self._evaluator(gt, preds[sample_id])\n        self._gt_labels.append(gt_label)\n        self._det_labels.append(det_label)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.metrics.rec_metrics","title":"<code>mindocr.metrics.rec_metrics</code>","text":"<p>Metric for accuracy evaluation.</p>"},{"location":"reference/api_doc/#mindocr.metrics.rec_metrics.RecMetric","title":"<code>mindocr.metrics.rec_metrics.RecMetric</code>","text":"<p>         Bases: <code>nn.Metric</code></p> <p>Define accuracy metric for warpctc network.</p> PARAMETER DESCRIPTION <code>ignore_space</code> <p>remove space in prediction and ground truth text if True</p> <p> DEFAULT: <code>True</code> </p> <code>filter_ood</code> <p>filter out-of-dictionary characters(e.g., '$' for the default digit+en dictionary) in ground truth text. Default is True.</p> <p> DEFAULT: <code>True</code> </p> <code>lower</code> <p>convert GT text to lower case. Recommend to set True if the dictionary does not contains upper letters</p> <p> DEFAULT: <code>True</code> </p> Notes <p>Since the OOD characters are skipped during label encoding in data transformation by default, filter_ood should be True. (Paddle skipped the OOD character in label encoding and then decoded the label indices back to text string, which has no ood character.</p> Source code in <code>mindocr\\metrics\\rec_metrics.py</code> <pre><code>class RecMetric(nn.Metric):\n\"\"\"\n    Define accuracy metric for warpctc network.\n\n    Args:\n        ignore_space: remove space in prediction and ground truth text if True\n        filter_ood: filter out-of-dictionary characters(e.g., '$' for the default digit+en dictionary) in\n            ground truth text. Default is True.\n        lower: convert GT text to lower case. Recommend to set True if the dictionary does not contains upper letters\n\n    Notes:\n        Since the OOD characters are skipped during label encoding in data transformation by default,\n        filter_ood should be True. (Paddle skipped the OOD character in label encoding and then decoded the label\n        indices back to text string, which has no ood character.\n    \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path=None,\n        ignore_space=True,\n        filter_ood=True,\n        lower=True,\n        print_flag=False,\n        device_num=1,\n        **kwargs\n    ):\n        super().__init__()\n        self.clear()\n        self.ignore_space = ignore_space\n        self.filter_ood = filter_ood\n        self.lower = lower\n        self.print_flag = print_flag\n\n        self.device_num = device_num\n        self.all_reduce = None if device_num == 1 else ops.AllReduce()\n        self.metric_names = [\"acc\", \"norm_edit_distance\"]\n\n        # TODO: use parsed dictionary object\n        if character_dict_path is None:\n            self.dict = [c for c in \"0123456789abcdefghijklmnopqrstuvwxyz\"]\n        else:\n            self.dict = []\n            with open(character_dict_path, \"r\") as f:\n                for line in f:\n                    c = line.rstrip(\"\\n\\r\")\n                    self.dict.append(c)\n\n    def clear(self):\n        self._correct_num = ms.Tensor(0, dtype=ms.int32)\n        self._total_num = ms.Tensor(0, dtype=ms.float32)  # avoid int divisor\n        self._norm_edit_dis = ms.Tensor(0.0, dtype=ms.float32)\n\n    def update(self, *inputs):\n\"\"\"\n        Updates the internal evaluation result\n\n        Args:\n            inputs (tuple): contain two elements preds, gt\n                    preds (dict): prediction output by postprocess, keys:\n                        - texts, List[str], batch of predicted text strings, shape [BS, ]\n                        - confs (optional), List[float], batch of confidence values for the prediction\n                    gt (tuple or list): ground truth, order defined by output_columns in eval dataloader.\n                        require element:\n                        gt_texts, for the grouth truth texts (padded to the fixed length), shape [BS, ]\n                        gt_lens (optional), length of original text if padded, shape [BS, ]\n\n        Raises:\n            ValueError: If the number of the inputs is not 2.\n        \"\"\"\n\n        if len(inputs) != 2:\n            raise ValueError(\"Length of inputs should be 2\")\n        preds, gt = inputs\n        pred_texts = preds[\"texts\"]\n        # pred_confs = preds['confs']\n        # print('pred: ', pred_texts, len(pred_texts))\n\n        # remove padded chars in GT\n        if isinstance(gt, tuple) or isinstance(gt, list):\n            gt_texts = gt[0]  # text string padded\n            gt_lens = gt[1]  # text length\n\n            if isinstance(gt_texts, ms.Tensor):\n                gt_texts = gt_texts.asnumpy()\n                gt_lens = gt_lens.asnumpy()\n\n            gt_texts = [gt_texts[i][:l] for i, l in enumerate(gt_lens)]\n        else:\n            gt_texts = gt\n            if isinstance(gt_texts, ms.Tensor):\n                gt_texts = gt_texts.asnumpy()\n\n        # print('2: ', gt_texts)\n        for pred, label in zip(pred_texts, gt_texts):\n            # print('pred', pred, 'END')\n            # print('label ', label, 'END')\n\n            if self.ignore_space:\n                pred = pred.replace(\" \", \"\")\n                label = label.replace(\" \", \"\")\n\n            if self.lower:  # convert to lower case\n                label = label.lower()\n                pred = pred.lower()\n\n            if self.filter_ood:  # filter out of dictionary characters\n                label = \"\".join([c for c in label if c in self.dict])\n\n            if self.print_flag:\n                print(pred, \" :: \", label)\n\n            edit_distance = Levenshtein.normalized_distance(pred, label)\n            self._norm_edit_dis += edit_distance\n            if pred == label:\n                self._correct_num += 1\n\n            self._total_num += 1\n\n    @ms_function\n    def all_reduce_fun(self, x):\n        res = self.all_reduce(x)\n        return res\n\n    def eval(self):\n        if self._total_num == 0:\n            raise RuntimeError(\"Accuary can not be calculated, because the number of samples is 0.\")\n        print(\"correct num: \", self._correct_num, \", total num: \", self._total_num)\n\n        if self.all_reduce:\n            # sum over all devices\n            correct_num = self.all_reduce_fun(self._correct_num)\n            norm_edit_dis = self.all_reduce_fun(self._norm_edit_dis)\n            total_num = self.all_reduce_fun(self._total_num)\n        else:\n            correct_num = self._correct_num\n            norm_edit_dis = self._norm_edit_dis\n            total_num = self._total_num\n\n        sequence_accurancy = float((correct_num / total_num).asnumpy())\n        norm_edit_distance = float((1 - norm_edit_dis / total_num).asnumpy())\n\n        return {\"acc\": sequence_accurancy, \"norm_edit_distance\": norm_edit_distance}\n</code></pre> <code>mindocr.metrics.rec_metrics.RecMetric.update(*inputs)</code> \u00b6 <p>Updates the internal evaluation result</p> PARAMETER DESCRIPTION <code>inputs</code> <p>contain two elements preds, gt     preds (dict): prediction output by postprocess, keys:         - texts, List[str], batch of predicted text strings, shape [BS, ]         - confs (optional), List[float], batch of confidence values for the prediction     gt (tuple or list): ground truth, order defined by output_columns in eval dataloader.         require element:         gt_texts, for the grouth truth texts (padded to the fixed length), shape [BS, ]         gt_lens (optional), length of original text if padded, shape [BS, ]</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the number of the inputs is not 2.</p> Source code in <code>mindocr\\metrics\\rec_metrics.py</code> <pre><code>def update(self, *inputs):\n\"\"\"\n    Updates the internal evaluation result\n\n    Args:\n        inputs (tuple): contain two elements preds, gt\n                preds (dict): prediction output by postprocess, keys:\n                    - texts, List[str], batch of predicted text strings, shape [BS, ]\n                    - confs (optional), List[float], batch of confidence values for the prediction\n                gt (tuple or list): ground truth, order defined by output_columns in eval dataloader.\n                    require element:\n                    gt_texts, for the grouth truth texts (padded to the fixed length), shape [BS, ]\n                    gt_lens (optional), length of original text if padded, shape [BS, ]\n\n    Raises:\n        ValueError: If the number of the inputs is not 2.\n    \"\"\"\n\n    if len(inputs) != 2:\n        raise ValueError(\"Length of inputs should be 2\")\n    preds, gt = inputs\n    pred_texts = preds[\"texts\"]\n    # pred_confs = preds['confs']\n    # print('pred: ', pred_texts, len(pred_texts))\n\n    # remove padded chars in GT\n    if isinstance(gt, tuple) or isinstance(gt, list):\n        gt_texts = gt[0]  # text string padded\n        gt_lens = gt[1]  # text length\n\n        if isinstance(gt_texts, ms.Tensor):\n            gt_texts = gt_texts.asnumpy()\n            gt_lens = gt_lens.asnumpy()\n\n        gt_texts = [gt_texts[i][:l] for i, l in enumerate(gt_lens)]\n    else:\n        gt_texts = gt\n        if isinstance(gt_texts, ms.Tensor):\n            gt_texts = gt_texts.asnumpy()\n\n    # print('2: ', gt_texts)\n    for pred, label in zip(pred_texts, gt_texts):\n        # print('pred', pred, 'END')\n        # print('label ', label, 'END')\n\n        if self.ignore_space:\n            pred = pred.replace(\" \", \"\")\n            label = label.replace(\" \", \"\")\n\n        if self.lower:  # convert to lower case\n            label = label.lower()\n            pred = pred.lower()\n\n        if self.filter_ood:  # filter out of dictionary characters\n            label = \"\".join([c for c in label if c in self.dict])\n\n        if self.print_flag:\n            print(pred, \" :: \", label)\n\n        edit_distance = Levenshtein.normalized_distance(pred, label)\n        self._norm_edit_dis += edit_distance\n        if pred == label:\n            self._correct_num += 1\n\n        self._total_num += 1\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models","title":"<code>mindocr.models</code>","text":""},{"location":"reference/api_doc/#mindocr.models.backbones","title":"<code>mindocr.models.backbones</code>","text":""},{"location":"reference/api_doc/#mindocr.models.backbones.builder","title":"<code>mindocr.models.backbones.builder</code>","text":"<code>mindocr.models.backbones.builder.build_backbone(name, **kwargs)</code> \u00b6 <p>Build the backbone network.</p> PARAMETER DESCRIPTION <code>name</code> <p>the backbone name, which can be a registered backbone class name             or a registered backbone (function) name.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>input args for the backbone 1) if <code>name</code> is in the registered backbones (e.g. det_resnet50), kwargs include args for backbone creating likes <code>pretrained</code> 2) if <code>name</code> is in the registered backbones class (e.g. DetResNet50), kwargs include args for the backbone configuration like <code>layers</code>. - pretrained: can be bool or str. If bool, load model weights from default url defined in the backbone py file. If str, pretrained can be url or local path to a checkpoint.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell for backbone module</p> Construct Example Source code in <code>mindocr\\models\\backbones\\builder.py</code> <pre><code>def build_backbone(name, **kwargs):\n\"\"\"\n    Build the backbone network.\n\n    Args:\n        name (str): the backbone name, which can be a registered backbone class name\n                        or a registered backbone (function) name.\n        kwargs (dict): input args for the backbone\n           1) if `name` is in the registered backbones (e.g. det_resnet50), kwargs include args for backbone creating\n           likes `pretrained`\n           2) if `name` is in the registered backbones class (e.g. DetResNet50), kwargs include args for the backbone\n           configuration like `layers`.\n           - pretrained: can be bool or str. If bool, load model weights from default url defined in the backbone py\n           file. If str, pretrained can be url or local path to a checkpoint.\n\n\n    Return:\n        nn.Cell for backbone module\n\n    Construct:\n        Input: Tensor\n        Output: List[Tensor]\n\n    Example:\n        &gt;&gt;&gt; # build using backbone function name\n        &gt;&gt;&gt; from mindocr.models.backbones import build_backbone\n        &gt;&gt;&gt; backbone = build_backbone('det_resnet50', pretrained=True)\n        &gt;&gt;&gt; # build using backbone class name\n        &gt;&gt;&gt; from mindocr.models.backbones.mindcv_models.resnet import Bottleneck\n        &gt;&gt;&gt; cfg_from_class = dict(name='DetResNet', Bottleneck, layers=[3,4,6,3])\n        &gt;&gt;&gt; backbone = build_backbone(**cfg_from_class)\n        &gt;&gt;&gt; print(backbone)\n    \"\"\"\n    remove_prefix = kwargs.pop(\"remove_prefix\", False)\n\n    if is_backbone(name):\n        create_fn = backbone_entrypoint(name)\n        backbone = create_fn(**kwargs)\n    elif is_backbone_class(name):\n        backbone_class = backbone_class_entrypoint(name)\n        backbone = backbone_class(**kwargs)\n    elif 'mindcv' in name:\n        # you can add `feature_only` parameter and `out_indices` in kwargs to extract intermediate features.\n        backbone = MindCVBackboneWrapper(name, **kwargs)\n    else:\n        raise ValueError(f'Invalid backbone name: {name}, supported backbones are: {list_backbones()}')\n\n    if 'pretrained' in kwargs:\n        pretrained = kwargs['pretrained']\n        if not isinstance(pretrained, bool):\n            if remove_prefix:\n                # remove the prefix with `backbone.`\n                def fn(x): return {k.replace('backbone.', ''): v for k, v in x.items()}\n            else:\n                fn = None\n            load_model(backbone, pretrained, filter_fn=fn)\n        # No need to load again if pretrained is bool and True, because pretrained backbone is already loaded\n        # in the backbone definition function.')\n\n    return backbone\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.backbones.builder.build_backbone--build-using-backbone-function-name","title":"build using backbone function name","text":"<p>from mindocr.models.backbones import build_backbone backbone = build_backbone('det_resnet50', pretrained=True)</p>"},{"location":"reference/api_doc/#mindocr.models.backbones.builder.build_backbone--build-using-backbone-class-name","title":"build using backbone class name","text":"<p>from mindocr.models.backbones.mindcv_models.resnet import Bottleneck cfg_from_class = dict(name='DetResNet', Bottleneck, layers=[3,4,6,3]) backbone = build_backbone(**cfg_from_class) print(backbone)</p>"},{"location":"reference/api_doc/#mindocr.models.backbones.cls_mobilenet_v3","title":"<code>mindocr.models.backbones.cls_mobilenet_v3</code>","text":"<code>mindocr.models.backbones.cls_mobilenet_v3.cls_mobilenet_v3_small_100(pretrained=True, in_channels=3, **kwargs)</code> \u00b6 <p>Get small MobileNetV3 model without width scaling.</p> Source code in <code>mindocr\\models\\backbones\\cls_mobilenet_v3.py</code> <pre><code>@register_backbone\ndef cls_mobilenet_v3_small_100(pretrained: bool = True, in_channels: int = 3, **kwargs):\n\"\"\"Get small MobileNetV3 model without width scaling.\n    \"\"\"\n    model = ClsMobileNetV3(arch=\"small\", alpha=1.0, in_channels=in_channels, **kwargs)\n\n    # load pretrained weights\n    if pretrained:\n        default_cfg = default_cfgs['mobilenet_v3_small_1.0']\n        load_pretrained(model, default_cfg)\n\n    return model\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.backbones.det_mobilenet","title":"<code>mindocr.models.backbones.det_mobilenet</code>","text":""},{"location":"reference/api_doc/#mindocr.models.backbones.det_resnet","title":"<code>mindocr.models.backbones.det_resnet</code>","text":""},{"location":"reference/api_doc/#mindocr.models.backbones.mindcv_models","title":"<code>mindocr.models.backbones.mindcv_models</code>","text":"<p>models init</p> <code>mindocr.models.backbones.mindcv_models.bit</code> \u00b6 <p>MindSpore implementation of <code>BiT_ResNet</code>. Refer to Big Transfer (BiT): General Visual Representation Learning.</p> <code>mindocr.models.backbones.mindcv_models.bit.BiT_ResNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>BiT_ResNet model class, based on <code>\"Big Transfer (BiT): General Visual Representation Learning\" &lt;https://arxiv.org/abs/1912.11370&gt;</code>_</p> PARAMETER DESCRIPTION <code>block(Union[Bottleneck])</code> <p>block of BiT_ResNetv2.</p> <p> </p> <code>layers(tuple(int))</code> <p>number of layers of each stage.</p> <p> </p> <code>wf(int)</code> <p>width of each layer. Default: 1.</p> <p> </p> <code>num_classes(int)</code> <p>number of classification classes. Default: 1000.</p> <p> </p> <code>in_channels(int)</code> <p>number the channels of the input. Default: 3.</p> <p> </p> <code>groups(int)</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> </p> <code>base_width(int)</code> <p>base width of pre group hidden channel in blocks. Default: 64.</p> <p> </p> <code>norm(nn.Cell)</code> <p>normalization layer in blocks. Default: None.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>class BiT_ResNet(nn.Cell):\nr\"\"\"BiT_ResNet model class, based on\n    `\"Big Transfer (BiT): General Visual Representation Learning\" &lt;https://arxiv.org/abs/1912.11370&gt;`_\n    Args:\n        block(Union[Bottleneck]): block of BiT_ResNetv2.\n        layers(tuple(int)): number of layers of each stage.\n        wf(int): width of each layer. Default: 1.\n        num_classes(int): number of classification classes. Default: 1000.\n        in_channels(int): number the channels of the input. Default: 3.\n        groups(int): number of groups for group conv in blocks. Default: 1.\n        base_width(int): base width of pre group hidden channel in blocks. Default: 64.\n        norm(nn.Cell): normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[Bottleneck]],\n        layers: List[int],\n        wf: int = 1,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if norm is None:\n            norm = nn.GroupNorm\n\n        self.norm: nn.Cell = norm  # add type hints to make pylint happy\n        self.input_channels = 64 * wf\n        self.groups = groups\n        self.base_with = base_width\n\n        self.conv1 = StdConv2d(in_channels, self.input_channels, kernel_size=7,\n                               stride=2, pad_mode=\"pad\", padding=3)\n        self.pad = nn.ConstantPad2d(1, 0)\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"valid\")\n\n        self.layer1 = self._make_layer(block, 64 * wf, layers[0])\n        self.layer2 = self._make_layer(block, 128 * wf, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256 * wf, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512 * wf, layers[3], stride=2)\n\n        self.gn = norm(32, 2048 * wf)\n        self.relu = nn.ReLU()\n        self.pool = GlobalAvgPooling(keep_dims=True)\n        self.classifier = nn.Conv2d(512 * block.expansion * wf, num_classes, kernel_size=1, has_bias=True)\n\n    def _make_layer(\n        self,\n        block: Type[Union[Bottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n\"\"\"build model depending on cfgs\"\"\"\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                StdConv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def root(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.pad(x)\n        x = self.max_pool(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.root(x)\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n        return x[..., 0, 0]\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.BiT_ResNet.forward_features(x)</code> \u00b6 <p>Network forward feature extraction.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the basic block of BiT</p> PARAMETER DESCRIPTION <code>in_channels(int)</code> <p>The channel number of the input tensor of the Conv2d layer.</p> <p> </p> <code>channels(int)</code> <p>The channel number of the output tensor of the middle Conv2d layer.</p> <p> </p> <code>stride(int)</code> <p>The movement stride of the 2D convolution kernel. Default: 1.</p> <p> </p> <code>groups(int)</code> <p>Number of groups for group conv in blocks. Default: 1.</p> <p> </p> <code>base_width(int)</code> <p>Base width of pre group hidden channel in blocks. Default: 64.</p> <p> </p> <code>norm(nn.Cell)</code> <p>Normalization layer in blocks. Default: None.</p> <p> </p> <code>down_sample(nn.Cell)</code> <p>Down sample in blocks. Default: None.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"define the basic block of BiT\n    Args:\n          in_channels(int): The channel number of the input tensor of the Conv2d layer.\n          channels(int): The channel number of the output tensor of the middle Conv2d layer.\n          stride(int): The movement stride of the 2D convolution kernel. Default: 1.\n          groups(int): Number of groups for group conv in blocks. Default: 1.\n          base_width(int): Base width of pre group hidden channel in blocks. Default: 64.\n          norm(nn.Cell): Normalization layer in blocks. Default: None.\n          down_sample(nn.Cell): Down sample in blocks. Default: None.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.GroupNorm\n\n        width = int(channels * (base_width / 64.0)) * groups\n        self.gn1 = norm(32, in_channels)\n        self.conv1 = StdConv2d(in_channels, width, kernel_size=1, stride=1)\n        self.gn2 = norm(32, width)\n        self.conv2 = StdConv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, pad_mode=\"pad\", group=groups)\n        self.gn3 = norm(32, width)\n        self.conv3 = StdConv2d(width, channels * self.expansion,\n                               kernel_size=1, stride=1)\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n        out = self.gn1(x)\n        out = self.relu(out)\n\n        residual = out\n\n        out = self.conv1(out)\n\n        out = self.gn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.gn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(residual)\n\n        out += identity\n        # out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.StdConv2d</code> \u00b6 <p>         Bases: <code>nn.Conv2d</code></p> <p>Conv2d with Weight Standardization</p> PARAMETER DESCRIPTION <code>in_channels(int)</code> <p>The channel number of the input tensor of the Conv2d layer.</p> <p> </p> <code>out_channels(int)</code> <p>The channel number of the output tensor of the Conv2d layer.</p> <p> </p> <code>kernel_size(int)</code> <p>Specifies the height and width of the 2D convolution kernel.</p> <p> </p> <code>stride(int)</code> <p>The movement stride of the 2D convolution kernel. Default: 1.</p> <p> </p> <code>pad_mode(str)</code> <p>Specifies padding mode. The optional values are \"same\", \"valid\", \"pad\". Default: \"same\".</p> <p> </p> <code>padding(int)</code> <p>The number of padding on the height and width directions of the input. Default: 0.</p> <p> </p> <code>group(int)</code> <p>Splits filter into groups. Default: 1.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>class StdConv2d(nn.Conv2d):\nr\"\"\"Conv2d with Weight Standardization\n    Args:\n        in_channels(int): The channel number of the input tensor of the Conv2d layer.\n        out_channels(int): The channel number of the output tensor of the Conv2d layer.\n        kernel_size(int): Specifies the height and width of the 2D convolution kernel.\n        stride(int): The movement stride of the 2D convolution kernel. Default: 1.\n        pad_mode(str): Specifies padding mode. The optional values are \"same\", \"valid\", \"pad\". Default: \"same\".\n        padding(int): The number of padding on the height and width directions of the input. Default: 0.\n        group(int): Splits filter into groups. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        pad_mode=\"same\",\n        padding=0,\n        group=1,\n    ) -&gt; None:\n        super(StdConv2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            pad_mode,\n            padding,\n            group,\n        )\n        self.mean_op = ops.ReduceMean(keep_dims=True)\n\n    def construct(self, x):\n        w = self.weight\n        m = self.mean_op(w, [1, 2, 3])\n        v = w.var((1, 2, 3), keepdims=True)\n        w = (w - m) / mindspore.ops.sqrt(v + 1e-10)\n        output = self.conv2d(x, w)\n        return output\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.BiTresnet101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>@register_model\ndef BiTresnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 101 layers ResNet model.\n    Refer to the base class `models.BiT_Resnet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"BiTresnet101\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.BiTresnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>@register_model\ndef BiTresnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 50 layers ResNet model.\n    Refer to the base class `models.BiT_Resnet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"BiTresnet50\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.bit.BiTresnet50x3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\bit.py</code> <pre><code>@register_model\ndef BiTresnet50x3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 50 layers ResNet model.\n     Refer to the base class `models.BiT_Resnet` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"BiTresnet50x3\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 6, 3], wf=3, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.cait</code> \u00b6 <p>MindSpore implementation of <code>CaiT</code>. Refer to Going deeper with Image Transformers.</p> <code>mindocr.models.backbones.mindcv_models.cait.AttentionTalkingHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Talking head is a trick for multi-head attention, which has two more linear map before and after the softmax compared to normal attention.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\cait.py</code> <pre><code>class AttentionTalkingHead(nn.Cell):\n\"\"\"\n    Talking head is a trick for multi-head attention,\n    which has two more linear map before and after\n    the softmax compared to normal attention.\n    \"\"\"\n    def __init__(self,\n                 dim: int,\n                 num_heads: int = 8,\n                 qkv_bias: bool = False,\n                 qk_scale: float = None,\n                 attn_drop_rate: float = 0.,\n                 proj_drop_rate: float = 0.) -&gt; None:\n        super(AttentionTalkingHead, self).__init__()\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads.\"\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Dense(dim, dim * 3, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(1 - attn_drop_rate)\n\n        self.proj = nn.Dense(dim, dim, has_bias=False)\n\n        self.proj_l = nn.Dense(num_heads, num_heads, has_bias=False)\n        self.proj_w = nn.Dense(num_heads, num_heads, has_bias=False)\n\n        self.proj_drop = nn.Dropout(1 - proj_drop_rate)\n\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n\n    def construct(self, x) -&gt; Tensor:\n        B, N, C = x.shape\n        qkv = ops.reshape(self.qkv(x), (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n        q = ops.mul(q, self.scale)\n\n        attn = self.q_matmul_k(q, k)\n\n        attn = ops.transpose(attn, (0, 2, 3, 1))\n        attn = self.proj_l(attn)\n        attn = ops.transpose(attn, (0, 3, 1, 2))\n        attn = self.softmax(attn)\n        attn = ops.transpose(attn, (0, 2, 3, 1))\n        attn = self.proj_w(attn)\n        attn = ops.transpose(attn, (0, 3, 1, 2))\n\n        attn = self.attn_drop(attn)\n\n        x = self.attn_matmul_v(attn, v)\n        x = ops.transpose(x, (0, 2, 1, 3))\n        x = ops.reshape(x, (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat</code> \u00b6 <p>CoaT architecture. Modified from timm/models/vision_transformer.py</p> <code>mindocr.models.backbones.mindcv_models.coat.CoaT</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>CoaT class.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class CoaT(nn.Cell):\n\"\"\" CoaT class. \"\"\"\n\n    def __init__(\n        self,\n        image_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dims=[0, 0, 0, 0],\n        serial_depths=[0, 0, 0, 0],\n        parallel_depth=0,\n        num_heads=0,\n        mlp_ratios=[0, 0, 0, 0],\n        qkv_bias=True,\n        drop_rate=0.,\n        attn_drop_rate=0.,\n        drop_path_rate=0.,\n        return_interm_layers=False,\n        out_features=None,\n        crpe_window={3: 2, 5: 3, 7: 3},\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n        self.return_interm_layers = return_interm_layers\n        self.out_features = out_features\n        self.num_classes = num_classes\n\n        self.patch_embed1 = PatchEmbed(image_size=image_size, patch_size=patch_size,\n                                       in_chans=in_chans, embed_dim=embed_dims[0])\n        self.patch_embed2 = PatchEmbed(image_size=image_size // (2**2), patch_size=2,\n                                       in_chans=embed_dims[0], embed_dim=embed_dims[1])\n        self.patch_embed3 = PatchEmbed(image_size=image_size // (2**3), patch_size=2,\n                                       in_chans=embed_dims[1], embed_dim=embed_dims[2])\n        self.patch_embed4 = PatchEmbed(image_size=image_size // (2**4), patch_size=2,\n                                       in_chans=embed_dims[2], embed_dim=embed_dims[3])\n\n        self.cls_token1 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[0]), mindspore.float32))\n        self.cls_token2 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[1]), mindspore.float32))\n        self.cls_token3 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[2]), mindspore.float32))\n        self.cls_token4 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[3]), mindspore.float32))\n\n        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n\n        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] // num_heads, h=num_heads, window=crpe_window)\n\n        dpr = drop_path_rate\n\n        self.serial_blocks1 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe1, shared_crpe=self.crpe1\n            )\n            for _ in range(serial_depths[0])]\n        )\n\n        self.serial_blocks2 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe2, shared_crpe=self.crpe2\n            )\n            for _ in range(serial_depths[1])]\n        )\n\n        self.serial_blocks3 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe3, shared_crpe=self.crpe3\n            )\n            for _ in range(serial_depths[2])]\n        )\n\n        self.serial_blocks4 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe4, shared_crpe=self.crpe4\n            )\n            for _ in range(serial_depths[3])]\n        )\n\n        self.parallel_depth = parallel_depth\n        if self.parallel_depth &gt; 0:\n            self.parallel_blocks = nn.CellList([\n                ParallelBlock(dims=embed_dims,\n                              num_heads=num_heads,\n                              mlp_ratios=mlp_ratios,\n                              qkv_bias=qkv_bias,\n                              drop=drop_rate,\n                              attn_drop=attn_drop_rate,\n                              drop_path=dpr,\n                              shared_cpes=[self.cpe1, self.cpe2, self.cpe3, self.cpe4],\n                              shared_crpes=[self.crpe1, self.crpe2, self.crpe3, self.crpe4]\n                              )\n                for _ in range(parallel_depth)]\n            )\n        else:\n            self.parallel_blocks = None\n\n        if not self.return_interm_layers:\n            if self.parallel_blocks is not None:\n                self.norm2 = nn.LayerNorm((embed_dims[1],), epsilon=1e-6)\n                self.norm3 = nn.LayerNorm((embed_dims[2],), epsilon=1e-6)\n            else:\n                self.norm2 = None\n                self.norm3 = None\n\n            self.norm4 = nn.LayerNorm((embed_dims[3],), epsilon=1e-6)\n\n            if self.parallel_depth &gt; 0:\n                self.aggregate = nn.Conv1d(in_channels=3,\n                                           out_channels=1,\n                                           kernel_size=1,\n                                           has_bias=True)\n                self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n            else:\n                self.aggregate = None\n                self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n\n        self.cls_token1.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token1.data.shape))\n        self.cls_token2.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token2.data.shape))\n        self.cls_token3.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token3.data.shape))\n        self.cls_token4.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token4.data.shape))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1.0), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n\n    def insert_cls(self, x, cls_token) -&gt; Tensor:\n        t0 = x.shape[0]\n        t1 = cls_token.shape[1]\n        t2 = cls_token.shape[2]\n        y = Tensor(np.ones((t0, t1, t2)))\n        cls_tokens = cls_token.expand_as(y)\n\n        x = ops.concat((cls_tokens, x), axis=1)\n        return x\n\n    def remove_cls(self, x: Tensor) -&gt; Tensor:\n        return x[:, 1:, :]\n\n    def forward_features(self, x0: Tensor) -&gt; Union[dict, Tensor]:\n        B = x0.shape[0]\n\n        x1 = self.patch_embed1(x0)\n        H1, W1 = self.patch_embed1.patches_resolution\n        x1 = self.insert_cls(x1, self.cls_token1)\n        for blk in self.serial_blocks1:\n            x1 = blk(x1, size=(H1, W1))\n        x1_nocls = self.remove_cls(x1)\n        x1_nocls = ops.reshape(x1_nocls, (B, H1, W1, -1))\n        x1_nocls = ops.transpose(x1_nocls, (0, 3, 1, 2))\n\n        x2 = self.patch_embed2(x1_nocls)\n        H2, W2 = self.patch_embed2.patches_resolution\n        x2 = self.insert_cls(x2, self.cls_token2)\n        for blk in self.serial_blocks2:\n            x2 = blk(x2, size=(H2, W2))\n        x2_nocls = self.remove_cls(x2)\n        x2_nocls = ops.reshape(x2_nocls, (B, H2, W2, -1))\n        x2_nocls = ops.transpose(x2_nocls, (0, 3, 1, 2))\n\n        x3 = self.patch_embed3(x2_nocls)\n        H3, W3 = self.patch_embed3.patches_resolution\n        x3 = self.insert_cls(x3, self.cls_token3)\n        for blk in self.serial_blocks3:\n            x3 = blk(x3, size=(H3, W3))\n        x3_nocls = self.remove_cls(x3)\n        x3_nocls = ops.reshape(x3_nocls, (B, H3, W3, -1))\n        x3_nocls = ops.transpose(x3_nocls, (0, 3, 1, 2))\n\n        x4 = self.patch_embed4(x3_nocls)\n        H4, W4 = self.patch_embed4.patches_resolution\n        x4 = self.insert_cls(x4, self.cls_token4)\n        for blk in self.serial_blocks4:\n            x4 = blk(x4, size=(H4, W4))\n        x4_nocls = self.remove_cls(x4)\n        x4_nocls = ops.reshape(x4_nocls, (B, H4, W4, -1))\n        x4_nocls = ops.transpose(x4_nocls, (0, 3, 1, 2))\n\n        if self.parallel_depth == 0:\n            if self.return_interm_layers:\n                feat_out = {}\n                if 'x1_nocls' in self.out_features:\n                    feat_out['x1_nocls'] = x1_nocls\n                if 'x2_nocls' in self.out_features:\n                    feat_out['x2_nocls'] = x2_nocls\n                if 'x3_nocls' in self.out_features:\n                    feat_out['x3_nocls'] = x3_nocls\n                if 'x4_nocls' in self.out_features:\n                    feat_out['x4_nocls'] = x4_nocls\n                return feat_out\n            else:\n                x4 = self.norm4(x4)\n                x4_cls = x4[:, 0]\n                return x4_cls\n\n        for blk in self.parallel_blocks:\n            x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4)])\n\n        if self.return_interm_layers:\n            feat_out = {}\n            if 'x1_nocls' in self.out_features:\n                x1_nocls = x1[:, 1:, :].reshape((B, H1, W1, -1)).transpose((0, 3, 1, 2))\n                feat_out['x1_nocls'] = x1_nocls\n            if 'x2_nocls' in self.out_features:\n                x2_nocls = x2[:, 1:, :].reshape((B, H2, W2, -1)).transpose((0, 3, 1, 2))\n                feat_out['x2_nocls'] = x2_nocls\n            if 'x3_nocls' in self.out_features:\n                x3_nocls = x3[:, 1:, :].reshape((B, H3, W3, -1)).transpose((0, 3, 1, 2))\n                feat_out['x3_nocls'] = x3_nocls\n            if 'x4_nocls' in self.out_features:\n                x4_nocls = x4[:, 1:, :].reshape((B, H4, W4, -1)).transpose((0, 3, 1, 2))\n                feat_out['x4_nocls'] = x4_nocls\n            return feat_out\n        else:\n            x2 = self.norm2(x2)\n            x3 = self.norm3(x3)\n            x4 = self.norm4(x4)\n            x2_cls = x2[:, :1]\n            x3_cls = x3[:, :1]\n            x4_cls = x4[:, :1]\n            merged_cls = ops.concat((x2_cls, x3_cls, x4_cls), axis=1)\n            merged_cls = self.aggregate(merged_cls).squeeze(axis=1)\n            return merged_cls\n\n    def construct(self, x: Tensor) -&gt; Union[dict, Tensor]:\n        if self.return_interm_layers:\n            return self.forward_features(x)\n        else:\n            x = self.forward_features(x)\n            x = self.head(x)\n            return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.ConvPosEnc</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Convolutional Position Encoding. Note: This module is similar to the conditional position encoding in CPVT.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class ConvPosEnc(nn.Cell):\n\"\"\" Convolutional Position Encoding.\n        Note: This module is similar to the conditional position encoding in CPVT.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        k=3\n    ) -&gt; None:\n        super(ConvPosEnc, self).__init__()\n        self.proj = nn.Conv2d(in_channels=dim,\n                              out_channels=dim,\n                              kernel_size=k,\n                              stride=1,\n                              padding=k // 2,\n                              group=dim,\n                              pad_mode='pad',\n                              has_bias=True)\n\n    def construct(self, x, size) -&gt; Tensor:\n        B, N, C = x.shape\n        H, W = size\n\n        cls_token, img_tokens = x[:, :1], x[:, 1:]\n\n        feat = ops.transpose(img_tokens, (0, 2, 1))\n        feat = ops.reshape(feat, (B, C, H, W))\n        x = ops.add(self.proj(feat), feat)\n\n        x = ops.reshape(x, (B, C, H * W))\n        x = ops.transpose(x, (0, 2, 1))\n\n        x = ops.concat((cls_token, x), axis=1)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.FactorAtt_ConvRelPosEnc</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Factorized attention with convolutional relative position encoding class.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class FactorAtt_ConvRelPosEnc(nn.Cell):\n\"\"\"Factorized attention with convolutional relative position encoding class.\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        attn_drop=0.,\n        proj_drop=0.,\n        shared_crpe=None\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(keep_prob=1 - attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = nn.Dropout(keep_prob=1 - proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_matmul = ops.BatchMatMul()\n\n        self.crpe = shared_crpe\n\n    def construct(self, x, size) -&gt; Tensor:\n        B, N, C = x.shape\n        q = ops.reshape(self.q(x), (B, N, self.num_heads, C // self.num_heads))\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (B, N, self.num_heads, C // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (B, N, self.num_heads, C // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        k_softmax = self.softmax(k)\n        factor_att = self.batch_matmul(q, k_softmax)\n        factor_att = self.batch_matmul(factor_att, v)\n\n        crpe = self.crpe(q, v, size=size)\n\n        x = ops.mul(self.scale, factor_att)\n        x = ops.add(x, crpe)\n        x = ops.transpose(x, (0, 2, 1, 3))\n        x = ops.reshape(x, (B, N, C))\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.Mlp</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP Cell</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class Mlp(nn.Cell):\n\"\"\"MLP Cell\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        drop=0.0\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = nn.GELU(approximate=False)\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = nn.Dropout(keep_prob=1.0 - drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.ParallelBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Parallel block class.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class ParallelBlock(nn.Cell):\n\"\"\" Parallel block class. \"\"\"\n\n    def __init__(\n        self,\n        dims,\n        num_heads,\n        mlp_ratios=[],\n        qkv_bias=False,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        shared_cpes=None,\n        shared_crpes=None\n    ) -&gt; None:\n        super().__init__()\n\n        self.cpes = shared_cpes\n\n        self.norm12 = nn.LayerNorm((dims[1],), epsilon=1e-6)\n        self.norm13 = nn.LayerNorm((dims[2],), epsilon=1e-6)\n        self.norm14 = nn.LayerNorm((dims[3],), epsilon=1e-6)\n        self.factoratt_crpe2 = FactorAtt_ConvRelPosEnc(dims[1],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[1]\n                                                       )\n        self.factoratt_crpe3 = FactorAtt_ConvRelPosEnc(dims[2],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[2]\n                                                       )\n        self.factoratt_crpe4 = FactorAtt_ConvRelPosEnc(dims[3],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[3]\n                                                       )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else Identity()\n\n        self.norm22 = nn.LayerNorm((dims[1],), epsilon=1e-6)\n        self.norm23 = nn.LayerNorm((dims[2],), epsilon=1e-6)\n        self.norm24 = nn.LayerNorm((dims[3],), epsilon=1e-6)\n\n        mlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n        self.mlp2 = self.mlp3 = self.mlp4 = Mlp(in_features=dims[1], hidden_features=mlp_hidden_dim, drop=drop)\n\n    def upsample(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map up-sampling. \"\"\"\n        return self.interpolate(x, output_size=output_size, size=size)\n\n    def downsample(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map down-sampling. \"\"\"\n        return self.interpolate(x, output_size=output_size, size=size)\n\n    def interpolate(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map interpolation. \"\"\"\n        B, N, C = x.shape\n        H, W = size\n\n        cls_token = x[:, :1, :]\n        img_tokens = x[:, 1:, :]\n\n        img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n        img_tokens = ops.reshape(img_tokens, (B, C, H, W))\n        img_tokens = ops.interpolate(img_tokens,\n                                     sizes=output_size,\n                                     mode='bilinear'\n                                     )\n        img_tokens = ops.reshape(img_tokens, (B, C, -1))\n        img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n\n        out = ops.concat((cls_token, img_tokens), axis=1)\n        return out\n\n    def construct(self, x1, x2, x3, x4, sizes) -&gt; tuple:\n        _, (H2, W2), (H3, W3), (H4, W4) = sizes\n\n        # Conv-Attention.\n        x2 = self.cpes[1](x2, size=(H2, W2))  # Note: x1 is ignored.\n        x3 = self.cpes[2](x3, size=(H3, W3))\n        x4 = self.cpes[3](x4, size=(H4, W4))\n\n        cur2 = self.norm12(x2)\n        cur3 = self.norm13(x3)\n        cur4 = self.norm14(x4)\n        cur2 = self.factoratt_crpe2(cur2, size=(H2, W2))\n        cur3 = self.factoratt_crpe3(cur3, size=(H3, W3))\n        cur4 = self.factoratt_crpe4(cur4, size=(H4, W4))\n        upsample3_2 = self.upsample(cur3, output_size=(H2, W2), size=(H3, W3))\n        upsample4_3 = self.upsample(cur4, output_size=(H3, W3), size=(H4, W4))\n        upsample4_2 = self.upsample(cur4, output_size=(H2, W2), size=(H4, W4))\n        downsample2_3 = self.downsample(cur2, output_size=(H3, W3), size=(H2, W2))\n        downsample3_4 = self.downsample(cur3, output_size=(H4, W4), size=(H3, W3))\n        downsample2_4 = self.downsample(cur2, output_size=(H4, W4), size=(H2, W2))\n        cur2 = cur2 + upsample3_2 + upsample4_2\n        cur3 = cur3 + upsample4_3 + downsample2_3\n        cur4 = cur4 + downsample3_4 + downsample2_4\n        x2 = x2 + self.drop_path(cur2)\n        x3 = x3 + self.drop_path(cur3)\n        x4 = x4 + self.drop_path(cur4)\n\n        cur2 = self.norm22(x2)\n        cur3 = self.norm23(x3)\n        cur4 = self.norm24(x4)\n        cur2 = self.mlp2(cur2)\n        cur3 = self.mlp3(cur3)\n        cur4 = self.mlp4(cur4)\n        x2 = x2 + self.drop_path(cur2)\n        x3 = x3 + self.drop_path(cur3)\n        x4 = x4 + self.drop_path(cur4)\n\n        return x1, x2, x3, x4\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.ParallelBlock.downsample(x, output_size, size)</code> \u00b6 <p>Feature map down-sampling.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>def downsample(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map down-sampling. \"\"\"\n    return self.interpolate(x, output_size=output_size, size=size)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.ParallelBlock.interpolate(x, output_size, size)</code> \u00b6 <p>Feature map interpolation.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>def interpolate(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map interpolation. \"\"\"\n    B, N, C = x.shape\n    H, W = size\n\n    cls_token = x[:, :1, :]\n    img_tokens = x[:, 1:, :]\n\n    img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n    img_tokens = ops.reshape(img_tokens, (B, C, H, W))\n    img_tokens = ops.interpolate(img_tokens,\n                                 sizes=output_size,\n                                 mode='bilinear'\n                                 )\n    img_tokens = ops.reshape(img_tokens, (B, C, -1))\n    img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n\n    out = ops.concat((cls_token, img_tokens), axis=1)\n    return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.ParallelBlock.upsample(x, output_size, size)</code> \u00b6 <p>Feature map up-sampling.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>def upsample(self, x, output_size, size) -&gt; Tensor:\n\"\"\" Feature map up-sampling. \"\"\"\n    return self.interpolate(x, output_size=output_size, size=size)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\" Image to Patch Embedding \"\"\"\n\n    def __init__(\n        self,\n        image_size=224,\n        patch_size=4,\n        in_chans=3,\n        embed_dim=96\n    ) -&gt; None:\n        super().__init__()\n        image_size = (image_size, image_size)\n        patch_size = (patch_size, patch_size)\n        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans,\n                              out_channels=embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size,\n                              pad_mode='valid',\n                              has_bias=True)\n\n        self.norm = nn.LayerNorm((embed_dim,), epsilon=1e-5)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B = x.shape[0]\n\n        x = ops.reshape(self.proj(x), (B, self.embed_dim, -1))\n        x = ops.transpose(x, (0, 2, 1))\n        x = self.norm(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.coat.SerialBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Serial block class.     Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\coat.py</code> <pre><code>class SerialBlock(nn.Cell):\n\"\"\"\n    Serial block class.\n        Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.,\n        qkv_bias=False,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        shared_cpe=None,\n        shared_crpe=None\n    ) -&gt; None:\n        super().__init__()\n\n        self.cpe = shared_cpe\n\n        self.norm1 = nn.LayerNorm((dim,), epsilon=1e-6)\n        self.factoratt_crpe = FactorAtt_ConvRelPosEnc(dim,\n                                                      num_heads=num_heads,\n                                                      qkv_bias=qkv_bias,\n                                                      attn_drop=attn_drop,\n                                                      proj_drop=drop,\n                                                      shared_crpe=shared_crpe\n                                                      )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else Identity()\n\n        self.norm2 = nn.LayerNorm((dim,), epsilon=1e-6)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n\n    def construct(self, x, size) -&gt; Tensor:\n        x = x + self.drop_path(self.factoratt_crpe(self.norm1(self.cpe(x, size)), size))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit</code> \u00b6 <p>MindSpore implementation of <code>ConViT</code>. Refer to ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</p> <code>mindocr.models.backbones.mindcv_models.convit.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic module of ConViT</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>class Block(nn.Cell):\n\"\"\"Basic module of ConViT\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float,\n        qkv_bias: bool = False,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        use_gpsa: bool = True,\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm((dim,))\n        if use_gpsa:\n            self.attn = GPSA(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                             attn_drop=attn_drop, proj_drop=drop, **kwargs)\n        else:\n            self.attn = MHSA(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                             attn_drop=attn_drop, proj_drop=drop, **kwargs)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = nn.LayerNorm((dim,))\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU, drop=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.ConViT</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ConViT model class, based on '\"Improving Vision Transformers with Soft Convolutional Inductive Biases\" https://arxiv.org/pdf/2103.10697.pdf'</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>image_size</code> <p>images input size. Default: 224.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>image patch size. Default: 16.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>16</code> </p> <code>embed_dim</code> <p>embedding dimension in all head. Default: 48.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>48</code> </p> <code>num_heads</code> <p>number of heads. Default: 12.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>12</code> </p> <code>drop_rate</code> <p>dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>drop path rate. Default: 0.1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.1</code> </p> <code>depth</code> <p>model block depth. Default: 12.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>12</code> </p> <code>mlp_ratio</code> <p>ratio of hidden features in Mlp. Default: 4.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>have bias in qkv layers or not. Default: False.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>attn_drop_rate</code> <p>attention layers dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>locality_strength</code> <p>determines how focused each head is around its attention center. Default: 1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>1.0</code> </p> <code>local_up_to_layer</code> <p>number of GPSA layers. Default: 10.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>10</code> </p> <code>use_pos_embed</code> <p>whether use the embeded position.  Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>locality_strength\uff08float\uff09</code> <p>the strength of locality. Default: 1.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>class ConViT(nn.Cell):\nr\"\"\"ConViT model class, based on\n    '\"Improving Vision Transformers with Soft Convolutional Inductive Biases\"\n    &lt;https://arxiv.org/pdf/2103.10697.pdf&gt;'\n\n    Args:\n        in_channels (int): number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        image_size (int) : images input size. Default: 224.\n        patch_size (int) : image patch size. Default: 16.\n        embed_dim (int) : embedding dimension in all head. Default: 48.\n        num_heads (int) : number of heads. Default: 12.\n        drop_rate (float) : dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.1.\n        depth (int) : model block depth. Default: 12.\n        mlp_ratio (float) : ratio of hidden features in Mlp. Default: 4.\n        qkv_bias (bool) : have bias in qkv layers or not. Default: False.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        locality_strength (float) : determines how focused each head is around its attention center. Default: 1.\n        local_up_to_layer (int) : number of GPSA layers. Default: 10.\n        use_pos_embed (bool): whether use the embeded position.  Default: True.\n        locality_strength\uff08float\uff09: the strength of locality. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        image_size: int = 224,\n        patch_size: int = 16,\n        embed_dim: int = 48,\n        num_heads: int = 12,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        depth: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        attn_drop_rate: float = 0.0,\n        local_up_to_layer: int = 10,\n        use_pos_embed: bool = True,\n        locality_strength: float = 1.0,\n    ) -&gt; None:\n        super().__init__()\n\n        self.local_up_to_layer = local_up_to_layer\n        self.use_pos_embed = use_pos_embed\n        self.num_heads = num_heads\n        self.locality_strength = locality_strength\n        self.embed_dim = embed_dim\n\n        self.patch_embed = PatchEmbed(\n            image_size=image_size, patch_size=patch_size, in_chans=in_channels, embed_dim=embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n\n        self.cls_token = Parameter(ops.Zeros()((1, 1, embed_dim), ms.float32))\n        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n\n        if self.use_pos_embed:\n            self.pos_embed = Parameter(ops.Zeros()((1, self.num_patches, embed_dim), ms.float32))\n            self.pos_embed.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), self.pos_embed.data.shape))\n\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.CellList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                use_gpsa=True)\n            if i &lt; local_up_to_layer else\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                use_gpsa=False)\n            for i in range(depth)])\n        self.norm = nn.LayerNorm((embed_dim,))\n\n        self.classifier = nn.Dense(in_channels=embed_dim, out_channels=num_classes) if num_classes &gt; 0 else Identity()\n        self.cls_token.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), self.cls_token.data.shape))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n        # local init\n        for i in range(self.local_up_to_layer):\n            self.blocks[i].attn.v.weight.set_data(ops.eye(self.embed_dim, self.embed_dim, ms.float32), slice_shape=True)\n            locality_distance = 1\n            kernel_size = int(self.num_heads**0.5)\n            center = (kernel_size - 1) / 2 if kernel_size % 2 == 0 else kernel_size // 2\n            pos_weight_data = self.blocks[i].attn.pos_proj.weight.data\n            for h1 in range(kernel_size):\n                for h2 in range(kernel_size):\n                    position = h1 + kernel_size * h2\n                    pos_weight_data[position, 2] = -1\n                    pos_weight_data[position, 1] = 2 * (h1 - center) * locality_distance\n                    pos_weight_data[position, 0] = 2 * (h2 - center) * locality_distance\n            pos_weight_data = pos_weight_data * self.locality_strength\n            self.blocks[i].attn.pos_proj.weight.set_data(pos_weight_data)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        if self.use_pos_embed:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        cls_tokens = ops.tile(self.cls_token, (x.shape[0], 1, 1))\n        for u, blk in enumerate(self.blocks):\n            if u == self.local_up_to_layer:\n                x = ops.Cast()(x, cls_tokens.dtype)\n                x = ops.concat((cls_tokens, x), 1)\n            x = blk(x)\n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT base model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_base(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT base model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_base\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=16, embed_dim=768, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_base_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT base+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_base_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT base+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_base_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=16, embed_dim=1024, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT small model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT small model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_small\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=9, embed_dim=432, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_small_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT small+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_small_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT small+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_small_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=9, embed_dim=576, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT tiny model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT tiny model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_tiny\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=4, embed_dim=192, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convit.convit_tiny_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConViT tiny+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convit.py</code> <pre><code>@register_model\ndef convit_tiny_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n\"\"\"Get ConViT tiny+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_tiny_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=4, embed_dim=256, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext</code> \u00b6 <p>MindSpore implementation of <code>ConvNeXt</code>. Refer to: A ConvNet for the 2020s</p> <code>mindocr.models.backbones.mindcv_models.convnext.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ConvNeXt Block</p> There are two equivalent implementations <p>(1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W) (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</p> <p>Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time &amp; w/ different HW.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>drop_path</code> <p>Stochastic depth rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>Init value for Layer Scale. Default: 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>class Block(nn.Cell):\n\"\"\"ConvNeXt Block\n    There are two equivalent implementations:\n      (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)\n      (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back\n    Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate\n    choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear\n    is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time &amp; w/ different HW.\n    Args:\n        dim (int): Number of input channels.\n        drop_path (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        drop_path: float = 0.0,\n        layer_scale_init_value: float = 1e-6,\n    ) -&gt; None:\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, group=dim, has_bias=True)  # depthwise conv\n        self.norm = ConvNextLayerNorm((dim,), epsilon=1e-6)\n        self.pwconv1 = nn.Dense(dim, 4 * dim)  # pointwise/1x1 convs, implemented with Dense layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Dense(4 * dim, dim)\n        self.gamma_ = Parameter(Tensor(layer_scale_init_value * np.ones((dim)), dtype=mstype.float32),\n                                requires_grad=True) if layer_scale_init_value &gt; 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        downsample = x\n        x = self.dwconv(x)\n        x = ops.transpose(x, (0, 2, 3, 1))\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma_ is not None:\n            x = self.gamma_ * x\n        x = ops.transpose(x, (0, 3, 1, 2))\n        x = downsample + self.drop_path(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.ConvNeXt</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ConvNeXt model class, based on '\"A ConvNet for the 2020s\" https://arxiv.org/abs/2201.03545'</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>dim of the input channel.</p> <p> TYPE: <code>int) </code> </p> <code>num_classes</code> <p>dim of the classes predicted.</p> <p> TYPE: <code>int) </code> </p> <code>depths</code> <p>the depths of each layer.</p> <p> TYPE: <code>List[int]) </code> </p> <code>dims</code> <p>the middle dim of each layer.</p> <p> TYPE: <code>List[int]) </code> </p> <code>drop_path_rate</code> <p>the rate of droppath default : 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>the parameter of init for the classifier default : 1e-6.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>1e-06</code> </p> <code>head_init_scale</code> <p>the parameter of init for the head default : 1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>1.0</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>class ConvNeXt(nn.Cell):\nr\"\"\"ConvNeXt model class, based on\n    '\"A ConvNet for the 2020s\" &lt;https://arxiv.org/abs/2201.03545&gt;'\n    Args:\n        in_channels (int) : dim of the input channel.\n        num_classes (int) : dim of the classes predicted.\n        depths (List[int]) : the depths of each layer.\n        dims (List[int]) : the middle dim of each layer.\n        drop_path_rate (float) : the rate of droppath default : 0.\n        layer_scale_init_value (float) : the parameter of init for the classifier default : 1e-6.\n        head_init_scale (float) : the parameter of init for the head default : 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        num_classes: int,\n        depths: List[int],\n        dims: List[int],\n        drop_path_rate: float = 0.0,\n        layer_scale_init_value: float = 1e-6,\n        head_init_scale: float = 1.0,\n    ):\n        super().__init__()\n\n        self.downsample_layers = nn.CellList()  # stem and 3 intermediate down_sampling conv layers\n        stem = nn.SequentialCell(\n            nn.Conv2d(in_channels, dims[0], kernel_size=4, stride=4, has_bias=True),\n            ConvNextLayerNorm((dims[0],), epsilon=1e-6, norm_axis=1),\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.SequentialCell(\n                ConvNextLayerNorm((dims[i],), epsilon=1e-6, norm_axis=1),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2, has_bias=True),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.CellList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = list(np.linspace(0, drop_path_rate, sum(depths)))\n        cur = 0\n        for i in range(4):\n            blocks = []\n            for j in range(depths[i]):\n                blocks.append(Block(dim=dims[i], drop_path=dp_rates[cur + j],\n                                    layer_scale_init_value=layer_scale_init_value))\n            stage = nn.SequentialCell(blocks)\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = ConvNextLayerNorm((dims[-1],), epsilon=1e-6)  # final norm layer\n        self.classifier = nn.Dense(dims[-1], num_classes)  # classifier\n        self.feature = nn.SequentialCell([\n            self.downsample_layers[0],\n            self.stages[0],\n            self.downsample_layers[1],\n            self.stages[1],\n            self.downsample_layers[2],\n            self.stages[2],\n            self.downsample_layers[3],\n            self.stages[3]\n        ])\n        self.head_init_scale = head_init_scale\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Dense, nn.Conv2d)):\n                cell.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n                )\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n        self.classifier.weight.set_data(self.classifier.weight * self.head_init_scale)\n        self.classifier.bias.set_data(self.classifier.bias * self.head_init_scale)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.feature(x)\n        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -&gt; (N, C)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.ConvNextLayerNorm</code> \u00b6 <p>         Bases: <code>nn.LayerNorm</code></p> <p>LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>class ConvNextLayerNorm(nn.LayerNorm):\nr\"\"\"LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\"\"\"\n\n    def __init__(\n        self,\n        normalized_shape: Tuple[int],\n        epsilon: float,\n        norm_axis: int = -1,\n    ) -&gt; None:\n        super().__init__(normalized_shape=normalized_shape, epsilon=epsilon)\n        assert norm_axis in (-1, 1), \"ConvNextLayerNorm's norm_axis must be 1 or -1.\"\n        self.norm_axis = norm_axis\n\n    def construct(self, input_x: Tensor) -&gt; Tensor:\n        if self.norm_axis == -1:\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n        else:\n            input_x = ops.transpose(input_x, (0, 2, 3, 1))\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n            y = ops.transpose(y, (0, 3, 1, 2))\n        return y\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.convnext_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConvNeXt base model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>@register_model\ndef convnext_base(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n\"\"\"Get ConvNeXt base model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_base\"]\n    model = ConvNeXt(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.convnext_large(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConvNeXt large model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>@register_model\ndef convnext_large(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n\"\"\"Get ConvNeXt large model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_large\"]\n    model = ConvNeXt(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.convnext_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConvNeXt small model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>@register_model\ndef convnext_small(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n\"\"\"Get ConvNeXt small model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_small\"]\n    model = ConvNeXt(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.convnext_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConvNeXt tiny model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>@register_model\ndef convnext_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n\"\"\"Get ConvNeXt tiny model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_tiny\"]\n    model = ConvNeXt(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.convnext.convnext_xlarge(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ConvNeXt xlarge model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\convnext.py</code> <pre><code>@register_model\ndef convnext_xlarge(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n\"\"\"Get ConvNeXt xlarge model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_xlarge\"]\n    model = ConvNeXt(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.crossvit</code> \u00b6 <p>MindSpore implementation of <code>crossvit</code>. Refer to crossvit: Cross-Attention Multi-Scale Vision Transformer for Image Classification</p> <code>mindocr.models.backbones.mindcv_models.crossvit.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\crossvit.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=True):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        if multi_conv:\n            if patch_size[0] == 12:\n                self.proj = nn.SequentialCell(\n                    nn.Conv2d(in_chans, embed_dim // 4, pad_mode='pad', kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, pad_mode='pad', kernel_size=3, stride=3, padding=0),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 2, embed_dim, pad_mode='pad', kernel_size=3, stride=1, padding=1),\n                )\n            elif patch_size[0] == 16:\n                self.proj = nn.SequentialCell(\n                    nn.Conv2d(in_chans, embed_dim // 4, pad_mode='pad', kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, pad_mode='pad', kernel_size=3, stride=2, padding=1),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 2, embed_dim, pad_mode='pad', kernel_size=3, stride=2, padding=1),\n                )\n        else:\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, pad_mode='valid',\n                                  has_bias=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        # f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H * W)\n        x = ops.transpose(x, (0, 2, 1))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.crossvit.VisionTransformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Vision Transformer with support for patch or hybrid CNN input stage</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\crossvit.py</code> <pre><code>class VisionTransformer(nn.Cell):\n\"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=(224, 224), patch_size=(8, 16), in_channels=3, num_classes=1000, embed_dim=(192, 384),\n                 depth=([1, 3, 1], [1, 3, 1], [1, 3, 1]),\n                 num_heads=(6, 12), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False):\n        super().__init__()\n\n        self.num_classes = num_classes\n        if not isinstance(img_size, list):\n            img_size = to_2tuple(img_size)\n        self.img_size = img_size\n\n        num_patches = _compute_num_patches(img_size, patch_size)\n        self.num_branches = len(patch_size)\n\n        patch_embed = []\n        if hybrid_backbone is None:\n            b = []\n            for i in range(self.num_branches):\n                c = ms.Parameter(Tensor(np.zeros([1, 1 + num_patches[i], embed_dim[i]], np.float32)),\n                                 name='pos_embed.' + str(i))\n                b.append(c)\n            b = tuple(b)\n            self.pos_embed = ms.ParameterTuple(b)\n            for im_s, p, d in zip(img_size, patch_size, embed_dim):\n                patch_embed.append(\n                    PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_channels, embed_dim=d, multi_conv=multi_conv))\n            self.patch_embed = nn.CellList(patch_embed)\n\n        d = []\n        for i in range(self.num_branches):\n            c = ms.Parameter(Tensor(np.zeros([1, 1, embed_dim[i]], np.float32)), name='cls_token.' + str(i))\n            d.append(c)\n        d = tuple(d)\n        self.cls_token = ms.ParameterTuple(d)\n        self.pos_drop = nn.Dropout(1.0 - drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = np.linspace(0, drop_path_rate, total_depth)  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.CellList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n                                  drop_path=dpr_,\n                                  norm_layer=norm_layer)\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.CellList([norm_layer((embed_dim[i],), epsilon=1e-6) for i in range(self.num_branches)])\n        self.head = nn.CellList([nn.Dense(embed_dim[i], num_classes) if num_classes &gt; 0 else Identity() for i in\n                                 range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            if self.pos_embed[i].requires_grad:\n                tensor1 = init.initializer(TruncatedNormal(sigma=.02), self.pos_embed[i].data.shape, ms.float32)\n                self.pos_embed[i].set_data(tensor1)\n            tensor2 = init.initializer(TruncatedNormal(sigma=.02), self.cls_token[i].data.shape, ms.float32)\n            self.cls_token[i].set_data(tensor2)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n\n    def no_weight_decay(self):\n        out = {'cls_token'}\n        if self.pos_embed[0].requires_grad:\n            out.add('pos_embed')\n        return out\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        xs = []\n        # print(x)\n        for i in range(self.num_branches):\n            x_ = ops.interpolate(x, sizes=(self.img_size[i], self.img_size[i]), mode='bilinear') if H != self.img_size[\n                i] else x\n            tmp = self.patch_embed[i](x_)\n            z = self.cls_token[i].shape\n            y = Tensor(np.ones((B, z[1], z[2])), dtype=mstype.float32)\n            cls_tokens = self.cls_token[i]\n            cls_tokens = cls_tokens.expand_as(y)  # stole cls_tokens impl from Phil Wang, thanks\n            con = ops.Concat(1)\n            cls_tokens = cls_tokens.astype(\"float32\")\n            tmp = tmp.astype(\"float32\")\n            tmp = con((cls_tokens, tmp))\n            tmp = tmp + self.pos_embed[i]\n            tmp = self.pos_drop(tmp)\n            xs.append(tmp)\n\n        for blk in self.blocks:\n            xs = blk(xs)\n\n        # NOTE: was before branch token section, move to here to assure all branch token are before layer norm\n        k = 0\n        xs2 = []\n        for x in xs:\n            xs2.append(self.norm[k](x))\n            k = k + 1\n        xs = xs2\n        out = []\n        for x in xs:\n            out.append(x[:, 0])\n        return out\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        ce_logits = []\n        zz = 0\n        for c in x:\n            ce_logits.append(self.head[zz](c))\n            zz = zz + 1\n        z = ops.stack([ce_logits[0], ce_logits[1]])\n        op = ops.ReduceMean(keep_dims=False)\n        ce_logits = op(z, 0)\n        return ce_logits\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.densenet</code> \u00b6 <p>MindSpore implementation of <code>DenseNet</code>. Refer to: Densely Connected Convolutional Networks</p> <code>mindocr.models.backbones.mindcv_models.densenet.DenseNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Densenet-BC model class, based on <code>\"Densely Connected Convolutional Networks\" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>growth_rate</code> <p>how many filters to add each layer (<code>k</code> in paper). Default: 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>block_config</code> <p>how many layers in each pooling block. Default: (6, 12, 24, 16).</p> <p> TYPE: <code>Tuple[int, int, int, int]</code> DEFAULT: <code>(6, 12, 24, 16)</code> </p> <code>num_init_features</code> <p>number of filters in the first Conv2d. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>bn_size</code> <p>multiplicative factor for number of bottleneck layers (i.e. bn_size * k features in the bottleneck layer). Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>drop_rate</code> <p>dropout rate after each dense layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\densenet.py</code> <pre><code>class DenseNet(nn.Cell):\nr\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_\n\n    Args:\n        growth_rate: how many filters to add each layer (`k` in paper). Default: 32.\n        block_config: how many layers in each pooling block. Default: (6, 12, 24, 16).\n        num_init_features: number of filters in the first Conv2d. Default: 64.\n        bn_size (int): multiplicative factor for number of bottleneck layers\n          (i.e. bn_size * k features in the bottleneck layer). Default: 4.\n        drop_rate: dropout rate after each dense layer. Default: 0.\n        in_channels: number of input channels. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        growth_rate: int = 32,\n        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n        num_init_features: int = 64,\n        bn_size: int = 4,\n        drop_rate: float = 0.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        layers = OrderedDict()\n        # first Conv2d\n        num_features = num_init_features\n        layers[\"conv0\"] = nn.Conv2d(in_channels, num_features, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3)\n        layers[\"norm0\"] = nn.BatchNorm2d(num_features)\n        layers[\"relu0\"] = nn.ReLU()\n        layers[\"pool0\"] = nn.SequentialCell([\n            nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"CONSTANT\"),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        ])\n\n        # DenseBlock\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n            )\n            layers[f\"denseblock{i + 1}\"] = block\n            num_features += num_layers * growth_rate\n            if i != len(block_config) - 1:\n                transition = _Transition(num_features, num_features // 2)\n                layers[f\"transition{i + 1}\"] = transition\n                num_features = num_features // 2\n\n        # final bn+ReLU\n        layers[\"norm5\"] = nn.BatchNorm2d(num_features)\n        layers[\"relu5\"] = nn.ReLU()\n\n        self.num_features = num_features\n        self.features = nn.SequentialCell(layers)\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.densenet.densenet121(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 121 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\densenet.py</code> <pre><code>@register_model\ndef densenet121(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n\"\"\"Get 121 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet121\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.densenet.densenet161(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 161 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\densenet.py</code> <pre><code>@register_model\ndef densenet161(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n\"\"\"Get 161 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet161\"]\n    model = DenseNet(growth_rate=48, block_config=(6, 12, 36, 24), num_init_features=96, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.densenet.densenet169(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 169 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\densenet.py</code> <pre><code>@register_model\ndef densenet169(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n\"\"\"Get 169 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet169\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 32, 32), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.densenet.densenet201(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 201 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\densenet.py</code> <pre><code>@register_model\ndef densenet201(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n\"\"\"Get 201 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet201\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 48, 32), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download</code> \u00b6 <p>Utility of downloading</p> <code>mindocr.models.backbones.mindcv_models.download.DownLoad</code> \u00b6 <p>Base utility class for downloading.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>class DownLoad:\n\"\"\"Base utility class for downloading.\"\"\"\n\n    USER_AGENT: str = (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n        \"Chrome/92.0.4515.131 Safari/537.36\"\n    )\n\n    @staticmethod\n    def calculate_md5(file_path: str, chunk_size: int = 1024 * 1024) -&gt; str:\n\"\"\"Calculate md5 value.\"\"\"\n        md5 = hashlib.md5()\n        with open(file_path, \"rb\") as fp:\n            for chunk in iter(lambda: fp.read(chunk_size), b\"\"):\n                md5.update(chunk)\n        return md5.hexdigest()\n\n    def check_md5(self, file_path: str, md5: Optional[str] = None) -&gt; bool:\n\"\"\"Check md5 value.\"\"\"\n        return md5 == self.calculate_md5(file_path)\n\n    @staticmethod\n    def extract_tar(from_path: str, to_path: Optional[str] = None, compression: Optional[str] = None) -&gt; None:\n\"\"\"Extract tar format file.\"\"\"\n\n        with tarfile.open(from_path, f\"r:{compression[1:]}\" if compression else \"r\") as tar:\n            tar.extractall(to_path)\n\n    @staticmethod\n    def extract_zip(from_path: str, to_path: Optional[str] = None, compression: Optional[str] = None) -&gt; None:\n\"\"\"Extract zip format file.\"\"\"\n\n        compression_mode = zipfile.ZIP_BZIP2 if compression else zipfile.ZIP_STORED\n        with zipfile.ZipFile(from_path, \"r\", compression=compression_mode) as zip_file:\n            zip_file.extractall(to_path)\n\n    def extract_archive(self, from_path: str, to_path: str = None) -&gt; str:\n\"\"\"Extract and  archive from path to path.\"\"\"\n        archive_extractors = {\n            \".tar\": self.extract_tar,\n            \".zip\": self.extract_zip,\n        }\n        compress_file_open = {\n            \".bz2\": bz2.open,\n            \".gz\": gzip.open,\n        }\n\n        if not to_path:\n            to_path = os.path.dirname(from_path)\n\n        suffix, archive_type, compression = detect_file_type(from_path)  # pylint: disable=unused-variable\n\n        if not archive_type:\n            to_path = from_path.replace(suffix, \"\")\n            compress = compress_file_open[compression]\n            with compress(from_path, \"rb\") as rf, open(to_path, \"wb\") as wf:\n                wf.write(rf.read())\n            return to_path\n\n        extractor = archive_extractors[archive_type]\n        extractor(from_path, to_path, compression)\n\n        return to_path\n\n    def download_file(self, url: str, file_path: str, chunk_size: int = 1024):\n\"\"\"Download a file.\"\"\"\n\n        # no check certificate\n        ctx = ssl.create_default_context()\n        ctx.check_hostname = False\n        ctx.verify_mode = ssl.CERT_NONE\n\n        # Define request headers.\n        headers = {\"User-Agent\": self.USER_AGENT}\n\n        _logger.info(f\"Downloading from {url} to {file_path} ...\")\n        with open(file_path, \"wb\") as f:\n            request = urllib.request.Request(url, headers=headers)\n            with urllib.request.urlopen(request, context=ctx) as response:\n                with tqdm(total=response.length, unit=\"B\") as pbar:\n                    for chunk in iter(lambda: response.read(chunk_size), b\"\"):\n                        if not chunk:\n                            break\n                        pbar.update(chunk_size)\n                        f.write(chunk)\n\n    def download_url(\n        self,\n        url: str,\n        path: Optional[str] = None,\n        filename: Optional[str] = None,\n        md5: Optional[str] = None,\n    ) -&gt; None:\n\"\"\"Download a file from a url and place it in root.\"\"\"\n        if path is None:\n            path = get_default_download_root()\n        path = os.path.expanduser(path)\n        os.makedirs(path, exist_ok=True)\n\n        if not filename:\n            filename = os.path.basename(url)\n\n        file_path = os.path.join(path, filename)\n\n        # Check if the file is exists.\n        if os.path.isfile(file_path):\n            if not md5 or self.check_md5(file_path, md5):\n                return file_path\n\n        # Download the file.\n        try:\n            self.download_file(url, file_path)\n        except (urllib.error.URLError, IOError) as e:\n            if url.startswith(\"https\"):\n                url = url.replace(\"https\", \"http\")\n                try:\n                    self.download_file(url, file_path)\n                except (urllib.error.URLError, IOError):\n                    # pylint: disable=protected-access\n                    ssl._create_default_https_context = ssl._create_unverified_context\n                    self.download_file(url, file_path)\n                    ssl._create_default_https_context = ssl.create_default_context\n            else:\n                raise e\n\n        return file_path\n\n    def download_and_extract_archive(\n        self,\n        url: str,\n        download_path: Optional[str] = None,\n        extract_path: Optional[str] = None,\n        filename: Optional[str] = None,\n        md5: Optional[str] = None,\n        remove_finished: bool = False,\n    ) -&gt; None:\n\"\"\"Download and extract archive.\"\"\"\n        if download_path is None:\n            download_path = get_default_download_root()\n        download_path = os.path.expanduser(download_path)\n\n        if not filename:\n            filename = os.path.basename(url)\n\n        self.download_url(url, download_path, filename, md5)\n\n        archive = os.path.join(download_path, filename)\n        self.extract_archive(archive, extract_path)\n\n        if remove_finished:\n            os.remove(archive)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.calculate_md5(file_path, chunk_size=1024 * 1024)</code> <code>staticmethod</code> \u00b6 <p>Calculate md5 value.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>@staticmethod\ndef calculate_md5(file_path: str, chunk_size: int = 1024 * 1024) -&gt; str:\n\"\"\"Calculate md5 value.\"\"\"\n    md5 = hashlib.md5()\n    with open(file_path, \"rb\") as fp:\n        for chunk in iter(lambda: fp.read(chunk_size), b\"\"):\n            md5.update(chunk)\n    return md5.hexdigest()\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.check_md5(file_path, md5=None)</code> \u00b6 <p>Check md5 value.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>def check_md5(self, file_path: str, md5: Optional[str] = None) -&gt; bool:\n\"\"\"Check md5 value.\"\"\"\n    return md5 == self.calculate_md5(file_path)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.download_and_extract_archive(url, download_path=None, extract_path=None, filename=None, md5=None, remove_finished=False)</code> \u00b6 <p>Download and extract archive.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>def download_and_extract_archive(\n    self,\n    url: str,\n    download_path: Optional[str] = None,\n    extract_path: Optional[str] = None,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n    remove_finished: bool = False,\n) -&gt; None:\n\"\"\"Download and extract archive.\"\"\"\n    if download_path is None:\n        download_path = get_default_download_root()\n    download_path = os.path.expanduser(download_path)\n\n    if not filename:\n        filename = os.path.basename(url)\n\n    self.download_url(url, download_path, filename, md5)\n\n    archive = os.path.join(download_path, filename)\n    self.extract_archive(archive, extract_path)\n\n    if remove_finished:\n        os.remove(archive)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.download_file(url, file_path, chunk_size=1024)</code> \u00b6 <p>Download a file.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>def download_file(self, url: str, file_path: str, chunk_size: int = 1024):\n\"\"\"Download a file.\"\"\"\n\n    # no check certificate\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    # Define request headers.\n    headers = {\"User-Agent\": self.USER_AGENT}\n\n    _logger.info(f\"Downloading from {url} to {file_path} ...\")\n    with open(file_path, \"wb\") as f:\n        request = urllib.request.Request(url, headers=headers)\n        with urllib.request.urlopen(request, context=ctx) as response:\n            with tqdm(total=response.length, unit=\"B\") as pbar:\n                for chunk in iter(lambda: response.read(chunk_size), b\"\"):\n                    if not chunk:\n                        break\n                    pbar.update(chunk_size)\n                    f.write(chunk)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.download_url(url, path=None, filename=None, md5=None)</code> \u00b6 <p>Download a file from a url and place it in root.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>def download_url(\n    self,\n    url: str,\n    path: Optional[str] = None,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n) -&gt; None:\n\"\"\"Download a file from a url and place it in root.\"\"\"\n    if path is None:\n        path = get_default_download_root()\n    path = os.path.expanduser(path)\n    os.makedirs(path, exist_ok=True)\n\n    if not filename:\n        filename = os.path.basename(url)\n\n    file_path = os.path.join(path, filename)\n\n    # Check if the file is exists.\n    if os.path.isfile(file_path):\n        if not md5 or self.check_md5(file_path, md5):\n            return file_path\n\n    # Download the file.\n    try:\n        self.download_file(url, file_path)\n    except (urllib.error.URLError, IOError) as e:\n        if url.startswith(\"https\"):\n            url = url.replace(\"https\", \"http\")\n            try:\n                self.download_file(url, file_path)\n            except (urllib.error.URLError, IOError):\n                # pylint: disable=protected-access\n                ssl._create_default_https_context = ssl._create_unverified_context\n                self.download_file(url, file_path)\n                ssl._create_default_https_context = ssl.create_default_context\n        else:\n            raise e\n\n    return file_path\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.extract_archive(from_path, to_path=None)</code> \u00b6 <p>Extract and  archive from path to path.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>def extract_archive(self, from_path: str, to_path: str = None) -&gt; str:\n\"\"\"Extract and  archive from path to path.\"\"\"\n    archive_extractors = {\n        \".tar\": self.extract_tar,\n        \".zip\": self.extract_zip,\n    }\n    compress_file_open = {\n        \".bz2\": bz2.open,\n        \".gz\": gzip.open,\n    }\n\n    if not to_path:\n        to_path = os.path.dirname(from_path)\n\n    suffix, archive_type, compression = detect_file_type(from_path)  # pylint: disable=unused-variable\n\n    if not archive_type:\n        to_path = from_path.replace(suffix, \"\")\n        compress = compress_file_open[compression]\n        with compress(from_path, \"rb\") as rf, open(to_path, \"wb\") as wf:\n            wf.write(rf.read())\n        return to_path\n\n    extractor = archive_extractors[archive_type]\n    extractor(from_path, to_path, compression)\n\n    return to_path\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.extract_tar(from_path, to_path=None, compression=None)</code> <code>staticmethod</code> \u00b6 <p>Extract tar format file.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>@staticmethod\ndef extract_tar(from_path: str, to_path: Optional[str] = None, compression: Optional[str] = None) -&gt; None:\n\"\"\"Extract tar format file.\"\"\"\n\n    with tarfile.open(from_path, f\"r:{compression[1:]}\" if compression else \"r\") as tar:\n        tar.extractall(to_path)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.download.DownLoad.extract_zip(from_path, to_path=None, compression=None)</code> <code>staticmethod</code> \u00b6 <p>Extract zip format file.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\download.py</code> <pre><code>@staticmethod\ndef extract_zip(from_path: str, to_path: Optional[str] = None, compression: Optional[str] = None) -&gt; None:\n\"\"\"Extract zip format file.\"\"\"\n\n    compression_mode = zipfile.ZIP_BZIP2 if compression else zipfile.ZIP_STORED\n    with zipfile.ZipFile(from_path, \"r\", compression=compression_mode) as zip_file:\n        zip_file.extractall(to_path)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn</code> \u00b6 <p>MindSpore implementation of <code>DPN</code>. Refer to: Dual Path Networks</p> <code>mindocr.models.backbones.mindcv_models.dpn.BottleBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>A block for the Dual Path Architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>class BottleBlock(nn.Cell):\n\"\"\"A block for the Dual Path Architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channel: int,\n        num_1x1_a: int,\n        num_3x3_b: int,\n        num_1x1_c: int,\n        inc: int,\n        g: int,\n        key_stride: int,\n    ):\n        super().__init__()\n        self.bn1 = nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)\n        self.conv1 = nn.Conv2d(in_channel, num_1x1_a, 1, stride=1)\n        self.bn2 = nn.BatchNorm2d(num_1x1_a, eps=1e-3, momentum=0.9)\n        self.conv2 = nn.Conv2d(num_1x1_a, num_3x3_b, 3, key_stride, pad_mode=\"pad\", padding=1, group=g)\n        self.bn3 = nn.BatchNorm2d(num_3x3_b, eps=1e-3, momentum=0.9)\n        self.conv3_r = nn.Conv2d(num_3x3_b, num_1x1_c, 1, stride=1)\n        self.conv3_d = nn.Conv2d(num_3x3_b, inc, 1, stride=1)\n\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor):\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        return (self.conv3_r(x), self.conv3_d(x))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.DPN</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>DPN model class, based on <code>\"Dual Path Networks\" &lt;https://arxiv.org/pdf/1707.01629.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_init_channel</code> <p>int type, the output channel of first blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>k_r</code> <p>int type, the first channel of each stage. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>g</code> <p>int type,number of group in the conv2d. Default: 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>k_sec</code> <p>multiplicative factor for number of bottleneck layers. Default: 4.</p> <p> TYPE: <code>Tuple[int]</code> DEFAULT: <code>(3, 4, 20, 3)</code> </p> <code>inc_sec</code> <p>the first output channel in each stage. Default: (16, 32, 24, 128).</p> <p> TYPE: <code>Tuple[int]</code> DEFAULT: <code>(16, 32, 24, 128)</code> </p> <code>in_channels</code> <p>int type, number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>int type, number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>class DPN(nn.Cell):\nr\"\"\"DPN model class, based on\n    `\"Dual Path Networks\" &lt;https://arxiv.org/pdf/1707.01629.pdf&gt;`_\n\n    Args:\n        num_init_channel: int type, the output channel of first blocks. Default: 64.\n        k_r: int type, the first channel of each stage. Default: 96.\n        g: int type,number of group in the conv2d. Default: 32.\n        k_sec Tuple[int]: multiplicative factor for number of bottleneck layers. Default: 4.\n        inc_sec Tuple[int]: the first output channel in each stage. Default: (16, 32, 24, 128).\n        in_channels: int type, number of input channels. Default: 3.\n        num_classes: int type, number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_init_channel: int = 64,\n        k_r: int = 96,\n        g: int = 32,\n        k_sec: Tuple[int, int, int, int] = (3, 4, 20, 3),\n        inc_sec: Tuple[int, int, int, int] = (16, 32, 24, 128),\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ):\n        super().__init__()\n        blocks = OrderedDict()\n\n        # conv1\n        blocks[\"conv1\"] = nn.SequentialCell(OrderedDict([\n            (\"conv\", nn.Conv2d(in_channels, num_init_channel, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3)),\n            (\"norm\", nn.BatchNorm2d(num_init_channel, eps=1e-3, momentum=0.9)),\n            (\"relu\", nn.ReLU()),\n            (\"maxpool\", nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")),\n        ]))\n\n        # conv2\n        bw = 256\n        inc = inc_sec[0]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv2_1\"] = DualPathBlock(num_init_channel, r, r, bw, inc, g, \"proj\", False)\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks[f\"conv2_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv3\n        bw = 512\n        inc = inc_sec[1]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv3_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks[f\"conv3_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv4\n        bw = 1024\n        inc = inc_sec[2]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv4_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks[f\"conv4_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv5\n        bw = 2048\n        inc = inc_sec[3]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv5_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks[f\"conv5_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        self.features = nn.SequentialCell(blocks)\n        self.conv5_x = nn.SequentialCell(OrderedDict([\n            (\"norm\", nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)),\n            (\"relu\", nn.ReLU()),\n        ]))\n        self.avgpool = GlobalAvgPooling()\n        self.classifier = nn.Dense(in_channel, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_feature(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        x = ops.concat(x, axis=1)\n        x = self.conv5_x(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.avgpool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_feature(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.DualPathBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>A block for Dual Path Networks to combine proj, residual and densely network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>class DualPathBlock(nn.Cell):\n\"\"\"A block for Dual Path Networks to combine proj, residual and densely network\"\"\"\n\n    def __init__(\n        self,\n        in_channel: int,\n        num_1x1_a: int,\n        num_3x3_b: int,\n        num_1x1_c: int,\n        inc: int,\n        g: int,\n        _type: str = \"normal\",\n        cat_input: bool = True,\n    ):\n        super().__init__()\n        self.num_1x1_c = num_1x1_c\n\n        if _type == \"proj\":\n            key_stride = 1\n            self.has_proj = True\n        if _type == \"down\":\n            key_stride = 2\n            self.has_proj = True\n        if _type == \"normal\":\n            key_stride = 1\n            self.has_proj = False\n\n        self.cat_input = cat_input\n\n        if self.has_proj:\n            self.c1x1_w_bn = nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)\n            self.c1x1_w_relu = nn.ReLU()\n            self.c1x1_w_r = nn.Conv2d(in_channel, num_1x1_c, kernel_size=1, stride=key_stride,\n                                      pad_mode=\"pad\", padding=0)\n            self.c1x1_w_d = nn.Conv2d(in_channel, 2 * inc, kernel_size=1, stride=key_stride,\n                                      pad_mode=\"pad\", padding=0)\n\n        self.layers = BottleBlock(in_channel, num_1x1_a, num_3x3_b, num_1x1_c, inc, g, key_stride)\n\n    def construct(self, x: Tensor):\n        if self.cat_input:\n            data_in = ops.concat(x, axis=1)\n        else:\n            data_in = x\n\n        if self.has_proj:\n            data_o = self.c1x1_w_bn(data_in)\n            data_o = self.c1x1_w_relu(data_o)\n            data_o1 = self.c1x1_w_r(data_o)\n            data_o2 = self.c1x1_w_d(data_o)\n        else:\n            data_o1 = x[0]\n            data_o2 = x[1]\n\n        out = self.layers(data_in)\n        summ = ops.add(data_o1, out[0])\n        dense = ops.concat((data_o2, out[1]), axis=1)\n        return (summ, dense)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.dpn107(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 107 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>@register_model\ndef dpn107(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n\"\"\"Get 107 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn107\"]\n    model = DPN(num_init_channel=128, k_r=200, g=50, k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.dpn131(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 131 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>@register_model\ndef dpn131(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n\"\"\"Get 131 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn131\"]\n    model = DPN(num_init_channel=128, k_r=160, g=40, k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.dpn92(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 92 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>@register_model\ndef dpn92(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n\"\"\"Get 92 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn92\"]\n    model = DPN(num_init_channel=64, k_r=96, g=32, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.dpn.dpn98(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 98 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\dpn.py</code> <pre><code>@register_model\ndef dpn98(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n\"\"\"Get 98 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn98\"]\n    model = DPN(num_init_channel=96, k_r=160, g=40, k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext</code> \u00b6 <p>MindSpore implementation of <code>edgenext</code>. Refer to EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications.</p> <code>mindocr.models.backbones.mindcv_models.edgenext.EdgeNeXt</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>EdgeNeXt model class, based on <code>\"Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision\" &lt;https://arxiv.org/abs/2206.10589&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of input channels. Default: 3</p> <p> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> DEFAULT: <code>1000</code> </p> <code>depths</code> <p>the depths of each layer. Default: [0, 0, 0, 3]</p> <p> DEFAULT: <code>[3, 3, 9, 3]</code> </p> <code>dims</code> <p>the middle dim of each layer. Default: [24, 48, 88, 168]</p> <p> DEFAULT: <code>[24, 48, 88, 168]</code> </p> <code>global_block</code> <p>number of global block. Default: [0, 0, 0, 3]</p> <p> DEFAULT: <code>[0, 0, 0, 3]</code> </p> <code>global_block_type</code> <p>type of global block. Default: ['None', 'None', 'None', 'SDTA']</p> <p> DEFAULT: <code>['None', 'None', 'None', 'SDTA']</code> </p> <code>drop_path_rate</code> <p>Stochastic Depth. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>value of layer scale initialization. Default: 1e-6</p> <p> DEFAULT: <code>1e-06</code> </p> <code>head_init_scale</code> <p>scale of head initialization. Default: 1.</p> <p> DEFAULT: <code>1.0</code> </p> <code>expan_ratio</code> <p>ratio of expansion. Default: 4</p> <p> DEFAULT: <code>4</code> </p> <code>kernel_sizes</code> <p>kernel sizes of different stages. Default: [7, 7, 7, 7]</p> <p> DEFAULT: <code>[7, 7, 7, 7]</code> </p> <code>heads</code> <p>number of attention heads. Default: [8, 8, 8, 8]</p> <p> DEFAULT: <code>[8, 8, 8, 8]</code> </p> <code>use_pos_embd_xca</code> <p>use position embedding in xca or not. Default: [False, False, False, False]</p> <p> DEFAULT: <code>[False, False, False, False]</code> </p> <code>use_pos_embd_global</code> <p>use position embedding globally or not. Default: False</p> <p> DEFAULT: <code>False</code> </p> <code>d2_scales</code> <p>scales of splitting channels</p> <p> DEFAULT: <code>[2, 3, 4, 5]</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>class EdgeNeXt(nn.Cell):\nr\"\"\"EdgeNeXt model class, based on\n    `\"Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision\" &lt;https://arxiv.org/abs/2206.10589&gt;`_\n\n    Args:\n        in_channels: number of input channels. Default: 3\n        num_classes: number of classification classes. Default: 1000\n        depths: the depths of each layer. Default: [0, 0, 0, 3]\n        dims: the middle dim of each layer. Default: [24, 48, 88, 168]\n        global_block: number of global block. Default: [0, 0, 0, 3]\n        global_block_type: type of global block. Default: ['None', 'None', 'None', 'SDTA']\n        drop_path_rate: Stochastic Depth. Default: 0.\n        layer_scale_init_value: value of layer scale initialization. Default: 1e-6\n        head_init_scale: scale of head initialization. Default: 1.\n        expan_ratio: ratio of expansion. Default: 4\n        kernel_sizes: kernel sizes of different stages. Default: [7, 7, 7, 7]\n        heads: number of attention heads. Default: [8, 8, 8, 8]\n        use_pos_embd_xca: use position embedding in xca or not. Default: [False, False, False, False]\n        use_pos_embd_global: use position embedding globally or not. Default: False\n        d2_scales: scales of splitting channels\n    \"\"\"\n    def __init__(self, in_chans=3, num_classes=1000,\n                 depths=[3, 3, 9, 3], dims=[24, 48, 88, 168],\n                 global_block=[0, 0, 0, 3], global_block_type=[\"None\", \"None\", \"None\", \"SDTA\"],\n                 drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1., expan_ratio=4,\n                 kernel_sizes=[7, 7, 7, 7], heads=[8, 8, 8, 8], use_pos_embd_xca=[False, False, False, False],\n                 use_pos_embd_global=False, d2_scales=[2, 3, 4, 5], **kwargs):\n        super().__init__()\n        for g in global_block_type:\n            assert g in [\"None\", \"SDTA\"]\n        if use_pos_embd_global:\n            self.pos_embd = PositionalEncodingFourier(dim=dims[0])\n        else:\n            self.pos_embd = None\n        self.downsample_layers = nn.CellList()  # stem and 3 intermediate downsampling conv layers\n        stem = nn.SequentialCell(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, has_bias=True),\n            LayerNorm((dims[0],), epsilon=1e-6, norm_axis=1),\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.SequentialCell(\n                LayerNorm((dims[i],), epsilon=1e-6, norm_axis=1),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2, has_bias=True),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.CellList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = list(np.linspace(0, drop_path_rate, sum(depths)))\n        cur = 0\n        for i in range(4):\n            stage_blocks = []\n            for j in range(depths[i]):\n                if j &gt; depths[i] - global_block[i] - 1:\n                    if global_block_type[i] == \"SDTA\":\n                        stage_blocks.append(SDTAEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                        expan_ratio=expan_ratio, scales=d2_scales[i],\n                                                        use_pos_emb=use_pos_embd_xca[i], num_heads=heads[i]))\n                    else:\n                        raise NotImplementedError\n                else:\n                    stage_blocks.append(ConvEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                    layer_scale_init_value=layer_scale_init_value,\n                                                    expan_ratio=expan_ratio, kernel_size=kernel_sizes[i]))\n\n            self.stages.append(nn.SequentialCell(*stage_blocks))\n            cur += depths[i]\n        self.norm = nn.LayerNorm((dims[-1],), epsilon=1e-6)  # Final norm layer\n        self.head = nn.Dense(dims[-1], num_classes)\n\n        # self.head_dropout = nn.Dropout(kwargs[\"classifier_dropout\"])\n        self.head_dropout = nn.Dropout(1.0)\n        self.head_init_scale = head_init_scale\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Dense, nn.Conv2d)):\n                cell.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n                )\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, (nn.LayerNorm)):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n        self.head.weight.set_data(self.head.weight * self.head_init_scale)\n        self.head.bias.set_data(self.head.bias * self.head_init_scale)\n\n    def forward_features(self, x):\n        x = self.downsample_layers[0](x)\n        x = self.stages[0](x)\n        if self.pos_embd is not None:\n            B, C, H, W = x.shape\n            x = x + self.pos_embd(B, H, W)\n        for i in range(1, 4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return self.norm(x.mean([-2, -1]))  # Global average pooling, (N, C, H, W) -&gt; (N, C)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.head(self.head_dropout(x))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext.LayerNorm</code> \u00b6 <p>         Bases: <code>nn.LayerNorm</code></p> <p>LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>class LayerNorm(nn.LayerNorm):\nr\"\"\"LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\"\"\"\n\n    def __init__(\n        self,\n        normalized_shape: Tuple[int],\n        epsilon: float,\n        norm_axis: int = -1,\n    ) -&gt; None:\n        super().__init__(normalized_shape=normalized_shape, epsilon=epsilon)\n        assert norm_axis in (-1, 1), \"ConvNextLayerNorm's norm_axis must be 1 or -1.\"\n        self.norm_axis = norm_axis\n\n    def construct(self, input_x: Tensor) -&gt; Tensor:\n        if self.norm_axis == -1:\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n        else:\n            input_x = ops.transpose(input_x, (0, 2, 3, 1))\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n            y = ops.transpose(y, (0, 3, 1, 2))\n        return y\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext.edgenext_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get edgenext_base model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_base(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n\"\"\"Get edgenext_base model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_base\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[80, 160, 288, 584],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext.edgenext_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get edgenext_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n\"\"\"Get edgenext_small model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_small\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[48, 96, 160, 304],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext.edgenext_x_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get edgenext_x_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_x_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n\"\"\"Get edgenext_x_small model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_x_small\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[32, 64, 100, 192],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        heads=[4, 4, 4, 4],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.edgenext.edgenext_xx_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get edgenext_xx_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_xx_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n\"\"\"Get edgenext_xx_small model.\n        Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_xx_small\"]\n    model = EdgeNeXt(\n        depths=[2, 2, 6, 2],\n        dims=[24, 48, 88, 168],\n        expan_ratio=4,\n        global_block=[0, 1, 1, 1],\n        global_block_type=['None', 'SDTA', 'SDTA', 'SDTA'],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        heads=[4, 4, 4, 4],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet</code> \u00b6 <p>EfficientNet Architecture.</p> <code>mindocr.models.backbones.mindcv_models.efficientnet.EfficientNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>EfficientNet architecture. <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>arch</code> <p>The name of the model.</p> <p> TYPE: <code>str</code> </p> <code>dropout_rate</code> <p>The dropout rate of efficientnet.</p> <p> TYPE: <code>float</code> </p> <code>width_mult</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_mult</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>in_channels</code> <p>The input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>The number of class. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>inverted_residual_setting</code> <p>The settings of block. Default: None.</p> <p> TYPE: <code>Sequence[Union[MBConvConfig, FusedMBConvConfig]]</code> DEFAULT: <code>None</code> </p> <code>keep_prob</code> <p>The dropout rate of MBConv. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>norm_layer</code> <p>The normalization layer. Default: None.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>None</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, 1000)</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>class EfficientNet(nn.Cell):\n\"\"\"\n    EfficientNet architecture.\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        arch (str): The name of the model.\n        dropout_rate (float): The dropout rate of efficientnet.\n        width_mult (float): The ratio of the channel. Default: 1.0.\n        depth_mult (float): The ratio of num_layers. Default: 1.0.\n        in_channels (int): The input channels. Default: 3.\n        num_classes (int): The number of class. Default: 1000.\n        inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]], optional): The settings of block.\n            Default: None.\n        keep_prob (float): The dropout rate of MBConv. Default: 0.2.\n        norm_layer (nn.Cell, optional): The normalization layer. Default: None.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, 1000)`.\n    \"\"\"\n\n    def __init__(\n        self,\n        arch: str,\n        dropout_rate: float,\n        width_mult: float = 1.0,\n        depth_mult: float = 1.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        inverted_residual_setting: Optional[Sequence[Union[MBConvConfig, FusedMBConvConfig]]] = None,\n        keep_prob: float = 0.2,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.last_channel = None\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n            if width_mult &gt;= 1.6:\n                norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.99)\n\n        layers: List[nn.Cell] = []\n\n        if not inverted_residual_setting:\n            if arch.startswith(\"efficientnet_b\"):\n                bneck_conf = partial(MBConvConfig, width_cnf=width_mult, depth_cnf=depth_mult)\n                inverted_residual_setting = [\n                    bneck_conf(1, 3, 1, 32, 16, 1),\n                    bneck_conf(6, 3, 2, 16, 24, 2),\n                    bneck_conf(6, 5, 2, 24, 40, 2),\n                    bneck_conf(6, 3, 2, 40, 80, 3),\n                    bneck_conf(6, 5, 1, 80, 112, 3),\n                    bneck_conf(6, 5, 2, 112, 192, 4),\n                    bneck_conf(6, 3, 1, 192, 320, 1),\n                ]\n            elif arch.startswith(\"efficientnet_v2_s\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n                    FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n                    FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n                    MBConvConfig(4, 3, 2, 64, 128, 6),\n                    MBConvConfig(6, 3, 1, 128, 160, 9),\n                    MBConvConfig(6, 3, 2, 160, 256, 15),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_m\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 24, 24, 3),\n                    FusedMBConvConfig(4, 3, 2, 24, 48, 5),\n                    FusedMBConvConfig(4, 3, 2, 48, 80, 5),\n                    MBConvConfig(4, 3, 2, 80, 160, 7),\n                    MBConvConfig(6, 3, 1, 160, 176, 14),\n                    MBConvConfig(6, 3, 2, 176, 304, 18),\n                    MBConvConfig(6, 3, 1, 304, 512, 5),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_l\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n                    FusedMBConvConfig(4, 3, 2, 32, 64, 7),\n                    FusedMBConvConfig(4, 3, 2, 64, 96, 7),\n                    MBConvConfig(4, 3, 2, 96, 192, 10),\n                    MBConvConfig(6, 3, 1, 192, 224, 19),\n                    MBConvConfig(6, 3, 2, 224, 384, 25),\n                    MBConvConfig(6, 3, 1, 384, 640, 7),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_xl\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n                    FusedMBConvConfig(4, 3, 2, 32, 64, 8),\n                    FusedMBConvConfig(4, 3, 2, 64, 96, 8),\n                    MBConvConfig(4, 3, 2, 96, 192, 16),\n                    MBConvConfig(6, 3, 1, 192, 256, 24),\n                    MBConvConfig(6, 3, 2, 256, 512, 32),\n                    MBConvConfig(6, 3, 1, 512, 640, 8),\n                ]\n                self.last_channel = 1280\n\n        # building first layer\n        firstconv_output_channels = inverted_residual_setting[0].input_channels\n        layers.extend([\n            nn.Conv2d(in_channels, firstconv_output_channels, kernel_size=3, stride=2),\n            norm_layer(firstconv_output_channels),\n            Swish(),\n        ])\n\n        # building MBConv blocks\n        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n        stage_block_id = 0\n\n        # cnf is the settings of block\n        for cnf in inverted_residual_setting:\n            stage: List[nn.Cell] = []\n\n            # cnf.num_layers is the num of the same block\n            for _ in range(cnf.num_layers):\n                # copy to avoid modifications. shallow copy is enough\n                block_cnf = copy.copy(cnf)\n\n                block = MBConv\n\n                if \"FusedMBConvConfig\" in str(type(block_cnf)):\n                    block = FusedMBConv\n\n                # overwrite info if not the first conv in the stage\n                if stage:\n                    block_cnf.input_channels = block_cnf.out_channels\n                    block_cnf.stride = 1\n\n                # adjust dropout rate of blocks based on the depth of the stage block\n                sd_prob = keep_prob * float(stage_block_id + 0.00001) / total_stage_blocks\n\n                stage.append(block(block_cnf, sd_prob, norm_layer))\n                stage_block_id += 1\n\n            layers.append(nn.SequentialCell(stage))\n\n        # building last several layers\n        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n        lastconv_output_channels = self.last_channel if self.last_channel is not None else 4 * lastconv_input_channels\n        layers.extend([\n            nn.Conv2d(lastconv_input_channels, lastconv_output_channels, kernel_size=1),\n            norm_layer(lastconv_output_channels),\n            Swish(),\n        ])\n\n        self.features = nn.SequentialCell(layers)\n        self.avgpool = GlobalAvgPooling()\n        self.dropout = nn.Dropout(1 - dropout_rate)\n        self.mlp_head = nn.Dense(lastconv_output_channels, num_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n\n        x = self.avgpool(x)\n\n        if self.training:\n            x = self.dropout(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"construct\"\"\"\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                init_range = 1.0 / np.sqrt(cell.weight.shape[0])\n                cell.weight.set_data(weight_init.initializer(Uniform(init_range), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            if isinstance(cell, nn.Conv2d):\n                out_channel, _, kernel_size_h, kernel_size_w = cell.weight.shape\n                stddev = np.sqrt(2 / int(out_channel * kernel_size_h * kernel_size_w))\n                cell.weight.set_data(\n                    weight_init.initializer(Normal(sigma=stddev), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.EfficientNet.construct(x)</code> \u00b6 <p>construct</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"construct\"\"\"\n    x = self.forward_features(x)\n    return self.forward_head(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.FusedMBConv</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>FusedMBConv</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>class FusedMBConv(nn.Cell):\n\"\"\"FusedMBConv\"\"\"\n\n    def __init__(\n        self,\n        cnf: FusedMBConvConfig,\n        keep_prob: float,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if not 1 &lt;= cnf.stride &lt;= 2:\n            raise ValueError(\"illegal stride value\")\n\n        self.shortcut = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Cell] = []\n\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            # fused expand\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, expanded_channels, kernel_size=cnf.kernel_size,\n                          stride=cnf.stride),\n                norm(expanded_channels),\n                Swish(),\n            ])\n\n            # project\n            layers.extend([\n                nn.Conv2d(expanded_channels, cnf.out_channels, kernel_size=1),\n                norm(cnf.out_channels),\n            ])\n        else:\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, cnf.out_channels, kernel_size=cnf.kernel_size,\n                          stride=cnf.stride),\n                norm(cnf.out_channels),\n                Swish(),\n            ])\n\n        self.block = nn.SequentialCell(layers)\n        self.dropout = DropPath(keep_prob)\n        self.out_channels = cnf.out_channels\n\n    def construct(self, x) -&gt; Tensor:\n        result = self.block(x)\n        if self.shortcut:\n            result = self.dropout(result)\n            result += x\n        return result\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.FusedMBConvConfig</code> \u00b6 <p>         Bases: <code>MBConvConfig</code></p> <p>FusedMBConvConfig</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>class FusedMBConvConfig(MBConvConfig):\n\"\"\"FusedMBConvConfig\"\"\"\n\n    # Stores information listed at Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel_size: int,\n        stride: int,\n        in_chs: int,\n        out_chs: int,\n        num_layers: int,\n    ) -&gt; None:\n        super().__init__(expand_ratio, kernel_size, stride, in_chs, out_chs, num_layers)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.MBConv</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MBConv Module.</p> PARAMETER DESCRIPTION <code>cnf</code> <p>The class which contains the parameters(in_channels, out_channels, nums_layers) and the functions which help calculate the parameters after multipling the expand_ratio.</p> <p> TYPE: <code>MBConvConfig</code> </p> <code>keep_prob</code> <p>The dropout rate in MBConv. Default: 0.8.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>norm</code> <p>The BatchNorm Method. Default: None.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>None</code> </p> <code>se_layer</code> <p>The squeeze-excite Module. Default: SqueezeExcite.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>SqueezeExcite</code> </p> RETURNS DESCRIPTION <p>Tensor</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>class MBConv(nn.Cell):\n\"\"\"\n    MBConv Module.\n\n    Args:\n        cnf (MBConvConfig): The class which contains the parameters(in_channels, out_channels, nums_layers) and\n            the functions which help calculate the parameters after multipling the expand_ratio.\n        keep_prob: The dropout rate in MBConv. Default: 0.8.\n        norm (nn.Cell): The BatchNorm Method. Default: None.\n        se_layer (nn.Cell): The squeeze-excite Module. Default: SqueezeExcite.\n\n    Returns:\n        Tensor\n    \"\"\"\n\n    def __init__(\n        self,\n        cnf: MBConvConfig,\n        keep_prob: float = 0.8,\n        norm: Optional[nn.Cell] = None,\n        se_layer: Callable[..., nn.Cell] = SqueezeExcite,\n    ) -&gt; None:\n        super().__init__()\n\n        self.shortcut = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Cell] = []\n\n        # expand conv: the out_channels is cnf.expand_ratio times of the in_channels.\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, expanded_channels, kernel_size=1),\n                norm(expanded_channels),\n                Swish(),\n            ])\n\n        # depthwise conv: splits the filter into groups.\n        layers.extend([\n            nn.Conv2d(expanded_channels, expanded_channels, kernel_size=cnf.kernel_size,\n                      stride=cnf.stride, group=expanded_channels),\n            norm(expanded_channels),\n            Swish(),\n        ])\n\n        # squeeze and excitation\n        squeeze_channels = max(1, cnf.input_channels // 4)\n        layers.append(se_layer(in_channels=expanded_channels, rd_channels=squeeze_channels, act_layer=Swish))\n\n        # project\n        layers.extend([\n            nn.Conv2d(expanded_channels, cnf.out_channels, kernel_size=1),\n            norm(cnf.out_channels),\n        ])\n\n        self.block = nn.SequentialCell(layers)\n        self.dropout = DropPath(keep_prob)\n        self.out_channels = cnf.out_channels\n\n    def construct(self, x) -&gt; Tensor:\n        result = self.block(x)\n        if self.shortcut:\n            result = self.dropout(result)\n            result += x\n        return result\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.MBConvConfig</code> \u00b6 <p>The Parameters of MBConv which need to multiply the expand_ration.</p> PARAMETER DESCRIPTION <code>expand_ratio</code> <p>The Times of the num of out_channels with respect to in_channels.</p> <p> TYPE: <code>float</code> </p> <code>kernel_size</code> <p>The kernel size of the depthwise conv.</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>The stride of the depthwise conv.</p> <p> TYPE: <code>int</code> </p> <code>in_chs</code> <p>The input_channels of the MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>out_chs</code> <p>The output_channels of the MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>The num of MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>width_cnf</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_cnf</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cnf = MBConvConfig(1, 3, 1, 32, 16, 1)\n&gt;&gt;&gt; print(cnf.input_channels)\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>class MBConvConfig:\n\"\"\"\n    The Parameters of MBConv which need to multiply the expand_ration.\n\n    Args:\n        expand_ratio (float): The Times of the num of out_channels with respect to in_channels.\n        kernel_size (int): The kernel size of the depthwise conv.\n        stride (int): The stride of the depthwise conv.\n        in_chs (int): The input_channels of the MBConv Module.\n        out_chs (int): The output_channels of the MBConv Module.\n        num_layers (int): The num of MBConv Module.\n        width_cnf: The ratio of the channel. Default: 1.0.\n        depth_cnf: The ratio of num_layers. Default: 1.0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; cnf = MBConvConfig(1, 3, 1, 32, 16, 1)\n        &gt;&gt;&gt; print(cnf.input_channels)\n    \"\"\"\n\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel_size: int,\n        stride: int,\n        in_chs: int,\n        out_chs: int,\n        num_layers: int,\n        width_cnf: float = 1.0,\n        depth_cnf: float = 1.0,\n    ) -&gt; None:\n        self.expand_ratio = expand_ratio\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.input_channels = self.adjust_channels(in_chs, width_cnf)\n        self.out_channels = self.adjust_channels(out_chs, width_cnf)\n        self.num_layers = self.adjust_depth(num_layers, depth_cnf)\n\n    @staticmethod\n    def adjust_channels(channels: int, width_cnf: float, min_value: Optional[int] = None) -&gt; int:\n\"\"\"\n        Calculate the width of MBConv.\n\n        Args:\n            channels (int): The number of channel.\n            width_cnf (float): The ratio of channel.\n            min_value (int, optional): The minimum number of channel. Default: None.\n\n        Returns:\n            int, the width of MBConv.\n        \"\"\"\n\n        return make_divisible(channels * width_cnf, 8, min_value)\n\n    @staticmethod\n    def adjust_depth(num_layers: int, depth_cnf: float) -&gt; int:\n\"\"\"\n        Calculate the depth of MBConv.\n\n        Args:\n            num_layers (int): The number of MBConv Module.\n            depth_cnf (float): The ratio of num_layers.\n\n        Returns:\n            int, the depth of MBConv.\n        \"\"\"\n\n        return int(math.ceil(num_layers * depth_cnf))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.MBConvConfig.adjust_channels(channels, width_cnf, min_value=None)</code> <code>staticmethod</code> \u00b6 <p>Calculate the width of MBConv.</p> PARAMETER DESCRIPTION <code>channels</code> <p>The number of channel.</p> <p> TYPE: <code>int</code> </p> <code>width_cnf</code> <p>The ratio of channel.</p> <p> TYPE: <code>float</code> </p> <code>min_value</code> <p>The minimum number of channel. Default: None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>int, the width of MBConv.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@staticmethod\ndef adjust_channels(channels: int, width_cnf: float, min_value: Optional[int] = None) -&gt; int:\n\"\"\"\n    Calculate the width of MBConv.\n\n    Args:\n        channels (int): The number of channel.\n        width_cnf (float): The ratio of channel.\n        min_value (int, optional): The minimum number of channel. Default: None.\n\n    Returns:\n        int, the width of MBConv.\n    \"\"\"\n\n    return make_divisible(channels * width_cnf, 8, min_value)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.MBConvConfig.adjust_depth(num_layers, depth_cnf)</code> <code>staticmethod</code> \u00b6 <p>Calculate the depth of MBConv.</p> PARAMETER DESCRIPTION <code>num_layers</code> <p>The number of MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>depth_cnf</code> <p>The ratio of num_layers.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>int</code> <p>int, the depth of MBConv.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@staticmethod\ndef adjust_depth(num_layers: int, depth_cnf: float) -&gt; int:\n\"\"\"\n    Calculate the depth of MBConv.\n\n    Args:\n        num_layers (int): The number of MBConv Module.\n        depth_cnf (float): The ratio of num_layers.\n\n    Returns:\n        int, the depth of MBConv.\n    \"\"\"\n\n    return int(math.ceil(num_layers * depth_cnf))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B0 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B0 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b0\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B1 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B1 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b1\", 1.0, 1.1, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B2 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B2 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b2\", 1.1, 1.2, 0.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B3 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B3 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b3\", 1.2, 1.4, 0.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b4\", 1.4, 1.8, 0.4, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B5 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B5 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b5\", 1.6, 2.2, 0.4, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b6(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B6 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b6(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B6 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b6\", 1.8, 2.6, 0.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_b7(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B7 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b7(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B7 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b7\", 2.0, 3.1, 0.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_v2_l(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_l(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_l\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_v2_m(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_m(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_m\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_v2_s(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_s(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_s\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.efficientnet.efficientnet_v2_xl(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_xl(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n\"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_xl\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.ghostnet</code> \u00b6 <p>MindSpore implementation of <code>GhostNet</code>.</p> <code>mindocr.models.backbones.mindcv_models.ghostnet.ConvBnAct</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>A block for conv bn and relu</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\ghostnet.py</code> <pre><code>class ConvBnAct(nn.Cell):\n\"\"\"A block for conv bn and relu\"\"\"\n\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.ReLU):\n        super().__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,\n                              padding=kernel_size // 2, pad_mode=\"pad\", has_bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer()\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.ghostnet.GhostGate</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Implementation for (relu6 + 3) / 6</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\ghostnet.py</code> <pre><code>class GhostGate(nn.Cell):\n\"\"\"Implementation for (relu6 + 3) / 6\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.relu6 = nn.ReLU6()\n\n    def construct(self, x):\n        return self.relu6(x + 3.0) * 0.16666667\n</code></pre> <code>mindocr.models.backbones.mindcv_models.ghostnet.GhostNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>GhostNet model class, based on <code>\"GhostNet: More Features from Cheap Operations \" &lt;https://arxiv.org/abs/1911.11907&gt;</code>_</p> PARAMETER DESCRIPTION <code>cfgs</code> <p>the config of the GhostNet.</p> <p> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>width</code> <p>base width of hidden channel in blocks. Default: 1.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>droupout</code> <p>the probability of the features before classification. Default: 0.2</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\ghostnet.py</code> <pre><code>class GhostNet(nn.Cell):\nr\"\"\"GhostNet model class, based on\n    `\"GhostNet: More Features from Cheap Operations \" &lt;https://arxiv.org/abs/1911.11907&gt;`_\n\n    Args:\n        cfgs: the config of the GhostNet.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of input channels. Default: 3.\n        width: base width of hidden channel in blocks. Default: 1.0\n        droupout: the probability of the features before classification. Default: 0.2\n    \"\"\"\n\n    def __init__(\n        self,\n        cfgs,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        width: float = 1.0,\n        dropout: float = 0.2,\n    ) -&gt; None:\n        super().__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        self.dropout_rate = dropout\n\n        # building first layer\n        output_channel = make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(in_channels, output_channel, kernel_size=3,\n                                   padding=1, stride=2, has_bias=False, pad_mode=\"pad\")\n        self.bn1 = nn.BatchNorm2d(output_channel)\n        self.act1 = nn.ReLU()\n        input_channel = output_channel\n\n        # building inverted residual blocks\n        stages = []\n        block = GhostBottleneck\n        exp_size = 128\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                output_channel = make_divisible(c * width, 4)\n                hidden_channel = make_divisible(exp_size * width, 4)\n                layers.append(block(input_channel, hidden_channel, output_channel, k, s, se_ratio=se_ratio))\n                input_channel = output_channel\n            stages.append(nn.SequentialCell([*layers]))\n\n        output_channel = make_divisible(exp_size * width, 4)\n        stages.append(nn.SequentialCell([ConvBnAct(input_channel, output_channel, 1)]))\n        input_channel = output_channel\n\n        self.blocks = nn.SequentialCell([*stages])\n\n        # building last several layers\n        output_channel = 1280\n        self.global_pool = GlobalAvgPooling(keep_dims=True)\n        self.conv_head = nn.Conv2d(input_channel, output_channel, kernel_size=1,\n                                   padding=0, stride=1, has_bias=True, pad_mode=\"pad\")\n        self.act2 = nn.ReLU()\n        if self.dropout_rate &gt; 0:\n            self.dropout = nn.Dropout(self.dropout_rate)\n        self.classifier = nn.Dense(output_channel, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        self.init_parameters_data()\n        for _, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.set_data(Tensor(np.random.normal(0, np.sqrt(2.0 / n), m.weight.data.shape).astype(\"float32\")))\n                if m.bias is not None:\n                    m.bias.set_data(Tensor(np.zeros(m.bias.data.shape, dtype=\"float32\")))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.gamma.set_data(Tensor(np.ones(m.gamma.data.shape, dtype=\"float32\")))\n                m.beta.set_data(Tensor(np.zeros(m.beta.data.shape, dtype=\"float32\")))\n            elif isinstance(m, nn.Dense):\n                m.weight.set_data(Tensor(np.random.normal(0, 0.01, m.weight.data.shape).astype(\"float32\")))\n                if m.bias is not None:\n                    m.bias.set_data(Tensor(np.zeros(m.bias.data.shape, dtype=\"float32\")))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = ops.flatten(x)\n        if self.dropout_rate &gt; 0.0:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.ghostnet.ghostnet_1x(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get GhostNet model. Refer to the base class 'models.GhostNet' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\ghostnet.py</code> <pre><code>@register_model\ndef ghostnet_1x(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; GhostNet:\n\"\"\"Get GhostNet model.\n    Refer to the base class 'models.GhostNet' for more details.\n    \"\"\"\n    model_args = model_cfgs[\"1x\"][\"cfg\"]\n    model = GhostNet(cfgs=model_args, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, model_args, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.ghostnet.ghostnet_nose_1x(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get GhostNet model without SEModule. Refer to the base class 'models.GhostNet' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\ghostnet.py</code> <pre><code>@register_model\ndef ghostnet_nose_1x(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; GhostNet:\n\"\"\"Get GhostNet model without SEModule.\n    Refer to the base class 'models.GhostNet' for more details.\n    \"\"\"\n    model_args = model_cfgs[\"nose_1x\"][\"cfg\"]\n    model = GhostNet(cfgs=model_args, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, model_args, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet</code> \u00b6 <p>MindSpore implementation of <code>HRNet</code>. Refer to Deep High-Resolution Representation Learning for Visual Recognition</p> <code>mindocr.models.backbones.mindcv_models.hrnet.BasicBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic block of HRNet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>class BasicBlock(nn.Cell):\n\"\"\"Basic block of HRNet\"\"\"\n\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n        assert groups == 1, \"BasicBlock only supports groups=1\"\n        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            pad_mode=\"pad\",\n        )\n        self.bn1 = norm(channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(\n            channels, channels, kernel_size=3, stride=1, padding=1, pad_mode=\"pad\"\n        )\n        self.bn2 = norm(channels)\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Bottleneck block of HRNet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"Bottleneck block of HRNet\"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        width = int(channels * (base_width / 64.0)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n        self.bn1 = norm(width)\n        self.conv2 = nn.Conv2d(\n            width,\n            width,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            pad_mode=\"pad\",\n            group=groups,\n        )\n        self.bn2 = norm(width)\n        self.conv3 = nn.Conv2d(\n            width, channels * self.expansion, kernel_size=1, stride=1\n        )\n        self.bn3 = norm(channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.HRModule</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>High-Resolution Module for HRNet. In this module, every branch has 4 BasicBlocks/Bottlenecks. Fusion/Exchange is in this module.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>class HRModule(nn.Cell):\n\"\"\"High-Resolution Module for HRNet.\n    In this module, every branch has 4 BasicBlocks/Bottlenecks. Fusion/Exchange\n    is in this module.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_branches: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_inchannels: List[int],\n        num_channels: List[int],\n        multi_scale_output: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self._check_branches(num_branches, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, block, num_blocks, num_channels\n        )\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU()\n\n    @staticmethod\n    def _check_branches(\n        num_branches: int,\n        num_blocks: List[int],\n        num_inchannels: List[int],\n        num_channels: List[int],\n    ) -&gt; None:\n\"\"\"Check input to avoid ValueError.\"\"\"\n        if num_branches != len(num_blocks):\n            error_msg = f\"NUM_BRANCHES({num_branches})!= NUM_BLOCKS({len(num_blocks)})\"\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = (\n                f\"NUM_BRANCHES({num_branches})!= NUM_CHANNELS({len(num_channels)})\"\n            )\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = (\n                f\"NUM_BRANCHES({num_branches}) != NUM_INCHANNELS({len(num_inchannels)})\"\n            )\n            raise ValueError(error_msg)\n\n    def _make_one_branch(\n        self,\n        branch_index: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_channels: List[int],\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.SequentialCell(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                ),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                down_sample=downsample,\n            )\n        )\n        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n        for _ in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(self.num_inchannels[branch_index], num_channels[branch_index])\n            )\n\n        return nn.SequentialCell(layers)\n\n    def _make_branches(\n        self,\n        num_branches: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_channels: List[int],\n    ) -&gt; nn.CellList:\n\"\"\"Make branches.\"\"\"\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.CellList(branches)\n\n    def _make_fuse_layers(self) -&gt; nn.CellList:\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j &gt; i:\n                    fuse_layer.append(\n                        nn.SequentialCell(\n                            nn.Conv2d(\n                                num_inchannels[j], num_inchannels[i], kernel_size=1\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(IdentityCell())\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.SequentialCell(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        pad_mode=\"pad\",\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.SequentialCell(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        pad_mode=\"pad\",\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(),\n                                )\n                            )\n                    fuse_layer.append(nn.SequentialCell(conv3x3s))\n            fuse_layers.append(nn.CellList(fuse_layer))\n\n        return nn.CellList(fuse_layers)\n\n    def construct(self, x: List[Tensor]) -&gt; List[Tensor]:\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j &gt; i:\n                    _, _, height, width = x[i].shape\n                    t = self.fuse_layers[i][j](x[j])\n                    t = ops.ResizeNearestNeighbor((height, width))(t)\n                    y = y + t\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        if not self.multi_scale_output:\n            x_fuse = x_fuse[0]\n\n        return x_fuse\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.HRNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>HRNet Backbone, based on <code>\"Deep High-Resolution Representation Learning for Visual Recognition\" &lt;https://arxiv.org/abs/1908.07919&gt;</code>_.</p> PARAMETER DESCRIPTION <code>stage_cfg</code> <p>Configuration of the extra blocks. It accepts a dictionay storing the detail config of each block. which include <code>num_modules</code>, <code>num_branches</code>, <code>block</code>, <code>num_blocks</code>, <code>num_channels</code>. For detail example, please check the implementation of <code>hrnet_w32</code> and <code>hrnet_w48</code>.</p> <p> TYPE: <code>Dict[str, Dict[str, int]]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>class HRNet(nn.Cell):\nr\"\"\"HRNet Backbone, based on\n    `\"Deep High-Resolution Representation Learning for Visual Recognition\"\n    &lt;https://arxiv.org/abs/1908.07919&gt;`_.\n\n    Args:\n        stage_cfg: Configuration of the extra blocks. It accepts a dictionay\n            storing the detail config of each block. which include `num_modules`,\n            `num_branches`, `block`, `num_blocks`, `num_channels`. For detail example,\n            please check the implementation of `hrnet_w32` and `hrnet_w48`.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: Number the channels of the input. Default: 3.\n    \"\"\"\n\n    blocks_dict = {\"BASIC\": BasicBlock, \"BOTTLENECK\": Bottleneck}\n\n    def __init__(\n        self,\n        stage_cfg: Dict[str, Dict[str, int]],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n    ) -&gt; None:\n        super().__init__()\n\n        self.stage_cfg = stage_cfg\n        # stem net\n        self.conv1 = nn.Conv2d(\n            in_channels, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\"\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(\n            64, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\"\n        )\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n\n        # stage 1\n        self.stage1_cfg = self.stage_cfg[\"stage1\"]\n        num_channels = self.stage1_cfg[\"num_channels\"][0]\n        num_blocks = self.stage1_cfg[\"num_blocks\"][0]\n        block = self.blocks_dict[self.stage1_cfg[\"block\"]]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n\n        # stage 2\n        self.stage2_cfg = self.stage_cfg[\"stage2\"]\n        num_channels = self.stage2_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage2_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n\n        self.transition1, self.transition1_flags = self._make_transition_layer(\n            [256], num_channels\n        )\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels\n        )\n\n        # stage 3\n        self.stage3_cfg = self.stage_cfg[\"stage3\"]\n        num_channels = self.stage3_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage3_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n\n        self.transition2, self.transition2_flags = self._make_transition_layer(\n            pre_stage_channels, num_channels\n        )\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels\n        )\n\n        # stage 4\n        self.stage4_cfg = self.stage_cfg[\"stage4\"]\n        num_channels = self.stage4_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage4_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3, self.transition3_flags = self._make_transition_layer(\n            pre_stage_channels, num_channels\n        )\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels\n        )\n\n        # head\n        self.pool = GlobalAvgPooling()\n        self.incre_modules, self.downsample_modules, self.final_layer = self._make_head(\n            pre_stage_channels\n        )\n        self.classifier = nn.Dense(2048, num_classes)\n\n    def _make_head(self, pre_stage_channels: List[int]):\n        head_block = Bottleneck\n        head_channels = [32, 64, 128, 256]\n\n        # increase the #channesl on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = list()\n        for i, channels in enumerate(pre_stage_channels):\n            incre_module = self._make_layer(\n                head_block, channels, head_channels[i], 1, stride=1\n            )\n            incre_modules.append(incre_module)\n        incre_modules = nn.CellList(incre_modules)\n\n        # downsample modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = head_channels[i] * head_block.expansion\n            out_channels = head_channels[i + 1] * head_block.expansion\n\n            downsamp_module = nn.SequentialCell(\n                nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    kernel_size=3,\n                    stride=2,\n                    pad_mode=\"pad\",\n                    padding=1,\n                ),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            )\n\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.CellList(downsamp_modules)\n\n        final_layer = nn.SequentialCell(\n            nn.Conv2d(\n                in_channels=head_channels[3] * head_block.expansion,\n                out_channels=2048,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.BatchNorm2d(2048),\n            nn.ReLU(),\n        )\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(\n        self, num_channels_pre_layer: List[int], num_channels_cur_layer: List[int]\n    ) -&gt; Tuple[nn.CellList, List[bool]]:\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        transition_layers_flags = []\n        for i in range(num_branches_cur):\n            if i &lt; num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.SequentialCell(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                kernel_size=3,\n                                padding=1,\n                                pad_mode=\"pad\",\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(),\n                        )\n                    )\n                    transition_layers_flags.append(True)\n                else:\n                    transition_layers.append(IdentityCell())\n                    transition_layers_flags.append(False)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = (\n                        num_channels_cur_layer[i]\n                        if j == i - num_branches_pre\n                        else inchannels\n                    )\n                    conv3x3s.append(\n                        nn.SequentialCell(\n                            [\n                                nn.Conv2d(\n                                    inchannels,\n                                    outchannels,\n                                    kernel_size=3,\n                                    stride=2,\n                                    padding=1,\n                                    pad_mode=\"pad\",\n                                ),\n                                nn.BatchNorm2d(outchannels),\n                                nn.ReLU(),\n                            ]\n                        )\n                    )\n                transition_layers.append(nn.SequentialCell(conv3x3s))\n                transition_layers_flags.append(True)\n\n        return nn.CellList(transition_layers), transition_layers_flags\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        in_channels: int,\n        out_channels: int,\n        blocks: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or in_channels != out_channels * block.expansion:\n            downsample = nn.SequentialCell(\n                nn.Conv2d(\n                    in_channels,\n                    out_channels * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                ),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(in_channels, out_channels, stride, down_sample=downsample))\n        for _ in range(1, blocks):\n            layers.append(block(out_channels * block.expansion, out_channels))\n\n        return nn.SequentialCell(layers)\n\n    def _make_stage(\n        self,\n        layer_config: Dict[str, int],\n        num_inchannels: int,\n        multi_scale_output: bool = True,\n    ) -&gt; Tuple[nn.SequentialCell, List[int]]:\n        num_modules = layer_config[\"num_modules\"]\n        num_branches = layer_config[\"num_branches\"]\n        num_blocks = layer_config[\"num_blocks\"]\n        num_channels = layer_config[\"num_channels\"]\n        block = self.blocks_dict[layer_config[\"block\"]]\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HRModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    reset_multi_scale_output,\n                )\n            )\n            num_inchannels = modules[-1].num_inchannels\n\n        return nn.SequentialCell(modules), num_inchannels\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Perform the feature extraction.\n\n        Args:\n            x: Tensor\n\n        Returns:\n            Extracted feature\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        # stage 1\n        x = self.layer1(x)\n\n        # stage 2\n        x_list = []\n        for i in range(self.stage2_cfg[\"num_branches\"]):\n            if self.transition1_flags[i]:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        # stage 3\n        x_list = []\n        for i in range(self.stage3_cfg[\"num_branches\"]):\n            if self.transition2_flags[i]:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        # stage 4\n        x_list = []\n        for i in range(self.stage4_cfg[\"num_branches\"]):\n            if self.transition3_flags[i]:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y = self.stage4(x_list)\n\n        return y\n\n    def forward_head(self, x: List[Tensor]) -&gt; Tensor:\n        y = self.incre_modules[0](x[0])\n        for i in range(len(self.downsample_modules)):\n            y = self.incre_modules[i + 1](x[i + 1]) + self.downsample_modules[i](y)\n\n        y = self.final_layer(y)\n        y = self.pool(y)\n        y = self.classifier(y)\n        return y\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.HRNet.forward_features(x)</code> \u00b6 <p>Perform the feature extraction.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Extracted feature</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Perform the feature extraction.\n\n    Args:\n        x: Tensor\n\n    Returns:\n        Extracted feature\n    \"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.relu(x)\n\n    # stage 1\n    x = self.layer1(x)\n\n    # stage 2\n    x_list = []\n    for i in range(self.stage2_cfg[\"num_branches\"]):\n        if self.transition1_flags[i]:\n            x_list.append(self.transition1[i](x))\n        else:\n            x_list.append(x)\n    y_list = self.stage2(x_list)\n\n    # stage 3\n    x_list = []\n    for i in range(self.stage3_cfg[\"num_branches\"]):\n        if self.transition2_flags[i]:\n            x_list.append(self.transition2[i](y_list[-1]))\n        else:\n            x_list.append(y_list[i])\n    y_list = self.stage3(x_list)\n\n    # stage 4\n    x_list = []\n    for i in range(self.stage4_cfg[\"num_branches\"]):\n        if self.transition3_flags[i]:\n            x_list.append(self.transition3[i](y_list[-1]))\n        else:\n            x_list.append(y_list[i])\n    y = self.stage4(x_list)\n\n    return y\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.IdentityCell</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Identity Cell</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>class IdentityCell(nn.Cell):\n\"\"\"Identity Cell\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def construct(self, x: Any) -&gt; Any:\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.hrnet_w32(pretrained=False, num_classes=1000, in_channels=3)</code> \u00b6 <p>Get HRNet with width=32 model. Refer to the base class <code>models.HRNet</code> for more details.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether the model is pretrained. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number of input channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>HRNet</code> <p>HRNet model</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>@register_model\ndef hrnet_w32(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3\n) -&gt; HRNet:\n\"\"\"Get HRNet with width=32 model.\n    Refer to the base class `models.HRNet` for more details.\n\n    Args:\n        pretrained: Whether the model is pretrained. Default: False\n        num_classes: number of classification classes. Default: 1000\n        in_channels: Number of input channels. Default: 3\n\n    Returns:\n        HRNet model\n    \"\"\"\n    default_cfg = default_cfgs[\"hrnet_w32\"]\n    stage_cfg = dict(\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block=\"BOTTLENECK\",\n            num_blocks=[4],\n            num_channels=[64],\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block=\"BASIC\",\n            num_blocks=[4, 4],\n            num_channels=[32, 64],\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4],\n            num_channels=[32, 64, 128],\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4, 4],\n            num_channels=[32, 64, 128, 256],\n        ),\n    )\n    model = HRNet(stage_cfg, num_classes=num_classes, in_channels=in_channels)\n    if pretrained:\n        load_pretrained(\n            model, default_cfg, num_classes=num_classes, in_channels=in_channels\n        )\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.hrnet.hrnet_w48(pretrained=False, num_classes=1000, in_channels=3)</code> \u00b6 <p>Get HRNet with width=48 model. Refer to the base class <code>models.HRNet</code> for more details.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether the model is pretrained. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number of input channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>HRNet</code> <p>HRNet model</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\hrnet.py</code> <pre><code>@register_model\ndef hrnet_w48(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3\n) -&gt; HRNet:\n\"\"\"Get HRNet with width=48 model.\n    Refer to the base class `models.HRNet` for more details.\n\n    Args:\n        pretrained: Whether the model is pretrained. Default: False\n        num_classes: number of classification classes. Default: 1000\n        in_channels: Number of input channels. Default: 3\n\n    Returns:\n        HRNet model\n    \"\"\"\n    default_cfg = default_cfgs[\"hrnet_w48\"]\n    stage_cfg = dict(\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block=\"BOTTLENECK\",\n            num_blocks=[4],\n            num_channels=[64],\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block=\"BASIC\",\n            num_blocks=[4, 4],\n            num_channels=[48, 96],\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4],\n            num_channels=[48, 96, 192],\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4, 4],\n            num_channels=[48, 96, 192, 384],\n        ),\n    )\n    model = HRNet(stage_cfg, num_classes=num_classes, in_channels=in_channels)\n    if pretrained:\n        load_pretrained(\n            model, default_cfg, num_classes=num_classes, in_channels=in_channels\n        )\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers</code> \u00b6 <p>layers init</p> <code>mindocr.models.backbones.mindcv_models.layers.activation</code> \u00b6 <p>Custom operators.</p> <code>mindocr.models.backbones.mindcv_models.layers.activation.Swish</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Swish activation function: x * sigmoid(x).</p> Return <p>Tensor</p> Example <p>x = Tensor(((20, 16), (50, 50)), mindspore.float32) Swish()(x)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\activation.py</code> <pre><code>class Swish(nn.Cell):\n\"\"\"\n    Swish activation function: x * sigmoid(x).\n\n    Args:\n        None\n\n    Return:\n        Tensor\n\n    Example:\n        &gt;&gt;&gt; x = Tensor(((20, 16), (50, 50)), mindspore.float32)\n        &gt;&gt;&gt; Swish()(x)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.result = None\n        self.sigmoid = nn.Sigmoid()\n\n    def construct(self, x):\n        result = x * self.sigmoid(x)\n        return result\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.conv_norm_act</code> \u00b6 <p>Conv2d + BN + Act</p> <code>mindocr.models.backbones.mindcv_models.layers.conv_norm_act.Conv2dNormActivation</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Conv2d + BN + Act</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\conv_norm_act.py</code> <pre><code>class Conv2dNormActivation(nn.Cell):\n\"\"\"Conv2d + BN + Act\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        pad_mode: str = \"pad\",\n        padding: Optional[int] = None,\n        dilation: int = 1,\n        groups: int = 1,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        has_bias: Optional[bool] = None,\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n\n        if pad_mode == \"pad\":\n            if padding is None:\n                padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n        else:\n            padding = 0\n\n        if has_bias is None:\n            has_bias = norm is None\n\n        layers = [\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride,\n                pad_mode=pad_mode,\n                padding=padding,\n                dilation=dilation,\n                group=groups,\n                has_bias=has_bias,\n                **kwargs\n            )\n        ]\n\n        if norm:\n            layers.append(norm(out_channels))\n        if activation:\n            layers.append(activation())\n\n        self.features = nn.SequentialCell(layers)\n\n    def construct(self, x):\n        output = self.features(x)\n        return output\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.drop_path</code> \u00b6 <p>DropPath Mindspore implementations of DropPath (Stochastic Depth) regularization layers. Papers: Deep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)</p> <code>mindocr.models.backbones.mindcv_models.layers.drop_path.DropPath</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>DropPath (Stochastic Depth) regularization layers</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\drop_path.py</code> <pre><code>class DropPath(nn.Cell):\n\"\"\"DropPath (Stochastic Depth) regularization layers\"\"\"\n\n    def __init__(\n        self,\n        drop_prob: float = 0.0,\n        scale_by_keep: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.keep_prob = 1.0 - drop_prob\n        self.scale_by_keep = scale_by_keep\n        self.dropout = nn.Dropout(self.keep_prob)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.keep_prob == 1.0 or not self.training:\n            return x\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = self.dropout(ones(shape))\n        if not self.scale_by_keep:\n            random_tensor = ops.mul(random_tensor, self.keep_prob)\n        return x * random_tensor\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.helpers</code> \u00b6 <p>Layer/Module Helpers</p> <code>mindocr.models.backbones.mindcv_models.layers.identity</code> \u00b6 <p>Identity Module</p> <code>mindocr.models.backbones.mindcv_models.layers.identity.Identity</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Identity</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\identity.py</code> <pre><code>class Identity(nn.Cell):\n\"\"\"Identity\"\"\"\n\n    def construct(self, x):\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.mlp</code> \u00b6 <p>MLP module w/ dropout and configurable activation layer</p> <code>mindocr.models.backbones.mindcv_models.layers.patch_embed</code> \u00b6 <p>Image to Patch Embedding using Conv2d A convolution based approach to patchifying a 2D image w/ embedding projection.</p> <code>mindocr.models.backbones.mindcv_models.layers.patch_embed.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Image size.  Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch token size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>embed_dim</code> <p>Number of linear projection output channels. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: None</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\patch_embed.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\"Image to Patch Embedding\n\n    Args:\n        image_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 96,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        image_size = to_2tuple(image_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n                              pad_mode='pad', has_bias=True, weight_init=\"TruncatedNormal\")\n\n        if norm_layer is not None:\n            if isinstance(embed_dim, int):\n                embed_dim = (embed_dim,)\n            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n        else:\n            self.norm = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"docstring\"\"\"\n        B = x.shape[0]\n        # FIXME look at relaxing size constraints\n        x = ops.Reshape()(self.proj(x), (B, self.embed_dim, -1))  # B Ph*Pw C\n        x = ops.Transpose()(x, (0, 2, 1))\n\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.patch_embed.PatchEmbed.construct(x)</code> \u00b6 <p>docstring</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\patch_embed.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"docstring\"\"\"\n    B = x.shape[0]\n    # FIXME look at relaxing size constraints\n    x = ops.Reshape()(self.proj(x), (B, self.embed_dim, -1))  # B Ph*Pw C\n    x = ops.Transpose()(x, (0, 2, 1))\n\n    if self.norm is not None:\n        x = self.norm(x)\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.pooling</code> \u00b6 <p>GlobalAvgPooling Module</p> <code>mindocr.models.backbones.mindcv_models.layers.pooling.GlobalAvgPooling</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\pooling.py</code> <pre><code>class GlobalAvgPooling(nn.Cell):\n\"\"\"\n    GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1\n    \"\"\"\n\n    def __init__(self, keep_dims: bool = False) -&gt; None:\n        super().__init__()\n        self.keep_dims = keep_dims\n\n    def construct(self, x):\n        x = ops.mean(x, axis=(2, 3), keep_dims=self.keep_dims)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.selective_kernel</code> \u00b6 <p>Selective Kernel Convolution/Attention Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)</p> <code>mindocr.models.backbones.mindcv_models.layers.selective_kernel.SelectiveKernel</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Selective Kernel Convolution Module As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications. Largest change is the input split, which divides the input channels across each convolution path, this can be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps the parameter count from ballooning when the convolutions themselves don't have groups, but still provides a noteworthy increase in performance over similar param count models without this attention layer. -Ross W</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>module input (feature) channel count</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>module output (feature) channel count</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_size</code> <p>kernel size for each convolution branch</p> <p> TYPE: <code>int, list</code> DEFAULT: <code>None</code> </p> <code>stride</code> <p>stride for convolutions</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>dilation for module as a whole, impacts dilation of each branch</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>groups</code> <p>number of groups for each branch</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>rd_ratio</code> <p>reduction factor for attention features</p> <p> TYPE: <code>int, float</code> DEFAULT: <code>1.0 / 16</code> </p> <code>rd_channels(int)</code> <p>reduction channels can be specified directly by arg (if rd_channels is set)</p> <p> </p> <code>rd_divisor(int)</code> <p>divisor can be specified to keep channels</p> <p> </p> <code>keep_3x3</code> <p>keep all branch convolution kernels as 3x3, changing larger kernels for dilations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>split_input</code> <p>split input channels evenly across each convolution branch, keeps param count lower, can be viewed as grouping by path, output expands to module out_channels count</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>activation</code> <p>activation layer to use</p> <p> TYPE: <code>nn.Module</code> DEFAULT: <code>nn.ReLU</code> </p> <code>norm</code> <p>batchnorm/norm layer to use</p> <p> TYPE: <code>nn.Module</code> DEFAULT: <code>nn.BatchNorm2d</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\selective_kernel.py</code> <pre><code>class SelectiveKernel(nn.Cell):\n\"\"\"Selective Kernel Convolution Module\n    As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n    Largest change is the input split, which divides the input channels across each convolution path, this can\n    be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n    the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n    a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n    Args:\n        in_channels (int):  module input (feature) channel count\n        out_channels (int):  module output (feature) channel count\n        kernel_size (int, list): kernel size for each convolution branch\n        stride (int): stride for convolutions\n        dilation (int): dilation for module as a whole, impacts dilation of each branch\n        groups (int): number of groups for each branch\n        rd_ratio (int, float): reduction factor for attention features\n        rd_channels(int): reduction channels can be specified directly by arg (if rd_channels is set)\n        rd_divisor(int): divisor can be specified to keep channels\n        keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n        split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n            can be viewed as grouping by path, output expands to module out_channels count\n        activation (nn.Module): activation layer to use\n        norm (nn.Module): batchnorm/norm layer to use\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        kernel_size: Optional[Union[int, List]] = None,\n        stride: int = 1,\n        dilation: int = 1,\n        groups: int = 1,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        keep_3x3: bool = True,\n        split_input: bool = True,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        out_channels = out_channels or in_channels\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -&gt; 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) // 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels // self.num_paths\n        groups = min(out_channels, groups)\n\n        self.paths = nn.CellList([\n            Conv2dNormActivation(in_channels, out_channels, kernel_size=k, stride=stride, groups=groups,\n                                 dilation=d, activation=activation, norm=norm)\n            for k, d in zip(kernel_size, dilation)\n        ])\n\n        attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)\n        self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_paths = []\n        if self.split_input:\n            x_split = ops.split(x, axis=1, output_num=self.num_paths)\n            for i, op in enumerate(self.paths):\n                x_paths.append(op(x_split[i]))\n        else:\n            for op in self.paths:\n                x_paths.append(op(x))\n\n        x = ops.stack(x_paths, axis=1)\n        x_attn = self.attn(x)\n        x = x * x_attn\n        x = x.sum(1)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.selective_kernel.SelectiveKernelAttn</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Selective Kernel Attention Module Selective Kernel attention mechanism factored out into its own module.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\selective_kernel.py</code> <pre><code>class SelectiveKernelAttn(nn.Cell):\n\"\"\"Selective Kernel Attention Module\n    Selective Kernel attention mechanism factored out into its own module.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        num_paths: int = 2,\n        attn_channels: int = 32,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        self.num_paths = num_paths\n        self.mean = GlobalAvgPooling(keep_dims=True)\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, has_bias=False)\n        self.bn = norm(attn_channels)\n        self.act = activation()\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1)\n        self.softmax = nn.Softmax(axis=1)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.mean((x.sum(1)))\n        x = self.fc_reduce(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.fc_select(x)\n        b, c, h, w = x.shape\n        x = x.reshape((b, self.num_paths, c // self.num_paths, h, w))\n        x = self.softmax(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.squeeze_excite</code> \u00b6 <p>Squeeze-and-Excitation Channel Attention An SE implementation originally based on PyTorch SE-Net impl. Has since evolved with additional functionality / configuration. Paper: <code>Squeeze-and-Excitation Networks</code> - https://arxiv.org/abs/1709.01507</p> <code>mindocr.models.backbones.mindcv_models.layers.squeeze_excite.SqueezeExcite</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions.</p> Additions include <ul> <li>divisor can be specified to keep channels % div == 0 (default: 8)</li> <li>reduction channels can be specified directly by arg (if rd_channels is set)</li> <li>reduction channels can be specified by float rd_ratio (default: 1/16)</li> <li>customizable activation, normalization, and gate layer</li> </ul> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\squeeze_excite.py</code> <pre><code>class SqueezeExcite(nn.Cell):\n\"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Conv2d(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x = x * x_se\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.layers.squeeze_excite.SqueezeExciteV2</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions. V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\layers\\squeeze_excite.py</code> <pre><code>class SqueezeExciteV2(nn.Cell):\n\"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Dense(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Dense(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=False)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x_se = ops.expand_dims(x_se, -1)\n        x_se = ops.expand_dims(x_se, -1)\n        x = x * x_se\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mixnet</code> \u00b6 <p>MindSpore implementation of <code>MixNet</code>. Refer to MixConv: Mixed Depthwise Convolutional Kernels</p> <code>mindocr.models.backbones.mindcv_models.mixnet.MDConv</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Mixed Depth-wise Convolution</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mixnet.py</code> <pre><code>class MDConv(nn.Cell):\n\"\"\"Mixed Depth-wise Convolution\"\"\"\n\n    def __init__(self, channels: int, kernel_size: list, stride: int) -&gt; None:\n        super(MDConv, self).__init__()\n        self.num_groups = len(kernel_size)\n\n        if self.num_groups == 1:\n            self.mixed_depthwise_conv = nn.Conv2d(\n                channels,\n                channels,\n                kernel_size[0],\n                stride=stride,\n                pad_mode=\"pad\",\n                padding=kernel_size[0] // 2,\n                group=channels,\n                has_bias=False\n            )\n        else:\n            self.split_channels = _splitchannels(channels, self.num_groups)\n\n            self.mixed_depthwise_conv = nn.CellList()\n            for i in range(self.num_groups):\n                self.mixed_depthwise_conv.append(nn.Conv2d(\n                    self.split_channels[i],\n                    self.split_channels[i],\n                    kernel_size[i],\n                    stride=stride,\n                    pad_mode=\"pad\",\n                    padding=kernel_size[i] // 2,\n                    group=self.split_channels[i],\n                    has_bias=False\n                ))\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.num_groups == 1:\n            return self.mixed_depthwise_conv(x)\n\n        output = []\n        start, end = 0, 0\n        for i in range(self.num_groups):\n            start, end = end, end + self.split_channels[i]\n            x_split = x[:, start:end]\n\n            conv = self.mixed_depthwise_conv[i]\n            output.append(conv(x_split))\n\n        return ops.concat(output, axis=1)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mixnet.MixNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MixNet model class, based on <code>\"MixConv: Mixed Depthwise Convolutional Kernels\" &lt;https://arxiv.org/abs/1907.09595&gt;</code>_</p> PARAMETER DESCRIPTION <code>arch</code> <p>size of the architecture. \"small\", \"medium\" or \"large\". Default: \"small\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'small'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>feature_size</code> <p>numbet of the channels of the output features. Default: 1536.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1536</code> </p> <code>drop_rate</code> <p>rate of dropout for classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>depth_multiplier</code> <p>expansion coefficient of channels. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mixnet.py</code> <pre><code>class MixNet(nn.Cell):\nr\"\"\"MixNet model class, based on\n    `\"MixConv: Mixed Depthwise Convolutional Kernels\" &lt;https://arxiv.org/abs/1907.09595&gt;`_\n\n    Args:\n        arch: size of the architecture. \"small\", \"medium\" or \"large\". Default: \"small\".\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of the channels of the input. Default: 3.\n        feature_size: numbet of the channels of the output features. Default: 1536.\n        drop_rate: rate of dropout for classifier. Default: 0.2.\n        depth_multiplier: expansion coefficient of channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        arch: str = \"small\",\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        feature_size: int = 1536,\n        drop_rate: float = 0.2,\n        depth_multiplier: float = 1.0\n    ) -&gt; None:\n        super(MixNet, self).__init__()\n        if arch == \"small\":\n            block_configs = [\n                [16, 16, [3], [1], [1], 1, 1, \"ReLU\", 0.0],\n                [16, 24, [3], [1, 1], [1, 1], 2, 6, \"ReLU\", 0.0],\n                [24, 24, [3], [1, 1], [1, 1], 1, 3, \"ReLU\", 0.0],\n                [24, 40, [3, 5, 7], [1], [1], 2, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 80, [3, 5, 7], [1], [1, 1], 2, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5], [1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5], [1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 120, [3, 5, 7], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 200, [3, 5, 7, 9, 11], [1], [1], 2, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5]\n            ]\n            stem_channels = 16\n            drop_rate = drop_rate\n        else:\n            block_configs = [\n                [24, 24, [3], [1], [1], 1, 1, \"ReLU\", 0.0],\n                [24, 32, [3, 5, 7], [1, 1], [1, 1], 2, 6, \"ReLU\", 0.0],\n                [32, 32, [3], [1, 1], [1, 1], 1, 3, \"ReLU\", 0.0],\n                [32, 40, [3, 5, 7, 9], [1], [1], 2, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 80, [3, 5, 7], [1], [1], 2, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 120, [3], [1], [1], 1, 6, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 200, [3, 5, 7, 9], [1], [1], 2, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5]\n            ]\n            if arch == \"medium\":\n                stem_channels = 24\n                drop_rate = drop_rate\n            elif arch == \"large\":\n                stem_channels = 24\n                depth_multiplier *= 1.3\n                drop_rate = drop_rate\n            else:\n                raise ValueError(f\"Unsupported model type {arch}\")\n\n        if depth_multiplier != 1.0:\n            stem_channels = _roundchannels(stem_channels * depth_multiplier)\n\n            for i, conf in enumerate(block_configs):\n                conf_ls = list(conf)\n                conf_ls[0] = _roundchannels(conf_ls[0] * depth_multiplier)\n                conf_ls[1] = _roundchannels(conf_ls[1] * depth_multiplier)\n                block_configs[i] = tuple(conf_ls)\n\n        # stem convolution\n        self.stem_conv = nn.SequentialCell([\n            nn.Conv2d(in_channels, stem_channels, 3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(stem_channels),\n            nn.ReLU()\n        ])\n\n        # building MixNet blocks\n        layers = []\n        for inc, outc, k, ek, pk, s, er, ac, se in block_configs:\n            layers.append(MixNetBlock(\n                inc,\n                outc,\n                kernel_size=k,\n                expand_ksize=ek,\n                project_ksize=pk,\n                stride=s,\n                expand_ratio=er,\n                activation=ac,\n                se_ratio=se\n            ))\n        self.layers = nn.SequentialCell(layers)\n\n        # head\n        self.head_conv = nn.SequentialCell([\n            nn.Conv2d(block_configs[-1][1], feature_size, 1, pad_mode=\"pad\", padding=0),\n            nn.BatchNorm2d(feature_size),\n            nn.ReLU()\n        ])\n\n        self.pool = GlobalAvgPooling()\n        self.dropout = nn.Dropout(keep_prob=1 - drop_rate)\n        self.classifier = nn.Dense(feature_size, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(math.sqrt(2.0 / fan_out)),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Uniform(1.0 / math.sqrt(cell.weight.shape[0])),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.stem_conv(x)\n        x = self.layers(x)\n        x = self.head_conv(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mixnet.MixNetBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic Block of MixNet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mixnet.py</code> <pre><code>class MixNetBlock(nn.Cell):\n\"\"\"Basic Block of MixNet\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: list = [3],\n        expand_ksize: list = [1],\n        project_ksize: list = [1],\n        stride: int = 1,\n        expand_ratio: int = 1,\n        activation: str = \"ReLU\",\n        se_ratio: float = 0.0,\n    ) -&gt; None:\n        super(MixNetBlock, self).__init__()\n        assert activation in [\"ReLU\", \"Swish\"]\n        self.activation = Swish if activation == \"Swish\" else nn.ReLU\n\n        expand_channels = in_channels * expand_ratio\n        self.residual_connection = (stride == 1 and in_channels == out_channels)\n\n        conv = []\n        if expand_ratio != 1:\n            # expand\n            conv.extend([\n                GroupedConv2d(in_channels, expand_channels, expand_ksize),\n                nn.BatchNorm2d(expand_channels),\n                self.activation()\n            ])\n\n        # depthwise\n        conv.extend([\n            MDConv(expand_channels, kernel_size, stride),\n            nn.BatchNorm2d(expand_channels),\n            self.activation()\n        ])\n\n        if se_ratio &gt; 0:\n            squeeze_channels = int(in_channels * se_ratio)\n            squeeze_excite = SqueezeExcite(expand_channels, rd_channels=squeeze_channels)\n            conv.append(squeeze_excite)\n\n        # projection phase\n        conv.extend([\n            GroupedConv2d(expand_channels, out_channels, project_ksize),\n            nn.BatchNorm2d(out_channels)\n        ])\n\n        self.convs = nn.SequentialCell(conv)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.residual_connection:\n            return x + self.convs(x)\n        else:\n            return self.convs(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mlpmixer</code> \u00b6 <p>MindSpore implementation of <code>MLP-Mixer</code>. Refer to MLP-Mixer: An all-MLP Architecture for Vision.</p> <code>mindocr.models.backbones.mindcv_models.mlpmixer.FeedForward</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Feed Forward Block. MLP Layer. FC -&gt; GELU -&gt; FC</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mlpmixer.py</code> <pre><code>class FeedForward(nn.Cell):\n\"\"\"Feed Forward Block. MLP Layer. FC -&gt; GELU -&gt; FC\"\"\"\n\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super(FeedForward, self).__init__()\n        self.net = nn.SequentialCell(\n            nn.Dense(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(keep_prob=1 - dropout),\n            nn.Dense(hidden_dim, dim),\n            nn.Dropout(keep_prob=1 - dropout)\n        )\n\n    def construct(self, x):\n        return self.net(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mlpmixer.MLPMixer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP-Mixer model class, based on <code>\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;</code>_</p> PARAMETER DESCRIPTION <code>depth</code> <p>number of MixerBlocks.</p> <p> TYPE: <code>int) </code> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int or tuple) </code> </p> <code>n_patches</code> <p>number of patches.</p> <p> TYPE: <code>int) </code> </p> <code>n_channels</code> <p>channels(dimension) of a single embedded patch.</p> <p> TYPE: <code>int) </code> </p> <code>token_dim</code> <p>hidden dim of token-mixing MLP.</p> <p> TYPE: <code>int) </code> </p> <code>channel_dim</code> <p>hidden dim of channel-mixing MLP.</p> <p> TYPE: <code>int) </code> </p> <code>n_classes</code> <p>number of classification classes.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mlpmixer.py</code> <pre><code>class MLPMixer(nn.Cell):\nr\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (int or tuple) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        n_classes (int) : number of classification classes.\n    \"\"\"\n\n    def __init__(self, depth, patch_size, n_patches, n_channels, token_dim, channel_dim, n_classes=1000):\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(3, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, n_classes)\n        self.mean = ops.ReduceMean()\n        self._initialize_weights()\n\n    def construct(self, x):\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        x = self.mean(x, 1)\n        return self.mlp_head(x)\n\n    def _initialize_weights(self):\n        # todo: implement weights init\n        pass\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mlpmixer.MixerBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Mixer Layer with token-mixing MLP and channel-mixing MLP</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mlpmixer.py</code> <pre><code>class MixerBlock(nn.Cell):\n\"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self, n_patches, n_channels, token_dim, channel_dim, dropout=0.):\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mlpmixer.TransPose</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>TransPose Layer. Wrap operator Transpose for easy integration in nn.SequentialCell</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mlpmixer.py</code> <pre><code>class TransPose(nn.Cell):\n\"\"\"TransPose Layer. Wrap operator Transpose for easy integration in nn.SequentialCell\"\"\"\n\n    def __init__(self, permutation=(0, 2, 1), embedding=False):\n        super(TransPose, self).__init__()\n        self.permutation = permutation\n        self.embedding = embedding\n        if embedding:\n            self.reshape = ops.Reshape()\n        self.transpose = ops.Transpose()\n\n    def construct(self, x):\n        if self.embedding:\n            b, c, h, w = x.shape\n            x = self.reshape(x, (b, c, h * w))\n        x = self.transpose(x, self.permutation)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet</code> \u00b6 <p>MindSpore implementation of <code>MnasNet</code>. Refer to MnasNet: Platform-Aware Neural Architecture Search for Mobile.</p> <code>mindocr.models.backbones.mindcv_models.mnasnet.Mnasnet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MnasNet model architecture from <code>\"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" &lt;https://arxiv.org/abs/1807.11626&gt;</code>_.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width.</p> <p> TYPE: <code>float</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>class Mnasnet(nn.Cell):\nr\"\"\"MnasNet model architecture from\n    `\"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" &lt;https://arxiv.org/abs/1807.11626&gt;`_.\n\n    Args:\n        alpha: scale factor of model width.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.2.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        drop_rate: float = 0.2,\n    ):\n        super().__init__()\n\n        inverted_residual_setting = [\n            # t, c, n, s, k\n            [3, 24, 3, 2, 3],  # -&gt; 56x56\n            [3, 40, 3, 2, 5],  # -&gt; 28x28\n            [6, 80, 3, 2, 5],  # -&gt; 14x14\n            [6, 96, 2, 1, 3],  # -&gt; 14x14\n            [6, 192, 4, 2, 5],  # -&gt; 7x7\n            [6, 320, 1, 1, 3],  # -&gt; 7x7\n        ]\n\n        mid_channels = make_divisible(32 * alpha, 8)\n        input_channels = make_divisible(16 * alpha, 8)\n\n        features: List[nn.Cell] = [\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(mid_channels, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1,\n                      group=mid_channels),\n            nn.BatchNorm2d(mid_channels, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n            nn.Conv2d(mid_channels, input_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(input_channels, momentum=0.99, eps=1e-3),\n        ]\n\n        for t, c, n, s, k in inverted_residual_setting:\n            output_channels = make_divisible(c * alpha, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channels, output_channels,\n                                                 stride=stride, kernel_size=k, expand_ratio=t))\n                input_channels = output_channels\n\n        features.extend([\n            nn.Conv2d(input_channels, 1280, kernel_size=1, stride=1),\n            nn.BatchNorm2d(1280, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n        ])\n        self.features = nn.SequentialCell(features)\n        self.pool = GlobalAvgPooling()\n        self.dropout = nn.Dropout(keep_prob=1 - drop_rate)\n        self.classifier = nn.Dense(1280, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode=\"fan_out\", nonlinearity=\"sigmoid\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet.mnasnet0_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MnasNet model with width scaled by 0.5. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet0_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n\"\"\"Get MnasNet model with width scaled by 0.5.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet0.5\"]\n    model = Mnasnet(alpha=0.5, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet.mnasnet0_75(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MnasNet model with width scaled by 0.75. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet0_75(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n\"\"\"Get MnasNet model with width scaled by 0.75.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet0.75\"]\n    model = Mnasnet(alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet.mnasnet1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MnasNet model with width scaled by 1.0. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n\"\"\"Get MnasNet model with width scaled by 1.0.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet1.0\"]\n    model = Mnasnet(alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet.mnasnet1_3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MnasNet model with width scaled by 1.3. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet1_3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n\"\"\"Get MnasNet model with width scaled by 1.3.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet1.3\"]\n    model = Mnasnet(alpha=1.3, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mnasnet.mnasnet1_4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MnasNet model with width scaled by 1.4. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet1_4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n\"\"\"Get MnasNet model with width scaled by 1.4.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet1.4\"]\n    model = Mnasnet(alpha=1.4, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1</code> \u00b6 <p>MindSpore implementation of <code>MobileNetV1</code>. Refer to MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.</p> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1.MobileNetV1</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MobileNetV1 model class, based on <code>\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\" &lt;https://arxiv.org/abs/1704.04861&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v1.py</code> <pre><code>class MobileNetV1(nn.Cell):\nr\"\"\"MobileNetV1 model class, based on\n    `\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\" &lt;https://arxiv.org/abs/1704.04861&gt;`_  # noqa: E501\n\n    Args:\n        alpha: scale factor of model width. Default: 1.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        input_channels = int(32 * alpha)\n        # Setting of depth-wise separable conv\n        # c: number of output channel\n        # s: stride of depth-wise conv\n        block_setting = [\n            # c, s\n            [64, 1],\n            [128, 2],\n            [128, 1],\n            [256, 2],\n            [256, 1],\n            [512, 2],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [1024, 2],\n            [1024, 1],\n        ]\n\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU(),\n        ]\n        for c, s in block_setting:\n            output_channel = int(c * alpha)\n            features.append(depthwise_separable_conv(input_channels, output_channel, s))\n            input_channels = output_channel\n        self.features = nn.SequentialCell(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(input_channels, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(init.initializer(init.XavierUniform(), cell.weight.shape, cell.weight.dtype))\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(), cell.weight.shape, cell.weight.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1.mobilenet_v1_025_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV1 model with width scaled by 0.25. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v1.py</code> <pre><code>@register_model\ndef mobilenet_v1_025_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n\"\"\"Get MobileNetV1 model with width scaled by 0.25.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_0.25_224\"]\n    model = MobileNetV1(alpha=0.25, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1.mobilenet_v1_050_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV1 model with width scaled by 0.5. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v1.py</code> <pre><code>@register_model\ndef mobilenet_v1_050_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n\"\"\"Get MobileNetV1 model with width scaled by 0.5.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_0.5_224\"]\n    model = MobileNetV1(alpha=0.5, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1.mobilenet_v1_075_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV1 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v1.py</code> <pre><code>@register_model\ndef mobilenet_v1_075_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n\"\"\"Get MobileNetV1 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_0.75_224\"]\n    model = MobileNetV1(alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v1.mobilenet_v1_100_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV1 model without width scaling. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v1.py</code> <pre><code>@register_model\ndef mobilenet_v1_100_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n\"\"\"Get MobileNetV1 model without width scaling.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_1.0_224\"]\n    model = MobileNetV1(alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2</code> \u00b6 <p>MindSpore implementation of <code>MobileNetV2</code>. Refer to MobileNetV2: Inverted Residuals and Linear Bottlenecks.</p> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.InvertedResidual</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Inverted Residual Block of MobileNetV2</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>class InvertedResidual(nn.Cell):\n\"\"\"Inverted Residual Block of MobileNetV2\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int,\n        expand_ratio: int,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        hidden_dim = round(in_channels * expand_ratio)\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.extend([\n                nn.Conv2d(in_channels, hidden_dim, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6()\n            ])\n        layers.extend([\n            # dw\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, pad_mode=\"pad\", padding=1, group=hidden_dim, has_bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(),\n            # pw-linear\n            nn.Conv2d(hidden_dim, out_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(out_channels),\n        ])\n        self.layers = nn.SequentialCell(layers)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.use_res_connect:\n            return x + self.layers(x)\n        return self.layers(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.MobileNetV2</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MobileNetV2 model class, based on <code>\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" &lt;https://arxiv.org/abs/1801.04381&gt;</code>_</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>round_nearest</code> <p>divisor of make divisible function. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>class MobileNetV2(nn.Cell):\nr\"\"\"MobileNetV2 model class, based on\n    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" &lt;https://arxiv.org/abs/1801.04381&gt;`_\n\n    Args:\n        alpha: scale factor of model width. Default: 1.\n        round_nearest: divisor of make divisible function. Default: 8.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        round_nearest: int = 8,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        input_channels = make_divisible(32 * alpha, round_nearest)\n        # Setting of inverted residual blocks.\n        # t: The expansion factor.\n        # c: Number of output channel.\n        # n: Number of block.\n        # s: First block stride.\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n        last_channels = make_divisible(1280 * max(1.0, alpha), round_nearest)\n\n        # Building stem conv layer.\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU6(),\n        ]\n        # Building inverted residual blocks.\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = make_divisible(c * alpha, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channels, output_channel, stride, expand_ratio=t))\n                input_channels = output_channel\n        # Building last point-wise layers.\n        features.extend([\n            nn.Conv2d(input_channels, last_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(last_channels),\n            nn.ReLU6(),\n        ])\n        self.features = nn.SequentialCell(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.SequentialCell([\n            nn.Dropout(keep_prob=0.8),  # confirmed by paper authors\n            nn.Dense(last_channels, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2. / n), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_035_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.35_128\"]\n    model = MobileNetV2(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_035_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.35_160\"]\n    model = MobileNetV2(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_035_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.35_192\"]\n    model = MobileNetV2(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_035_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.35_224\"]\n    model = MobileNetV2(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_035_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.35_96\"]\n    model = MobileNetV2(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_050_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.5_128\"]\n    model = MobileNetV2(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_050_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.5_160\"]\n    model = MobileNetV2(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_050_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.5_192\"]\n    model = MobileNetV2(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_050_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.5_224\"]\n    model = MobileNetV2(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_050_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.5_96\"]\n    model = MobileNetV2(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_075_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.75_128\"]\n    model = MobileNetV2(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_075_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.75_160\"]\n    model = MobileNetV2(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_075_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.75_192\"]\n    model = MobileNetV2(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_075_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.75_224\"]\n    model = MobileNetV2(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_075_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_0.75_96\"]\n    model = MobileNetV2(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_100_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model without width scaling and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model without width scaling and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.0_128\"]\n    model = MobileNetV2(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_100_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model without width scaling and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model without width scaling and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.0_160\"]\n    model = MobileNetV2(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_100_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model without width scaling and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model without width scaling and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.0_192\"]\n    model = MobileNetV2(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_100_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model without width scaling and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model without width scaling and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.0_224\"]\n    model = MobileNetV2(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_100_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model without width scaling and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model without width scaling and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.0_96\"]\n    model = MobileNetV2(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_130_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 1.3 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_130_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 1.3 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.3_224\"]\n    model = MobileNetV2(alpha=1.3, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v2.mobilenet_v2_140_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get MobileNetV2 model with width scaled by 1.4 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v2.py</code> <pre><code>@register_model\ndef mobilenet_v2_140_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n\"\"\"Get MobileNetV2 model with width scaled by 1.4 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_1.4_224\"]\n    model = MobileNetV2(alpha=1.4, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3</code> \u00b6 <p>MindSpore implementation of <code>MobileNetV3</code>. Refer to Searching for MobileNetV3.</p> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Bottleneck Block of MobilenetV3. depth-wise separable convolutions + inverted residual + squeeze excitation</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"Bottleneck Block of MobilenetV3. depth-wise separable convolutions + inverted residual + squeeze excitation\"\"\"\n\n    def __init__(\n            self,\n            in_channels: int,\n            mid_channels: int,\n            out_channels: int,\n            kernel_size: int,\n            stride: int = 1,\n            activation: str = \"relu\",\n            use_se: bool = False,\n            se_version: str = 'SqueezeExcite',\n            always_expand: bool = False\n    ) -&gt; None:\n        super().__init__()\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n        assert activation in [\"relu\", \"hswish\"]\n        self.activation = nn.HSwish if activation == \"hswish\" else nn.ReLU\n\n        layers = []\n        # Expand.\n        if in_channels != mid_channels or always_expand:\n            layers.extend([\n                nn.Conv2d(in_channels, mid_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n                nn.BatchNorm2d(mid_channels),\n                self.activation(),\n            ])\n\n        # DepthWise.\n        layers.extend([\n            nn.Conv2d(mid_channels, mid_channels, kernel_size, stride,\n                      pad_mode=\"same\", group=mid_channels, has_bias=False),\n            nn.BatchNorm2d(mid_channels),\n            self.activation(),\n        ])\n        # SqueezeExcitation.\n        if use_se and se_version == 'SqueezeExcite':\n            layers.append(SqueezeExcite(mid_channels, 1.0 / 4, act_layer=nn.ReLU, gate_layer=nn.HSigmoid))\n        elif use_se and se_version == 'SqueezeExciteV2':\n            layers.append(SqueezeExciteV2(mid_channels, rd_channels=mid_channels // 4))\n\n        # Project.\n        layers.extend([\n            nn.Conv2d(mid_channels, out_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(out_channels),\n        ])\n        self.layers = nn.SequentialCell(layers)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.use_res_connect:\n            return x + self.layers(x)\n        return self.layers(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.MobileNetV3</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MobileNetV3 model class, based on <code>\"Searching for MobileNetV3\" &lt;https://arxiv.org/abs/1905.02244&gt;</code>_</p> PARAMETER DESCRIPTION <code>arch</code> <p>size of the architecture. 'small' or 'large'.</p> <p> TYPE: <code>str</code> </p> <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>round_nearest</code> <p>divisor of make divisible function. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>class MobileNetV3(nn.Cell):\nr\"\"\"MobileNetV3 model class, based on\n    `\"Searching for MobileNetV3\" &lt;https://arxiv.org/abs/1905.02244&gt;`_\n\n    Args:\n        arch: size of the architecture. 'small' or 'large'.\n        alpha: scale factor of model width. Default: 1.\n        round_nearest: divisor of make divisible function. Default: 8.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n            self,\n            arch: str,\n            alpha: float = 1.0,\n            round_nearest: int = 8,\n            in_channels: int = 3,\n            num_classes: int = 1000,\n            scale_last: bool = True,\n            bottleneck_params: dict = None\n    ) -&gt; None:\n        super().__init__()\n        input_channels = make_divisible(16 * alpha, round_nearest)\n        # Setting of bottleneck blocks. ex: [k, e, c, se, nl, s]\n        # k: kernel size of depth-wise conv\n        # e: expansion size\n        # c: number of output channel\n        # se: whether there is a Squeeze-And-Excite in that block\n        # nl: type of non-linearity used\n        # s: stride of depth-wise conv\n        if arch == \"large\":\n            bottleneck_setting = [\n                [3, 16, 16, False, \"relu\", 1],\n                [3, 64, 24, False, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 1],\n                [5, 72, 40, True, \"relu\", 2],\n                [5, 120, 40, True, \"relu\", 1],\n                [5, 120, 40, True, \"relu\", 1],\n                [3, 240, 80, False, \"hswish\", 2],\n                [3, 200, 80, False, \"hswish\", 1],\n                [3, 184, 80, False, \"hswish\", 1],\n                [3, 184, 80, False, \"hswish\", 1],\n                [3, 480, 112, True, \"hswish\", 1],\n                [3, 672, 112, True, \"hswish\", 1],\n                [5, 672, 160, True, \"hswish\", 2],\n                [5, 960, 160, True, \"hswish\", 1],\n                [5, 960, 160, True, \"hswish\", 1],\n            ]\n            last_channels = make_divisible(alpha * 1280, round_nearest) if scale_last else 1280\n        elif arch == \"small\":\n            bottleneck_setting = [\n                [3, 16, 16, True, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 2],\n                [3, 88, 24, False, \"relu\", 1],\n                [5, 96, 40, True, \"hswish\", 2],\n                [5, 240, 40, True, \"hswish\", 1],\n                [5, 240, 40, True, \"hswish\", 1],\n                [5, 120, 48, True, \"hswish\", 1],\n                [5, 144, 48, True, \"hswish\", 1],\n                [5, 288, 96, True, \"hswish\", 2],\n                [5, 576, 96, True, \"hswish\", 1],\n                [5, 576, 96, True, \"hswish\", 1],\n            ]\n            last_channels = make_divisible(alpha * 1024, round_nearest) if scale_last else 1024\n        else:\n            raise ValueError(f\"Unsupported model type {arch}\")\n\n        # Building stem conv layer.\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.HSwish(),\n        ]\n        total_reduction = 2\n        self.feature_info = [dict(chs=input_channels, reduction=total_reduction, name=f'features.{len(features) - 1}')]\n\n        if bottleneck_params is None:\n            bottleneck_params = {}\n\n        # Building bottleneck blocks.\n        for k, e, c, se, nl, s in bottleneck_setting:\n            exp_channels = make_divisible(alpha * e, round_nearest)\n            output_channels = make_divisible(alpha * c, round_nearest)\n            features.append(Bottleneck(input_channels, exp_channels, output_channels,\n                                       kernel_size=k, stride=s, activation=nl, use_se=se, **bottleneck_params))\n            input_channels = output_channels\n            total_reduction *= s\n            self.feature_info.append(\n                dict(chs=input_channels, reduction=total_reduction, name=f'features.{len(features) - 1}'))\n        # Building last point-wise conv layers.\n        output_channels = input_channels * 6\n        features.extend([\n            nn.Conv2d(input_channels, output_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.HSwish(),\n        ])\n        self.feature_info.append(\n            dict(chs=output_channels, reduction=total_reduction, name=f'features.{len(features) - 1}'))\n        self.flatten_sequential = True\n        self.features = nn.CellList(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.SequentialCell([\n            nn.Dense(output_channels, last_channels),\n            nn.HSwish(),\n            nn.Dropout(keep_prob=0.8),\n            nn.Dense(last_channels, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2. / n), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        for feature in self.features:\n            x = feature(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.mobilenet_v3_large_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get large MobileNetV3 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>@register_model\ndef mobilenet_v3_large_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n\"\"\"Get large MobileNetV3 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_large_0.75\"]\n    model = MobileNetV3(arch=\"large\", alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.mobilenet_v3_large_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get large MobileNetV3 model without width scaling. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>@register_model\ndef mobilenet_v3_large_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n\"\"\"Get large MobileNetV3 model without width scaling.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_large_1.0\"]\n    model = MobileNetV3(arch=\"large\", alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.mobilenet_v3_small_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get small MobileNetV3 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>@register_model\ndef mobilenet_v3_small_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n\"\"\"Get small MobileNetV3 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_small_0.75\"]\n    model = MobileNetV3(arch=\"small\", alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.mobilenet_v3.mobilenet_v3_small_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get small MobileNetV3 model without width scaling. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\mobilenet_v3.py</code> <pre><code>@register_model\ndef mobilenet_v3_small_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n\"\"\"Get small MobileNetV3 model without width scaling.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_small_1.0\"]\n    model = MobileNetV3(arch=\"small\", alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.model_factory</code> \u00b6 <code>mindocr.models.backbones.mindcv_models.model_factory.create_model(model_name, num_classes=1000, pretrained=False, in_channels=3, checkpoint_path='', ema=False, features_only=False, out_indices=[0, 1, 2, 3, 4], **kwargs)</code> \u00b6 <p>Creates model by name.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of model.</p> <p> TYPE: <code>str</code> </p> <code>num_classes</code> <p>The number of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>pretrained</code> <p>Whether to load the pretrained model. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>in_channels</code> <p>The input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>checkpoint_path</code> <p>The path of checkpoint files. Default: \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>ema</code> <p>Whether use ema method. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>features_only</code> <p>Output the features at different strides instead. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>out_indices</code> <p>The indicies of the output features when <code>features_only</code> is <code>True</code>. Default: [0, 1, 2, 3, 4]</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[0, 1, 2, 3, 4]</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\model_factory.py</code> <pre><code>def create_model(\n    model_name: str,\n    num_classes: int = 1000,\n    pretrained=False,\n    in_channels: int = 3,\n    checkpoint_path: str = \"\",\n    ema: bool = False,\n    features_only: bool = False,\n    out_indices: List[int] = [0, 1, 2, 3, 4],\n    **kwargs,\n):\nr\"\"\"Creates model by name.\n\n    Args:\n        model_name (str):  The name of model.\n        num_classes (int): The number of classes. Default: 1000.\n        pretrained (bool): Whether to load the pretrained model. Default: False.\n        in_channels (int): The input channels. Default: 3.\n        checkpoint_path (str): The path of checkpoint files. Default: \"\".\n        ema (bool): Whether use ema method. Default: False.\n        features_only (bool): Output the features at different strides instead. Default: False\n        out_indices (list[int]): The indicies of the output features when `features_only` is `True`.\n            Default: [0, 1, 2, 3, 4]\n    \"\"\"\n\n    if checkpoint_path != \"\" and pretrained:\n        raise ValueError(\"checkpoint_path is mutually exclusive with pretrained\")\n\n    model_args = dict(num_classes=num_classes, pretrained=pretrained, in_channels=in_channels)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    if not is_model(model_name):\n        raise RuntimeError(f\"Unknown model {model_name}\")\n\n    create_fn = model_entrypoint(model_name)\n    model = create_fn(**model_args, **kwargs)\n\n    if os.path.exists(checkpoint_path):\n        checkpoint_param = load_checkpoint(checkpoint_path)\n        ema_param_dict = dict()\n        for param in checkpoint_param:\n            if param.startswith(\"ema\"):\n                new_name = param.split(\"ema.\")[1]\n                ema_data = checkpoint_param[param]\n                ema_data.name = new_name\n                ema_param_dict[new_name] = ema_data\n\n        if ema_param_dict and ema:\n            load_param_into_net(model, ema_param_dict)\n        elif bool(ema_param_dict) is False and ema:\n            raise ValueError(\"chekpoint_param does not contain ema_parameter, please set ema is False.\")\n        else:\n            load_param_into_net(model, checkpoint_param)\n\n    if features_only:\n        # wrap the model, output the feature pyramid instead\n        try:\n            model = FeatureExtractWrapper(model, out_indices=out_indices)\n        except AttributeError as e:\n            raise RuntimeError(f\"`feature_only` is not implemented for `{model_name}` model.\") from e\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet</code> \u00b6 <p>MindSpore implementation of <code>NasNet</code>. Refer to: Learning Transferable Architectures for Scalable Image Recognition</p> <code>mindocr.models.backbones.mindcv_models.nasnet.BranchSeparables</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class BranchSeparables(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, in_channels, kernel_size, stride, padding, bias=bias\n        )\n        self.bn_sep_1 = nn.BatchNorm2d(num_features=in_channels, eps=0.001, momentum=0.9, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(\n            in_channels, out_channels, kernel_size, 1, padding, bias=bias\n        )\n        self.bn_sep_2 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.BranchSeparablesReduction</code> \u00b6 <p>         Bases: <code>BranchSeparables</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class BranchSeparablesReduction(BranchSeparables):\n\"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        z_padding: int = 1,\n        bias: bool = False,\n    ) -&gt; None:\n        BranchSeparables.__init__(\n            self, in_channels, out_channels, kernel_size, stride, padding, bias\n        )\n        self.padding = nn.Pad(paddings=((0, 0), (0, 0), (z_padding, 0), (z_padding, 0)), mode=\"CONSTANT\")\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:]\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.BranchSeparablesStem</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class BranchSeparablesStem(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, out_channels, kernel_size, stride, padding, bias=bias\n        )\n        self.bn_sep_1 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(\n            out_channels, out_channels, kernel_size, 1, padding, bias=bias\n        )\n        self.bn_sep_2 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.CellStem0</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class CellStem0(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        stem_filters: int,\n        num_filters: int = 42,\n    ) -&gt; None:\n        super().__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)\n        ])\n\n        self.comb_iter_0_left = BranchSeparables(\n            self.num_filters, self.num_filters, 5, 2, 2\n        )\n        self.comb_iter_0_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 5, 2, 2, bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            self.num_filters, self.num_filters, 3, 1, 1, bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.CellStem1</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class CellStem1(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        stem_filters: int,\n        num_filters: int,\n    ) -&gt; None:\n        super().__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=2 * self.num_filters, out_channels=self.num_filters, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)])\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\"),\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters // 2, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)])\n\n        self.path_2 = nn.CellList([])\n        self.path_2.append(nn.Pad(paddings=((0, 0), (0, 0), (0, 1), (0, 1)), mode=\"CONSTANT\"))\n        self.path_2.append(\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\")\n        )\n        self.path_2.append(\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters // 2, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)\n        )\n\n        self.final_path_bn = nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            5,\n            2,\n            2,\n            bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            5,\n            2,\n            2,\n            bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            3,\n            1,\n            1,\n            bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x_conv0: Tensor, x_stem_0: Tensor) -&gt; Tensor:\n        x_left = self.conv_1x1(x_stem_0)\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2[0](x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2[1](x_path2)\n        x_path2 = self.path_2[2](x_path2)\n        # final path\n        x_right = self.final_path_bn(ops.concat((x_path1, x_path2), axis=1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.FirstCell</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class FirstCell(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\"),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)])\n\n        self.path_2 = nn.CellList([])\n        self.path_2.append(nn.Pad(paddings=((0, 0), (0, 0), (0, 1), (0, 1)), mode=\"CONSTANT\"))\n        self.path_2.append(\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\")\n        )\n        self.path_2.append(\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)\n        )\n\n        self.final_path_bn = nn.BatchNorm2d(num_features=out_channels_left * 2, eps=0.001, momentum=0.9, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_3_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_relu = self.relu(x_prev)\n        x_path1 = self.path_1(x_relu)\n        x_path2 = self.path_2[0](x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2[1](x_path2)\n        x_path2 = self.path_2[2](x_path2)\n        # final path\n        x_left = self.final_path_bn(ops.concat((x_path1, x_path2), axis=1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = ops.concat((x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.NASNetAMobile</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model class, based on <code>\"Learning Transferable Architectures for Scalable Image Recognition\" &lt;https://arxiv.org/pdf/1707.07012v4.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>stem_filters</code> <p>number of stem filters. Default: 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>penultimate_filters</code> <p>number of penultimate filters. Default: 1056.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1056</code> </p> <code>filters_multiplier</code> <p>size of filters multiplier. Default: 2.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class NASNetAMobile(nn.Cell):\nr\"\"\"NasNet model class, based on\n    `\"Learning Transferable Architectures for Scalable Image Recognition\" &lt;https://arxiv.org/pdf/1707.07012v4.pdf&gt;`_\n    Args:\n        num_classes: number of classification classes.\n        stem_filters: number of stem filters. Default: 32.\n        penultimate_filters: number of penultimate filters. Default: 1056.\n        filters_multiplier: size of filters multiplier. Default: 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        stem_filters: int = 32,\n        penultimate_filters: int = 1056,\n        filters_multiplier: int = 2,\n    ) -&gt; None:\n        super().__init__()\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.SequentialCell([\n            nn.Conv2d(in_channels=in_channels, out_channels=self.stem_filters, kernel_size=3, stride=2, pad_mode=\"pad\",\n                      padding=0,\n                      has_bias=False),\n            nn.BatchNorm2d(num_features=self.stem_filters, eps=0.001, momentum=0.9, affine=True)\n        ])\n\n        self.cell_stem_0 = CellStem0(\n            self.stem_filters, num_filters=filters // (filters_multiplier ** 2)\n        )\n        self.cell_stem_1 = CellStem1(\n            self.stem_filters, num_filters=filters // filters_multiplier\n        )\n\n        self.cell_0 = FirstCell(\n            in_channels_left=filters,\n            out_channels_left=filters // 2,  # 1, 0.5\n            in_channels_right=2 * filters,\n            out_channels_right=filters,\n        )  # 2, 1\n        self.cell_1 = NormalCell(\n            in_channels_left=2 * filters,\n            out_channels_left=filters,  # 2, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n        self.cell_2 = NormalCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n        self.cell_3 = NormalCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n\n        self.reduction_cell_0 = ReductionCell0(\n            in_channels_left=6 * filters,\n            out_channels_left=2 * filters,  # 6, 2\n            in_channels_right=6 * filters,\n            out_channels_right=2 * filters,\n        )  # 6, 2\n\n        self.cell_6 = FirstCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=8 * filters,\n            out_channels_right=2 * filters,\n        )  # 8, 2\n        self.cell_7 = NormalCell(\n            in_channels_left=8 * filters,\n            out_channels_left=2 * filters,  # 8, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n        self.cell_8 = NormalCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n        self.cell_9 = NormalCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n\n        self.reduction_cell_1 = ReductionCell1(\n            in_channels_left=12 * filters,\n            out_channels_left=4 * filters,  # 12, 4\n            in_channels_right=12 * filters,\n            out_channels_right=4 * filters,\n        )  # 12, 4\n\n        self.cell_12 = FirstCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=16 * filters,\n            out_channels_right=4 * filters,\n        )  # 16, 4\n        self.cell_13 = NormalCell(\n            in_channels_left=16 * filters,\n            out_channels_left=4 * filters,  # 16, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n        self.cell_14 = NormalCell(\n            in_channels_left=24 * filters,\n            out_channels_left=4 * filters,  # 24, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n        self.cell_15 = NormalCell(\n            in_channels_left=24 * filters,\n            out_channels_left=4 * filters,  # 24, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(keep_prob=0.5)\n        self.classifier = nn.Dense(in_channels=24 * filters, out_channels=num_classes)\n        self.pool = GlobalAvgPooling()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        self.init_parameters_data()\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(init.initializer(init.Normal(math.sqrt(2. / n), 0),\n                                                      cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n        x_conv0 = self.conv0(x)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n\n        x_cell_15 = self.relu(x_cell_15)\n        return x_cell_15\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)  # global average pool\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.NASNetAMobile.forward_features(x)</code> \u00b6 <p>Network forward feature extraction.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n    x_conv0 = self.conv0(x)\n    x_stem_0 = self.cell_stem_0(x_conv0)\n    x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n    x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n    x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n    x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n    x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n    x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n    x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n    x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n    x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n    x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n    x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n    x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n    x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n    x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n    x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n\n    x_cell_15 = self.relu(x_cell_15)\n    return x_cell_15\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.NormalCell</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class NormalCell(nn.Cell):\n\"\"\"NasNet model basic architecture\"\"\"\n    def __init__(self,\n                 in_channels_left: int,\n                 out_channels_left: int,\n                 in_channels_right: int,\n                 out_channels_right: int) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_left, out_channels_left, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_channels_left, out_channels_left, 5, 1, 2, bias=False\n        )\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_left, out_channels_left, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_3_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = ops.concat((x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.ReductionCell0</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class ReductionCell0(nn.Cell):\n\"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 5, 2, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 5, 2, 2, bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.ReductionCell1</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class ReductionCell1(nn.Cell):\n\"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            5,\n            2,\n            2,\n            bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            5,\n            2,\n            2,\n            bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            3,\n            1,\n            1,\n            bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.SeparableConv2d</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>depth-wise convolutions + point-wise convolutions</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>class SeparableConv2d(nn.Cell):\n\"\"\"depth-wise convolutions + point-wise convolutions\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dw_kernel: int,\n        dw_stride: int,\n        dw_padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=dw_kernel,\n                                          stride=dw_stride, pad_mode=\"pad\", padding=dw_padding, group=in_channels,\n                                          has_bias=bias)\n        self.pointwise_conv2d = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1,\n                                          pad_mode=\"pad\", has_bias=bias)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.nasnet.nasnet_a_4x1056(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get NasNet model. Refer to the base class <code>models.NASNetAMobile</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\nasnet.py</code> <pre><code>@register_model\ndef nasnet_a_4x1056(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; NASNetAMobile:\n\"\"\"Get NasNet model.\n    Refer to the base class `models.NASNetAMobile` for more details.\"\"\"\n    default_cfg = default_cfgs[\"nasnet_a_4x1056\"]\n    model = NASNetAMobile(in_channels=in_channels, num_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.path</code> \u00b6 <p>Utility of file path</p> <code>mindocr.models.backbones.mindcv_models.path.detect_file_type(filename)</code> \u00b6 <p>Detect file type by suffixes and return tuple(suffix, archive_type, compression).</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\path.py</code> <pre><code>def detect_file_type(filename: str):  # pylint: disable=inconsistent-return-statements\n\"\"\"Detect file type by suffixes and return tuple(suffix, archive_type, compression).\"\"\"\n    suffixes = pathlib.Path(filename).suffixes\n    if not suffixes:\n        raise RuntimeError(f\"File `{filename}` has no suffixes that could be used to detect.\")\n    suffix = suffixes[-1]\n\n    # Check if the suffix is a known alias.\n    if suffix in FILE_TYPE_ALIASES:\n        return suffix, FILE_TYPE_ALIASES[suffix][0], FILE_TYPE_ALIASES[suffix][1]\n\n    # Check if the suffix is an archive type.\n    if suffix in ARCHIVE_TYPE_SUFFIX:\n        return suffix, suffix, None\n\n    # Check if the suffix is a compression.\n    if suffix in COMPRESS_TYPE_SUFFIX:\n        # Check for suffix hierarchy.\n        if len(suffixes) &gt; 1:\n            suffix2 = suffixes[-2]\n            # Check if the suffix2 is an archive type.\n            if suffix2 in ARCHIVE_TYPE_SUFFIX:\n                return suffix2 + suffix, suffix2, suffix\n        return suffix, None, suffix\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit</code> \u00b6 <p>MindSpore implementation of <code>PiT</code>. Refer to Rethinking Spatial Dimensions of Vision Transformers.</p> <code>mindocr.models.backbones.mindcv_models.pit.Attention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define multi-head self attention block</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class Attention(nn.Cell):\n\"\"\"define multi-head self attention block\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        # get pair-wise relative position index for each token inside the window\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(keep_prob=1 - attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = nn.Dropout(keep_prob=1 - proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.batchmatmul = ops.BatchMatMul()\n\n    def construct(self, x):\n        B, N, C = x.shape\n        q = ops.reshape(self.q(x), (B, N, self.num_heads, C // self.num_heads)) * self.scale\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (B, N, self.num_heads, C // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (B, N, self.num_heads, C // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        attn = self.batchmatmul(q, k)\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = self.batchmatmul(attn, v)\n        x = ops.reshape(ops.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the basic block of PiT</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class Block(nn.Cell):\n\"\"\"define the basic block of PiT\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: nn.cell = nn.GELU,\n        norm_layer: nn.cell = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer((dim,), epsilon=1e-6)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer((dim,), epsilon=1e-6)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def construct(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.Mlp</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP as used in Vision Transformer, MLP-Mixer and related networks</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class Mlp(nn.Cell):\n\"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: int = None,\n        out_features: int = None,\n        act_layer: nn.cell = nn.GELU,\n        drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = nn.Dropout(keep_prob=1.0 - drop)\n\n    def construct(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.PoolingTransformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>PiT model class, based on <code>\"Rethinking Spatial Dimensions of Vision Transformers\" &lt;https://arxiv.org/abs/2103.16302&gt;</code></p> PARAMETER DESCRIPTION <code>image_size</code> <p>images input size.</p> <p> TYPE: <code>int) </code> </p> <code>patch_size</code> <p>image patch size.</p> <p> TYPE: <code>int) </code> </p> <code>stride</code> <p>stride of the depthwise conv.</p> <p> TYPE: <code>int) </code> </p> <code>base_dims</code> <p>middle dim of each layer.</p> <p> TYPE: <code>List[int]) </code> </p> <code>depth</code> <p>model block depth of each layer.</p> <p> TYPE: <code>List[int]) </code> </p> <code>heads</code> <p>number of heads of multi-head attention of each layer</p> <p> TYPE: <code>List[int]) </code> </p> <code>mlp_ratio</code> <p>ratio of hidden features in Mlp.</p> <p> TYPE: <code>float) </code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>in_chans</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>3</code> </p> <code>attn_drop_rate</code> <p>attention layers dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_rate</code> <p>dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>drop path rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class PoolingTransformer(nn.Cell):\nr\"\"\"PiT model class, based on\n    `\"Rethinking Spatial Dimensions of Vision Transformers\"\n    &lt;https://arxiv.org/abs/2103.16302&gt;`\n    Args:\n        image_size (int) : images input size.\n        patch_size (int) : image patch size.\n        stride (int) : stride of the depthwise conv.\n        base_dims (List[int]) : middle dim of each layer.\n        depth (List[int]) : model block depth of each layer.\n        heads (List[int]) : number of heads of multi-head attention of each layer\n        mlp_ratio (float) : ratio of hidden features in Mlp.\n        num_classes (int) : number of classification classes. Default: 1000.\n        in_chans (int) : number the channels of the input. Default: 3.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        drop_rate (float) : dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        stride: int,\n        base_dims: List[int],\n        depth: List[int],\n        heads: List[int],\n        mlp_ratio: float,\n        num_classes: int = 1000,\n        in_chans: int = 3,\n        attn_drop_rate: float = 0.0,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n\n        total_block = sum(depth)\n        padding = 0\n        block_idx = 0\n\n        width = math.floor((image_size + 2 * padding - patch_size) / stride + 1)\n\n        self.base_dims = base_dims\n        self.heads = heads\n        self.num_classes = num_classes\n\n        self.patch_size = patch_size\n        self.pos_embed = Parameter(Tensor(np.random.randn(1, base_dims[0] * heads[0], width, width), mstype.float32))\n        self.patch_embed = conv_embedding(in_chans, base_dims[0] * heads[0], patch_size, stride, padding)\n        self.cls_token = Parameter(Tensor(np.random.randn(1, 1, base_dims[0] * heads[0]), mstype.float32))\n\n        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n        self.tile = ops.Tile()\n\n        self.transformers = nn.CellList([])\n        self.pools = nn.CellList([])\n\n        for stage in range(len(depth)):\n            drop_path_prob = [drop_path_rate * i / total_block for i in range(block_idx, block_idx + depth[stage])]\n            block_idx += depth[stage]\n            self.transformers.append(\n                Transformer(\n                    base_dims[stage], depth[stage], heads[stage], mlp_ratio, drop_rate, attn_drop_rate, drop_path_prob\n                )\n            )\n            if stage &lt; len(heads) - 1:\n                self.pools.append(\n                    conv_head_pooling(\n                        base_dims[stage] * heads[stage], base_dims[stage + 1] * heads[stage + 1], stride=2\n                    )\n                )\n\n        self.norm = nn.LayerNorm((base_dims[-1] * heads[-1],), epsilon=1e-6)\n\n        self.embed_dim = base_dims[-1] * heads[-1]\n\n        # Classifier head\n        if num_classes &gt; 0:\n            self.head = nn.Dense(in_channels=base_dims[-1] * heads[-1], out_channels=num_classes, has_bias=True)\n        else:\n            self.head = Identity()\n\n        self.pos_embed.set_data(\n            init.initializer(init.TruncatedNormal(sigma=0.02), self.pos_embed.shape, self.pos_embed.dtype)\n        )\n        self.cls_token.set_data(\n            init.initializer(init.TruncatedNormal(sigma=0.02), self.cls_token.shape, self.cls_token.dtype)\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"init_weights\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.in_channels\n                cell.weight.set_data(\n                    init.initializer(init.Uniform(math.sqrt(1.0 / n)), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.Uniform(math.sqrt(1.0 / n)), cell.bias.shape, cell.bias.dtype)\n                    )\n            if isinstance(cell, nn.Dense):\n                init_range = 1.0 / np.sqrt(cell.weight.shape[0])\n                cell.weight.set_data(init.initializer(init.Uniform(init_range), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(init_range), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n\n        pos_embed = self.pos_embed\n        x = self.pos_drop(x + pos_embed)\n\n        cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n\n        for stage in range(len(self.pools)):\n            x, cls_tokens = self.transformers[stage](x, cls_tokens)\n            x, cls_tokens = self.pools[stage](x, cls_tokens)\n        x, cls_tokens = self.transformers[-1](x, cls_tokens)\n\n        cls_tokens = self.norm(cls_tokens)\n\n        return cls_tokens\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        cls_token = self.head(x[:, 0])\n        return cls_token\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        cls_token = self.forward_features(x)\n        cls_token = self.forward_head(cls_token)\n        return cls_token\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.Transformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the transformer block of PiT</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class Transformer(nn.Cell):\n\"\"\"define the transformer block of PiT\"\"\"\n\n    def __init__(\n        self,\n        base_dim: List[int],\n        depth: List[int],\n        heads: List[int],\n        mlp_ratio: float,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_prob: float = None,\n    ) -&gt; None:\n        super().__init__()\n        self.layers = nn.CellList([])\n        embed_dim = base_dim * heads\n\n        if drop_path_prob is None:\n            drop_path_prob = [0.0 for _ in range(depth)]\n\n        self.blocks = nn.CellList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=True,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=drop_path_prob[i],\n                    norm_layer=nn.LayerNorm,\n                )\n                for i in range(depth)\n            ]\n        )\n\n    def construct(self, x, cls_tokens):\n        h, w = x.shape[2:4]\n        x = ops.reshape(x, (x.shape[0], x.shape[1], h * w))\n        x = ops.transpose(x, (0, 2, 1))\n        token_length = cls_tokens.shape[1]\n        x = ops.concat((cls_tokens, x), axis=1)\n        for blk in self.blocks:\n            x = blk(x)\n\n        cls_tokens = x[:, :token_length]\n        x = x[:, token_length:]\n        x = ops.transpose(x, (0, 2, 1))\n        x = ops.reshape(x, (x.shape[0], x.shape[1], h, w))\n        return x, cls_tokens\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.conv_embedding</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define embedding layer using conv2d</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class conv_embedding(nn.Cell):\n\"\"\"define embedding layer using conv2d\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        patch_size: int,\n        stride: int,\n        padding: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=patch_size,\n            stride=stride,\n            pad_mode=\"pad\",\n            padding=padding,\n            has_bias=True,\n        )\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.conv_head_pooling</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define pooling layer using conv in spatial tokens with an additional fully-connected layer (to adjust the channel size to match the spatial tokens)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>class conv_head_pooling(nn.Cell):\n\"\"\"define pooling layer using conv in spatial tokens with an additional fully-connected layer\n    (to adjust the channel size to match the spatial tokens)\"\"\"\n\n    def __init__(\n        self,\n        in_feature: int,\n        out_feature: int,\n        stride: int,\n        pad_mode: str = \"pad\",\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_feature,\n            out_feature,\n            kernel_size=stride + 1,\n            padding=stride // 2,\n            stride=stride,\n            pad_mode=pad_mode,\n            group=in_feature,\n            has_bias=True,\n        )\n        self.fc = nn.Dense(in_channels=in_feature, out_channels=out_feature, has_bias=True)\n\n    def construct(self, x, cls_token):\n        x = self.conv(x)\n        cls_token = self.fc(cls_token)\n\n        return x, cls_token\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.pit_b(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PiT-B model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>@register_model\ndef pit_b(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n\"\"\"Get PiT-B model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_b_224\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.pit_s(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PiT-S model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>@register_model\ndef pit_s(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n\"\"\"Get PiT-S model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_s_224\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[3, 6, 12],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.pit_ti(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PiT-Ti model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>@register_model\ndef pit_ti(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n\"\"\"Get PiT-Ti model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_ti_224\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[32, 32, 32],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pit.pit_xs(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PiT-XS model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pit.py</code> <pre><code>@register_model\ndef pit_xs(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n\"\"\"Get PiT-XS model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_xs_224\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer</code> \u00b6 <p>MindSpore implementation of <code>poolformer</code>. Refer to PoolFormer: MetaFormer Is Actually What You Need for Vision.</p> <code>mindocr.models.backbones.mindcv_models.poolformer.ConvMlp</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP using 1x1 convs that keeps spatial dims</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>class ConvMlp(nn.Cell):\n\"\"\"MLP using 1x1 convs that keeps spatial dims\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=None,\n        bias=True,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n\n        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, has_bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else Identity()\n        self.act = act_layer(approximate=False)\n        self.drop = nn.Dropout(1 - drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, has_bias=bias[1])\n        self.cls_init_weights()\n\n    def cls_init_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n                if m.bias is not None:\n                    m.bias.set_data(\n                        init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n\n    def construct(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.ConvMlp.cls_init_weights()</code> \u00b6 <p>Initialize weights for cells.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>def cls_init_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n    for name, m in self.cells_and_names():\n        if isinstance(m, nn.Conv2d):\n            m.weight.set_data(\n                init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n            if m.bias is not None:\n                m.bias.set_data(\n                    init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Patch Embedding that is implemented by a layer of conv. Input: tensor in shape [B, C, H, W] Output: tensor in shape [B, C, H/stride, W/stride]</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\"Patch Embedding that is implemented by a layer of conv.\n    Input: tensor in shape [B, C, H, W]\n    Output: tensor in shape [B, C, H/stride, W/stride]\"\"\"\n\n    def __init__(self, in_chs=3, embed_dim=768, patch_size=16, stride=16, padding=0, norm_layer=None):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        stride = to_2tuple(stride)\n        # padding = to_2tuple(padding)\n        self.proj = nn.Conv2d(in_chs, embed_dim, kernel_size=patch_size, stride=stride, padding=padding, pad_mode=\"pad\",\n                              has_bias=True)\n        self.norm = norm_layer(embed_dim) if norm_layer else Identity()\n\n    def construct(self, x):\n        x = self.proj(x)\n        x = self.norm(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.PoolFormer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>PoolFormer model class, based on <code>\"MetaFormer Is Actually What You Need for Vision\" &lt;https://arxiv.org/pdf/2111.11418v3.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>layers</code> <p>number of blocks for the 4 stages</p> <p> </p> <code>embed_dims</code> <p>the embedding dims for the 4 stages. Default: (64, 128, 320, 512)</p> <p> DEFAULT: <code>(64, 128, 320, 512)</code> </p> <code>mlp_ratios</code> <p>mlp ratios for the 4 stages. Default: (4, 4, 4, 4)</p> <p> DEFAULT: <code>(4, 4, 4, 4)</code> </p> <code>downsamples</code> <p>flags to apply downsampling or not. Default: (True, True, True, True)</p> <p> DEFAULT: <code>(True, True, True, True)</code> </p> <code>pool_size</code> <p>the pooling size for the 4 stages. Default: 3</p> <p> DEFAULT: <code>3</code> </p> <code>in_chans</code> <p>number of input channels. Default: 3</p> <p> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classes for the image classification. Default: 1000</p> <p> DEFAULT: <code>1000</code> </p> <code>global_pool</code> <p>define the types of pooling layer. Default: avg</p> <p> DEFAULT: <code>'avg'</code> </p> <code>norm_layer</code> <p>define the types of normalization. Default: nn.GroupNorm</p> <p> DEFAULT: <code>nn.GroupNorm</code> </p> <code>act_layer</code> <p>define the types of activation. Default: nn.GELU</p> <p> DEFAULT: <code>nn.GELU</code> </p> <code>in_patch_size</code> <p>specify the patch embedding for the input image. Default: 7</p> <p> DEFAULT: <code>7</code> </p> <code>in_stride</code> <p>specify the stride for the input image. Default: 4.</p> <p> DEFAULT: <code>4</code> </p> <code>in_pad</code> <p>specify the pad for the input image. Default: 2.</p> <p> DEFAULT: <code>2</code> </p> <code>down_patch_size</code> <p>specify the downsample. Default: 3.</p> <p> DEFAULT: <code>3</code> </p> <code>down_stride</code> <p>specify the downsample (patch embed.). Default: 2.</p> <p> DEFAULT: <code>2</code> </p> <code>down_pad</code> <p>specify the downsample (patch embed.). Default: 1.</p> <p> DEFAULT: <code>1</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>Stochastic Depth. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>LayerScale. Default: 1e-5.</p> <p> DEFAULT: <code>1e-05</code> </p> <code>fork_feat</code> <p>whether output features of the 4 stages, for dense prediction. Default: False.</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>class PoolFormer(nn.Cell):\nr\"\"\"PoolFormer model class, based on\n    `\"MetaFormer Is Actually What You Need for Vision\" &lt;https://arxiv.org/pdf/2111.11418v3.pdf&gt;`_\n\n    Args:\n        layers: number of blocks for the 4 stages\n        embed_dims: the embedding dims for the 4 stages. Default: (64, 128, 320, 512)\n        mlp_ratios: mlp ratios for the 4 stages. Default: (4, 4, 4, 4)\n        downsamples: flags to apply downsampling or not. Default: (True, True, True, True)\n        pool_size: the pooling size for the 4 stages. Default: 3\n        in_chans: number of input channels. Default: 3\n        num_classes: number of classes for the image classification. Default: 1000\n        global_pool: define the types of pooling layer. Default: avg\n        norm_layer: define the types of normalization. Default: nn.GroupNorm\n        act_layer: define the types of activation. Default: nn.GELU\n        in_patch_size: specify the patch embedding for the input image. Default: 7\n        in_stride: specify the stride for the input image. Default: 4.\n        in_pad: specify the pad for the input image. Default: 2.\n        down_patch_size: specify the downsample. Default: 3.\n        down_stride: specify the downsample (patch embed.). Default: 2.\n        down_pad: specify the downsample (patch embed.). Default: 1.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.\n        drop_path_rate: Stochastic Depth. Default: 0.\n        layer_scale_init_value: LayerScale. Default: 1e-5.\n        fork_feat: whether output features of the 4 stages, for dense prediction. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        layers,\n        embed_dims=(64, 128, 320, 512),\n        mlp_ratios=(4, 4, 4, 4),\n        downsamples=(True, True, True, True),\n        pool_size=3,\n        in_chans=3,\n        num_classes=1000,\n        global_pool=\"avg\",\n        norm_layer=nn.GroupNorm,\n        act_layer=nn.GELU,\n        in_patch_size=7,\n        in_stride=4,\n        in_pad=2,\n        down_patch_size=3,\n        down_stride=2,\n        down_pad=1,\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        layer_scale_init_value=1e-5,\n        fork_feat=False,\n    ):\n        super().__init__()\n\n        if not fork_feat:\n            self.num_classes = num_classes\n        self.fork_feat = fork_feat\n\n        self.global_pool = global_pool\n        self.num_features = embed_dims[-1]\n        self.grad_checkpointing = False\n\n        self.patch_embed = PatchEmbed(\n            patch_size=in_patch_size, stride=in_stride, padding=in_pad,\n            in_chs=in_chans, embed_dim=embed_dims[0])\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            network.append(basic_blocks(\n                embed_dims[i], i, layers,\n                pool_size=pool_size, mlp_ratio=mlp_ratios[i],\n                act_layer=act_layer, norm_layer=norm_layer,\n                drop_rate=drop_rate, drop_path_rate=drop_path_rate,\n                layer_scale_init_value=layer_scale_init_value)\n            )\n            if i &lt; len(layers) - 1 and (downsamples[i] or embed_dims[i] != embed_dims[i + 1]):\n                # downsampling between stages\n                network.append(PatchEmbed(\n                    in_chs=embed_dims[i], embed_dim=embed_dims[i + 1],\n                    patch_size=down_patch_size, stride=down_stride, padding=down_pad)\n                )\n\n        self.network = nn.SequentialCell(*network)\n        self.norm = norm_layer(1, embed_dims[-1])\n        self.head = nn.Dense(embed_dims[-1], num_classes, has_bias=True) if num_classes &gt; 0 else Identity()\n        # self._initialize_weights()\n        self.cls_init_weights()\n\n    def cls_init_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Dense):\n                m.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n                if m.bias is not None:\n                    m.bias.set_data(\n                        init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Dense(self.num_features, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        x = self.network(x)\n        if self.fork_feat:\n            # otuput features of four stages for dense prediction\n            return x\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x.mean([-2, -1]))\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.PoolFormer.cls_init_weights()</code> \u00b6 <p>Initialize weights for cells.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>def cls_init_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n    for name, m in self.cells_and_names():\n        if isinstance(m, nn.Dense):\n            m.weight.set_data(\n                init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n            if m.bias is not None:\n                m.bias.set_data(\n                    init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.PoolFormerBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Implementation of one PoolFormer block.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>class PoolFormerBlock(nn.Cell):\n\"\"\"Implementation of one PoolFormer block.\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        pool_size=3,\n        mlp_ratio=4.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.GroupNorm,\n        drop=0.0,\n        drop_path=0.0,\n        layer_scale_init_value=1e-5,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(1, dim)\n        self.token_mixer = Pooling(pool_size=pool_size)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer(1, dim)\n        self.mlp = ConvMlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n\n        if layer_scale_init_value:\n            layer_scale_init_tensor = Tensor(layer_scale_init_value * np.ones([dim]).astype(np.float32))\n            self.layer_scale_1 = mindspore.Parameter(layer_scale_init_tensor)\n            self.layer_scale_2 = mindspore.Parameter(layer_scale_init_tensor)\n        else:\n            self.layer_scale_1 = None\n            self.layer_scale_2 = None\n        self.expand_dims = ops.ExpandDims()\n\n    def construct(self, x):\n        if self.layer_scale_1 is not None:\n            x = x + self.drop_path(\n                self.expand_dims(self.expand_dims(self.layer_scale_1, -1), -1) * self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(\n                self.expand_dims(self.expand_dims(self.layer_scale_2, -1), -1) * self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.basic_blocks(dim, index, layers, pool_size=3, mlp_ratio=4.0, act_layer=nn.GELU, norm_layer=nn.GroupNorm, drop_rate=0.0, drop_path_rate=0.0, layer_scale_init_value=1e-05)</code> \u00b6 <p>generate PoolFormer blocks for a stage</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>def basic_blocks(\n    dim,\n    index,\n    layers,\n    pool_size=3,\n    mlp_ratio=4.0,\n    act_layer=nn.GELU,\n    norm_layer=nn.GroupNorm,\n    drop_rate=0.0,\n    drop_path_rate=0.0,\n    layer_scale_init_value=1e-5,\n):\n\"\"\"generate PoolFormer blocks for a stage\"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(PoolFormerBlock(\n            dim, pool_size=pool_size, mlp_ratio=mlp_ratio,\n            act_layer=act_layer, norm_layer=norm_layer,\n            drop=drop_rate, drop_path=block_dpr,\n            layer_scale_init_value=layer_scale_init_value,\n        ))\n    blocks = nn.SequentialCell(*blocks)\n    return blocks\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.poolformer_m36(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get poolformer_m36 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_m36(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n\"\"\"Get poolformer_m36 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_m36\"]\n    layers = (6, 6, 18, 6)\n    embed_dims = (96, 192, 384, 768)\n    model = PoolFormer(\n        in_chans=in_channels,\n        num_classes=num_classes,\n        layers=layers,\n        layer_scale_init_value=1e-6,\n        embed_dims=embed_dims,\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.poolformer_m48(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get poolformer_m48 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_m48(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n\"\"\"Get poolformer_m48 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_m48\"]\n    layers = (8, 8, 24, 8)\n    embed_dims = (96, 192, 384, 768)\n    model = PoolFormer(\n        in_chans=in_channels,\n        num_classes=num_classes,\n        layers=layers,\n        layer_scale_init_value=1e-6,\n        embed_dims=embed_dims,\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.poolformer_s12(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get poolformer_s12 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s12(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n\"\"\"Get poolformer_s12 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s12\"]\n    model = PoolFormer(in_chans=in_channels, num_classes=num_classes, layers=(2, 2, 6, 2), **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.poolformer_s24(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get poolformer_s24 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s24(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n\"\"\"Get poolformer_s24 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s24\"]\n    model = PoolFormer(in_chans=in_channels, num_classes=num_classes, layers=(4, 4, 12, 4), **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.poolformer.poolformer_s36(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get poolformer_s36 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s36(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n\"\"\"Get poolformer_s36 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s36\"]\n    model = PoolFormer(\n        in_chans=in_channels, num_classes=num_classes, layers=(6, 6, 18, 6), layer_scale_init_value=1e-6, **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt</code> \u00b6 <p>MindSpore implementation of <code>PVT</code>. Refer to PVT: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</p> <code>mindocr.models.backbones.mindcv_models.pvt.Attention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>spatial-reduction attention (SRA)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>class Attention(nn.Cell):\n\"\"\"spatial-reduction attention (SRA)\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        sr_ratio: int = 1,\n    ):\n        super(Attention, self).__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Dense(dim, dim, has_bias=qkv_bias)\n        self.kv = nn.Dense(dim, dim * 2, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(1 - attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = nn.Dropout(1 - proj_drop)\n        self.qk_batmatmul = ops.BatchMatMul(transpose_b=True)\n        self.batmatmul = ops.BatchMatMul()\n        self.softmax = nn.Softmax(axis=-1)\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio &gt; 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio, has_bias=True)\n            self.norm = nn.LayerNorm([dim])\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x)\n        q = self.reshape(q, (B, N, self.num_heads, C // self.num_heads))\n        q = self.transpose(q, (0, 2, 1, 3))\n        if self.sr_ratio &gt; 1:\n            x_ = self.reshape(self.transpose(x, (0, 2, 1)), (B, C, H, W))\n\n            x_ = self.transpose(self.reshape(self.sr(x_), (B, C, -1)), (0, 2, 1))\n            x_ = self.norm(x_)\n            kv = self.kv(x_)\n\n            kv = self.transpose(self.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n        else:\n            kv = self.kv(x)\n            kv = self.transpose(self.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n        k, v = kv[0], kv[1]\n        attn = self.qk_batmatmul(q, k) * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = self.batmatmul(attn, v)\n        x = self.reshape(self.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Block with spatial-reduction attention (SRA) and feed forward</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>class Block(nn.Cell):\n\"\"\" Block with spatial-reduction attention (SRA) and feed forward\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super(Block, self).__init__()\n        self.norm1 = norm_layer([dim], epsilon=1e-5)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def construct(self, x, H, W):\n        x1 = self.norm1(x)\n        x1 = self.attn(x1, H, W)\n        x = x + self.drop_path(x1)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n        self.norm = nn.LayerNorm([embed_dim], epsilon=1e-5)\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n\n    def construct(self, x):\n        B, C, H, W = x.shape\n\n        x = self.proj(x)\n        b, c, h, w = x.shape\n        x = self.reshape(x, (b, c, h * w))\n        x = self.transpose(x, (0, 2, 1))\n        x = self.norm(x)\n        H, W = H // self.patch_size[0], W // self.patch_size[1]\n\n        return x, (H, W)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.PyramidVisionTransformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Pyramid Vision Transformer model class, based on <code>\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\" &lt;https://arxiv.org/abs/2102.12122&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>img_size(int)</code> <p>size of a input image.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dims</code> <p>how many hidden dim in each PatchEmbed.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[64, 128, 320, 512]</code> </p> <code>num_heads</code> <p>number of attention head in each stage.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[1, 2, 5, 8]</code> </p> <code>mlp_ratios</code> <p>ratios of MLP hidden dims in each stage.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[8, 8, 4, 4]</code> </p> <code>qkv_bias(bool)</code> <p>use bias in attention.</p> <p> </p> <code>qk_scale(float)</code> <p>Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.</p> <p> </p> <code>drop_rate(float)</code> <p>The drop rate for each block. Default: 0.0.</p> <p> </p> <code>attn_drop_rate(float)</code> <p>The drop rate for attention. Default: 0.0.</p> <p> </p> <code>drop_path_rate(float)</code> <p>The drop rate for drop path. Default: 0.0.</p> <p> </p> <code>norm_layer(nn.Cell)</code> <p>Norm layer that will be used in blocks. Default: nn.LayerNorm.</p> <p> </p> <code>depths</code> <p>number of Blocks.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[2, 2, 2, 2]</code> </p> <code>sr_ratios(list)</code> <p>stride and kernel size of each attention.</p> <p> </p> <code>num_stages(int)</code> <p>number of stage. Default: 4.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>class PyramidVisionTransformer(nn.Cell):\nr\"\"\"Pyramid Vision Transformer model class, based on\n    `\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\" &lt;https://arxiv.org/abs/2102.12122&gt;`_  # noqa: E501\n\n    Args:\n        img_size(int) : size of a input image.\n        patch_size (int) : size of a single image patch.\n        in_chans (int) : number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dims (list) : how many hidden dim in each PatchEmbed.\n        num_heads (list) : number of attention head in each stage.\n        mlp_ratios (list): ratios of MLP hidden dims in each stage.\n        qkv_bias(bool) : use bias in attention.\n        qk_scale(float) : Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.\n        drop_rate(float) : The drop rate for each block. Default: 0.0.\n        attn_drop_rate(float) : The drop rate for attention. Default: 0.0.\n        drop_path_rate(float) : The drop rate for drop path. Default: 0.0.\n        norm_layer(nn.Cell) : Norm layer that will be used in blocks. Default: nn.LayerNorm.\n        depths (list) : number of Blocks.\n        sr_ratios(list) : stride and kernel size of each attention.\n        num_stages(int) : number of stage. Default: 4.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, embed_dims=[64, 128, 320, 512],\n                 num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True, qk_scale=None, drop_rate=0.0,\n                 attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm,\n                 depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], num_stages=4):\n        super(PyramidVisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_stages = num_stages\n        start = Tensor(0, mindspore.float32)\n        stop = Tensor(drop_path_rate, mindspore.float32)\n        dpr = [float(x) for x in ops.linspace(start, stop, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        b_list = []\n        self.pos_embed = []\n        self.pos_drop = nn.Dropout(1 - drop_rate)\n        for i in range(num_stages):\n            block = nn.CellList(\n                [Block(dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n                       qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j],\n                       norm_layer=norm_layer, sr_ratio=sr_ratios[i])\n                 for j in range(depths[i])\n                 ])\n\n            b_list.append(block)\n            cur += depths[0]\n\n        self.patch_embed1 = PatchEmbed(img_size=img_size,\n                                       patch_size=patch_size,\n                                       in_chans=in_chans,\n                                       embed_dim=embed_dims[0])\n        num_patches = self.patch_embed1.num_patches\n        self.pos_embed1 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[0]), mindspore.float16))\n        self.pos_drop1 = nn.Dropout(1 - drop_rate)\n\n        self.patch_embed2 = PatchEmbed(img_size=img_size // (2 ** (1 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[1 - 1],\n                                       embed_dim=embed_dims[1])\n        num_patches = self.patch_embed2.num_patches\n        self.pos_embed2 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[1]), mindspore.float16))\n        self.pos_drop2 = nn.Dropout(1 - drop_rate)\n\n        self.patch_embed3 = PatchEmbed(img_size=img_size // (2 ** (2 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[2 - 1],\n                                       embed_dim=embed_dims[2])\n        num_patches = self.patch_embed3.num_patches\n        self.pos_embed3 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[2]), mindspore.float16))\n        self.pos_drop3 = nn.Dropout(1 - drop_rate)\n\n        self.patch_embed4 = PatchEmbed(img_size // (2 ** (3 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[3 - 1],\n                                       embed_dim=embed_dims[3])\n        num_patches = self.patch_embed4.num_patches + 1\n        self.pos_embed4 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[3]), mindspore.float16))\n        self.pos_drop4 = nn.Dropout(1 - drop_rate)\n        self.Blocks = nn.CellList(b_list)\n\n        self.norm = norm_layer([embed_dims[3]])\n\n        # cls_token\n        self.cls_token = mindspore.Parameter(ops.zeros((1, 1, embed_dims[3]), mindspore.float32))\n\n        # classification head\n        self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n        self.tile = ops.Tile()\n        self.Concat = ops.Concat(axis=1)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(weight_init.initializer(weight_init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(weight_init.initializer(weight_init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                fan_out //= cell.group\n                cell.weight.set_data(weight_init.initializer(weight_init.Normal(sigma=math.sqrt(2.0 / fan_out)),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def _get_pos_embed(self, pos_embed, ph, pw, H, W):\n        if H * W == self.patch_embed1.num_patches:\n            return pos_embed\n        else:\n            ResizeBilinear = nn.ResizeBilinear()\n\n            pos_embed = self.transpose(self.reshape(pos_embed, (1, ph, pw, -1)), (0, 3, 1, 2))\n            pos_embed = ResizeBilinear(pos_embed, (H, W))\n\n            pos_embed = self.transpose(self.reshape(pos_embed, (1, -1, H * W)), (0, 2, 1))\n\n            return pos_embed\n\n    def forward_features(self, x):\n        B = x.shape[0]\n\n        x, (H, W) = self.patch_embed1(x)\n        pos_embed = self.pos_embed1\n        x = self.pos_drop1(x + pos_embed)\n        for blk in self.Blocks[0]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed2(x)\n        ph, pw = self.patch_embed2.H, self.patch_embed2.W\n        pos_embed = self._get_pos_embed(self.pos_embed2, ph, pw, H, W)\n        x = self.pos_drop2(x + pos_embed)\n        for blk in self.Blocks[1]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed3(x)\n        ph, pw = self.patch_embed3.H, self.patch_embed3.W\n        pos_embed = self._get_pos_embed(self.pos_embed3, ph, pw, H, W)\n        x = self.pos_drop3(x + pos_embed)\n        for blk in self.Blocks[2]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed4(x)\n        cls_tokens = self.tile(self.cls_token, (B, 1, 1))\n\n        x = self.Concat((cls_tokens, x))\n        ph, pw = self.patch_embed4.H, self.patch_embed4.W\n        pos_embed_ = self._get_pos_embed(self.pos_embed4[:, 1:], ph, pw, H, W)\n        pos_embed = self.Concat((self.pos_embed4[:, 0:1], pos_embed_))\n        x = self.pos_drop4(x + pos_embed)\n        for blk in self.Blocks[3]:\n            x = blk(x, H, W)\n\n        x = self.norm(x)\n\n        return x[:, 0]\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.pvt_large(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVT large model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>@register_model\ndef pvt_large(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n\"\"\"Get PVT large model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_large']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 8, 27, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.pvt_medium(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVT medium model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>@register_model\ndef pvt_medium(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n\"\"\"Get PVT medium model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_medium']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 18, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.pvt_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVT small model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>@register_model\ndef pvt_small(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n\"\"\"Get PVT small model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_small']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 6, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvt.pvt_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVT tiny model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvt.py</code> <pre><code>@register_model\ndef pvt_tiny(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n\"\"\"Get PVT tiny model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_tiny']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2</code> \u00b6 <p>MindSpore implementation of <code>PVTv2</code>. Refer to PVTv2: PVTv2: Improved Baselines with Pyramid Vision Transformer</p> <code>mindocr.models.backbones.mindcv_models.pvtv2.Attention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Linear Spatial Reduction Attention</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class Attention(nn.Cell):\n\"\"\"Linear Spatial Reduction Attention\"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1,\n                 linear=False):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Dense(dim, dim, has_bias=qkv_bias)\n        self.kv = nn.Dense(dim, dim * 2, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(1 - attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = nn.Dropout(1 - proj_drop)\n        self.qk_batmatmul = ops.BatchMatMul(transpose_b=True)\n        self.batmatmul = ops.BatchMatMul()\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.linear = linear\n        self.sr_ratio = sr_ratio\n        if not linear:\n            if sr_ratio &gt; 1:\n                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio, has_bias=True)\n                self.norm = nn.LayerNorm([dim])\n\n        else:\n            self.pool = nn.AdaptiveAvgPool2d(7)\n            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1, has_bias=True)\n            self.norm = nn.LayerNorm([dim])\n            self.act = nn.GELU()\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x)\n        q = ops.reshape(q, (B, N, self.num_heads, C // self.num_heads))\n        q = ops.transpose(q, (0, 2, 1, 3))\n\n        if not self.linear:\n            if self.sr_ratio &gt; 1:\n                x_ = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n\n                x_ = self.sr(x_)\n                x_ = ops.transpose(ops.reshape(x_, (B, C, -1)), (0, 2, 1))\n                x_ = self.norm(x_)\n\n                kv = self.kv(x_)\n                kv = ops.transpose(ops.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n            else:\n                kv = self.kv(x)\n                kv = ops.transpose(ops.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n\n        else:\n            x_ = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n            x_ = self.sr(self.pool(x_))\n            x_ = ops.reshape(ops.transpose(x_, (0, 2, 1)), (B, C, -1))\n            x_ = self.norm(x_)\n            x_ = self.act(x_)\n            kv = ops.transpose(ops.reshape(self.kv(x_), (B, -1, 2, self.num_heads, C // self.num_heads)),\n                               (2, 0, 3, 1, 4))\n        k, v = kv[0], kv[1]\n\n        attn = self.qk_batmatmul(q, k) * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = self.batmatmul(attn, v)\n        x = ops.reshape(ops.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Block with Linear Spatial Reduction Attention and Convolutional Feed-Forward</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class Block(nn.Cell):\n\"\"\"Block with Linear Spatial Reduction Attention and Convolutional Feed-Forward\"\"\"\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, linear=False, block_id=0):\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio, linear=linear)\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n\n        self.norm2 = norm_layer([dim])\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, linear=linear)\n\n    def construct(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.DWConv</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Depthwise separable convolution</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class DWConv(nn.Cell):\n\"\"\"Depthwise separable convolution\"\"\"\n\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, has_bias=True, group=dim)\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        x = ops.transpose(x, (0, 2, 1)).view((B, C, H, W))\n        x = self.dwconv(x)\n        x = ops.transpose(x.view((B, C, H * W)), (0, 2, 1))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.Mlp</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP with depthwise separable convolution</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class Mlp(nn.Cell):\n\"\"\"MLP with depthwise separable convolution\"\"\"\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0, linear=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(hidden_features, out_features)\n        self.drop = nn.Dropout(1 - drop)\n        self.linear = linear\n        if self.linear:\n            self.relu = nn.ReLU()\n\n    def construct(self, x, H, W):\n        x = self.fc1(x)\n        if self.linear:\n            x = self.relu(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.OverlapPatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Overlapping Patch Embedding</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class OverlapPatchEmbed(nn.Cell):\n\"\"\"Overlapping Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n\n        assert max(patch_size) &gt; stride, \"Set larger patch_size than stride\"\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // stride, img_size[1] // stride\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, has_bias=True)\n        self.norm = nn.LayerNorm([embed_dim])\n\n    def construct(self, x):\n        x = self.proj(x)\n        B, C, H, W = x.shape\n        x = ops.transpose(ops.reshape(x, (B, C, H * W)), (0, 2, 1))\n        x = self.norm(x)\n\n        return x, H, W\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.PyramidVisionTransformerV2</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Pyramid Vision Transformer V2 model class, based on <code>\"PVTv2: Improved Baselines with Pyramid Vision Transformer\" &lt;https://arxiv.org/abs/2106.13797&gt;</code>_</p> PARAMETER DESCRIPTION <code>img_size(int)</code> <p>size of a input image.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>16</code> </p> <code>in_chans</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dims</code> <p>how many hidden dim in each PatchEmbed.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[64, 128, 256, 512]</code> </p> <code>num_heads</code> <p>number of attention head in each stage.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[1, 2, 4, 8]</code> </p> <code>mlp_ratios</code> <p>ratios of MLP hidden dims in each stage.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[4, 4, 4, 4]</code> </p> <code>qkv_bias(bool)</code> <p>use bias in attention.</p> <p> </p> <code>qk_scale(float)</code> <p>Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.</p> <p> </p> <code>drop_rate(float)</code> <p>The drop rate for each block. Default: 0.0.</p> <p> </p> <code>attn_drop_rate(float)</code> <p>The drop rate for attention. Default: 0.0.</p> <p> </p> <code>drop_path_rate(float)</code> <p>The drop rate for drop path. Default: 0.0.</p> <p> </p> <code>norm_layer(nn.Cell)</code> <p>Norm layer that will be used in blocks. Default: nn.LayerNorm.</p> <p> </p> <code>depths</code> <p>number of Blocks.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[3, 4, 6, 3]</code> </p> <code>sr_ratios(list)</code> <p>stride and kernel size of each attention.</p> <p> </p> <code>num_stages(int)</code> <p>number of stage. Default: 4.</p> <p> </p> <code>linear(bool)</code> <p>use linear SRA.</p> <p> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>class PyramidVisionTransformerV2(nn.Cell):\nr\"\"\"Pyramid Vision Transformer V2 model class, based on\n    `\"PVTv2: Improved Baselines with Pyramid Vision Transformer\" &lt;https://arxiv.org/abs/2106.13797&gt;`_\n\n    Args:\n        img_size(int) : size of a input image.\n        patch_size (int) : size of a single image patch.\n        in_chans (int) : number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dims (list) : how many hidden dim in each PatchEmbed.\n        num_heads (list) : number of attention head in each stage.\n        mlp_ratios (list): ratios of MLP hidden dims in each stage.\n        qkv_bias(bool) : use bias in attention.\n        qk_scale(float) : Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.\n        drop_rate(float) : The drop rate for each block. Default: 0.0.\n        attn_drop_rate(float) : The drop rate for attention. Default: 0.0.\n        drop_path_rate(float) : The drop rate for drop path. Default: 0.0.\n        norm_layer(nn.Cell) : Norm layer that will be used in blocks. Default: nn.LayerNorm.\n        depths (list) : number of Blocks.\n        sr_ratios(list) : stride and kernel size of each attention.\n        num_stages(int) : number of stage. Default: 4.\n        linear(bool) :  use linear SRA.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, linear=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_stages = num_stages\n\n        start = Tensor(0, mindspore.float32)\n        stop = Tensor(drop_path_rate, mindspore.float32)\n        dpr = [float(x) for x in ops.linspace(start, stop, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n\n        patch_embed_list = []\n        block_list = []\n        norm_list = []\n\n        for i in range(num_stages):\n            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n                                            patch_size=7 if i == 0 else 3,\n                                            stride=4 if i == 0 else 2,\n                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n                                            embed_dim=embed_dims[i])\n\n            block = nn.CellList([Block(\n                dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,\n                sr_ratio=sr_ratios[i], linear=linear, block_id=j)\n                for j in range(depths[i])])\n\n            norm = norm_layer([embed_dims[i]])\n\n            cur += depths[i]\n\n            patch_embed_list.append(patch_embed)\n            block_list.append(block)\n            norm_list.append(norm)\n        self.patch_embed_list = nn.CellList(patch_embed_list)\n        self.block_list = nn.CellList(block_list)\n        self.norm_list = nn.CellList(norm_list)\n        # classification head\n        self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n        self._initialize_weights()\n\n    def freeze_patch_emb(self):\n        self.patch_embed_list[0].requires_grad = False\n\n    def _initialize_weights(self):\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(weight_init.initializer(weight_init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(weight_init.initializer(weight_init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                fan_out //= cell.group\n                cell.weight.set_data(weight_init.initializer(weight_init.Normal(sigma=math.sqrt(2.0 / fan_out)),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n\n        for i in range(self.num_stages):\n            patch_embed = self.patch_embed_list[i]\n            block = self.block_list[i]\n            norm = self.norm_list[i]\n            x, H, W = patch_embed(x)\n            for blk in block:\n                x = blk(x, H, W)\n            x = norm(x)\n            if i != self.num_stages - 1:\n                x = ops.transpose(ops.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        return x.mean(axis=1)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b0 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b0(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b0 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b0\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b1 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b1(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b1 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b1\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b2 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b2(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b2 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b2\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b3 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b3(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b3 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b3\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b4 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b4(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b4 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b4\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.pvtv2.pvt_v2_b5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get PVTV2-b5 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b5(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n\"\"\"Get PVTV2-b5 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b5\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry</code> \u00b6 <p>model registry and list</p> <code>mindocr.models.backbones.mindcv_models.registry.get_pretrained_cfg_value(model_name, cfg_key)</code> \u00b6 <p>Get a specific model default_cfg value by key. None if it doesn't exist.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def get_pretrained_cfg_value(model_name, cfg_key):\n\"\"\"Get a specific model default_cfg value by key. None if it doesn't exist.\"\"\"\n    if model_name in _model_pretrained_cfgs:\n        return _model_pretrained_cfgs[model_name].get(cfg_key, None)\n    return None\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry.has_pretrained_cfg_key(model_name, cfg_key)</code> \u00b6 <p>Query model default_cfgs for existence of a specific key.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def has_pretrained_cfg_key(model_name, cfg_key):\n\"\"\"Query model default_cfgs for existence of a specific key.\"\"\"\n    if model_name in _model_pretrained_cfgs and cfg_key in _model_pretrained_cfgs[model_name]:\n        return True\n    return False\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry.is_model(model_name)</code> \u00b6 <p>Check if a model name exists</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def is_model(model_name):\n\"\"\"\n    Check if a model name exists\n    \"\"\"\n    return model_name in _model_entrypoints\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry.is_model_in_modules(model_name, module_names)</code> \u00b6 <p>Check if a model exists within a subset of modules</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def is_model_in_modules(model_name, module_names):\n\"\"\"\n    Check if a model exists within a subset of modules\n    Args:\n        model_name (str) - name of model to check\n        module_names (tuple, list, set) - names of modules to search in\n    \"\"\"\n    assert isinstance(module_names, (tuple, list, set))\n    return any(model_name in _module_to_models[n] for n in module_names)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry.list_modules()</code> \u00b6 <p>Return list of module names that contain models / model entrypoints</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def list_modules():\n\"\"\"\n    Return list of module names that contain models / model entrypoints\n    \"\"\"\n    modules = _module_to_models.keys()\n    return list(sorted(modules))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.registry.model_entrypoint(model_name)</code> \u00b6 <p>Fetch a model entrypoint for specified model name</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\registry.py</code> <pre><code>def model_entrypoint(model_name):\n\"\"\"\n    Fetch a model entrypoint for specified model name\n    \"\"\"\n    return _model_entrypoints[model_name]\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet</code> \u00b6 <p>MindSpore implementation of <code>RegNet</code>. Refer to: Designing Network Design Spaces</p> <code>mindocr.models.backbones.mindcv_models.regnet.AnyHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>AnyNet head: optional conv, AvgPool, 1x1.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class AnyHead(nn.Cell):\n\"\"\"AnyNet head: optional conv, AvgPool, 1x1.\"\"\"\n\n    def __init__(self, w_in, head_width, num_classes):\n        super(AnyHead, self).__init__()\n        self.head_width = head_width\n        if head_width &gt; 0:\n            self.conv = conv2d(w_in, head_width, 1)\n            self.bn = norm2d(head_width)\n            self.af = activation()\n            w_in = head_width\n        self.avg_pool = gap2d()\n        self.fc = linear(w_in, num_classes, bias=True)\n\n    def construct(self, x):\n        x = self.af(self.bn(self.conv(x))) if self.head_width &gt; 0 else x\n        x = self.avg_pool(x)\n        x = self.fc(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.AnyNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>AnyNet model.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class AnyNet(nn.Cell):\n\"\"\"AnyNet model.\"\"\"\n\n    @staticmethod\n    def anynet_get_params(depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w,\n                          num_classes, se_r):\n        nones = [None for _ in depths]\n        return {\n            \"stem_type\": stem_type,\n            \"stem_w\": stem_w,\n            \"block_type\": block_type,\n            \"depths\": depths,\n            \"widths\": widths,\n            \"strides\": strides,\n            \"bot_muls\": bot_muls if bot_muls else nones,\n            \"group_ws\": group_ws if group_ws else nones,\n            \"head_w\": head_w,\n            \"se_r\": se_r,\n            \"num_classes\": num_classes,\n        }\n\n    def __init__(self, depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w, num_classes,\n                 se_r, in_channels):\n        super(AnyNet, self).__init__()\n        p = AnyNet.anynet_get_params(depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w,\n                                     num_classes, se_r)\n        stem_fun = get_stem_fun(p[\"stem_type\"])\n        block_fun = get_block_fun(p[\"block_type\"])\n        self.stem = stem_fun(in_channels, p[\"stem_w\"])\n        prev_w = p[\"stem_w\"]\n        keys = [\"depths\", \"widths\", \"strides\", \"bot_muls\", \"group_ws\"]\n        self.stages = nn.CellList()\n        for i, (d, w, s, b, g) in enumerate(zip(*[p[k] for k in keys])):\n            params = {\"bot_mul\": b, \"group_w\": g, \"se_r\": p[\"se_r\"]}\n            stage = AnyStage(prev_w, w, s, d, block_fun, params)\n            self.stages.append(stage)\n            prev_w = w\n        self.head = AnyHead(prev_w, p[\"head_w\"], p[\"num_classes\"])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2.0 / fan_out), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        for module in self.stages:\n            x = module(x)\n        return x\n\n    def forward_head(self, x):\n        x = self.head(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.AnyStage</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>AnyNet stage (sequence of blocks w/ the same output shape).</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class AnyStage(nn.Cell):\n\"\"\"AnyNet stage (sequence of blocks w/ the same output shape).\"\"\"\n\n    def __init__(self, w_in, w_out, stride, d, block_fun, params):\n        super(AnyStage, self).__init__()\n        self.blocks = nn.CellList()\n        for _ in range(d):\n            block = block_fun(w_in, w_out, stride, params)\n            self.blocks.append(block)\n            stride, w_in = 1, w_out\n\n    def construct(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.BasicTransform</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic transformation: [3x3 conv, BN, Relu] x2.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class BasicTransform(nn.Cell):\n\"\"\"Basic transformation: [3x3 conv, BN, Relu] x2.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, _params):\n        super(BasicTransform, self).__init__()\n        self.a = conv2d(w_in, w_out, 3, stride=stride)\n        self.a_bn = norm2d(w_out)\n        self.a_af = activation()\n        self.b = conv2d(w_out, w_out, 3)\n        self.b_bn = norm2d(w_out)\n        self.b_bn.final_bn = True\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.BottleneckTransform</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class BottleneckTransform(nn.Cell):\n\"\"\"Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(BottleneckTransform, self).__init__()\n        w_b = int(round(w_out * params[\"bot_mul\"]))\n        w_se = int(round(w_in * params[\"se_r\"]))\n        groups = w_b // params[\"group_w\"]\n        self.a = conv2d(w_in, w_b, 1)\n        self.a_bn = norm2d(w_b)\n        self.a_af = activation()\n        self.b = conv2d(w_b, w_b, 3, stride=stride, groups=groups)\n        self.b_bn = norm2d(w_b)\n        self.b_af = activation()\n        self.se = SqueezeExcite(in_channels=w_b, rd_channels=w_se) if w_se else None\n        self.c = conv2d(w_b, w_out, 1)\n        self.c_bn = norm2d(w_out)\n        self.c_bn.final_bn = True\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        x = self.b_af(x)\n        x = self.se(x) if self.se is not None else x\n        x = self.c(x)\n        x = self.c_bn(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.RegNet</code> \u00b6 <p>         Bases: <code>AnyNet</code></p> <p>RegNet model class, based on <code>\"Designing Network Design Spaces\" &lt;https://arxiv.org/abs/2003.13678&gt;</code>_</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class RegNet(AnyNet):\nr\"\"\"RegNet model class, based on\n    `\"Designing Network Design Spaces\" &lt;https://arxiv.org/abs/2003.13678&gt;`_\n    \"\"\"\n\n    @staticmethod\n    def regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w,\n                          num_classes, se_r):\n\"\"\"Get AnyNet parameters that correspond to the RegNet.\"\"\"\n        ws, ds, ss, bs, gs = generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)\n        return {\n            \"stem_type\": stem_type,\n            \"stem_w\": stem_w,\n            \"block_type\": block_type,\n            \"depths\": ds,\n            \"widths\": ws,\n            \"strides\": ss,\n            \"bot_muls\": bs,\n            \"group_ws\": gs,\n            \"head_w\": head_w,\n            \"se_r\": se_r,\n            \"num_classes\": num_classes,\n        }\n\n    def __init__(self, w_a, w_0, w_m, d, group_w, stride=2, bot_mul=1.0, stem_type=\"simple_stem_in\", stem_w=32,\n                 block_type=\"res_bottleneck_block\", head_w=0, num_classes=1000, se_r=0.0, in_channels=3):\n        params = RegNet.regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type,\n                                          head_w, num_classes, se_r)\n        print(params)\n        super(RegNet, self).__init__(params[\"depths\"], params[\"stem_type\"], params[\"stem_w\"], params[\"block_type\"],\n                                     params[\"widths\"], params[\"strides\"], params[\"bot_muls\"], params[\"group_ws\"],\n                                     params[\"head_w\"], params[\"num_classes\"], params[\"se_r\"], in_channels)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.RegNet.regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w, num_classes, se_r)</code> <code>staticmethod</code> \u00b6 <p>Get AnyNet parameters that correspond to the RegNet.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>@staticmethod\ndef regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w,\n                      num_classes, se_r):\n\"\"\"Get AnyNet parameters that correspond to the RegNet.\"\"\"\n    ws, ds, ss, bs, gs = generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)\n    return {\n        \"stem_type\": stem_type,\n        \"stem_w\": stem_w,\n        \"block_type\": block_type,\n        \"depths\": ds,\n        \"widths\": ws,\n        \"strides\": ss,\n        \"bot_muls\": bs,\n        \"group_ws\": gs,\n        \"head_w\": head_w,\n        \"se_r\": se_r,\n        \"num_classes\": num_classes,\n    }\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.ResBasicBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Residual basic block: x + f(x), f = basic transform.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class ResBasicBlock(nn.Cell):\n\"\"\"Residual basic block: x + f(x), f = basic transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBasicBlock, self).__init__()\n        self.proj, self.bn = None, None\n        if (w_in != w_out) or (stride != 1):\n            self.proj = conv2d(w_in, w_out, 1, stride=stride)\n            self.bn = norm2d(w_out)\n        self.f = BasicTransform(w_in, w_out, stride, params)\n        self.af = activation()\n\n    def construct(self, x):\n        x_p = self.bn(self.proj(x)) if self.proj is not None else x\n        return self.af(x_p + self.f(x))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.ResBottleneckBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Residual bottleneck block: x + f(x), f = bottleneck transform.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class ResBottleneckBlock(nn.Cell):\n\"\"\"Residual bottleneck block: x + f(x), f = bottleneck transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBottleneckBlock, self).__init__()\n        self.proj, self.bn = None, None\n        if (w_in != w_out) or (stride != 1):\n            self.proj = conv2d(w_in, w_out, 1, stride=stride)\n            self.bn = norm2d(w_out)\n        self.f = BottleneckTransform(w_in, w_out, stride, params)\n        self.af = activation()\n\n    def construct(self, x):\n        x_p = self.bn(self.proj(x)) if self.proj is not None else x\n        return self.af(x_p + self.f(x))\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.ResBottleneckLinearBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Residual linear bottleneck block: x + f(x), f = bottleneck transform.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class ResBottleneckLinearBlock(nn.Cell):\n\"\"\"Residual linear bottleneck block: x + f(x), f = bottleneck transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBottleneckLinearBlock, self).__init__()\n        self.has_skip = (w_in == w_out) and (stride == 1)\n        self.f = BottleneckTransform(w_in, w_out, stride, params)\n\n    def construct(self, x):\n        return x + self.f(x) if self.has_skip else self.f(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.ResStem</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ResNet stem for ImageNet: 7x7, BN, AF, MaxPool.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class ResStem(nn.Cell):\n\"\"\"ResNet stem for ImageNet: 7x7, BN, AF, MaxPool.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(ResStem, self).__init__()\n        self.conv = conv2d(w_in, w_out, 7, stride=2)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n        self.pool = pool2d(w_out, 3, stride=2)\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        x = self.pool(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.ResStemCifar</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ResNet stem for CIFAR: 3x3, BN, AF.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class ResStemCifar(nn.Cell):\n\"\"\"ResNet stem for CIFAR: 3x3, BN, AF.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(ResStemCifar, self).__init__()\n        self.conv = conv2d(w_in, w_out, 3)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.SimpleStem</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Simple stem for ImageNet: 3x3, BN, AF.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class SimpleStem(nn.Cell):\n\"\"\"Simple stem for ImageNet: 3x3, BN, AF.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(SimpleStem, self).__init__()\n        self.conv = conv2d(w_in, w_out, 3, stride=2)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.VanillaBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Vanilla block: [3x3 conv, BN, Relu] x2.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>class VanillaBlock(nn.Cell):\n\"\"\"Vanilla block: [3x3 conv, BN, Relu] x2.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, _params):\n        super(VanillaBlock, self).__init__()\n        self.a = conv2d(w_in, w_out, 3, stride=stride)\n        self.a_bn = norm2d(w_out)\n        self.a_af = activation()\n        self.b = conv2d(w_out, w_out, 3)\n        self.b_bn = norm2d(w_out)\n        self.b_af = activation()\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        x = self.b_af(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.activation()</code> \u00b6 <p>Helper for building an activation layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def activation():\n\"\"\"Helper for building an activation layer.\"\"\"\n    return nn.ReLU()\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.adjust_block_compatibility(ws, bs, gs)</code> \u00b6 <p>Adjusts the compatibility of widths, bottlenecks, and groups.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def adjust_block_compatibility(ws, bs, gs):\n\"\"\"Adjusts the compatibility of widths, bottlenecks, and groups.\"\"\"\n    assert len(ws) == len(bs) == len(gs)\n    assert all(w &gt; 0 and b &gt; 0 and g &gt; 0 for w, b, g in zip(ws, bs, gs))\n    assert all(b &lt; 1 or b % 1 == 0 for b in bs)\n    vs = [int(max(1, w * b)) for w, b in zip(ws, bs)]\n    gs = [int(min(g, v)) for g, v in zip(gs, vs)]\n    ms = [np.lcm(g, int(b)) if b &gt; 1 else g for g, b in zip(gs, bs)]\n    vs = [max(m, int(round(v / m) * m)) for v, m in zip(vs, ms)]\n    ws = [int(v / b) for v, b in zip(vs, bs)]\n    assert all(w * b % g == 0 for w, b, g in zip(ws, bs, gs))\n    return ws, bs, gs\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.conv2d(w_in, w_out, k, *, stride=1, groups=1, bias=False)</code> \u00b6 <p>Helper for building a conv2d layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def conv2d(w_in, w_out, k, *, stride=1, groups=1, bias=False):\n\"\"\"Helper for building a conv2d layer.\"\"\"\n    assert k % 2 == 1, \"Only odd size kernels supported to avoid padding issues.\"\n    s, p, g, b = stride, (k - 1) // 2, groups, bias\n    return nn.Conv2d(w_in, w_out, k, stride=s, pad_mode=\"pad\", padding=p, group=g, has_bias=b)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.gap2d(keep_dims=False)</code> \u00b6 <p>Helper for building a gap2d layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def gap2d(keep_dims=False):\n\"\"\"Helper for building a gap2d layer.\"\"\"\n    return GlobalAvgPooling(keep_dims)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.generate_regnet(w_a, w_0, w_m, d, q=8)</code> \u00b6 <p>Generates per stage widths and depths from RegNet parameters.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def generate_regnet(w_a, w_0, w_m, d, q=8):\n\"\"\"Generates per stage widths and depths from RegNet parameters.\"\"\"\n    assert w_a &gt;= 0 and w_0 &gt; 0 and w_m &gt; 1 and w_0 % q == 0\n    # Generate continuous per-block ws\n    ws_cont = np.arange(d) * w_a + w_0\n    # Generate quantized per-block ws\n    ks = np.round(np.log(ws_cont / w_0) / np.log(w_m))\n    ws_all = w_0 * np.power(w_m, ks)\n    ws_all = np.round(np.divide(ws_all, q)).astype(int) * q\n    # Generate per stage ws and ds (assumes ws_all are sorted)\n    ws, ds = np.unique(ws_all, return_counts=True)\n    # Compute number of actual stages and total possible stages\n    num_stages, total_stages = len(ws), ks.max() + 1\n    # Convert numpy arrays to lists and return\n    ws, ds, ws_all, ws_cont = (x.tolist() for x in (ws, ds, ws_all, ws_cont))\n    return ws, ds, num_stages, total_stages, ws_all, ws_cont\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)</code> \u00b6 <p>Generates per stage ws, ds, gs, bs, and ss from RegNet cfg.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w):\n\"\"\"Generates per stage ws, ds, gs, bs, and ss from RegNet cfg.\"\"\"\n    ws, ds = generate_regnet(w_a, w_0, w_m, d)[0:2]\n    ss = [stride for _ in ws]\n    bs = [bot_mul for _ in ws]\n    gs = [group_w for _ in ws]\n    ws, bs, gs = adjust_block_compatibility(ws, bs, gs)\n    return ws, ds, ss, bs, gs\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.get_block_fun(block_type)</code> \u00b6 <p>Retrieves the block function by name.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def get_block_fun(block_type):\n\"\"\"Retrieves the block function by name.\"\"\"\n    block_funs = {\n        \"vanilla_block\": VanillaBlock,\n        \"res_basic_block\": ResBasicBlock,\n        \"res_bottleneck_block\": ResBottleneckBlock,\n        \"res_bottleneck_linear_block\": ResBottleneckLinearBlock,\n    }\n    err_str = \"Block type '{}' not supported\"\n    assert block_type in block_funs.keys(), err_str.format(block_type)\n    return block_funs[block_type]\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.get_stem_fun(stem_type)</code> \u00b6 <p>Retrieves the stem function by name.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def get_stem_fun(stem_type):\n\"\"\"Retrieves the stem function by name.\"\"\"\n    stem_funs = {\n        \"res_stem_cifar\": ResStemCifar,\n        \"res_stem_in\": ResStem,\n        \"simple_stem_in\": SimpleStem,\n    }\n    err_str = \"Stem type '{}' not supported\"\n    assert stem_type in stem_funs.keys(), err_str.format(stem_type)\n    return stem_funs[stem_type]\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.linear(w_in, w_out, *, bias=False)</code> \u00b6 <p>Helper for building a linear layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def linear(w_in, w_out, *, bias=False):\n\"\"\"Helper for building a linear layer.\"\"\"\n    return nn.Dense(w_in, w_out, has_bias=bias)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.norm2d(w_in, eps=1e-05, mom=0.9)</code> \u00b6 <p>Helper for building a norm2d layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def norm2d(w_in, eps=1e-5, mom=0.9):\n\"\"\"Helper for building a norm2d layer.\"\"\"\n    return nn.BatchNorm2d(num_features=w_in, eps=eps, momentum=mom)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.regnet.pool2d(_w_in, k, *, stride=1)</code> \u00b6 <p>Helper for building a pool2d layer.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\regnet.py</code> <pre><code>def pool2d(_w_in, k, *, stride=1):\n\"\"\"Helper for building a pool2d layer.\"\"\"\n    assert k % 2 == 1, \"Only odd size kernels supported to avoid padding issues.\"\n    padding = (k - 1) // 2\n    pad2d = nn.Pad(((0, 0), (0, 0), (padding, padding), (padding, padding)), mode=\"CONSTANT\")\n    max_pool = nn.MaxPool2d(kernel_size=k, stride=stride, pad_mode=\"valid\")\n    return nn.SequentialCell([pad2d, max_pool])\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp</code> \u00b6 <p>MindSpore implementation of <code>RepMLP</code>. Refer to RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality.</p> <code>mindocr.models.backbones.mindcv_models.repmlp.FFNBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Common FFN layer</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>class FFNBlock(nn.Cell):\n\"\"\"Common FFN layer\"\"\"\n\n    def __init__(self, in_channels, hidden_channels=None, out_channels=None, act_layer=nn.GELU):\n        super().__init__()\n        out_features = out_channels or in_channels\n        hidden_features = hidden_channels or in_channels\n        self.ffn_fc1 = conv_bn(in_channels, hidden_features, 1, 1, 0, has_bias=False)\n        self.ffn_fc2 = conv_bn(hidden_features, out_features, 1, 1, 0, has_bias=False)\n        self.act = act_layer()\n\n    def construct(self, inputs):\n        x = self.ffn_fc1(inputs)\n        x = self.act(x)\n        x = self.ffn_fc2(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.GlobalPerceptron</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>GlobalPerceptron Layers provides global information(One of the three components of RepMLPBlock)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>class GlobalPerceptron(nn.Cell):\n\"\"\"GlobalPerceptron Layers provides global information(One of the three components of RepMLPBlock)\"\"\"\n\n    def __init__(self, input_channels, internal_neurons):\n        super(GlobalPerceptron, self).__init__()\n        self.fc1 = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons, kernel_size=(1, 1), stride=1,\n                             has_bias=True)\n        self.fc2 = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels, kernel_size=(1, 1), stride=1,\n                             has_bias=True)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.input_channels = input_channels\n        self.shape = ops.Shape()\n\n    def construct(self, x):\n        shape = self.shape(x)\n        pool = nn.AvgPool2d(kernel_size=(shape[2], shape[3]), stride=1)\n        x = pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        x = x.view(-1, self.input_channels, 1, 1)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic RepMLPBlock Layer(compose of Global Perceptron, Channel Perceptron and Local Perceptron)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>class RepMLPBlock(nn.Cell):\n\"\"\"Basic RepMLPBlock Layer(compose of Global Perceptron, Channel Perceptron and Local Perceptron)\"\"\"\n\n    def __init__(self, in_channels, out_channels,\n                 h, w,\n                 reparam_conv_k=None,\n                 globalperceptron_reduce=4,\n                 num_sharesets=1,\n                 deploy=False):\n        super().__init__()\n\n        self.C = in_channels  # noqa: E741\n        self.O = out_channels  # noqa: E741\n        self.S = num_sharesets  # noqa: E741\n\n        self.h, self.w = h, w\n\n        self.deploy = deploy\n        self.transpose = ops.Transpose()\n        self.shape = ops.Shape()\n        self.reshape = ops.Reshape()\n\n        assert in_channels == out_channels\n        self.gp = GlobalPerceptron(input_channels=in_channels, internal_neurons=in_channels // globalperceptron_reduce)\n\n        self.fc3 = nn.Conv2d(in_channels=self.h * self.w * num_sharesets, out_channels=self.h * self.w * num_sharesets,\n                             kernel_size=(1, 1), stride=1, padding=0, has_bias=deploy, group=num_sharesets)\n        if deploy:\n            self.fc3_bn = ops.Identity()\n        else:\n            self.fc3_bn = nn.BatchNorm2d(num_sharesets).set_train()\n\n        self.reparam_conv_k = reparam_conv_k\n        self.conv_branch_k = []\n        if not deploy and reparam_conv_k is not None:\n            for k in reparam_conv_k:\n                conv_branch = conv_bn(num_sharesets, num_sharesets, kernel_size=k, stride=1, padding=k // 2,\n                                      group=num_sharesets, has_bias=False)\n                self.__setattr__(\"repconv{}\".format(k), conv_branch)\n                self.conv_branch_k.append(conv_branch)\n                # print(conv_branch)\n\n    def partition(self, x, h_parts, w_parts):\n        x = x.reshape(-1, self.C, h_parts, self.h, w_parts, self.w)\n        input_perm = (0, 2, 4, 1, 3, 5)\n        x = self.transpose(x, input_perm)\n        return x\n\n    def partition_affine(self, x, h_parts, w_parts):\n        fc_inputs = x.reshape(-1, self.S * self.h * self.w, 1, 1)\n        out = self.fc3(fc_inputs)\n        out = out.reshape(-1, self.S, self.h, self.w)\n        out = self.fc3_bn(out)\n        out = out.reshape(-1, h_parts, w_parts, self.S, self.h, self.w)\n        return out\n\n    def construct(self, inputs):\n        # Global Perceptron\n        global_vec = self.gp(inputs)\n\n        origin_shape = self.shape(inputs)\n\n        h_parts = origin_shape[2] // self.h\n        w_parts = origin_shape[3] // self.w\n\n        partitions = self.partition(inputs, h_parts, w_parts)\n\n        #   Channel Perceptron\n        fc3_out = self.partition_affine(partitions, h_parts, w_parts)\n\n        #   Local Perceptron\n        if self.reparam_conv_k is not None and not self.deploy:\n            conv_inputs = self.reshape(partitions, (-1, self.S, self.h, self.w))\n            conv_out = 0\n            for k in self.conv_branch_k:\n                conv_out += k(conv_inputs)\n            conv_out = self.reshape(conv_out, (-1, h_parts, w_parts, self.S, self.h, self.w))\n            fc3_out += conv_out\n\n        input_perm = (0, 3, 1, 4, 2, 5)\n        fc3_out = self.transpose(fc3_out, input_perm)  # N, O, h_parts, out_h, w_parts, out_w\n        out = fc3_out.reshape(*origin_shape)\n        out = out * global_vec\n        return out\n\n    def get_equivalent_fc3(self):\n        fc_weight, fc_bias = fuse_bn(self.fc3, self.fc3_bn)\n        if self.reparam_conv_k is not None:\n            largest_k = max(self.reparam_conv_k)\n            largest_branch = self.__getattr__(\"repconv{}\".format(largest_k))\n            total_kernel, total_bias = fuse_bn(largest_branch.conv, largest_branch.bn)\n            for k in self.reparam_conv_k:\n                if k != largest_k:\n                    k_branch = self.__getattr__(\"repconv{}\".format(k))\n                    kernel, bias = fuse_bn(k_branch.conv, k_branch.bn)\n                    total_kernel += nn.Pad(kernel, [(largest_k - k) // 2] * 4)\n                    total_bias += bias\n            rep_weight, rep_bias = self._convert_conv_to_fc(total_kernel, total_bias)\n            final_fc3_weight = rep_weight.reshape_as(fc_weight) + fc_weight\n            final_fc3_bias = rep_bias + fc_bias\n        else:\n            final_fc3_weight = fc_weight\n            final_fc3_bias = fc_bias\n        return final_fc3_weight, final_fc3_bias\n\n    def local_inject(self):\n        self.deploy = True\n        #   Locality Injection\n        fc3_weight, fc3_bias = self.get_equivalent_fc3()\n        #   Remove Local Perceptron\n        if self.reparam_conv_k is not None:\n            for k in self.reparam_conv_k:\n                self.__delattr__(\"repconv{}\".format(k))\n        self.__delattr__(\"fc3\")\n        self.__delattr__(\"fc3_bn\")\n        self.fc3 = nn.Conv2d(self.S * self.h * self.w, self.S * self.h * self.w, 1, 1, 0, has_bias=True, group=self.S)\n        self.fc3_bn = ops.Identity()\n        self.fc3.weight.data = fc3_weight\n        self.fc3.bias.data = fc3_bias\n\n    def _convert_conv_to_fc(self, conv_kernel, conv_bias):\n        I = ops.eye(self.h * self.w).repeat(1, self.S).reshape(self.h * self.w, self.S, self.h, self.w)  # noqa: E741\n        fc_k = ops.Conv2D(I, conv_kernel, pad=(conv_kernel.size(2) // 2, conv_kernel.size(3) // 2), group=self.S)\n        fc_k = fc_k.reshape(self.h * self.w, self.S * self.h * self.w).t()\n        fc_bias = conv_bias.repeat_interleave(self.h * self.w)\n        return fc_k, fc_bias\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>RepMLPNet model class, based on <code>\"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\" &lt;https://arxiv.org/pdf/2112.11081v2.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch. Default: (4, 4)</p> <p> DEFAULT: <code>(4, 4)</code> </p> <code>num_blocks</code> <p>number of blocks per stage. Default: (2,2,6,2)</p> <p> DEFAULT: <code>(2, 2, 6, 2)</code> </p> <code>channels</code> <p>number of in_channels(channels[stage_idx]) and out_channels(channels[stage_idx + 1]) per stage. Default: (192,384,768,1536)</p> <p> DEFAULT: <code>(192, 384, 768, 1536)</code> </p> <code>hs</code> <p>height of picture per stage. Default: (64,32,16,8)</p> <p> DEFAULT: <code>(64, 32, 16, 8)</code> </p> <code>ws</code> <p>width of picture per stage. Default: (64,32,16,8)</p> <p> DEFAULT: <code>(64, 32, 16, 8)</code> </p> <code>sharesets_nums</code> <p>number of share sets per stage. Default: (4,8,16,32)</p> <p> DEFAULT: <code>(4, 8, 16, 32)</code> </p> <code>reparam_conv_k</code> <p>convolution kernel size in local Perceptron. Default: (3,)</p> <p> DEFAULT: <code>(3)</code> </p> <code>globalperceptron_reduce</code> <p>Intermediate convolution output size (in_channal = inchannal, out_channel = in_channel/globalperceptron_reduce) in globalperceptron. Default: 4</p> <p> DEFAULT: <code>4</code> </p> <code>use_checkpoint</code> <p>whether to use checkpoint</p> <p> DEFAULT: <code>False</code> </p> <code>deploy</code> <p>whether to use bias</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>class RepMLPNet(nn.Cell):\nr\"\"\"RepMLPNet model class, based on\n    `\"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\" &lt;https://arxiv.org/pdf/2112.11081v2.pdf&gt;`_\n\n    Args:\n        in_channels: number of input channels. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n        patch_size: size of a single image patch. Default: (4, 4)\n        num_blocks: number of blocks per stage. Default: (2,2,6,2)\n        channels: number of in_channels(channels[stage_idx]) and out_channels(channels[stage_idx + 1]) per stage.\n            Default: (192,384,768,1536)\n        hs: height of picture per stage. Default: (64,32,16,8)\n        ws: width of picture per stage. Default: (64,32,16,8)\n        sharesets_nums: number of share sets per stage. Default: (4,8,16,32)\n        reparam_conv_k: convolution kernel size in local Perceptron. Default: (3,)\n        globalperceptron_reduce: Intermediate convolution output size\n            (in_channal = inchannal, out_channel = in_channel/globalperceptron_reduce) in globalperceptron. Default: 4\n        use_checkpoint: whether to use checkpoint\n        deploy: whether to use bias\n    \"\"\"\n\n    def __init__(self,\n                 in_channels=3, num_class=1000,\n                 patch_size=(4, 4),\n                 num_blocks=(2, 2, 6, 2), channels=(192, 384, 768, 1536),\n                 hs=(64, 32, 16, 8), ws=(64, 32, 16, 8),\n                 sharesets_nums=(4, 8, 16, 32),\n                 reparam_conv_k=(3,),\n                 globalperceptron_reduce=4, use_checkpoint=False,\n                 deploy=False):\n        super().__init__()\n        num_stages = len(num_blocks)\n        assert num_stages == len(channels)\n        assert num_stages == len(hs)\n        assert num_stages == len(ws)\n        assert num_stages == len(sharesets_nums)\n\n        self.conv_embedding = conv_bn_relu(in_channels, channels[0], kernel_size=patch_size, stride=patch_size,\n                                           padding=0, has_bias=False)\n        self.conv2d = nn.Conv2d(in_channels, channels[0], kernel_size=patch_size, stride=patch_size, padding=0)\n\n        stages = []\n        embeds = []\n        for stage_idx in range(num_stages):\n            stage_blocks = [RepMLPNetUnit(channels=channels[stage_idx], h=hs[stage_idx], w=ws[stage_idx],\n                                          reparam_conv_k=reparam_conv_k,\n                                          globalperceptron_reduce=globalperceptron_reduce, ffn_expand=4,\n                                          num_sharesets=sharesets_nums[stage_idx],\n                                          deploy=deploy) for _ in range(num_blocks[stage_idx])]\n            stages.append(nn.CellList(stage_blocks))\n            if stage_idx &lt; num_stages - 1:\n                embeds.append(\n                    conv_bn_relu(in_channels=channels[stage_idx], out_channels=channels[stage_idx + 1], kernel_size=2,\n                                 stride=2, padding=0))\n        self.stages = nn.CellList(stages)\n        self.embeds = nn.CellList(embeds)\n        self.head_norm = nn.BatchNorm2d(channels[-1]).set_train()\n        self.head = nn.Dense(channels[-1], num_class)\n\n        self.use_checkpoint = use_checkpoint\n        self.shape = ops.Shape()\n        self.reshape = ops.Reshape()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                k = cell.group / (cell.in_channels * cell.kernel_size[0] * cell.kernel_size[1])\n                k = k**0.5\n                cell.weight.set_data(init.initializer(init.Uniform(k), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(k), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                k = 1 / cell.in_channels\n                k = k**0.5\n                cell.weight.set_data(init.initializer(init.Uniform(k), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(k), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_embedding(x)\n\n        for i, stage in enumerate(self.stages):\n            for block in stage:\n                x = block(x)\n\n            if i &lt; len(self.stages) - 1:\n                embed = self.embeds[i]\n                x = embed(x)\n        x = self.head_norm(x)\n        shape = self.shape(x)\n        pool = nn.AvgPool2d(kernel_size=(shape[2], shape[3]))\n        x = pool(x)\n        return x.view(shape[0], -1)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNetUnit</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic unit of RepMLPNet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>class RepMLPNetUnit(nn.Cell):\n\"\"\"Basic unit of RepMLPNet\"\"\"\n\n    def __init__(self, channels, h, w, reparam_conv_k, globalperceptron_reduce, ffn_expand=4,\n                 num_sharesets=1, deploy=False):\n        super().__init__()\n        self.repmlp_block = RepMLPBlock(in_channels=channels, out_channels=channels, h=h, w=w,\n                                        reparam_conv_k=reparam_conv_k, globalperceptron_reduce=globalperceptron_reduce,\n                                        num_sharesets=num_sharesets, deploy=deploy)\n        self.ffn_block = FFNBlock(channels, channels * ffn_expand)\n        self.prebn1 = nn.BatchNorm2d(channels).set_train()\n        self.prebn2 = nn.BatchNorm2d(channels).set_train()\n\n    def construct(self, x):\n        y = x + self.repmlp_block(self.prebn1(x))\n        # print(y)\n        z = y + self.ffn_block(self.prebn2(y))\n        return z\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_B224(pretrained=False, image_size=224, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_B224 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_B224(pretrained: bool = False, image_size: int = 224, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_B224 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_B224\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(56, 28, 14, 7),\n                      ws=(56, 28, 14, 7),\n                      num_blocks=(2, 2, 12, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_B256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_B256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_B256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_B256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_B256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 12, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_D256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_D256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_D256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_D256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_D256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(80, 160, 320, 640), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 18, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_L256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_L256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_L256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_L256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_L256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 18, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 256),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_T224(pretrained=False, image_size=224, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_T224 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_T224(pretrained: bool = False, image_size: int = 224, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_T224 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_T224\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(64, 128, 256, 512), hs=(56, 28, 14, 7),\n                      ws=(56, 28, 14, 7),\n                      num_blocks=(2, 2, 6, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repmlp.RepMLPNet_T256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code> \u00b6 <p>Get RepMLPNet_T256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repmlp.py</code> <pre><code>@register_model\ndef RepMLPNet_T256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                   deploy=False, **kwargs):\n\"\"\"Get RepMLPNet_T256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"RepMLPNet_T256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(64, 128, 256, 512), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 6, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg</code> \u00b6 <p>MindSpore implementation of <code>RepVGG</code>. Refer to RepVGG: Making VGG_style ConvNets Great Again</p> <code>mindocr.models.backbones.mindcv_models.repvgg.RepVGG</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>RepVGG model class, based on <code>\"RepVGGBlock: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/pdf/2101.03697&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_blocks</code> <p>number of RepVGGBlocks</p> <p> TYPE: <code>list) </code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>in_channels) </code> DEFAULT: <code>3</code> </p> <code>width_multiplier</code> <p>the numbers of MLP Architecture.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>None</code> </p> <code>override_group_map</code> <p>the numbers of MLP Architecture.</p> <p> TYPE: <code>dict) </code> DEFAULT: <code>None</code> </p> <code>deploy</code> <p>use rbr_reparam block or not. Default: False</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>use_se</code> <p>use se_block or not. Default: False</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>class RepVGG(nn.Cell):\nr\"\"\"RepVGG model class, based on\n    `\"RepVGGBlock: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/pdf/2101.03697&gt;`_\n\n    Args:\n        num_blocks (list) : number of RepVGGBlocks\n        num_classes (int) : number of classification classes. Default: 1000.\n        in_channels (in_channels) : number the channels of the input. Default: 3.\n        width_multiplier (list) : the numbers of MLP Architecture.\n        override_group_map (dict) : the numbers of MLP Architecture.\n        deploy (bool) : use rbr_reparam block or not. Default: False\n        use_se (bool) : use se_block or not. Default: False\n    \"\"\"\n\n    def __init__(self, num_blocks, num_classes=1000, in_channels=3, width_multiplier=None, override_group_map=None,\n                 deploy=False, use_se=False):\n        super().__init__()\n\n        assert len(width_multiplier) == 4\n\n        self.deploy = deploy\n        self.override_group_map = override_group_map or {}\n        self.use_se = use_se\n\n        assert 0 not in self.override_group_map\n\n        self.in_planes = min(64, int(64 * width_multiplier[0]))\n\n        self.stage0 = RepVGGBlock(in_channels=in_channels, out_channels=self.in_planes, kernel_size=3, stride=2,\n                                  padding=1,\n                                  deploy=self.deploy, use_se=self.use_se)\n        self.cur_layer_idx = 1\n        self.stage1 = self._make_stage(\n            int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n        self.stage2 = self._make_stage(\n            int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n        self.stage3 = self._make_stage(\n            int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n        self.stage4 = self._make_stage(\n            int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n        self.gap = GlobalAvgPooling()\n        self.linear = nn.Dense(int(512 * width_multiplier[3]), num_classes)\n        self._initialize_weights()\n\n    def _make_stage(self, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        blocks = []\n        for s in strides:\n            cur_group = self.override_group_map.get(self.cur_layer_idx, 1)\n            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n                                      stride=s, padding=1, group=cur_group, deploy=self.deploy,\n                                      use_se=self.use_se))\n            self.in_planes = planes\n            self.cur_layer_idx += 1\n        return nn.SequentialCell(blocks)\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode='fan_out', nonlinearity='relu'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer('ones', cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer('zeros', cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode='fan_in', nonlinearity='sigmoid'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n\n    def construct(self, x):\n        x = self.stage0(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.gap(x)\n        x = self.linear(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.RepVGGBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic Block of RepVGG</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>class RepVGGBlock(nn.Cell):\n\"\"\"Basic Block of RepVGG\"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n                 stride: int = 1, padding: int = 0, dilation: int = 1,\n                 group: int = 1, padding_mode: str = \"zeros\",\n                 deploy: bool = False, use_se: bool = False) -&gt; None:\n        super().__init__()\n        self.deploy = deploy\n        self.group = group\n        self.in_channels = in_channels\n\n        assert kernel_size == 3\n        assert padding == 1\n\n        padding_11 = padding - kernel_size // 2\n\n        self.nonlinearity = nn.ReLU()\n\n        if use_se:\n            self.se = SqueezeExcite(\n                in_channels=out_channels, rd_channels=out_channels // 16)\n        else:\n            self.se = Identity()\n\n        if deploy:\n            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                         stride=stride, padding=padding, dilation=dilation, group=group, has_bias=True,\n                                         pad_mode=padding_mode)\n        else:\n            self.rbr_reparam = None\n            self.rbr_identity = nn.BatchNorm2d(\n                num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n\n            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, group=group)\n            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride,\n                                   padding=padding_11, group=group)\n\n    def construct(self, inputs: Tensor) -&gt; Tensor:\n        if self.rbr_reparam is not None:\n            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n\n        if self.rbr_identity is None:\n            id_out = 0\n        else:\n            id_out = self.rbr_identity(inputs)\n\n        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n\n    def get_custom_l2(self):\n\"\"\"This may improve the accuracy and facilitates quantization in some cases.\"\"\"\n        k3 = self.rbr_dense.conv.weight\n        k1 = self.rbr_1x1.conv.weight\n\n        t3 = self.rbr_dense.bn.weight / (\n            ops.sqrt((self.rbr_dense.bn.moving_variance + self.rbr_dense.bn.eps)))\n        t3 = ops.reshape(t3, (-1, 1, 1, 1))\n\n        t1 = (self.rbr_1x1.bn.weight /\n              ((self.rbr_1x1.bn.moving_variance + self.rbr_1x1.bn.eps).sqrt()))\n        t1 = ops.reshape(t1, (-1, 1, 1, 1))\n\n        l2_loss_circle = ops.reduce_sum(k3**2) - ops.reduce_sum(k3[:, :, 1:2, 1:2] ** 2)\n        eq_kernel = k3[:, :, 1:2, 1:2] * t3 + k1 * t1\n        l2_loss_eq_kernel = ops.reduce_sum(eq_kernel**2 / (t3**2 + t1**2))\n        return l2_loss_eq_kernel + l2_loss_circle\n\n    #   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n    #   You can get the equivalent kernel and bias at any time and do whatever you want,\n    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n    #   May be useful for quantization or pruning.\n    def get_equivalent_kernel_bias(self):\n        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n\n    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n        if kernel1x1 is None:\n            return 0\n        return ops.pad(kernel1x1, ((1, 1), (1, 1)))\n\n    def _fuse_bn_tensor(self, branch):\n        if branch is None:\n            return 0, 0\n        if isinstance(branch, nn.SequentialCell):\n            kernel = branch.conv.weight\n            moving_mean = branch.bn.moving_mean\n            moving_variance = branch.bn.moving_variance\n            gamma = branch.bn.gamma\n            beta = branch.bn.beta\n            eps = branch.bn.eps\n        else:\n            assert isinstance(branch, (nn.BatchNorm2d, nn.SyncBatchNorm))\n            if not hasattr(self, \"id_tensor\"):\n                input_dim = self.in_channels // self.group\n                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n                for i in range(self.in_channels):\n                    kernel_value[i, i % input_dim, 1, 1] = 1\n                self.id_tensor = Tensor(kernel_value, dtype=branch.weight.dtype)\n            kernel = self.id_tensor\n            moving_mean = branch.moving_mean\n            moving_variance = branch.moving_variance\n            gamma = branch.gamma\n            beta = branch.beta\n            eps = branch.eps\n        std = ops.sqrt(moving_variance + eps)\n        t = ops.reshape(gamma / std, (-1, 1, 1, 1))\n        return kernel * t, beta - moving_mean * gamma / std\n\n    def switch_to_deploy(self):\n\"\"\"Model_convert\"\"\"\n        if self.rbr_reparam is not None:\n            return\n        kernel, bias = self.get_equivalent_kernel_bias()\n        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels,\n                                     out_channels=self.rbr_dense.conv.out_channels,\n                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation,\n                                     group=self.rbr_dense.conv.group, has_bias=True, pad_mode=\"pad\")\n        self.rbr_reparam.weight.data = kernel\n        self.rbr_reparam.bias.data = bias\n        for para in self.parameters():\n            para.detach_()\n        self.__delattr__(\"rbr_dense\")\n        self.__delattr__(\"rbr_1x1\")\n        if hasattr(self, \"rbr_identity\"):\n            self.__delattr__(\"rbr_identity\")\n        if hasattr(self, \"id_tensor\"):\n            self.__delattr__(\"id_tensor\")\n        self.deploy = True\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.RepVGGBlock.get_custom_l2()</code> \u00b6 <p>This may improve the accuracy and facilitates quantization in some cases.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>def get_custom_l2(self):\n\"\"\"This may improve the accuracy and facilitates quantization in some cases.\"\"\"\n    k3 = self.rbr_dense.conv.weight\n    k1 = self.rbr_1x1.conv.weight\n\n    t3 = self.rbr_dense.bn.weight / (\n        ops.sqrt((self.rbr_dense.bn.moving_variance + self.rbr_dense.bn.eps)))\n    t3 = ops.reshape(t3, (-1, 1, 1, 1))\n\n    t1 = (self.rbr_1x1.bn.weight /\n          ((self.rbr_1x1.bn.moving_variance + self.rbr_1x1.bn.eps).sqrt()))\n    t1 = ops.reshape(t1, (-1, 1, 1, 1))\n\n    l2_loss_circle = ops.reduce_sum(k3**2) - ops.reduce_sum(k3[:, :, 1:2, 1:2] ** 2)\n    eq_kernel = k3[:, :, 1:2, 1:2] * t3 + k1 * t1\n    l2_loss_eq_kernel = ops.reduce_sum(eq_kernel**2 / (t3**2 + t1**2))\n    return l2_loss_eq_kernel + l2_loss_circle\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.RepVGGBlock.switch_to_deploy()</code> \u00b6 <p>Model_convert</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>def switch_to_deploy(self):\n\"\"\"Model_convert\"\"\"\n    if self.rbr_reparam is not None:\n        return\n    kernel, bias = self.get_equivalent_kernel_bias()\n    self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels,\n                                 out_channels=self.rbr_dense.conv.out_channels,\n                                 kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n                                 padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation,\n                                 group=self.rbr_dense.conv.group, has_bias=True, pad_mode=\"pad\")\n    self.rbr_reparam.weight.data = kernel\n    self.rbr_reparam.bias.data = bias\n    for para in self.parameters():\n        para.detach_()\n    self.__delattr__(\"rbr_dense\")\n    self.__delattr__(\"rbr_1x1\")\n    if hasattr(self, \"rbr_identity\"):\n        self.__delattr__(\"rbr_identity\")\n    if hasattr(self, \"id_tensor\"):\n        self.__delattr__(\"id_tensor\")\n    self.deploy = True\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_a0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[0.75, 0.75, 0.75, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[0.75, 0.75, 0.75, 2.5].\n    Refer to the base class `models.RepVGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a0\"]\n    model = RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[0.75, 0.75, 0.75, 2.5], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_a1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a1\"]\n    model = RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[1.0, 1.0, 1.0, 2.5], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_a2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.5, 1.5, 1.5, 2.75]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.5, 1.5, 1.5, 2.75].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a2\"]\n    model = RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[1.5, 1.5, 1.5, 2.75], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b0']\n    model = RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[1.0, 1.0, 1.0, 2.5], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b1']\n    model = RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[2.0, 2.0, 2.0, 4.0], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b2']\n    model = RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[2.5, 2.5, 2.5, 5.0], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[3.0, 3.0, 3.0, 5.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n\"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[3.0, 3.0, 3.0, 5.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b3']\n    model = RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                   width_multiplier=[3.0, 3.0, 3.0, 5.0], override_group_map=None, deploy=False, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.repvgg.repvgg_model_convert(model, save_path=None, do_copy=True)</code> \u00b6 <p>repvgg_model_convert</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\repvgg.py</code> <pre><code>def repvgg_model_convert(model: nn.Cell, save_path=None, do_copy=True):\n\"\"\"repvgg_model_convert\"\"\"\n    if do_copy:\n        model = copy.deepcopy(model)\n    for module in model.modules():\n        if hasattr(module, \"switch_to_deploy\"):\n            module.switch_to_deploy()\n    if save_path is not None:\n        save_checkpoint(model.parameters_and_names(), save_path)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.res2net</code> \u00b6 <p>MindSpore implementation of <code>Res2Net</code>. Refer to Res2Net: A New Multi-scale Backbone Architecture.</p> <code>mindocr.models.backbones.mindcv_models.res2net.Res2Net</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Res2Net model class, based on <code>\"Res2Net: A New Multi-scale Backbone Architecture\" &lt;https://arxiv.org/abs/1904.01169&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of resnet.</p> <p> TYPE: <code>Type[nn.Cell]</code> </p> <code>layer_nums</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>version</code> <p>variety of Res2Net, 'res2net' or 'res2net_v1b'. Default: 'res2net'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'res2net'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 26.</p> <p> TYPE: <code>int</code> DEFAULT: <code>26</code> </p> <code>scale</code> <p>scale factor of Bottle2neck. Default: 4.</p> <p> DEFAULT: <code>4</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[nn.Cell]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\res2net.py</code> <pre><code>class Res2Net(nn.Cell):\nr\"\"\"Res2Net model class, based on\n    `\"Res2Net: A New Multi-scale Backbone Architecture\" &lt;https://arxiv.org/abs/1904.01169&gt;`_\n\n    Args:\n        block: block of resnet.\n        layer_nums: number of layers of each stage.\n        version: variety of Res2Net, 'res2net' or 'res2net_v1b'. Default: 'res2net'.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 26.\n        scale: scale factor of Bottle2neck. Default: 4.\n        norm: normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[nn.Cell],\n        layer_nums: List[int],\n        version: str = \"res2net\",\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 26,\n        scale=4,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        assert version in [\"res2net\", \"res2net_v1b\"]\n        self.version = version\n\n        if norm is None:\n            norm = nn.BatchNorm2d\n        self.norm = norm\n\n        self.num_classes = num_classes\n        self.input_channels = 64\n        self.groups = groups\n        self.base_width = base_width\n        self.scale = scale\n        if self.version == \"res2net\":\n            self.conv1 = nn.Conv2d(in_channels, self.input_channels, kernel_size=7,\n                                   stride=2, padding=3, pad_mode=\"pad\")\n        elif self.version == \"res2net_v1b\":\n            self.conv1 = nn.SequentialCell([\n                nn.Conv2d(in_channels, self.input_channels // 2, kernel_size=3,\n                          stride=2, padding=1, pad_mode=\"pad\"),\n                norm(self.input_channels // 2),\n                nn.ReLU(),\n                nn.Conv2d(self.input_channels // 2, self.input_channels // 2, kernel_size=3,\n                          stride=1, padding=1, pad_mode=\"pad\"),\n                norm(self.input_channels // 2),\n                nn.ReLU(),\n                nn.Conv2d(self.input_channels // 2, self.input_channels, kernel_size=3,\n                          stride=1, padding=1, pad_mode=\"pad\"),\n            ])\n\n        self.bn1 = norm(self.input_channels)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.SequentialCell([\n            nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"CONSTANT\"),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ])\n        self.layer1 = self._make_layer(block, 64, layer_nums[0])\n        self.layer2 = self._make_layer(block, 128, layer_nums[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layer_nums[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layer_nums[3], stride=2)\n\n        self.pool = GlobalAvgPooling()\n        self.num_features = 512 * block.expansion\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[nn.Cell],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            if stride == 1 or self.version == \"res2net\":\n                down_sample = nn.SequentialCell([\n                    nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                    self.norm(channels * block.expansion)\n                ])\n            else:\n                down_sample = nn.SequentialCell([\n                    nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"same\"),\n                    nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=1),\n                    self.norm(channels * block.expansion)\n                ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_width,\n                scale=self.scale,\n                stype=\"stage\",\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    scale=self.scale,\n                    norm=self.norm,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.res2net.res2net101(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\res2net.py</code> <pre><code>@register_model\ndef res2net101(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n\"\"\"Get 101 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net101\"]\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.res2net.res2net152(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code> \u00b6 <p>Get 152 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\res2net.py</code> <pre><code>@register_model\ndef res2net152(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n\"\"\"Get 152 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net152\"]\n    model = Res2Net(Bottle2neck, [3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.res2net.res2net50(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\res2net.py</code> <pre><code>@register_model\ndef res2net50(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n\"\"\"Get 50 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net50\"]\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnest</code> \u00b6 <p>MindSpore implementation of <code>ResNeSt</code>. Refer to ResNeSt: Split-Attention Networks.</p> <code>mindocr.models.backbones.mindcv_models.resnest.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ResNeSt Bottleneck</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnest.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"ResNeSt Bottleneck\"\"\"\n\n    expansion = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride=1,\n        downsample: Optional[nn.SequentialCell] = None,\n        radix: int = 1,\n        cardinality: int = 1,\n        bottleneck_width: int = 64,\n        avd: bool = False,\n        avd_first: bool = False,\n        dilation: int = 1,\n        is_first: bool = False,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, has_bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride &gt; 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, pad_mode=\"same\")\n            stride = 1\n\n        if radix &gt;= 1:\n            self.conv2 = SplitAttn(group_width, group_width, kernel_size=3, stride=stride,\n                                   padding=dilation, dilation=dilation, group=cardinality,\n                                   bias=False, radix=radix, norm_layer=norm_layer)\n        else:\n            self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride,\n                                   pad_mode=\"pad\", padding=dilation, dilation=dilation,\n                                   group=cardinality, has_bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(group_width, planes * 4, kernel_size=1, has_bias=False)\n        self.bn3 = norm_layer(planes * 4)\n\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnest.ResNeSt</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ResNeSt model class, based on <code>\"ResNeSt: Split-Attention Networks\" &lt;https://arxiv.org/abs/2004.08955&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>Class for the residual block. Option is Bottleneck.</p> <p> TYPE: <code>Type[Bottleneck]</code> </p> <code>layers</code> <p>Numbers of layers in each block.</p> <p> TYPE: <code>List[int]</code> </p> <code>radix</code> <p>Number of groups for Split-Attention conv. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>group</code> <p>Number of groups for the conv in each bottleneck block. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>bottleneck_width</code> <p>bottleneck channels factor. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>num_classes</code> <p>Number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>dilated</code> <p>Applying dilation strategy to pretrained ResNeSt yielding a stride-8 model,      typically used in Semantic Segmentation. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dilation</code> <p>Number of dilation in the conv. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>deep_stem</code> <p>three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2.        Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stem_width</code> <p>number of channels in stem convolutions. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>avg_down</code> <p>use avg pooling for projection skip connection between stages/downsample.       Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>avd</code> <p>use avg pooling before or after split-attention conv. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>avd_first</code> <p>use avg pooling before or after split-attention conv. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>drop_rate</code> <p>Drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>norm_layer</code> <p>Normalization layer used in backbone network. Default: nn.BatchNorm2d.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.BatchNorm2d</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnest.py</code> <pre><code>class ResNeSt(nn.Cell):\nr\"\"\"ResNeSt model class, based on\n    `\"ResNeSt: Split-Attention Networks\" &lt;https://arxiv.org/abs/2004.08955&gt;`_\n\n    Args:\n        block: Class for the residual block. Option is Bottleneck.\n        layers: Numbers of layers in each block.\n        radix: Number of groups for Split-Attention conv. Default: 1.\n        group: Number of groups for the conv in each bottleneck block. Default: 1.\n        bottleneck_width: bottleneck channels factor. Default: 64.\n        num_classes: Number of classification classes. Default: 1000.\n        dilated: Applying dilation strategy to pretrained ResNeSt yielding a stride-8 model,\n                 typically used in Semantic Segmentation. Default: False.\n        dilation: Number of dilation in the conv. Default: 1.\n        deep_stem: three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2.\n                   Default: False.\n        stem_width: number of channels in stem convolutions. Default: 64.\n        avg_down: use avg pooling for projection skip connection between stages/downsample.\n                  Default: False.\n        avd: use avg pooling before or after split-attention conv. Default: False.\n        avd_first: use avg pooling before or after split-attention conv. Default: False.\n        drop_rate: Drop probability for the Dropout layer. Default: 0.\n        norm_layer: Normalization layer used in backbone network. Default: nn.BatchNorm2d.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Bottleneck],\n        layers: List[int],\n        radix: int = 1,\n        group: int = 1,\n        bottleneck_width: int = 64,\n        num_classes: int = 1000,\n        dilated: bool = False,\n        dilation: int = 1,\n        deep_stem: bool = False,\n        stem_width: int = 64,\n        avg_down: bool = False,\n        avd: bool = False,\n        avd_first: bool = False,\n        drop_rate: float = 0.0,\n        norm_layer: nn.Cell = nn.BatchNorm2d,\n    ) -&gt; None:\n        super(ResNeSt, self).__init__()\n        self.cardinality = group\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width * 2 if deep_stem else 64\n        self.avg_down = avg_down\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        if deep_stem:\n            self.conv1 = nn.SequentialCell([\n                nn.Conv2d(3, stem_width, kernel_size=3, stride=2, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(),\n                nn.Conv2d(stem_width, stem_width, kernel_size=3, stride=1, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(),\n                nn.Conv2d(stem_width, stem_width * 2, kernel_size=3, stride=1, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n            ])\n        else:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3,\n                                   has_bias=False)\n\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer)\n        elif dilation == 2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation=1, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = GlobalAvgPooling()\n        self.drop = nn.Dropout(keep_prob=1.0 - drop_rate) if drop_rate &gt; 0.0 else None\n        self.fc = nn.Dense(512 * block.expansion, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(\n                        init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"), cell.weight.shape, cell.weight.dtype\n                    )\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(\n                        init.HeUniform(mode=\"fan_in\", nonlinearity=\"sigmoid\"), cell.weight.shape, cell.weight.dtype\n                    )\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[Bottleneck],\n        planes: int,\n        blocks: int,\n        stride: int = 1,\n        dilation: int = 1,\n        norm_layer: Optional[nn.Cell] = None,\n        is_first: bool = True,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"valid\"))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1, pad_mode=\"valid\"))\n\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1,\n                                             stride=1, has_bias=False))\n            else:\n                down_layers.append(\n                    nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride,\n                              has_bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.SequentialCell(down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=1,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                )\n            )\n        elif dilation == 4:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=2,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                )\n            )\n        else:\n            raise ValueError(f\"Unsupported model type {dilation}\")\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.avgpool(x)\n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnest.SplitAttn</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Split-Attention Conv2d</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnest.py</code> <pre><code>class SplitAttn(nn.Cell):\n\"\"\"Split-Attention Conv2d\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        group: int = 1,\n        bias: bool = False,\n        radix: int = 2,\n        rd_ratio: float = 0.25,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        act_layer: nn.Cell = nn.ReLU,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super(SplitAttn, self).__init__()\n        out_channels = out_channels or in_channels\n        self.radix = radix\n        mid_chs = out_channels * radix\n\n        if rd_channels is None:\n            attn_chs = make_divisible(in_channels * radix * rd_ratio, min_value=32, divisor=rd_divisor)\n        else:\n            attn_chs = rd_channels * radix\n\n        padding = kernel_size // 2 if padding is None else padding\n\n        self.conv = nn.Conv2d(in_channels, mid_chs, kernel_size=kernel_size, stride=stride,\n                              pad_mode=\"pad\", padding=padding, dilation=dilation,\n                              group=group * radix, has_bias=bias)\n        self.bn0 = norm_layer(mid_chs) if norm_layer else Identity()\n        self.act0 = act_layer()\n        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, group=group, has_bias=True)\n        self.bn1 = norm_layer(attn_chs) if norm_layer else nn.Identity()\n        self.act1 = act_layer()\n        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, group=group, has_bias=True)\n        self.rsoftmax = RadixSoftmax(radix, group)\n        self.pool = GlobalAvgPooling(keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        x = self.bn0(x)\n        x = self.act0(x)\n\n        B, RC, H, W = x.shape\n        if self.radix &gt; 1:\n            x = ops.reshape(x, (B, self.radix, RC // self.radix, H, W))\n            x_gap = x.sum(axis=1)\n        else:\n            x_gap = x\n        x_gap = self.pool(x_gap)\n        x_gap = self.fc1(x_gap)\n        x_gap = self.bn1(x_gap)\n        x_gap = self.act1(x_gap)\n        x_attn = self.fc2(x_gap)\n\n        x_attn = self.rsoftmax(x_attn)\n        x_attn = ops.reshape(x_attn, (B, -1, 1, 1))\n        if self.radix &gt; 1:\n            out = x * ops.reshape(x_attn, (B, self.radix, RC // self.radix, 1, 1))\n            out = out.sum(axis=1)\n        else:\n            out = x * x_attn\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet</code> \u00b6 <p>MindSpore implementation of <code>ResNet</code>. Refer to Deep Residual Learning for Image Recognition.</p> <code>mindocr.models.backbones.mindcv_models.resnet.BasicBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the basic block of resnet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>class BasicBlock(nn.Cell):\n\"\"\"define the basic block of resnet\"\"\"\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n        assert groups == 1, \"BasicBlock only supports groups=1\"\n        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3,\n                               stride=stride, padding=1, pad_mode=\"pad\")\n        self.bn1 = norm(channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,\n                               stride=1, padding=1, pad_mode=\"pad\")\n        self.bn2 = norm(channels)\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Bottleneck here places the stride for downsampling at 3x3 convolution(self.conv2) as torchvision does, while original implementation places the stride at the first 1x1 convolution(self.conv1)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"\n    Bottleneck here places the stride for downsampling at 3x3 convolution(self.conv2) as torchvision does,\n    while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    \"\"\"\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        width = int(channels * (base_width / 64.0)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n        self.bn1 = norm(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, pad_mode=\"pad\", group=groups)\n        self.bn2 = norm(width)\n        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n                               kernel_size=1, stride=1)\n        self.bn3 = norm(channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.ResNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ResNet model class, based on <code>\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/abs/1512.03385&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of resnet.</p> <p> TYPE: <code>Type[Union[BasicBlock, Bottleneck]]</code> </p> <code>layers</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[nn.Cell]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>class ResNet(nn.Cell):\nr\"\"\"ResNet model class, based on\n    `\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/abs/1512.03385&gt;`_\n\n    Args:\n        block: block of resnet.\n        layers: number of layers of each stage.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 64.\n        norm: normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        self.norm: nn.Cell = norm  # add type hints to make pylint happy\n        self.input_channels = 64\n        self.groups = groups\n        self.base_with = base_width\n\n        self.conv1 = nn.Conv2d(in_channels, self.input_channels, kernel_size=7,\n                               stride=2, pad_mode=\"pad\", padding=3)\n        self.bn1 = norm(self.input_channels)\n        self.relu = nn.ReLU()\n        self.feature_info = [dict(chs=self.input_channels, reduction=2, name=\"relu\")]\n\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.layer1 = self._make_layer(block, 64, layers[0], name=\"layer1\", reduction=4)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, name=\"layer2\", reduction=8)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, name=\"layer3\", reduction=16)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, name=\"layer4\", reduction=32)\n\n        self.pool = GlobalAvgPooling()\n        self.num_features = 512 * block.expansion\n        self.classifier = nn.Dense(self.num_features, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode='fan_out', nonlinearity='relu'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer('ones', cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer('zeros', cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode='fan_in', nonlinearity='sigmoid'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n        name: str = \"\",\n        reduction: int = 1,\n    ) -&gt; nn.SequentialCell:\n\"\"\"build model depending on cfgs\"\"\"\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                self.norm(channels * block.expansion)\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm\n                )\n            )\n\n        self.feature_info.append(dict(chs=self.input_channels, reduction=reduction, name=name))\n\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.ResNet.forward_features(x)</code> \u00b6 <p>Network forward feature extraction.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n\"\"\"Network forward feature extraction.\"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.max_pool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnet101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 101 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet101\"]\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnet152(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 152 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnet152(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 152 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet152\"]\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnet18(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 18 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 18 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet18\"]\n    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnet34(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 34 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 34 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet34\"]\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 50 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet50\"]\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnext101_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers ResNeXt model with 32 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnext101_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 101 layers ResNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext101_32x4d\"]\n    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=32, base_width=4, num_classes=num_classes,\n                   in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnext101_64x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers ResNeXt model with 64 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnext101_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 101 layers ResNeXt model with 64 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext101_64x4d\"]\n    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=64, base_width=4, num_classes=num_classes,\n                   in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnet.resnext50_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers ResNeXt model with 32 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnet.py</code> <pre><code>@register_model\ndef resnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 50 layers ResNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext50_32x4d\"]\n    model = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, base_width=4, num_classes=num_classes,\n                   in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnetv2</code> \u00b6 <p>MindSpore implementation of <code>ResNetV2</code>. Refer to Identity Mappings in Deep Residual Networks.</p> <code>mindocr.models.backbones.mindcv_models.resnetv2.resnetv2_101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 101 layers ResNetV2 model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnetv2.py</code> <pre><code>@register_model\ndef resnetv2_101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 101 layers ResNetV2 model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnetv2_101\"]\n    model = ResNet(PreActBottleneck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.resnetv2.resnetv2_50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers ResNetV2 model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\resnetv2.py</code> <pre><code>@register_model\ndef resnetv2_50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n\"\"\"Get 50 layers ResNetV2 model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs['resnetv2_50']\n    model = ResNet(PreActBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet</code> \u00b6 <p>MindSpore implementation of <code>ReXNet</code>. Refer to ReXNet: Rethinking Channel Dimensions for Efficient Model Design.</p> <code>mindocr.models.backbones.mindcv_models.rexnet.LinearBottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>LinearBottleneck</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>class LinearBottleneck(nn.Cell):\n\"\"\"LinearBottleneck\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        exp_ratio,\n        stride,\n        use_se=True,\n        se_ratio=1 / 12,\n        ch_div=1,\n        act_layer=nn.SiLU,\n        dw_act_layer=nn.ReLU6,\n        drop_path=None,\n        **kwargs,\n    ):\n        super(LinearBottleneck, self).__init__(**kwargs)\n        self.use_shortcut = stride == 1 and in_channels &lt;= out_channels\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if exp_ratio != 1:\n            dw_channels = in_channels * exp_ratio\n            self.conv_exp = Conv2dNormActivation(in_channels, dw_channels, 1, activation=act_layer)\n        else:\n            dw_channels = in_channels\n            self.conv_exp = None\n\n        self.conv_dw = Conv2dNormActivation(dw_channels, dw_channels, 3, stride, padding=1,\n                                            groups=dw_channels, activation=None)\n\n        if use_se:\n            self.se = SqueezeExcite(dw_channels,\n                                    rd_channels=make_divisible(int(dw_channels * se_ratio), ch_div),\n                                    norm=nn.BatchNorm2d)\n        else:\n            self.se = None\n        self.act_dw = dw_act_layer()\n\n        self.conv_pwl = Conv2dNormActivation(dw_channels, out_channels, 1, padding=0, activation=None)\n        self.drop_path = drop_path\n\n    def construct(self, x):\n        shortcut = x\n        if self.conv_exp is not None:\n            x = self.conv_exp(x)\n        x = self.conv_dw(x)\n        if self.se is not None:\n            x = self.se(x)\n        x = self.act_dw(x)\n        x = self.conv_pwl(x)\n        if self.use_shortcut:\n            if self.drop_path is not None:\n                x = self.drop_path(x)\n            x[:, 0:self.in_channels] += shortcut\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.ReXNetV1</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ReXNet model class, based on <code>\"Rethinking Channel Dimensions for Efficient Model Design\" &lt;https://arxiv.org/abs/2007.00992&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of the input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>fi_channels</code> <p>number of the final channels. Default: 180.</p> <p> TYPE: <code>int</code> DEFAULT: <code>180</code> </p> <code>initial_channels</code> <p>initialize inplanes. Default: 16.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>width_mult</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_mult</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>use_se</code> <p>use SENet in LinearBottleneck. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>se_ratio</code> <p>(float): SENet reduction ratio. Default 1/12.</p> <p> DEFAULT: <code>1 / 12</code> </p> <code>drop_rate</code> <p>dropout ratio. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>ch_div</code> <p>divisible by ch_div. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>act_layer</code> <p>activation function in ConvNormAct. Default: nn.SiLU.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.SiLU</code> </p> <code>dw_act_layer</code> <p>activation function after dw_conv. Default: nn.ReLU6.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.ReLU6</code> </p> <code>cls_useconv</code> <p>use conv in classification. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>class ReXNetV1(nn.Cell):\nr\"\"\"ReXNet model class, based on\n    `\"Rethinking Channel Dimensions for Efficient Model Design\" &lt;https://arxiv.org/abs/2007.00992&gt;`_\n\n    Args:\n        in_channels (int): number of the input channels. Default: 3.\n        fi_channels (int): number of the final channels. Default: 180.\n        initial_channels (int): initialize inplanes. Default: 16.\n        width_mult (float): The ratio of the channel. Default: 1.0.\n        depth_mult (float): The ratio of num_layers. Default: 1.0.\n        num_classes (int) : number of classification classes. Default: 1000.\n        use_se (bool): use SENet in LinearBottleneck. Default: True.\n        se_ratio: (float): SENet reduction ratio. Default 1/12.\n        drop_rate (float): dropout ratio. Default: 0.2.\n        ch_div (int): divisible by ch_div. Default: 1.\n        act_layer (nn.Cell): activation function in ConvNormAct. Default: nn.SiLU.\n        dw_act_layer (nn.Cell): activation function after dw_conv. Default: nn.ReLU6.\n        cls_useconv (bool): use conv in classification. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=3,\n        fi_channels=180,\n        initial_channels=16,\n        width_mult=1.0,\n        depth_mult=1.0,\n        num_classes=1000,\n        use_se=True,\n        se_ratio=1 / 12,\n        drop_rate=0.2,\n        drop_path_rate=0.0,\n        ch_div=1,\n        act_layer=nn.SiLU,\n        dw_act_layer=nn.ReLU6,\n        cls_useconv=False,\n    ):\n        super(ReXNetV1, self).__init__()\n\n        layers = [1, 2, 2, 3, 3, 5]\n        strides = [1, 2, 2, 2, 1, 2]\n        use_ses = [False, False, True, True, True, True]\n\n        layers = [ceil(element * depth_mult) for element in layers]\n        strides = sum([[element] + [1] * (layers[idx] - 1)\n                       for idx, element in enumerate(strides)], [])\n        if use_se:\n            use_ses = sum([[element] * layers[idx] for idx, element in enumerate(use_ses)], [])\n        else:\n            use_ses = [False] * sum(layers[:])\n        exp_ratios = [1] * layers[0] + [6] * sum(layers[1:])\n\n        self.depth = sum(layers[:]) * 3\n        stem_channel = 32 / width_mult if width_mult &lt; 1.0 else 32\n        inplanes = initial_channels / width_mult if width_mult &lt; 1.0 else initial_channels\n\n        features = []\n        in_channels_group = []\n        out_channels_group = []\n\n        for i in range(self.depth // 3):\n            if i == 0:\n                in_channels_group.append(int(round(stem_channel * width_mult)))\n                out_channels_group.append(int(round(inplanes * width_mult)))\n            else:\n                in_channels_group.append(int(round(inplanes * width_mult)))\n                inplanes += fi_channels / (self.depth // 3 * 1.0)\n                out_channels_group.append(int(round(inplanes * width_mult)))\n\n        stem_chs = make_divisible(round(stem_channel * width_mult), divisor=ch_div)\n        self.stem = Conv2dNormActivation(in_channels, stem_chs, stride=2, padding=1, activation=act_layer)\n\n        feat_chs = [stem_chs]\n        feature_info = []\n        curr_stride = 2\n        features = []\n        num_blocks = len(in_channels_group)\n        for block_idx, (in_c, out_c, exp_ratio, stride, use_se) in enumerate(\n            zip(in_channels_group, out_channels_group, exp_ratios, strides, use_ses)\n        ):\n            if stride &gt; 1:\n                fname = \"stem\" if block_idx == 0 else f\"features.{block_idx - 1}\"\n                feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=fname)]\n                curr_stride *= stride\n            block_dpr = drop_path_rate * block_idx / (num_blocks - 1)  # stochastic depth linear decay rule\n            drop_path = DropPath(block_dpr) if block_dpr &gt; 0. else None\n            features.append(LinearBottleneck(in_channels=in_c,\n                                             out_channels=out_c,\n                                             exp_ratio=exp_ratio,\n                                             stride=stride,\n                                             use_se=use_se,\n                                             se_ratio=se_ratio,\n                                             act_layer=act_layer,\n                                             dw_act_layer=dw_act_layer,\n                                             drop_path=drop_path))\n\n        pen_channels = make_divisible(int(1280 * width_mult), divisor=ch_div)\n        features.append(Conv2dNormActivation(out_channels_group[-1],\n                                             pen_channels,\n                                             kernel_size=1,\n                                             activation=act_layer))\n\n        features.append(GlobalAvgPooling(keep_dims=True))\n        self.useconv = cls_useconv\n        self.features = nn.SequentialCell(*features)\n        if self.useconv:\n            self.cls = nn.SequentialCell(\n                nn.Dropout(1.0 - drop_rate),\n                nn.Conv2d(pen_channels, num_classes, 1, has_bias=True))\n        else:\n            self.cls = nn.SequentialCell(\n                nn.Dropout(1.0 - drop_rate),\n                nn.Dense(pen_channels, num_classes))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Conv2d, nn.Dense)):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         [1, cell.bias.shape[0]], cell.bias.dtype).reshape((-1)))\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x):\n        if not self.useconv:\n            x = x.reshape((x.shape[0], -1))\n            x = self.cls(x)\n        else:\n            x = self.cls(x).reshape((x.shape[0], -1))\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.rexnet_x09(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ReXNet model with width multiplier of 0.9. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_x09(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n\"\"\"Get ReXNet model with width multiplier of 0.9.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_x09\", 0.9, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.rexnet_x10(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ReXNet model with width multiplier of 1.0. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_x10(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n\"\"\"Get ReXNet model with width multiplier of 1.0.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_x10\", 1.0, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.rexnet_x13(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ReXNet model with width multiplier of 1.3. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_x13(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n\"\"\"Get ReXNet model with width multiplier of 1.3.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_x13\", 1.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.rexnet_x15(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ReXNet model with width multiplier of 1.5. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_x15(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n\"\"\"Get ReXNet model with width multiplier of 1.5.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_x15\", 1.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.rexnet.rexnet_x20(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ReXNet model with width multiplier of 2.0. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_x20(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n\"\"\"Get ReXNet model with width multiplier of 2.0.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_x20\", 2.0, in_channels, num_classes, pretrained, **kwargs)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet</code> \u00b6 <p>MindSpore implementation of <code>SENet</code>. Refer to Squeeze-and-Excitation Networks.</p> <code>mindocr.models.backbones.mindcv_models.senet.Bottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Define the base block class for [SEnet, SEResNet, SEResNext] bottlenecks that implements <code>construct</code> method.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class Bottleneck(nn.Cell):\n\"\"\"\n    Define the base block class for [SEnet, SEResNet, SEResNext] bottlenecks\n    that implements `construct` method.\n    \"\"\"\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet.SEBottleneck</code> \u00b6 <p>         Bases: <code>Bottleneck</code></p> <p>Define the Bottleneck for SENet154.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class SEBottleneck(Bottleneck):\n\"\"\"\n    Define the Bottleneck for SENet154.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels * 2, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels * 2)\n        self.conv2 = nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=stride,\n                               pad_mode=\"pad\", padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels * 4)\n        self.conv3 = nn.Conv2d(channels * 4, channels * 4, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet.SENet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>SENet model class, based on <code>\"Squeeze-and-Excitation Networks\" &lt;https://arxiv.org/abs/1709.01507&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block class of SENet.</p> <p> TYPE: <code>Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]]</code> </p> <code>layers</code> <p>Number of residual blocks for 4 layers.</p> <p> TYPE: <code>List[int]</code> </p> <code>group</code> <p>Number of groups for the conv in each bottleneck block.</p> <p> TYPE: <code>int</code> </p> <code>reduction</code> <p>Reduction ratio for Squeeze-and-Excitation modules.</p> <p> TYPE: <code>int</code> </p> <code>drop_rate</code> <p>Drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>inplanes</code> <p>Number of input channels for layer1. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>input3x3</code> <p>If <code>True</code>, use three 3x3 convolutions in layer0. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>downsample_kernel_size</code> <p>Kernel size for downsampling convolutions. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>downsample_padding</code> <p>Padding for downsampling convolutions. Default: 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class SENet(nn.Cell):\nr\"\"\"SENet model class, based on\n    `\"Squeeze-and-Excitation Networks\" &lt;https://arxiv.org/abs/1709.01507&gt;`_\n\n    Args:\n        block: block class of SENet.\n        layers: Number of residual blocks for 4 layers.\n        group: Number of groups for the conv in each bottleneck block.\n        reduction: Reduction ratio for Squeeze-and-Excitation modules.\n        drop_rate: Drop probability for the Dropout layer. Default: 0.\n        in_channels: number the channels of the input. Default: 3.\n        inplanes:  Number of input channels for layer1. Default: 64.\n        input3x3: If `True`, use three 3x3 convolutions in layer0. Default: False.\n        downsample_kernel_size: Kernel size for downsampling convolutions. Default: 1.\n        downsample_padding: Padding for downsampling convolutions. Default: 0.\n        num_classes (int): number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]],\n        layers: List[int],\n        group: int,\n        reduction: int,\n        drop_rate: float = 0.0,\n        in_channels: int = 3,\n        inplanes: int = 64,\n        input3x3: bool = False,\n        downsample_kernel_size: int = 1,\n        downsample_padding: int = 0,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if input3x3:\n            self.layer0 = nn.SequentialCell([\n                nn.Conv2d(in_channels, 64, 3, stride=2, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Conv2d(64, 64, 3, stride=1, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Conv2d(64, inplanes, 3, stride=1, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(inplanes),\n                nn.ReLU()\n            ])\n        else:\n            self.layer0 = nn.SequentialCell([\n                nn.Conv2d(in_channels, inplanes, kernel_size=7, stride=2, pad_mode=\"pad\",\n                          padding=3, has_bias=False),\n                nn.BatchNorm2d(inplanes),\n                nn.ReLU()\n            ])\n        self.pool0 = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n        self.layer1 = self._make_layer(block, planes=64, blocks=layers[0], group=group,\n                                       reduction=reduction, downsample_kernel_size=1,\n                                       downsample_padding=0)\n\n        self.layer2 = self._make_layer(block, planes=128, blocks=layers[1], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.layer3 = self._make_layer(block, planes=256, blocks=layers[2], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.layer4 = self._make_layer(block, planes=512, blocks=layers[3], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.num_features = 512 * block.expansion\n\n        self.pool = GlobalAvgPooling()\n        if self.drop_rate &gt; 0.:\n            self.dropout = nn.Dropout(keep_prob=1. - self.drop_rate)\n        self.classifier = nn.Dense(self.num_features, self.num_classes)\n\n        self._initialize_weights()\n\n    def _make_layer(\n        self,\n        block: Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]],\n        planes: int,\n        blocks: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample_kernel_size: int = 1,\n        downsample_padding: int = 0,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.SequentialCell([\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,\n                          stride=stride, pad_mode=\"pad\", padding=downsample_padding, has_bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            ])\n\n        layers = [block(self.inplanes, planes, group, reduction, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, group, reduction))\n\n        return nn.SequentialCell(layers)\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode=\"fan_in\", nonlinearity=\"sigmoid\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.layer0(x)\n        x = self.pool0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        if self.drop_rate &gt; 0.0:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet.SEResNeXtBottleneck</code> \u00b6 <p>         Bases: <code>Bottleneck</code></p> <p>Define the ResNeXt bottleneck type C with a Squeeze-and-Excitation module.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class SEResNeXtBottleneck(Bottleneck):\n\"\"\"\n    Define the ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n        base_width: int = 4,\n    ) -&gt; None:\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(channels * (base_width / 64)) * group\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, channels * 4, kernel_size=1, pad_mode=\"pad\", padding=0,\n                               has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet.SEResNetBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Define the basic block of resnet with a Squeeze-and-Excitation module.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class SEResNetBlock(nn.Cell):\n\"\"\"\n    Define the basic block of resnet with a Squeeze-and-Excitation module.\n    \"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, pad_mode=\"pad\", padding=1,\n                               group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.senet.SEResNetBottleneck</code> \u00b6 <p>         Bases: <code>Bottleneck</code></p> <p>Define the ResNet bottleneck with a Squeeze-and-Excitation module, and the latter is used in the torchvision implementation of ResNet.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\senet.py</code> <pre><code>class SEResNetBottleneck(Bottleneck):\n\"\"\"\n    Define the ResNet bottleneck with a Squeeze-and-Excitation module,\n    and the latter is used in the torchvision implementation of ResNet.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.conv3 = nn.Conv2d(channels, channels * 4, kernel_size=1, pad_mode=\"pad\", padding=0,\n                               has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1</code> \u00b6 <p>MindSpore implementation of <code>ShuffleNetV1</code>. Refer to ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</p> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.ShuffleNetV1</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ShuffleNetV1 model class, based on <code>\"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" &lt;https://arxiv.org/abs/1707.01083&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>model_size</code> <p>scale factor which controls the number of channels. Default: '2.0x'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'2.0x'</code> </p> <code>group</code> <p>number of group for group convolution. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>class ShuffleNetV1(nn.Cell):\nr\"\"\"ShuffleNetV1 model class, based on\n    `\"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" &lt;https://arxiv.org/abs/1707.01083&gt;`_  # noqa: E501\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of input channels. Default: 3.\n        model_size: scale factor which controls the number of channels. Default: '2.0x'.\n        group: number of group for group convolution. Default: 3.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        model_size: str = \"2.0x\",\n        group: int = 3,\n    ):\n        super().__init__()\n        self.stage_repeats = [4, 8, 4]\n        self.model_size = model_size\n        if group == 3:\n            if model_size == \"0.5x\":\n                self.stage_out_channels = [-1, 12, 120, 240, 480]\n            elif model_size == \"1.0x\":\n                self.stage_out_channels = [-1, 24, 240, 480, 960]\n            elif model_size == \"1.5x\":\n                self.stage_out_channels = [-1, 24, 360, 720, 1440]\n            elif model_size == \"2.0x\":\n                self.stage_out_channels = [-1, 48, 480, 960, 1920]\n            else:\n                raise NotImplementedError\n        elif group == 8:\n            if model_size == \"0.5x\":\n                self.stage_out_channels = [-1, 16, 192, 384, 768]\n            elif model_size == \"1.0x\":\n                self.stage_out_channels = [-1, 24, 384, 768, 1536]\n            elif model_size == \"1.5x\":\n                self.stage_out_channels = [-1, 24, 576, 1152, 2304]\n            elif model_size == \"2.0x\":\n                self.stage_out_channels = [-1, 48, 768, 1536, 3072]\n            else:\n                raise NotImplementedError\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.first_conv = nn.SequentialCell(\n            nn.Conv2d(in_channels, input_channel, kernel_size=3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU(),\n        )\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        features = []\n        for idxstage, numrepeat in enumerate(self.stage_repeats):\n            output_channel = self.stage_out_channels[idxstage + 2]\n            for i in range(numrepeat):\n                stride = 2 if i == 0 else 1\n                first_group = idxstage == 0 and i == 0\n                features.append(ShuffleV1Block(input_channel, output_channel,\n                                               group=group, first_group=first_group,\n                                               mid_channels=output_channel // 4, stride=stride))\n                input_channel = output_channel\n\n        self.features = nn.SequentialCell(features)\n        self.global_pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.stage_out_channels[-1], num_classes, has_bias=False)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if \"first\" in name:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(1.0 / cell.weight.shape[1], 0), cell.weight.shape,\n                                         cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.first_conv(x)\n        x = self.max_pool(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.global_pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.ShuffleV1Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Basic block of ShuffleNetV1. 1x1 GC -&gt; CS -&gt; 3x3 DWC -&gt; 1x1 GC</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>class ShuffleV1Block(nn.Cell):\n\"\"\"Basic block of ShuffleNetV1. 1x1 GC -&gt; CS -&gt; 3x3 DWC -&gt; 1x1 GC\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        mid_channels: int,\n        stride: int,\n        group: int,\n        first_group: bool,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        self.stride = stride\n        self.group = group\n\n        if stride == 2:\n            out_channels = out_channels - in_channels\n\n        branch_main_1 = [\n            # pw\n            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1,\n                      group=1 if first_group else group),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(),\n        ]\n\n        branch_main_2 = [\n            # dw\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, pad_mode=\"pad\", padding=1,\n                      group=mid_channels),\n            nn.BatchNorm2d(mid_channels),\n            # pw-linear\n            nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, group=group),\n            nn.BatchNorm2d(out_channels),\n        ]\n        self.branch_main_1 = nn.SequentialCell(branch_main_1)\n        self.branch_main_2 = nn.SequentialCell(branch_main_2)\n        if stride == 2:\n            self.branch_proj = nn.AvgPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identify = x\n        x = self.branch_main_1(x)\n        if self.group &gt; 1:\n            x = self.channel_shuffle(x)\n        x = self.branch_main_2(x)\n        if self.stride == 1:\n            out = self.relu(identify + x)\n        else:\n            out = self.relu(ops.concat((self.branch_proj(identify), x), axis=1))\n\n        return out\n\n    def channel_shuffle(self, x: Tensor) -&gt; Tensor:\n        batch_size, num_channels, height, width = x.shape\n\n        group_channels = num_channels // self.group\n        x = ops.reshape(x, (batch_size, group_channels, self.group, height, width))\n        x = ops.transpose(x, (0, 2, 1, 3, 4))\n        x = ops.reshape(x, (batch_size, num_channels, height, width))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g3_x0_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 0.5 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_x0_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 0.5 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_0.5\"]\n    model = ShuffleNetV1(group=3, model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g3_x1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 1.0 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_x1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 1.0 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_1.0\"]\n    model = ShuffleNetV1(group=3, model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g3_x1_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 1.5 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_x1_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 1.5 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_1.5\"]\n    model = ShuffleNetV1(group=3, model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g3_x2_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 2.0 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_x2_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 2.0 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_2.0\"]\n    model = ShuffleNetV1(group=3, model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g8_x0_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 0.5 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_x0_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 0.5 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_0.5\"]\n    model = ShuffleNetV1(group=8, model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g8_x1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 1.0 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_x1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 1.0 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_1.0\"]\n    model = ShuffleNetV1(group=8, model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g8_x1_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 1.5 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_x1_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 1.5 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_1.5\"]\n    model = ShuffleNetV1(group=8, model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv1.shufflenet_v1_g8_x2_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV1 model with width scaled by 2.0 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_x2_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n\"\"\"Get ShuffleNetV1 model with width scaled by 2.0 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_2.0\"]\n    model = ShuffleNetV1(group=8, model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2</code> \u00b6 <p>MindSpore implementation of <code>ShuffleNetV2</code>. Refer to ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</p> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.ShuffleNetV2</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>ShuffleNetV2 model class, based on <code>\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" &lt;https://arxiv.org/abs/1807.11164&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>model_size</code> <p>scale factor which controls the number of channels. Default: '1.5x'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'1.5x'</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>class ShuffleNetV2(nn.Cell):\nr\"\"\"ShuffleNetV2 model class, based on\n    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" &lt;https://arxiv.org/abs/1807.11164&gt;`_\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of input channels. Default: 3.\n        model_size: scale factor which controls the number of channels. Default: '1.5x'.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        model_size: str = \"1.5x\",\n    ):\n        super().__init__()\n\n        self.stage_repeats = [4, 8, 4]\n        self.model_size = model_size\n        if model_size == \"0.5x\":\n            self.stage_out_channels = [-1, 24, 48, 96, 192, 1024]\n        elif model_size == \"1.0x\":\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif model_size == \"1.5x\":\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif model_size == \"2.0x\":\n            self.stage_out_channels = [-1, 24, 244, 488, 976, 2048]\n        else:\n            raise NotImplementedError\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.first_conv = nn.SequentialCell([\n            nn.Conv2d(in_channels, input_channel, kernel_size=3, stride=2,\n                      pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU(),\n        ])\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.features = []\n        for idxstage, numrepeat in enumerate(self.stage_repeats):\n            output_channel = self.stage_out_channels[idxstage + 2]\n            for i in range(numrepeat):\n                if i == 0:\n                    self.features.append(ShuffleV2Block(input_channel, output_channel,\n                                                        mid_channels=output_channel // 2, kernel_size=3, stride=2))\n                else:\n                    self.features.append(ShuffleV2Block(input_channel // 2, output_channel,\n                                                        mid_channels=output_channel // 2, kernel_size=3, stride=1))\n                input_channel = output_channel\n\n        self.features = nn.SequentialCell(self.features)\n\n        self.conv_last = nn.SequentialCell([\n            nn.Conv2d(input_channel, self.stage_out_channels[-1], kernel_size=1, stride=1),\n            nn.BatchNorm2d(self.stage_out_channels[-1]),\n            nn.ReLU()\n        ])\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.stage_out_channels[-1], num_classes, has_bias=False)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if \"first\" in name:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(1.0 / cell.weight.shape[1], 0), cell.weight.shape,\n                                         cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.first_conv(x)\n        x = self.max_pool(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_last(x)\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.ShuffleV2Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the basic block of ShuffleV2</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>class ShuffleV2Block(nn.Cell):\n\"\"\"define the basic block of ShuffleV2\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        mid_channels: int,\n        kernel_size: int,\n        stride: int,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        self.stride = stride\n        pad = kernel_size // 2\n        out_channels = out_channels - in_channels\n        branch_main = [\n            # pw\n            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(),\n            # dw\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_size, stride=stride,\n                      pad_mode=\"pad\", padding=pad, group=mid_channels),\n            nn.BatchNorm2d(mid_channels),\n            # pw-linear\n            nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        ]\n        self.branch_main = nn.SequentialCell(branch_main)\n\n        if stride == 2:\n            branch_proj = [\n                # dw\n                nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride,\n                          pad_mode=\"pad\", padding=pad, group=in_channels),\n                nn.BatchNorm2d(in_channels),\n                # pw-linear\n                nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(),\n            ]\n            self.branch_proj = nn.SequentialCell(branch_proj)\n        else:\n            self.branch_proj = None\n\n    def construct(self, old_x: Tensor) -&gt; Tensor:\n        if self.stride == 1:\n            x_proj, x = self.channel_shuffle(old_x)\n            return ops.concat((x_proj, self.branch_main(x)), axis=1)\n\n        if self.stride == 2:\n            x_proj = old_x\n            x = old_x\n            return ops.concat((self.branch_proj(x_proj), self.branch_main(x)), axis=1)\n        return None\n\n    @staticmethod\n    def channel_shuffle(x: Tensor) -&gt; Tuple[Tensor, Tensor]:\n        batch_size, num_channels, height, width = x.shape\n        x = ops.reshape(x, (batch_size * num_channels // 2, 2, height * width,))\n        x = ops.transpose(x, (1, 0, 2,))\n        x = ops.reshape(x, (2, -1, num_channels // 2, height, width,))\n        return x[0], x[1]\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.shufflenet_v2_x0_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV2 model with width scaled by 0.5. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x0_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n\"\"\"Get ShuffleNetV2 model with width scaled by 0.5.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_0.5\"]\n    model = ShuffleNetV2(model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.shufflenet_v2_x1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV2 model with width scaled by 1.0. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n\"\"\"Get ShuffleNetV2 model with width scaled by 1.0.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_1.0\"]\n    model = ShuffleNetV2(model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.shufflenet_v2_x1_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV2 model with width scaled by 1.5. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x1_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n\"\"\"Get ShuffleNetV2 model with width scaled by 1.5.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_1.5\"]\n    model = ShuffleNetV2(model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.shufflenetv2.shufflenet_v2_x2_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get ShuffleNetV2 model with width scaled by 2.0. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x2_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n\"\"\"Get ShuffleNetV2 model with width scaled by 2.0.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_2.0\"]\n    model = ShuffleNetV2(model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet</code> \u00b6 <p>MindSpore implementation of <code>SKNet</code>. Refer to Selective Kernel Networks.</p> <code>mindocr.models.backbones.mindcv_models.sknet.SKNet</code> \u00b6 <p>         Bases: <code>ResNet</code></p> <p>SKNet model class, based on <code>\"Selective Kernel Networks\" &lt;https://arxiv.org/abs/1903.06586&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of sknet.</p> <p> TYPE: <code>Type[nn.Cell]</code> </p> <code>layers</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[nn.Cell]</code> DEFAULT: <code>None</code> </p> <code>sk_kwargs</code> <p>kwargs of selective kernel. Default: None.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>class SKNet(ResNet):\nr\"\"\"SKNet model class, based on\n    `\"Selective Kernel Networks\" &lt;https://arxiv.org/abs/1903.06586&gt;`_\n\n    Args:\n        block: block of sknet.\n        layers: number of layers of each stage.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 64.\n        norm: normalization layer in blocks. Default: None.\n        sk_kwargs: kwargs of selective kernel. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[nn.Cell],\n        layers: List[int],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ) -&gt; None:\n        self.sk_kwargs: Optional[Dict] = sk_kwargs  # make pylint happy\n        super().__init__(block, layers, num_classes, in_channels, groups, base_width, norm)\n\n    def _make_layer(\n        self,\n        block: Type[Union[SelectiveKernelBasic, SelectiveKernelBottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                self.norm(channels * block.expansion)\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n                sk_kwargs=self.sk_kwargs,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm,\n                    sk_kwargs=self.sk_kwargs,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.SelectiveKernelBasic</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>build basic block of sknet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>class SelectiveKernelBasic(nn.Cell):\n\"\"\"build basic block of sknet\"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        down_sample: Optional[nn.Cell] = None,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ):\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        if sk_kwargs is None:\n            sk_kwargs = {}\n\n        assert groups == 1, \"BasicBlock only supports cardinality of 1\"\n        assert base_width == 64, \"BasicBlock doest not support changing base width\"\n\n        self.conv1 = SelectiveKernel(\n            in_channels, out_channels, stride=stride, **sk_kwargs)\n        self.conv2 = nn.SequentialCell([\n            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, padding=1, pad_mode=\"pad\"),\n            norm(out_channels * self.expansion)\n        ])\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.SelectiveKernelBottleneck</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>build the bottleneck of the sknet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>class SelectiveKernelBottleneck(nn.Cell):\n\"\"\"build the bottleneck of the sknet\"\"\"\n\n    expansion = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        down_sample: Optional[nn.Cell] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ):\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        if sk_kwargs is None:\n            sk_kwargs = {}\n\n        width = int(out_channels * (base_width / 64.0)) * groups\n        self.conv1 = nn.SequentialCell([\n            nn.Conv2d(in_channels, width, kernel_size=1),\n            norm(width)\n        ])\n        self.conv2 = SelectiveKernel(\n            width, width, stride=stride, groups=groups, **sk_kwargs)\n        self.conv3 = nn.SequentialCell([\n            nn.Conv2d(width, out_channels * self.expansion, kernel_size=1),\n            norm(out_channels * self.expansion)\n        ])\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n\n        if self.down_sample:\n            identity = self.down_sample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.skresnet18(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 18 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>@register_model\ndef skresnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n\"\"\"Get 18 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet18\"]\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model = SKNet(SelectiveKernelBasic, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.skresnet34(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 34 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>@register_model\ndef skresnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n\"\"\"Get 34 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet34\"]\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model = SKNet(SelectiveKernelBasic, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.skresnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>@register_model\ndef skresnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n\"\"\"Get 50 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet50\"]\n    sk_kwargs = dict(split_input=True)\n    model = SKNet(SelectiveKernelBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.sknet.skresnext50_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 50 layers SKNeXt model with 32 groups of GPConv. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\sknet.py</code> <pre><code>@register_model\ndef skresnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n\"\"\"Get 50 layers SKNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnext50_32x4d\"]\n    sk_kwargs = dict(rd_ratio=1 / 16, rd_divisor=32, split_input=False)\n    model = SKNet(SelectiveKernelBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.squeezenet</code> \u00b6 <p>MindSpore implementation of <code>SqueezeNet</code>. Refer to SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size.</p> <code>mindocr.models.backbones.mindcv_models.squeezenet.Fire</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>define the basic block of squeezenet</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\squeezenet.py</code> <pre><code>class Fire(nn.Cell):\n\"\"\"define the basic block of squeezenet\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        squeeze_channels: int,\n        expand1x1_channels: int,\n        expand3x3_channels: int,\n    ) -&gt; None:\n        super().__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1, has_bias=True)\n        self.squeeze_activation = nn.ReLU()\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1, has_bias=True)\n        self.expand1x1_activation = nn.ReLU()\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, pad_mode=\"same\", has_bias=True)\n        self.expand3x3_activation = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.squeeze_activation(self.squeeze(x))\n        return ops.concat((self.expand1x1_activation(self.expand1x1(x)),\n                           self.expand3x3_activation(self.expand3x3(x))), axis=1)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.squeezenet.SqueezeNet</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>SqueezeNet model class, based on <code>\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\" &lt;https://arxiv.org/abs/1602.07360&gt;</code>_  # noqa: E501</p> <p>.. note::     Important: In contrast to the other models the inception_v3 expects tensors with a size of     N x 3 x 227 x 227, so ensure your images are sized accordingly.</p> PARAMETER DESCRIPTION <code>version</code> <p>version of the architecture, '1_0' or '1_1'. Default: '1_0'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'1_0'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>drop_rate</code> <p>dropout rate of the classifier. Default: 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\squeezenet.py</code> <pre><code>class SqueezeNet(nn.Cell):\nr\"\"\"SqueezeNet model class, based on\n    `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\" &lt;https://arxiv.org/abs/1602.07360&gt;`_  # noqa: E501\n\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 227 x 227, so ensure your images are sized accordingly.\n\n    Args:\n        version: version of the architecture, '1_0' or '1_1'. Default: '1_0'.\n        num_classes: number of classification classes. Default: 1000.\n        drop_rate: dropout rate of the classifier. Default: 0.5.\n        in_channels: number the channels of the input. Default: 3.\n    \"\"\"\n\n    def __init__(\n        self,\n        version: str = \"1_0\",\n        num_classes: int = 1000,\n        drop_rate: float = 0.5,\n        in_channels: int = 3,\n    ) -&gt; None:\n        super().__init__()\n        if version == \"1_0\":\n            self.features = nn.SequentialCell([\n                nn.Conv2d(in_channels, 96, kernel_size=7, stride=2, pad_mode=\"valid\", has_bias=True),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(512, 64, 256, 256),\n            ])\n        elif version == \"1_1\":\n            self.features = nn.SequentialCell([\n                nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\", has_bias=True),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            ])\n        else:\n            raise ValueError(f\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\")\n\n        self.final_conv = nn.Conv2d(512, num_classes, kernel_size=1, has_bias=True)\n        self.classifier = nn.SequentialCell([\n            nn.Dropout(keep_prob=1 - drop_rate),\n            self.final_conv,\n            nn.ReLU(),\n            GlobalAvgPooling()\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if cell is self.final_conv:\n                    cell.weight.set_data(init.initializer(init.Normal(), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.squeezenet.squeezenet1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get SqueezeNet model of version 1.0. Refer to the base class <code>models.SqueezeNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\squeezenet.py</code> <pre><code>@register_model\ndef squeezenet1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SqueezeNet:\n\"\"\"Get SqueezeNet model of version 1.0.\n    Refer to the base class `models.SqueezeNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"squeezenet1_0\"]\n    model = SqueezeNet(version=\"1_0\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.squeezenet.squeezenet1_1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get SqueezeNet model of version 1.1. Refer to the base class <code>models.SqueezeNet</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\squeezenet.py</code> <pre><code>@register_model\ndef squeezenet1_1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SqueezeNet:\n\"\"\"Get SqueezeNet model of version 1.1.\n    Refer to the base class `models.SqueezeNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"squeezenet1_1\"]\n    model = SqueezeNet(version=\"1_1\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer</code> \u00b6 <p>Define SwinTransformer model</p> <code>mindocr.models.backbones.mindcv_models.swin_transformer.BasicLayer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>A basic Swin Transformer layer for one stage.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>input_resolution</code> <p>Input resolution.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>depth</code> <p>Number of blocks.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>Local window size.</p> <p> TYPE: <code>int</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>drop</code> <p>Dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop</code> <p>Attention dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path</code> <p>Stochastic depth rate. Default: 0.0</p> <p> TYPE: <code>float | tuple[float]</code> DEFAULT: <code>0.0</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: nn.LayerNorm</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.LayerNorm</code> </p> <code>downsample</code> <p>Downsample layer at the end of the layer. Default: None</p> <p> TYPE: <code>nn.Cell | None</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class BasicLayer(nn.Cell):\n\"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Cell, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Cell | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        input_resolution: Tuple[int],\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: Optional[float] = 0.0,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n        downsample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.CellList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,  # TODO: \u8fd9\u91ccwindow_size//2\u7684\u65f6\u5019\u7279\u522b\u6162\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        for blk in self.blocks:\n            x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.PatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Image size.  Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch token size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>embed_dim</code> <p>Number of linear projection output channels. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: None</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class PatchEmbed(nn.Cell):\n\"\"\"Image to Patch Embedding\n\n    Args:\n        image_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 96,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        image_size = to_2tuple(image_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n                              pad_mode=\"pad\", has_bias=True, weight_init=\"TruncatedNormal\")\n\n        if norm_layer is not None:\n            if isinstance(embed_dim, int):\n                embed_dim = (embed_dim,)\n            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n        else:\n            self.norm = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        b = x.shape[0]\n        # FIXME look at relaxing size constraints\n        x = ops.reshape(self.proj(x), (b, self.embed_dim, -1))  # b Ph*Pw c\n        x = ops.transpose(x, (0, 2, 1))\n\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.PatchMerging</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Patch Merging Layer.</p> PARAMETER DESCRIPTION <code>input_resolution</code> <p>Resolution of input feature.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>norm_layer</code> <p>Normalization layer.  Default: nn.LayerNorm</p> <p> TYPE: <code>nn.Module</code> DEFAULT: <code>nn.LayerNorm</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class PatchMerging(nn.Cell):\n\"\"\"Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        input_resolution: Tuple[int],\n        dim: int,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim[0] if isinstance(dim, tuple) and len(dim) == 1 else dim\n        # Default False\n        self.reduction = nn.Dense(in_channels=4 * dim, out_channels=2 * dim, has_bias=False)\n        self.norm = norm_layer([dim * 4, ])\n        self.H, self.W = self.input_resolution\n        self.H_2, self.W_2 = self.H // 2, self.W // 2\n        self.H2W2 = int(self.H * self.W // 4)\n        self.dim_mul_4 = int(dim * 4)\n        self.H2W2 = int(self.H * self.W // 4)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"\n        x: B, H*W, C\n        \"\"\"\n        b = x.shape[0]\n        x = ops.reshape(x, (b, self.H_2, 2, self.W_2, 2, self.dim))\n        x = ops.transpose(x, (0, 1, 3, 4, 2, 5))\n        x = ops.reshape(x, (b, self.H2W2, self.dim_mul_4))\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.PatchMerging.construct(x)</code> \u00b6 Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"\n    x: B, H*W, C\n    \"\"\"\n    b = x.shape[0]\n    x = ops.reshape(x, (b, self.H_2, 2, self.W_2, 2, self.dim))\n    x = ops.transpose(x, (0, 1, 3, 4, 2, 5))\n    x = ops.reshape(x, (b, self.H2W2, self.dim_mul_4))\n    x = self.norm(x)\n    x = self.reduction(x)\n\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.SwinTransformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>SwinTransformer model class, based on <code>\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" &lt;https://arxiv.org/pdf/2103.14030&gt;</code>_</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Input image size. Default 224</p> <p> TYPE: <code>int | tuple(int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch size. Default: 4</p> <p> TYPE: <code>int | tuple(int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>Number of classes for classification head. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>Patch embedding dimension. Default: 96</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>depths</code> <p>Depth of each Swin Transformer layer.</p> <p> TYPE: <code>tuple(int</code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>Number of attention heads in different layers.</p> <p> TYPE: <code>tuple(int</code> DEFAULT: <code>None</code> </p> <code>window_size</code> <p>Window size. Default: 7</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>drop_rate</code> <p>Dropout rate. Default: 0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>Attention dropout rate. Default: 0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>Stochastic depth rate. Default: 0.1</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: nn.LayerNorm.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.LayerNorm</code> </p> <code>ape</code> <p>If True, add absolute position embedding to the patch embedding. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patch_norm</code> <p>If True, add normalization after patch embedding. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class SwinTransformer(nn.Cell):\nr\"\"\"SwinTransformer model class, based on\n    `\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" &lt;https://arxiv.org/pdf/2103.14030&gt;`_\n\n    Args:\n        image_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Cell): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 96,\n        depths: Optional[List[int]] = None,\n        num_heads: Optional[List[int]] = None,\n        window_size: int = 7,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[int] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n        ape: bool = False,\n        patch_norm: bool = True,\n    ) -&gt; None:\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            image_size=image_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = Parameter(Tensor(np.zeros(1, num_patches, embed_dim), dtype=mstype.float32))\n\n        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n\n        # stochastic depth\n        dpr = [x for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.CellList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer &lt; self.num_layers - 1) else None)\n            self.layers.append(layer)\n\n        self.norm = norm_layer([self.num_features, ], epsilon=1e-5)\n        self.classifier = nn.Dense(in_channels=self.num_features,\n                                   out_channels=num_classes, has_bias=True) if num_classes &gt; 0 else Identity()\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=0.02),\n                                                      cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n\n    def no_weight_decay(self) -&gt; None:\n        return {\"absolute_pos_embed\"}\n\n    def no_weight_decay_keywords(self) -&gt; None:\n        return {\"relative_position_bias_table\"}\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)  # B L C\n        x = ops.mean(ops.transpose(x, (0, 2, 1)), 2)  # B C 1\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.SwinTransformerBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Swin Transformer Block.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>input_resolution</code> <p>Input resolution.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>Window size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>shift_size</code> <p>Shift size for SW-MSA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>drop</code> <p>Dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop</code> <p>Attention dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path</code> <p>Stochastic depth rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>act_layer</code> <p>Activation layer. Default: nn.GELU</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.GELU</code> </p> <code>norm_layer</code> <p>Normalization layer.  Default: nn.LayerNorm</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.LayerNorm</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class SwinTransformerBlock(nn.Cell):\n\"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Cell, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Cell, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        input_resolution: Tuple[int],\n        num_heads: int,\n        window_size: int = 7,\n        shift_size: int = 0,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: Optional[nn.Cell] = nn.GELU,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n    ) -&gt; None:\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) &lt;= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n\n        if isinstance(dim, int):\n            dim = (dim,)\n\n        self.norm1 = norm_layer(dim, epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer(dim, epsilon=1e-5)\n        mlp_hidden_dim = int((dim[0] if isinstance(dim, tuple) else dim) * mlp_ratio)\n        self.mlp = Mlp(in_features=dim[0] if isinstance(dim, tuple) else dim, hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer, drop=drop)\n        if self.shift_size &gt; 0:\n            # calculate attention mask for SW-MSA\n            h_, w_ = self.input_resolution\n            img_mask = np.zeros((1, h_, w_, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            # img_mask: [1, 56, 56, 1] window_size: 7\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.reshape(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows[:, np.newaxis] - mask_windows[:, :, np.newaxis]\n            # [64, 49, 49] ==&gt; [1, 64, 1, 49, 49]\n            attn_mask = np.expand_dims(attn_mask, axis=1)\n            attn_mask = np.expand_dims(attn_mask, axis=0)\n            attn_mask = Tensor(np.where(attn_mask == 0, 0.0, -100.0), dtype=mstype.float32)\n            self.attn_mask = Parameter(attn_mask, requires_grad=False)\n            self.roll_pos = Roll(self.shift_size)\n            self.roll_neg = Roll(-self.shift_size)\n        else:\n            self.attn_mask = None\n\n        self.window_partition = WindowPartition(self.window_size)\n        self.window_reverse = WindowReverse()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        h, w = self.input_resolution\n        b, _, c = x.shape\n\n        shortcut = x\n        x = self.norm1(x)\n        x = ops.reshape(x, (b, h, w, c,))\n\n        # cyclic shift\n        if self.shift_size &gt; 0:\n            shifted_x = self.roll_neg(x)\n            # shifted_x = numpy.roll(x, (-self.shift_size, -self.shift_size), (1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = self.window_partition(shifted_x)  # nW*B, window_size, window_size, C\n        x_windows = ops.reshape(x_windows,\n                                (-1, self.window_size * self.window_size, c,))  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = ops.reshape(attn_windows, (-1, self.window_size, self.window_size, c,))\n        shifted_x = self.window_reverse(attn_windows, self.window_size, h, w)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size &gt; 0:\n            x = self.roll_pos(shifted_x)\n        else:\n            x = shifted_x\n\n        x = ops.reshape(x, (b, h * w, c,))\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowAttention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Window based multi-head self attention (W-MSA) Cell with relative position bias. It supports both of shifted and non-shifted window.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>The height and width of the window.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qZk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set</p> <p> TYPE: <code>float | None</code> </p> <code>attn_drop</code> <p>Dropout ratio of attention weight. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>proj_drop</code> <p>Dropout ratio of output. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class WindowAttention(nn.Cell):\nr\"\"\"Window based multi-head self attention (W-MSA) Cell with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qZk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        window_size: int,\n        num_heads: int,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        if isinstance(dim, tuple) and len(dim) == 1:\n            dim = dim[0]\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = Tensor(qk_scale or head_dim**-0.5, mstype.float32)\n        self.relative_bias = RelativeBias(self.window_size, num_heads)\n\n        # get pair-wise relative position index for each token inside the window\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(keep_prob=1.0 - attn_drop)\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim, has_bias=True)\n        self.proj_drop = nn.Dropout(keep_prob=1.0 - proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_matmul = ops.BatchMatMul()\n\n    def construct(self, x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        b_, n, c = x.shape\n        q = ops.reshape(self.q(x), (b_, n, self.num_heads, c // self.num_heads)) * self.scale\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (b_, n, self.num_heads, c // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (b_, n, self.num_heads, c // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        attn = self.batch_matmul(q, k)\n        attn = attn + self.relative_bias()\n\n        if mask is not None:\n            nw = mask.shape[1]\n            attn = ops.reshape(attn, (b_ // nw, nw, self.num_heads, n, n,)) + mask\n            attn = ops.reshape(attn, (-1, self.num_heads, n, n,))\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = ops.reshape(ops.transpose(self.batch_matmul(attn, v), (0, 2, 1, 3)), (b_, n, c))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowAttention.construct(x, mask=None)</code> \u00b6 PARAMETER DESCRIPTION <code>x</code> <p>input features with shape of (num_windows*B, N, C)</p> <p> TYPE: <code>Tensor</code> </p> <code>mask</code> <p>(0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>def construct(self, x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"\n    Args:\n        x: input features with shape of (num_windows*B, N, C)\n        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n    \"\"\"\n    b_, n, c = x.shape\n    q = ops.reshape(self.q(x), (b_, n, self.num_heads, c // self.num_heads)) * self.scale\n    q = ops.transpose(q, (0, 2, 1, 3))\n    k = ops.reshape(self.k(x), (b_, n, self.num_heads, c // self.num_heads))\n    k = ops.transpose(k, (0, 2, 3, 1))\n    v = ops.reshape(self.v(x), (b_, n, self.num_heads, c // self.num_heads))\n    v = ops.transpose(v, (0, 2, 1, 3))\n\n    attn = self.batch_matmul(q, k)\n    attn = attn + self.relative_bias()\n\n    if mask is not None:\n        nw = mask.shape[1]\n        attn = ops.reshape(attn, (b_ // nw, nw, self.num_heads, n, n,)) + mask\n        attn = ops.reshape(attn, (-1, self.num_heads, n, n,))\n        attn = self.softmax(attn)\n    else:\n        attn = self.softmax(attn)\n    attn = self.attn_drop(attn)\n    x = ops.reshape(ops.transpose(self.batch_matmul(attn, v), (0, 2, 1, 3)), (b_, n, c))\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowPartition</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class WindowPartition(nn.Cell):\n    def __init__(\n        self,\n        window_size: int,\n    ) -&gt; None:\n        super(WindowPartition, self).__init__()\n\n        self.window_size = window_size\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"\n        Args:\n            x: (b, h, w, c)\n            window_size (int): window size\n\n        Returns:\n            windows: Tensor(num_windows*b, window_size, window_size, c)\n        \"\"\"\n        b, h, w, c = x.shape\n        x = ops.reshape(x, (b, h // self.window_size, self.window_size, w // self.window_size, self.window_size, c))\n        x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n        x = ops.reshape(x, (b * h * w // (self.window_size**2), self.window_size, self.window_size, c))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowPartition.construct(x)</code> \u00b6 PARAMETER DESCRIPTION <code>x</code> <p>(b, h, w, c)</p> <p> TYPE: <code>Tensor</code> </p> <code>window_size</code> <p>window size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>windows</code> <p>Tensor(num_windows*b, window_size, window_size, c)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n\"\"\"\n    Args:\n        x: (b, h, w, c)\n        window_size (int): window size\n\n    Returns:\n        windows: Tensor(num_windows*b, window_size, window_size, c)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = ops.reshape(x, (b, h // self.window_size, self.window_size, w // self.window_size, self.window_size, c))\n    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = ops.reshape(x, (b * h * w // (self.window_size**2), self.window_size, self.window_size, c))\n\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowReverse</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>class WindowReverse(nn.Cell):\n    def construct(\n        self,\n        windows: Tensor,\n        window_size: int,\n        h: int,\n        w: int,\n    ) -&gt; Tensor:\n\"\"\"\n        Args:\n            windows: (num_windows*B, window_size, window_size, C)\n            window_size (int): Window size\n            h (int): Height of image\n            w (int): Width of image\n\n        Returns:\n            x: (B, H, W, C)\n        \"\"\"\n        b = windows.shape[0] // (h * w // window_size // window_size)\n        x = ops.reshape(windows, (b, h // window_size, w // window_size, window_size, window_size, -1))\n        x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n        x = ops.reshape(x, (b, h, w, -1))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.WindowReverse.construct(windows, window_size, h, w)</code> \u00b6 PARAMETER DESCRIPTION <code>windows</code> <p>(num_windows*B, window_size, window_size, C)</p> <p> TYPE: <code>Tensor</code> </p> <code>window_size</code> <p>Window size</p> <p> TYPE: <code>int</code> </p> <code>h</code> <p>Height of image</p> <p> TYPE: <code>int</code> </p> <code>w</code> <p>Width of image</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>x</code> <p>(B, H, W, C)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>def construct(\n    self,\n    windows: Tensor,\n    window_size: int,\n    h: int,\n    w: int,\n) -&gt; Tensor:\n\"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        h (int): Height of image\n        w (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    b = windows.shape[0] // (h * w // window_size // window_size)\n    x = ops.reshape(windows, (b, h // window_size, w // window_size, window_size, window_size, -1))\n    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = ops.reshape(x, (b, h, w, -1))\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.swin_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get SwinTransformer tiny model. Refer to the base class 'models.SwinTransformer' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>@register_model\ndef swin_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SwinTransformer:\n\"\"\"Get SwinTransformer tiny model.\n    Refer to the base class 'models.SwinTransformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"swin_tiny\"]\n    model = SwinTransformer(image_size=224, patch_size=4, in_chans=in_channels, num_classes=num_classes,\n                            embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7,\n                            mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,\n                            norm_layer=nn.LayerNorm, ape=False, patch_norm=True, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.swin_transformer.window_partition(x, window_size)</code> \u00b6 PARAMETER DESCRIPTION <code>x</code> <p>(B, H, W, C)</p> <p> </p> <code>window_size</code> <p>window size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>windows</code> <p>numpy(num_windows*B, window_size, window_size, C)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\swin_transformer.py</code> <pre><code>def window_partition(x, window_size: int):\n\"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: numpy(num_windows*B, window_size, window_size, C)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = np.reshape(x, (b, h // window_size, window_size, w // window_size, window_size, c))\n    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, c)\n    return windows\n</code></pre> <code>mindocr.models.backbones.mindcv_models.utils</code> \u00b6 <p>Some utils while building models</p> <code>mindocr.models.backbones.mindcv_models.utils.ConfigDict</code> \u00b6 <p>         Bases: <code>dict</code></p> <p>dot.notation access to dictionary attributes</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\utils.py</code> <pre><code>class ConfigDict(dict):\n\"\"\"dot.notation access to dictionary attributes\"\"\"\n\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n</code></pre> <code>mindocr.models.backbones.mindcv_models.utils.auto_map(model, param_dict)</code> \u00b6 <p>Raname part of the param_dict such that names from checkpoint and model are consistent</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\utils.py</code> <pre><code>def auto_map(model, param_dict):\n\"\"\"Raname part of the param_dict such that names from checkpoint and model are consistent\"\"\"\n    updated_param_dict = deepcopy(param_dict)\n    net_param = model.get_parameters()\n    ckpt_param = list(updated_param_dict.keys())\n    remap = {}\n    for param in net_param:\n        if param.name not in ckpt_param:\n            print('Cannot find a param to load: ', param.name)\n            poss = difflib.get_close_matches(param.name, ckpt_param, n=3, cutoff=0.6)\n            if len(poss) &gt; 0:\n                print('=&gt; Find most matched param: ', poss[0], ', loaded')\n                updated_param_dict[param.name] = updated_param_dict.pop(poss[0])  # replace\n                remap[param.name] = poss[0]\n            else:\n                raise ValueError('Cannot find any matching param from: ', ckpt_param)\n\n    if remap != {}:\n        print('WARNING: Auto mapping succeed. Please check the found mapping names to ensure correctness')\n        print('\\tNet Param\\t&lt;---\\tCkpt Param')\n        for k in remap:\n            print(f'\\t{k}\\t&lt;---\\t{remap[k]}')\n    return updated_param_dict\n</code></pre> <code>mindocr.models.backbones.mindcv_models.utils.download_pretrained(default_cfg)</code> \u00b6 <p>Download the pretrained ckpt from url to local path</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\utils.py</code> <pre><code>def download_pretrained(default_cfg):\n\"\"\"Download the pretrained ckpt from url to local path\"\"\"\n    if \"url\" not in default_cfg or not default_cfg[\"url\"]:\n        logging.warning(\"Pretrained model URL is invalid\")\n        return\n\n    # download files\n    download_path = get_checkpoint_download_root()\n    os.makedirs(download_path, exist_ok=True)\n    file_path = DownLoad().download_url(default_cfg[\"url\"], path=download_path)\n    return file_path\n</code></pre> <code>mindocr.models.backbones.mindcv_models.utils.load_pretrained(model, default_cfg, num_classes=1000, in_channels=3, filter_fn=None, auto_mapping=False)</code> \u00b6 <p>load pretrained model depending on cfgs of model</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\utils.py</code> <pre><code>def load_pretrained(model, default_cfg, num_classes=1000, in_channels=3, filter_fn=None, auto_mapping=False):\n\"\"\"load pretrained model depending on cfgs of model\"\"\"\n    file_path = download_pretrained(default_cfg)\n\n    try:\n        param_dict = load_checkpoint(file_path)\n    except Exception:\n        print(f'ERROR: Fails to load the checkpoint. Please check whether the checkpoint is downloaded successfully as '\n              f'`{file_path}` and is not zero-byte. You may try to manually download the checkpoint from ',\n              default_cfg[\"url\"])\n        param_dict = dict()\n\n    if auto_mapping:\n        param_dict = auto_map(model, param_dict)\n\n    if in_channels == 1:\n        conv1_name = default_cfg[\"first_conv\"]\n        logging.info(\"Converting first conv (%s) from 3 to 1 channel\", conv1_name)\n        con1_weight = param_dict[conv1_name + \".weight\"]\n        con1_weight.set_data(con1_weight.sum(axis=1, keepdims=True), slice_shape=True)\n    elif in_channels != 3:\n        raise ValueError(\"Invalid in_channels for pretrained weights\")\n\n    if 'classifier' in default_cfg:\n        classifier_name = default_cfg[\"classifier\"]\n        if num_classes == 1000 and default_cfg[\"num_classes\"] == 1001:\n            classifier_weight = param_dict[classifier_name + \".weight\"]\n            classifier_weight.set_data(classifier_weight[:1000], slice_shape=True)\n            classifier_bias = param_dict[classifier_name + \".bias\"]\n            classifier_bias.set_data(classifier_bias[:1000], slice_shape=True)\n        elif num_classes != default_cfg[\"num_classes\"]:\n            params_names = list(param_dict.keys())\n            param_dict.pop(\n                _search_param_name(params_names, classifier_name + \".weight\"),\n                \"No Parameter {} in ParamDict\".format(classifier_name + \".weight\"),\n            )\n            param_dict.pop(\n                _search_param_name(params_names, classifier_name + \".bias\"),\n                \"No Parameter {} in ParamDict\".format(classifier_name + \".bias\"),\n            )\n\n    if filter_fn is not None:\n        param_dict = filter_fn(param_dict)\n\n    load_param_into_net(model, param_dict)\n\n    print('INFO: Finish loading model checkpoint from: ', file_path)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.utils.make_divisible(v, divisor, min_value=None)</code> \u00b6 <p>Find the smallest integer larger than v and divisible by divisor.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\utils.py</code> <pre><code>def make_divisible(\n    v: float,\n    divisor: int,\n    min_value: Optional[int] = None,\n) -&gt; int:\n\"\"\"Find the smallest integer larger than v and divisible by divisor.\"\"\"\n    if not min_value:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vgg</code> \u00b6 <p>MindSpore implementation of <code>VGGNet</code>. Refer to SqueezeNet: Very Deep Convolutional Networks for Large-Scale Image Recognition.</p> <code>mindocr.models.backbones.mindcv_models.vgg.VGG</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>VGGNet model class, based on <code>\"Very Deep Convolutional Networks for Large-Scale Image Recognition\" &lt;https://arxiv.org/abs/1409.1556&gt;</code>_</p> PARAMETER DESCRIPTION <code>model_name</code> <p>name of the architecture. 'vgg11', 'vgg13', 'vgg16' or 'vgg19'.</p> <p> TYPE: <code>str</code> </p> <code>batch_norm</code> <p>use batch normalization or not. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>drop_rate</code> <p>dropout rate of the classifier. Default: 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vgg.py</code> <pre><code>class VGG(nn.Cell):\nr\"\"\"VGGNet model class, based on\n    `\"Very Deep Convolutional Networks for Large-Scale Image Recognition\" &lt;https://arxiv.org/abs/1409.1556&gt;`_\n\n    Args:\n        model_name: name of the architecture. 'vgg11', 'vgg13', 'vgg16' or 'vgg19'.\n        batch_norm: use batch normalization or not. Default: False.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        drop_rate: dropout rate of the classifier. Default: 0.5.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        batch_norm: bool = False,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        drop_rate: float = 0.5,\n    ) -&gt; None:\n        super().__init__()\n        cfg = cfgs[model_name]\n        self.features = _make_layers(cfg, batch_norm=batch_norm, in_channels=in_channels)\n        self.flatten = nn.Flatten()\n        self.classifier = nn.SequentialCell([\n            nn.Dense(512 * 7 * 7, 4096),\n            nn.ReLU(),\n            nn.Dropout(keep_prob=1 - drop_rate),\n            nn.Dense(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(keep_prob=1 - drop_rate),\n            nn.Dense(4096, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n\"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.flatten(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vgg.vgg11(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 11 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vgg.py</code> <pre><code>@register_model\ndef vgg11(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n\"\"\"Get 11 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg11\"]\n    model = VGG(model_name=\"vgg11\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vgg.vgg13(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 13 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vgg.py</code> <pre><code>@register_model\ndef vgg13(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n\"\"\"Get 13 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg13\"]\n    model = VGG(model_name=\"vgg13\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vgg.vgg16(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 16 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vgg.py</code> <pre><code>@register_model\ndef vgg16(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n\"\"\"Get 16 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg16\"]\n    model = VGG(model_name=\"vgg16\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vgg.vgg19(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get 19 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vgg.py</code> <pre><code>@register_model\ndef vgg19(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n\"\"\"Get 19 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg19\"]\n    model = VGG(model_name=\"vgg19\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer</code> \u00b6 <p>MindSpore implementation of <code>Visformer</code>. Refer to: Visformer: The Vision-friendly Transformer</p> <code>mindocr.models.backbones.mindcv_models.visformer.Attention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Attention layer</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>class Attention(nn.Cell):\n\"\"\"Attention layer\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        head_dim_ratio: float = 1.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super(Attention, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = round(dim // num_heads * head_dim_ratio)\n        self.head_dim = head_dim\n\n        qk_scale_factor = qk_scale if qk_scale is not None else -0.25\n        self.scale = head_dim**qk_scale_factor\n\n        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, 1, pad_mode=\"pad\", padding=0, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(1 - attn_drop)\n        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, 1, pad_mode=\"pad\", padding=0)\n        self.proj_drop = nn.Dropout(1 - proj_drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        x = self.qkv(x)\n        qkv = ops.reshape(x, (B, 3, self.num_heads, self.head_dim, H * W))\n        qkv = qkv.transpose((1, 0, 2, 4, 3))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = ops.matmul(q * self.scale, k.transpose(0, 1, 3, 2) * self.scale)\n        attn = ops.Softmax(axis=-1)(attn)\n        attn = self.attn_drop(attn)\n        x = ops.matmul(attn, v)\n\n        x = x.transpose((0, 1, 3, 2)).reshape((B, -1, H, W))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.Block</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>visformer basic block</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>class Block(nn.Cell):\n\"\"\"visformer basic block\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        head_dim_ratio: float = 1.0,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: nn.Cell = nn.GELU,\n        group: int = 8,\n        attn_disabled: bool = False,\n        spatial_conv: bool = False,\n    ) -&gt; None:\n        super(Block, self).__init__()\n        self.attn_disabled = attn_disabled\n        self.spatial_conv = spatial_conv\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        if not attn_disabled:\n            self.norm1 = nn.BatchNorm2d(dim)\n            self.attn = Attention(dim, num_heads=num_heads, head_dim_ratio=head_dim_ratio, qkv_bias=qkv_bias,\n                                  qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm2 = nn.BatchNorm2d(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,\n                       group=group, spatial_conv=spatial_conv)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if not self.attn_disabled:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.Mlp</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MLP layer</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>class Mlp(nn.Cell):\n\"\"\"MLP layer\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: int = None,\n        out_features: int = None,\n        act_layer: nn.Cell = nn.GELU,\n        drop: float = 0.0,\n        group: int = 8,\n        spatial_conv: bool = False,\n    ) -&gt; None:\n        super(Mlp, self).__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.in_features = in_features\n        self.out_features = out_features\n        self.spatial_conv = spatial_conv\n        if self.spatial_conv:\n            if group &lt; 2:\n                hidden_features = in_features * 5 // 6\n            else:\n                hidden_features = in_features * 2\n        self.hidden_features = hidden_features\n        self.group = group\n        self.drop = nn.Dropout(1 - drop)\n        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, 1, pad_mode=\"pad\", padding=0)\n        self.act1 = act_layer()\n        if self.spatial_conv:\n            self.conv2 = nn.Conv2d(hidden_features, hidden_features, 3, 1, pad_mode=\"pad\", padding=1, group=self.group)\n            self.act2 = act_layer()\n        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, 1, pad_mode=\"pad\", padding=0)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.drop(x)\n\n        if self.spatial_conv:\n            x = self.conv2(x)\n            x = self.act2(x)\n\n        x = self.conv3(x)\n        x = self.drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.Visformer</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Visformer model class, based on '\"Visformer: The Vision-friendly Transformer\" https://arxiv.org/pdf/2104.12533.pdf'</p> PARAMETER DESCRIPTION <code>image_size</code> <p>images input size. Default: 224.</p> <p> TYPE: <code>int) </code> </p> <code>number</code> <p>32.</p> <p> TYPE: <code>the channels of the input. Default</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>embedding dimension in all head. Default: 384.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>384</code> </p> <code>depth</code> <p>model block depth. Default: None.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>number of heads. Default: None.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>None</code> </p> <code>mlp_ratio</code> <p>ratio of hidden features in Mlp. Default: 4.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>have bias in qkv layers or not. Default: False.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>None</code> </p> <code>drop_rate</code> <p>dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>attention layers dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>drop path rate. Default: 0.1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.1</code> </p> <code>attn_stage</code> <p>block will have a attention layer if value = '1' else not. Default: '1111'.</p> <p> TYPE: <code>str) </code> DEFAULT: <code>'1111'</code> </p> <code>pos_embed</code> <p>position embedding. Default: True.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>True</code> </p> <code>spatial_conv</code> <p>block will have a spatial convolution layer if value = '1' else not. Default: '1111'.</p> <p> TYPE: <code>str) </code> DEFAULT: <code>'1111'</code> </p> <code>group</code> <p>convolution group. Default: 8.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>8</code> </p> <code>pool</code> <p>if true will use global_pooling else not. Default: True.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>True</code> </p> <code>conv_init</code> <p>if true will init convolution weights else not. Default: False.</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>class Visformer(nn.Cell):\nr\"\"\"Visformer model class, based on\n    '\"Visformer: The Vision-friendly Transformer\"\n    &lt;https://arxiv.org/pdf/2104.12533.pdf&gt;'\n\n    Args:\n        image_size (int) : images input size. Default: 224.\n        number the channels of the input. Default: 32.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dim (int) : embedding dimension in all head. Default: 384.\n        depth (int) : model block depth. Default: None.\n        num_heads (int) : number of heads. Default: None.\n        mlp_ratio (float) : ratio of hidden features in Mlp. Default: 4.\n        qkv_bias (bool) : have bias in qkv layers or not. Default: False.\n        qk_scale (float) : Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float) : dropout rate. Default: 0.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.1.\n        attn_stage (str) : block will have a attention layer if value = '1' else not. Default: '1111'.\n        pos_embed (bool) : position embedding. Default: True.\n        spatial_conv (str) : block will have a spatial convolution layer if value = '1' else not. Default: '1111'.\n        group (int) : convolution group. Default: 8.\n        pool (bool) : if true will use global_pooling else not. Default: True.\n        conv_init : if true will init convolution weights else not. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 224,\n        init_channels: int = 32,\n        num_classes: int = 1000,\n        embed_dim: int = 384,\n        depth: List[int] = None,\n        num_heads: List[int] = None,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        attn_stage: str = \"1111\",\n        pos_embed: bool = True,\n        spatial_conv: str = \"1111\",\n        group: int = 8,\n        pool: bool = True,\n        conv_init: bool = False,\n    ) -&gt; None:\n        super(Visformer, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        self.init_channels = init_channels\n        self.img_size = img_size\n        self.pool = pool\n        self.conv_init = conv_init\n        self.depth = depth\n        assert (isinstance(depth, list) or isinstance(depth, tuple)) and len(depth) == 4\n        if not (isinstance(num_heads, list) or isinstance(num_heads, tuple)):\n            num_heads = [num_heads] * 4\n\n        self.pos_embed = pos_embed\n        dpr = np.linspace(0, drop_path_rate, sum(depth)).tolist()\n\n        self.stem = nn.SequentialCell([\n            nn.Conv2d(3, self.init_channels, 7, 2, pad_mode=\"pad\", padding=3),\n            nn.BatchNorm2d(self.init_channels),\n            nn.ReLU()\n        ])\n        img_size //= 2\n\n        self.pos_drop = nn.Dropout(1 - drop_rate)\n        # stage0\n        if depth[0]:\n            self.patch_embed0 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=self.init_channels,\n                                           embed_dim=embed_dim // 4)\n            img_size //= 2\n            if self.pos_embed:\n                self.pos_embed0 = mindspore.Parameter(\n                    ops.zeros((1, embed_dim // 4, img_size, img_size), mindspore.float32))\n            self.stage0 = nn.CellList([\n                Block(dim=embed_dim // 4, num_heads=num_heads[0], head_dim_ratio=0.25, mlp_ratio=mlp_ratio,\n                      qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                      group=group, attn_disabled=(attn_stage[0] == \"0\"), spatial_conv=(spatial_conv[0] == \"1\"))\n                for i in range(depth[0])\n            ])\n\n        # stage1\n        if depth[0]:\n            self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim // 4,\n                                           embed_dim=embed_dim // 2)\n            img_size //= 2\n        else:\n            self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=4, in_chans=self.init_channels,\n                                           embed_dim=embed_dim // 2)\n            img_size //= 4\n\n        if self.pos_embed:\n            self.pos_embed1 = mindspore.Parameter(ops.zeros((1, embed_dim // 2, img_size, img_size), mindspore.float32))\n\n        self.stage1 = nn.CellList([\n            Block(\n                dim=embed_dim // 2, num_heads=num_heads[1], head_dim_ratio=0.5, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[1] == \"0\"), spatial_conv=(spatial_conv[1] == \"1\")\n            )\n            for i in range(sum(depth[:1]), sum(depth[:2]))\n        ])\n\n        # stage2\n        self.patch_embed2 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim // 2, embed_dim=embed_dim)\n        img_size //= 2\n        if self.pos_embed:\n            self.pos_embed2 = mindspore.Parameter(ops.zeros((1, embed_dim, img_size, img_size), mindspore.float32))\n        self.stage2 = nn.CellList([\n            Block(\n                dim=embed_dim, num_heads=num_heads[2], head_dim_ratio=1.0, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[2] == \"0\"), spatial_conv=(spatial_conv[2] == \"1\")\n            )\n            for i in range(sum(depth[:2]), sum(depth[:3]))\n        ])\n\n        # stage3\n        self.patch_embed3 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim, embed_dim=embed_dim * 2)\n        img_size //= 2\n        if self.pos_embed:\n            self.pos_embed3 = mindspore.Parameter(ops.zeros((1, embed_dim * 2, img_size, img_size), mindspore.float32))\n        self.stage3 = nn.CellList([\n            Block(\n                dim=embed_dim * 2, num_heads=num_heads[3], head_dim_ratio=1.0, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[3] == \"0\"), spatial_conv=(spatial_conv[3] == \"1\")\n            )\n            for i in range(sum(depth[:3]), sum(depth[:4]))\n        ])\n\n        # head\n        if self.pool:\n            self.global_pooling = GlobalAvgPooling()\n\n        self.norm = nn.BatchNorm2d(embed_dim * 2)\n        self.head = nn.Dense(embed_dim * 2, num_classes)\n\n        # weight init\n        if self.pos_embed:\n            if depth[0]:\n                self.pos_embed0.set_data(initializer(TruncatedNormal(0.02),\n                                                     self.pos_embed0.shape, self.pos_embed0.dtype))\n            self.pos_embed1.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed1.shape, self.pos_embed1.dtype))\n            self.pos_embed2.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed2.shape, self.pos_embed2.dtype))\n            self.pos_embed3.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed3.shape, self.pos_embed3.dtype))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(initializer(TruncatedNormal(0.02), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(initializer(Constant(0), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.beta.set_data(initializer(Constant(0), cell.beta.shape, cell.beta.dtype))\n                cell.gamma.set_data(initializer(Constant(1), cell.gamma.shape, cell.gamma.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.beta.set_data(initializer(Constant(0), cell.beta.shape, cell.beta.dtype))\n                cell.gamma.set_data(initializer(Constant(1), cell.gamma.shape, cell.gamma.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                if self.conv_init:\n                    cell.weight.set_data(initializer(HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"), cell.weight.shape,\n                                                     cell.weight.dtype))\n                else:\n                    cell.weight.set_data(initializer(TruncatedNormal(0.02), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(initializer(Constant(0), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.stem(x)\n\n        # stage 0\n        if self.depth[0]:\n            x = self.patch_embed0(x)\n            if self.pos_embed:\n                x = x + self.pos_embed0\n                x = self.pos_drop(x)\n            for b in self.stage0:\n                x = b(x)\n\n        # stage 1\n        x = self.patch_embed1(x)\n        if self.pos_embed:\n            x = x + self.pos_embed1\n            x = self.pos_drop(x)\n        for b in self.stage1:\n            x = b(x)\n\n        # stage 2\n        x = self.patch_embed2(x)\n        if self.pos_embed:\n            x = x + self.pos_embed2\n            x = self.pos_drop(x)\n        for b in self.stage2:\n            x = b(x)\n\n        # stage 3\n        x = self.patch_embed3(x)\n        if self.pos_embed:\n            x = x + self.pos_embed3\n            x = self.pos_drop(x)\n        for b in self.stage3:\n            x = b(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        # head\n        if self.pool:\n            x = self.global_pooling(x)\n        else:\n            x = x[:, :, 0, 0]\n        x = self.head(x.view(x.shape[0], -1))\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.visformer_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get visformer small model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>@register_model\ndef visformer_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n\"\"\"Get visformer small model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_small\"]\n    model = Visformer(img_size=224, init_channels=32, num_classes=num_classes, embed_dim=384,\n                      depth=[0, 7, 4, 4], num_heads=[6, 6, 6, 6], mlp_ratio=4., group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.visformer_small_v2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get visformer small2 model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>@register_model\ndef visformer_small_v2(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n\"\"\"Get visformer small2 model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_small_v2\"]\n    model = Visformer(img_size=224, init_channels=32, num_classes=num_classes, embed_dim=256,\n                      depth=[1, 10, 14, 3], num_heads=[2, 4, 8, 16], mlp_ratio=4., qk_scale=-0.5,\n                      group=8, attn_stage=\"0011\", spatial_conv=\"1100\", conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.visformer_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get visformer tiny model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>@register_model\ndef visformer_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n\"\"\"Get visformer tiny model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_tiny\"]\n    model = Visformer(img_size=224, init_channels=16, num_classes=num_classes, embed_dim=192,\n                      depth=[0, 7, 4, 4], num_heads=[3, 3, 3, 3], mlp_ratio=4., group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", drop_path_rate=0.03, conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.visformer.visformer_tiny_v2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get visformer tiny2 model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\visformer.py</code> <pre><code>@register_model\ndef visformer_tiny_v2(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n\"\"\"Get visformer tiny2 model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_tiny_v2\"]\n    model = Visformer(img_size=224, init_channels=24, num_classes=num_classes, embed_dim=192,\n                      depth=[1, 4, 6, 3], num_heads=[1, 3, 6, 12], mlp_ratio=4., qk_scale=-0.5, group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", drop_path_rate=0.03, conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit</code> \u00b6 <p>ViT</p> <code>mindocr.models.backbones.mindcv_models.vit.Attention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Attention layer implementation, Rearrange Input -&gt; B x N x hidden size.</p> PARAMETER DESCRIPTION <code>dim</code> <p>The dimension of input features.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>The number of attention heads. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>keep_prob</code> <p>The keep rate, greater than 0 and less equal than 1. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>attention_keep_prob</code> <p>The keep rate for attention. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = Attention(768, 12)\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class Attention(nn.Cell):\n\"\"\"\n    Attention layer implementation, Rearrange Input -&gt; B x N x hidden size.\n\n    Args:\n        dim (int): The dimension of input features.\n        num_heads (int): The number of attention heads. Default: 8.\n        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n        attention_keep_prob (float): The keep rate for attention. Default: 1.0.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = Attention(768, 12)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        keep_prob: float = 1.0,\n        attention_keep_prob: float = 1.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = Tensor(head_dim**-0.5)\n\n        self.qkv = nn.Dense(dim, dim * 3)\n        self.attn_drop = nn.Dropout(attention_keep_prob)\n        self.out = nn.Dense(dim, dim)\n        self.out_drop = nn.Dropout(keep_prob)\n\n        self.mul = ops.Mul()\n        self.reshape = ops.Reshape()\n        self.transpose = ops.Transpose()\n        self.unstack = ops.Unstack(axis=0)\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n        self.softmax = nn.Softmax(axis=-1)\n\n    def construct(self, x):\n\"\"\"Attention construct.\"\"\"\n        b, n, c = x.shape\n        qkv = self.qkv(x)\n        qkv = self.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n        qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = self.unstack(qkv)\n\n        attn = self.q_matmul_k(q, k)\n        attn = self.mul(attn, self.scale)\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        out = self.attn_matmul_v(attn, v)\n        out = self.transpose(out, (0, 2, 1, 3))\n        out = self.reshape(out, (b, n, c))\n        out = self.out(out)\n        out = self.out_drop(out)\n\n        return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.Attention.construct(x)</code> \u00b6 <p>Attention construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"Attention construct.\"\"\"\n    b, n, c = x.shape\n    qkv = self.qkv(x)\n    qkv = self.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n    qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n    q, k, v = self.unstack(qkv)\n\n    attn = self.q_matmul_k(q, k)\n    attn = self.mul(attn, self.scale)\n    attn = self.softmax(attn)\n    attn = self.attn_drop(attn)\n\n    out = self.attn_matmul_v(attn, v)\n    out = self.transpose(out, (0, 2, 1, 3))\n    out = self.reshape(out, (b, n, c))\n    out = self.out(out)\n    out = self.out_drop(out)\n\n    return out\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.BaseClassifier</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>generate classifier to combine the backbone and head</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class BaseClassifier(nn.Cell):\n\"\"\"\n    generate classifier to combine the backbone and head\n    \"\"\"\n\n    def __init__(self, backbone, neck=None, head=None):\n        super().__init__()\n        self.backbone = backbone\n        if neck:\n            self.neck = neck\n            self.with_neck = True\n        else:\n            self.with_neck = False\n        if head:\n            self.head = head\n            self.with_head = True\n        else:\n            self.with_head = False\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.backbone(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.head(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        if self.with_neck:\n            x = self.neck(x)\n        if self.with_head:\n            x = self.forward_head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.DenseHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>LinearClsHead architecture.</p> PARAMETER DESCRIPTION <code>input_channel</code> <p>The number of input channel.</p> <p> TYPE: <code>int</code> </p> <code>num_classes</code> <p>Number of classes.</p> <p> TYPE: <code>int</code> </p> <code>has_bias</code> <p>Specifies whether the layer uses a bias vector. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>activation</code> <p>activate function applied to the output. Eg. <code>ReLU</code>. Default: None.</p> <p> TYPE: <code>Union[str, Cell, Primitive]</code> DEFAULT: <code>None</code> </p> <code>keep_prob</code> <p>Dropout keeping rate, between [0, 1]. E.g. rate=0.9, means dropping out 10% of input. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class DenseHead(nn.Cell):\n\"\"\"\n    LinearClsHead architecture.\n\n    Args:\n        input_channel (int): The number of input channel.\n        num_classes (int): Number of classes.\n        has_bias (bool): Specifies whether the layer uses a bias vector. Default: True.\n        activation (Union[str, Cell, Primitive]): activate function applied to the output. Eg. `ReLU`. Default: None.\n        keep_prob (float): Dropout keeping rate, between [0, 1]. E.g. rate=0.9, means dropping out 10% of input.\n            Default: 1.0.\n\n    Returns:\n        Tensor, output tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_channel: int,\n        num_classes: int,\n        has_bias: bool = True,\n        activation: Optional[Union[str, nn.Cell]] = None,\n        keep_prob: float = 1.0,\n    ) -&gt; None:\n        super().__init__()\n\n        self.dropout = nn.Dropout(keep_prob)\n        self.classifier = nn.Dense(input_channel, num_classes, has_bias=has_bias, activation=activation)\n\n    def construct(self, x):\n        if self.training:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.DropPath</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class DropPath(nn.Cell):\n\"\"\"\n    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, keep_prob=None, seed=0):\n        super().__init__()\n        self.keep_prob = 1 - keep_prob\n        seed = min(seed, 0)\n        self.rand = P.UniformReal(seed=seed)\n        self.shape = P.Shape()\n        self.floor = P.Floor()\n\n    def construct(self, x):\n        if self.training:\n            x_shape = self.shape(x)\n            random_tensor = self.rand((x_shape[0], 1, 1))\n            random_tensor = random_tensor + self.keep_prob\n            random_tensor = self.floor(random_tensor)\n            x = x / self.keep_prob\n            x = x * random_tensor\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.FeedForward</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Feed Forward layer implementation.</p> PARAMETER DESCRIPTION <code>in_features</code> <p>The dimension of input features.</p> <p> TYPE: <code>int</code> </p> <code>hidden_features</code> <p>The dimension of hidden features. Default: None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>out_features</code> <p>The dimension of output features. Default: None</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function which will be stacked on top of the</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.GELU</code> </p> <code>normalization</code> <p>nn.GELU.</p> <p> TYPE: <code>layer (if not None), otherwise on top of the conv layer. Default</code> </p> <code>keep_prob</code> <p>The keep rate, greater than 0 and less equal than 1. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = FeedForward(768, 3072)\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class FeedForward(nn.Cell):\n\"\"\"\n    Feed Forward layer implementation.\n\n    Args:\n        in_features (int): The dimension of input features.\n        hidden_features (int): The dimension of hidden features. Default: None.\n        out_features (int): The dimension of output features. Default: None\n        activation (nn.Cell): Activation function which will be stacked on top of the\n        normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = FeedForward(768, 3072)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        activation: nn.Cell = nn.GELU,\n        keep_prob: float = 1.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.dense1 = nn.Dense(in_features, hidden_features)\n        self.activation = activation()\n        self.dense2 = nn.Dense(hidden_features, out_features)\n        self.dropout = nn.Dropout(keep_prob)\n\n    def construct(self, x):\n\"\"\"Feed Forward construct.\"\"\"\n        x = self.dense1(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.dense2(x)\n        x = self.dropout(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.FeedForward.construct(x)</code> \u00b6 <p>Feed Forward construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"Feed Forward construct.\"\"\"\n    x = self.dense1(x)\n    x = self.activation(x)\n    x = self.dropout(x)\n    x = self.dense2(x)\n    x = self.dropout(x)\n\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.MultilayerDenseHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>MultilayerDenseHead architecture.</p> PARAMETER DESCRIPTION <code>input_channel</code> <p>The number of input channel.</p> <p> TYPE: <code>int</code> </p> <code>num_classes</code> <p>Number of classes.</p> <p> TYPE: <code>int</code> </p> <code>mid_channel</code> <p>Number of channels in the hidden fc layers.</p> <p> TYPE: <code>list</code> </p> <code>keep_prob</code> <p>Dropout keeping rate, between [0, 1]. E.g. rate=0.9, means dropping out 10% of</p> <p> TYPE: <code>list</code> </p> <code>activation</code> <p>activate function applied to the output. Eg. <code>ReLU</code>.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class MultilayerDenseHead(nn.Cell):\n\"\"\"\n    MultilayerDenseHead architecture.\n\n    Args:\n        input_channel (int): The number of input channel.\n        num_classes (int): Number of classes.\n        mid_channel (list): Number of channels in the hidden fc layers.\n        keep_prob (list): Dropout keeping rate, between [0, 1]. E.g. rate=0.9, means dropping out 10% of\n        input.\n        activation (list): activate function applied to the output. Eg. `ReLU`.\n\n    Returns:\n        Tensor, output tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_channel: int,\n        num_classes: int,\n        mid_channel: List[int],\n        keep_prob: List[float],\n        activation: List[Optional[Union[str, nn.Cell]]],\n    ) -&gt; None:\n        super().__init__()\n        mid_channel.append(num_classes)\n        assert len(mid_channel) == len(activation) == len(keep_prob), \"The length of the list should be the same.\"\n\n        length = len(activation)\n        head = []\n\n        for i in range(length):\n            linear = DenseHead(\n                input_channel,\n                mid_channel[i],\n                activation=activation[i],\n                keep_prob=keep_prob[i],\n            )\n            head.append(linear)\n            input_channel = mid_channel[i]\n\n        self.classifier = nn.SequentialCell(head)\n\n    def construct(self, x):\n        x = self.classifier(x)\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.PatchEmbedding</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Path embedding layer for ViT. First rearrange b c (h p) (w p) -&gt; b (h w) (p p c).</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Input image size. Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch size of image. Default: 16.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>embed_dim</code> <p>The dimension of embedding. Default: 768.</p> <p> TYPE: <code>int</code> DEFAULT: <code>768</code> </p> <code>input_channels</code> <p>The number of input channel. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = PathEmbedding(224, 16, 768, 3)\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class PatchEmbedding(nn.Cell):\n\"\"\"\n    Path embedding layer for ViT. First rearrange b c (h p) (w p) -&gt; b (h w) (p p c).\n\n    Args:\n        image_size (int): Input image size. Default: 224.\n        patch_size (int): Patch size of image. Default: 16.\n        embed_dim (int): The dimension of embedding. Default: 768.\n        input_channels (int): The number of input channel. Default: 3.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = PathEmbedding(224, 16, 768, 3)\n    \"\"\"\n\n    MIN_NUM_PATCHES = 4\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 16,\n        embed_dim: int = 768,\n        input_channels: int = 3,\n    ):\n        super().__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n        self.reshape = ops.Reshape()\n        self.transpose = ops.Transpose()\n\n    def construct(self, x):\n\"\"\"Path Embedding construct.\"\"\"\n        x = self.conv(x)\n        b, c, h, w = x.shape\n        x = self.reshape(x, (b, c, h * w))\n        x = self.transpose(x, (0, 2, 1))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.PatchEmbedding.construct(x)</code> \u00b6 <p>Path Embedding construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"Path Embedding construct.\"\"\"\n    x = self.conv(x)\n    b, c, h, w = x.shape\n    x = self.reshape(x, (b, c, h * w))\n    x = self.transpose(x, (0, 2, 1))\n\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.ResidualCell</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Cell which implements Residual function:</p> \\[output = x + f(x)\\] PARAMETER DESCRIPTION <code>cell</code> <p>Cell needed to add residual block.</p> <p> TYPE: <code>Cell</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = ResidualCell(nn.Dense(3,4))\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class ResidualCell(nn.Cell):\n\"\"\"\n    Cell which implements Residual function:\n\n    $$output = x + f(x)$$\n\n    Args:\n        cell (Cell): Cell needed to add residual block.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = ResidualCell(nn.Dense(3,4))\n    \"\"\"\n\n    def __init__(self, cell):\n        super().__init__()\n        self.cell = cell\n\n    def construct(self, x):\n\"\"\"ResidualCell construct.\"\"\"\n        return self.cell(x) + x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.ResidualCell.construct(x)</code> \u00b6 <p>ResidualCell construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"ResidualCell construct.\"\"\"\n    return self.cell(x) + x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.TransformerEncoder</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>TransformerEncoder implementation.</p> PARAMETER DESCRIPTION <code>dim</code> <p>The dimension of embedding.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>The depth of transformer.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>The number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>mlp_dim</code> <p>The dimension of MLP hidden layer.</p> <p> TYPE: <code>int</code> </p> <code>keep_prob</code> <p>The keep rate, greater than 0 and less equal than 1. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>attention_keep_prob</code> <p>The keep rate for attention. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>drop_path_keep_prob</code> <p>The keep rate for drop path. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>activation</code> <p>Activation function which will be stacked on top of the</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.GELU</code> </p> <code>normalization</code> <p>nn.GELU.</p> <p> TYPE: <code>layer (if not None), otherwise on top of the conv layer. Default</code> </p> <code>norm</code> <p>Norm layer that will be stacked on top of the convolution</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.LayerNorm</code> </p> <code>layer.</code> <p>nn.LayerNorm.</p> <p> TYPE: <code>Default</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = TransformerEncoder(768, 12, 12, 3072)\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class TransformerEncoder(nn.Cell):\n\"\"\"\n    TransformerEncoder implementation.\n\n    Args:\n        dim (int): The dimension of embedding.\n        num_layers (int): The depth of transformer.\n        num_heads (int): The number of attention heads.\n        mlp_dim (int): The dimension of MLP hidden layer.\n        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n        attention_keep_prob (float): The keep rate for attention. Default: 1.0.\n        drop_path_keep_prob (float): The keep rate for drop path. Default: 1.0.\n        activation (nn.Cell): Activation function which will be stacked on top of the\n        normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n        norm (nn.Cell, optional): Norm layer that will be stacked on top of the convolution\n        layer. Default: nn.LayerNorm.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = TransformerEncoder(768, 12, 12, 3072)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_layers: int,\n        num_heads: int,\n        mlp_dim: int,\n        keep_prob: float = 1.0,\n        attention_keep_prob: float = 1.0,\n        drop_path_keep_prob: float = 1.0,\n        activation: nn.Cell = nn.GELU,\n        norm: nn.Cell = nn.LayerNorm,\n    ):\n        super().__init__()\n        drop_path_rate = 1 - drop_path_keep_prob\n        dpr = [i.item() for i in np.linspace(0, drop_path_rate, num_layers)]\n        attn_seeds = [np.random.randint(1024) for _ in range(num_layers)]\n        mlp_seeds = [np.random.randint(1024) for _ in range(num_layers)]\n\n        layers = []\n        for i in range(num_layers):\n            normalization1 = norm((dim,))\n            normalization2 = norm((dim,))\n            attention = Attention(dim=dim,\n                                  num_heads=num_heads,\n                                  keep_prob=keep_prob,\n                                  attention_keep_prob=attention_keep_prob)\n\n            feedforward = FeedForward(in_features=dim,\n                                      hidden_features=mlp_dim,\n                                      activation=activation,\n                                      keep_prob=keep_prob)\n\n            if drop_path_rate &gt; 0:\n                layers.append(\n                    nn.SequentialCell([\n                        ResidualCell(nn.SequentialCell([normalization1,\n                                                        attention,\n                                                        DropPath(dpr[i], attn_seeds[i])])),\n                        ResidualCell(nn.SequentialCell([normalization2,\n                                                        feedforward,\n                                                        DropPath(dpr[i], mlp_seeds[i])]))]))\n            else:\n                layers.append(\n                    nn.SequentialCell([\n                        ResidualCell(nn.SequentialCell([normalization1,\n                                                        attention])),\n                        ResidualCell(nn.SequentialCell([normalization2,\n                                                        feedforward]))\n                    ])\n                )\n        self.layers = nn.SequentialCell(layers)\n\n    def construct(self, x):\n\"\"\"Transformer construct.\"\"\"\n        return self.layers(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.TransformerEncoder.construct(x)</code> \u00b6 <p>Transformer construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"Transformer construct.\"\"\"\n    return self.layers(x)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.ViT</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Vision Transformer architecture implementation.</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Input image size. Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>input_channels</code> <p>The number of input channel. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>patch_size</code> <p>Patch size of image. Default: 16.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>embed_dim</code> <p>The dimension of embedding. Default: 768.</p> <p> TYPE: <code>int</code> DEFAULT: <code>768</code> </p> <code>num_layers</code> <p>The depth of transformer. Default: 12.</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>num_heads</code> <p>The number of attention heads. Default: 12.</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>mlp_dim</code> <p>The dimension of MLP hidden layer. Default: 3072.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3072</code> </p> <code>keep_prob</code> <p>The keep rate, greater than 0 and less equal than 1. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>attention_keep_prob</code> <p>The keep rate for attention layer. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>drop_path_keep_prob</code> <p>The keep rate for drop path. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>activation</code> <p>Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.GELU</code> </p> <code>norm</code> <p>Norm layer that will be stacked on top of the convolution layer. Default: nn.LayerNorm.</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>nn.LayerNorm</code> </p> <code>pool</code> <p>The method of pooling. Default: 'cls'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cls'</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, 768)</code></p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>split</code> is not 'train', 'test' or 'infer'.</p> Supported Platforms <p><code>GPU</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = ViT()\n&gt;&gt;&gt; x = ms.Tensor(np.ones([1, 3, 224, 224]), ms.float32)\n&gt;&gt;&gt; output = net(x)\n&gt;&gt;&gt; print(output.shape)\n(1, 768)\n</code></pre> <p>About ViT:</p> <p>Vision Transformer (ViT) shows that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p> <p>Citation:</p> <p>.. code-block::</p> <pre><code>@article{2020An,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Houlsby, N.},\nyear={2020},\n}\n</code></pre> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>class ViT(nn.Cell):\n\"\"\"\n    Vision Transformer architecture implementation.\n\n    Args:\n        image_size (int): Input image size. Default: 224.\n        input_channels (int): The number of input channel. Default: 3.\n        patch_size (int): Patch size of image. Default: 16.\n        embed_dim (int): The dimension of embedding. Default: 768.\n        num_layers (int): The depth of transformer. Default: 12.\n        num_heads (int): The number of attention heads. Default: 12.\n        mlp_dim (int): The dimension of MLP hidden layer. Default: 3072.\n        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n        attention_keep_prob (float): The keep rate for attention layer. Default: 1.0.\n        drop_path_keep_prob (float): The keep rate for drop path. Default: 1.0.\n        activation (nn.Cell): Activation function which will be stacked on top of the\n            normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n        norm (nn.Cell, optional): Norm layer that will be stacked on top of the convolution\n            layer. Default: nn.LayerNorm.\n        pool (str): The method of pooling. Default: 'cls'.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, 768)`\n\n    Raises:\n        ValueError: If `split` is not 'train', 'test' or 'infer'.\n\n    Supported Platforms:\n        ``GPU``\n\n    Examples:\n        &gt;&gt;&gt; net = ViT()\n        &gt;&gt;&gt; x = ms.Tensor(np.ones([1, 3, 224, 224]), ms.float32)\n        &gt;&gt;&gt; output = net(x)\n        &gt;&gt;&gt; print(output.shape)\n        (1, 768)\n\n    About ViT:\n\n    Vision Transformer (ViT) shows that a pure transformer applied directly to sequences of image\n    patches can perform very well on image classification tasks. When pre-trained on large amounts\n    of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet,\n    CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art\n    convolutional networks while requiring substantially fewer computational resources to train.\n\n    Citation:\n\n    .. code-block::\n\n        @article{2020An,\n        title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n        author={Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Houlsby, N.},\n        year={2020},\n        }\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        input_channels: int = 3,\n        patch_size: int = 16,\n        embed_dim: int = 768,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        keep_prob: float = 1.0,\n        attention_keep_prob: float = 1.0,\n        drop_path_keep_prob: float = 1.0,\n        activation: nn.Cell = nn.GELU,\n        norm: Optional[nn.Cell] = nn.LayerNorm,\n        pool: str = \"cls\",\n    ) -&gt; None:\n        super().__init__()\n\n        # Validator.check_string(pool, [\"cls\", \"mean\"], \"pool type\")\n\n        self.patch_embedding = PatchEmbedding(image_size=image_size,\n                                              patch_size=patch_size,\n                                              embed_dim=embed_dim,\n                                              input_channels=input_channels)\n        num_patches = self.patch_embedding.num_patches\n\n        if pool == \"cls\":\n            self.cls_token = init(init_type=Normal(sigma=1.0),\n                                  shape=(1, 1, embed_dim),\n                                  dtype=ms.float32,\n                                  name=\"cls\",\n                                  requires_grad=True)\n            self.pos_embedding = init(init_type=Normal(sigma=1.0),\n                                      shape=(1, num_patches + 1, embed_dim),\n                                      dtype=ms.float32,\n                                      name=\"pos_embedding\",\n                                      requires_grad=True)\n            self.concat = ops.Concat(axis=1)\n        else:\n            self.pos_embedding = init(init_type=Normal(sigma=1.0),\n                                      shape=(1, num_patches, embed_dim),\n                                      dtype=ms.float32,\n                                      name=\"pos_embedding\",\n                                      requires_grad=True)\n            self.mean = ops.ReduceMean(keep_dims=False)\n\n        self.pool = pool\n        self.pos_dropout = nn.Dropout(keep_prob)\n        self.norm = norm((embed_dim,))\n        self.tile = ops.Tile()\n        self.transformer = TransformerEncoder(\n            dim=embed_dim,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            mlp_dim=mlp_dim,\n            keep_prob=keep_prob,\n            attention_keep_prob=attention_keep_prob,\n            drop_path_keep_prob=drop_path_keep_prob,\n            activation=activation,\n            norm=norm,\n        )\n\n    def construct(self, x):\n\"\"\"ViT construct.\"\"\"\n        x = self.patch_embedding(x)\n\n        if self.pool == \"cls\":\n            cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n            x = self.concat((cls_tokens, x))\n            x += self.pos_embedding\n        else:\n            x += self.pos_embedding\n        x = self.pos_dropout(x)\n        x = self.transformer(x)\n        x = self.norm(x)\n\n        if self.pool == \"cls\":\n            x = x[:, 0]\n        else:\n            x = self.mean(x, (1, 2))  # (1,) or (1,2)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.ViT.construct(x)</code> \u00b6 <p>ViT construct.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def construct(self, x):\n\"\"\"ViT construct.\"\"\"\n    x = self.patch_embedding(x)\n\n    if self.pool == \"cls\":\n        cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n        x = self.concat((cls_tokens, x))\n        x += self.pos_embedding\n    else:\n        x += self.pos_embedding\n    x = self.pos_dropout(x)\n    x = self.transformer(x)\n    x = self.norm(x)\n\n    if self.pool == \"cls\":\n        x = x[:, 0]\n    else:\n        x = self.mean(x, (1, 2))  # (1,) or (1,2)\n    return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit(image_size, input_channels, patch_size, embed_dim, num_layers, num_heads, num_classes, mlp_dim, dropout=0.0, attention_dropout=0.0, drop_path_rate=0.0, activation=nn.GELU, norm=nn.LayerNorm, pool='cls', representation_size=None, pretrained=False, url_cfg=None)</code> \u00b6 <p>Vision Transformer architecture.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>def vit(\n    image_size: int,\n    input_channels: int,\n    patch_size: int,\n    embed_dim: int,\n    num_layers: int,\n    num_heads: int,\n    num_classes: int,\n    mlp_dim: int,\n    dropout: float = 0.0,\n    attention_dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n    activation: nn.Cell = nn.GELU,\n    norm: nn.Cell = nn.LayerNorm,\n    pool: str = \"cls\",\n    representation_size: Optional[int] = None,\n    pretrained: bool = False,\n    url_cfg: dict = None,\n) -&gt; ViT:\n\"\"\"Vision Transformer architecture.\"\"\"\n    backbone = ViT(\n        image_size=image_size,\n        input_channels=input_channels,\n        patch_size=patch_size,\n        embed_dim=embed_dim,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        mlp_dim=mlp_dim,\n        keep_prob=1.0 - dropout,\n        attention_keep_prob=1.0 - attention_dropout,\n        drop_path_keep_prob=1.0 - drop_path_rate,\n        activation=activation,\n        norm=norm,\n        pool=pool,\n    )\n    if representation_size:\n        head = MultilayerDenseHead(\n            input_channel=embed_dim,\n            num_classes=num_classes,\n            mid_channel=[representation_size],\n            activation=[\"tanh\", None],\n            keep_prob=[1.0, 1.0],\n        )\n    else:\n        head = DenseHead(input_channel=embed_dim, num_classes=num_classes)\n\n    model = BaseClassifier(backbone=backbone, head=head)\n\n    if pretrained:\n        # Download the pre-trained checkpoint file from url, and load ckpt file.\n        load_pretrained(model, url_cfg, num_classes=num_classes, in_channels=input_channels)\n\n    return model\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_b_16_224(pretrained=False, num_classes=1000, in_channels=3, image_size=224, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>Constructs a vit_b_16 architecture from <code>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale &lt;https://arxiv.org/abs/2010.11929&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether to download and load the pre-trained model. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The number of classification. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>image_size</code> <p>The input image size. Default: 224 for ImageNet.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>has_logits</code> <p>Whether has logits or not. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>drop_rate</code> <p>The drop out rate. Default: 0.0.s</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>The stochastic depth rate. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> RETURNS DESCRIPTION <code>ViT</code> <p>ViT network, MindSpore.nn.Cell</p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = vit_b_16_224()\n&gt;&gt;&gt; x = ms.Tensor(np.ones([1, 3, 224, 224]), ms.float32)\n&gt;&gt;&gt; output = net(x)\n&gt;&gt;&gt; print(output.shape)\n(1, 1000)\n</code></pre> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code></p> Supported Platforms <p><code>GPU</code></p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_b_16_224(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 224,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"\n    Constructs a vit_b_16 architecture from\n    `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale &lt;https://arxiv.org/abs/2010.11929&gt;`_.\n\n    Args:\n        pretrained (bool): Whether to download and load the pre-trained model. Default: False.\n        num_classes (int): The number of classification. Default: 1000.\n        in_channels (int): The number of input channels. Default: 3.\n        image_size (int): The input image size. Default: 224 for ImageNet.\n        has_logits (bool): Whether has logits or not. Default: False.\n        drop_rate (float): The drop out rate. Default: 0.0.s\n        drop_path_rate (float): The stochastic depth rate. Default: 0.0.\n\n    Returns:\n        ViT network, MindSpore.nn.Cell\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Examples:\n        &gt;&gt;&gt; net = vit_b_16_224()\n        &gt;&gt;&gt; x = ms.Tensor(np.ones([1, 3, 224, 224]), ms.float32)\n        &gt;&gt;&gt; output = net(x)\n        &gt;&gt;&gt; print(output.shape)\n        (1, 1000)\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`\n\n    Supported Platforms:\n        ``GPU``\n    \"\"\"\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 16\n    config.embed_dim = 768\n    config.mlp_dim = 3072\n    config.num_heads = 12\n    config.num_layers = 12\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.pretrained = pretrained\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.representation_size = 768 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_b_16_224\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_b_16_384(pretrained=False, num_classes=1000, in_channels=3, image_size=384, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_b_16_384(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 384,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 16\n    config.embed_dim = 768\n    config.mlp_dim = 3072\n    config.num_heads = 12\n    config.num_layers = 12\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.pretrained = pretrained\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.representation_size = 768 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_b_16_384\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_b_32_224(pretrained=False, num_classes=1000, in_channels=3, image_size=224, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_b_32_224(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 224,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 32\n    config.embed_dim = 768\n    config.mlp_dim = 3072\n    config.num_heads = 12\n    config.num_layers = 12\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.pretrained = pretrained\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.representation_size = 768 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_b_32_224\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_b_32_384(pretrained=False, num_classes=1000, in_channels=3, image_size=384, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_b_32_384(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 384,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention_dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 32\n    config.embed_dim = 768\n    config.mlp_dim = 3072\n    config.num_heads = 12\n    config.num_layers = 12\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention_dropout\n    config.drop_path_rate = drop_path_rate\n    config.pretrained = pretrained\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.representation_size = 768 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_b_32_384\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_l_16_224(pretrained=False, num_classes=1000, in_channels=3, image_size=224, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_l_16_224(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 224,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 16\n    config.embed_dim = 1024\n    config.mlp_dim = 4096\n    config.num_heads = 16\n    config.num_layers = 24\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.pretrained = pretrained\n    config.representation_size = 1024 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_l_16_224\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_l_16_384(pretrained=False, num_classes=1000, in_channels=3, image_size=384, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_l_16_384(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 384,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 16\n    config.embed_dim = 1024\n    config.mlp_dim = 4096\n    config.num_heads = 16\n    config.num_layers = 24\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.pretrained = pretrained\n    config.representation_size = 1024 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_l_16_384\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.vit.vit_l_32_224(pretrained=False, num_classes=1000, in_channels=3, image_size=224, has_logits=False, drop_rate=0.0, drop_path_rate=0.0)</code> \u00b6 <p>construct and return a ViT network</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\vit.py</code> <pre><code>@register_model\ndef vit_l_32_224(\n    pretrained: bool = False,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    image_size: int = 224,\n    has_logits: bool = False,\n    drop_rate: float = 0.0,\n    # attention-dropout: float = 0.0,\n    drop_path_rate: float = 0.0,\n) -&gt; ViT:\n\"\"\"construct and return a ViT network\"\"\"\n    config = ConfigDict()\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.patch_size = 32\n    config.embed_dim = 1024\n    config.mlp_dim = 4096\n    config.num_heads = 16\n    config.num_layers = 24\n    config.dropout = drop_rate\n    config.attention_dropout = drop_rate  # attention-dropout\n    config.drop_path_rate = drop_path_rate\n    config.pretrained = pretrained\n    config.input_channels = in_channels\n    config.pool = \"cls\"\n    config.representation_size = 1024 if has_logits else None\n\n    config.url_cfg = default_cfgs[\"vit_l_32_224\"]\n\n    return vit(**config)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit</code> \u00b6 <p>MindSpore implementation of XCiT Refer to: XCiT: Cross-Covariance Image Transformers</p> <code>mindocr.models.backbones.mindcv_models.xcit.ClassAttention</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class ClassAttention(nn.Cell):\n\"\"\"Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Dense(\n            in_channels=dim, out_channels=dim * 3, has_bias=qkv_bias)\n        self.attn_drop = nn.Dropout(keep_prob=1 - attn_drop)\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim)\n        self.proj_drop = nn.Dropout(keep_prob=1 - proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.attn_matmul_v = ops.BatchMatMul()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, N, C = x.shape\n\n        qkv = self.qkv(x)\n        qkv = ops.reshape(qkv, (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n        qc = q[:, :, 0:1]\n        attn_cls = (qc * k).sum(-1) * self.scale\n        attn_cls = self.softmax(attn_cls)\n        attn_cls = self.attn_drop(attn_cls)\n\n        attn_cls = ops.expand_dims(attn_cls, 2)\n        cls_tkn = self.attn_matmul_v(attn_cls, v)\n        cls_tkn = ops.transpose(cls_tkn, (0, 2, 1, 3))\n        cls_tkn = ops.reshape(cls_tkn, (B, 1, C))\n        cls_tkn = self.proj(cls_tkn)\n        x = ops.concat((self.proj_drop(cls_tkn), x[:, 1:]), axis=1)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.ClassAttentionBlock</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class ClassAttentionBlock(nn.Cell):\n\"\"\"Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239\n    \"\"\"\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0.,\n                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, eta=None,\n                 tokens_norm=False):\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n        self.attn = ClassAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop,\n            proj_drop=drop\n        )\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path &gt; 0. else ops.Identity()\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer,\n                       drop=drop)\n\n        # LayerScale Initialization (no layerscale when None)\n        if eta is not None:\n            self.gamma1 = Parameter(\n                eta * ops.Ones()((dim), mstype.float32), requires_grad=True)\n            self.gamma2 = Parameter(\n                eta * ops.Ones()((dim), mstype.float32), requires_grad=True)\n        else:\n            self.gamma1, self.gamma2 = 1.0, 1.0\n\n        # FIXME: A hack for models pre-trained with layernorm over all the tokens not just the CLS\n        self.tokens_norm = tokens_norm\n\n    def construct(self, x, H, W, mask=None):\n\n        x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x)))\n\n        if self.tokens_norm:\n            x = self.norm2(x)\n        else:\n            x[:, 0:1] = self.norm2(x[:, 0:1])\n        x_res = x\n        cls_token = x[:, 0:1]\n        cls_token = self.gamma2 * self.mlp(cls_token)\n        x = ops.concat((cls_token, x[:, 1:]), axis=1)\n        x = x_res + self.drop_path(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.ConvPatchEmbed</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Image to Patch Embedding using multiple convolutional layers</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class ConvPatchEmbed(nn.Cell):\n\"\"\" Image to Patch Embedding using multiple convolutional layers\n    \"\"\"\n\n    def __init__(self,\n                 img_size: int = 224,\n                 patch_size: int = 16,\n                 in_chans: int = 3,\n                 embed_dim: int = 768\n                 ) -&gt; None:\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * \\\n            (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        if patch_size[0] == 16:\n            self.proj = nn.SequentialCell([\n                conv3x3(3, embed_dim // 8, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 8, embed_dim // 4, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            ])\n        elif patch_size[0] == 8:\n            self.proj = nn.SequentialCell([\n                conv3x3(3, embed_dim // 4, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            ])\n        else:\n            raise ValueError(\n                \"For convolutional projection, patch size has to be in [8, 16]\")\n\n    def construct(self, x, padding_size=None) -&gt; Tensor:\n        x = self.proj(x)\n        B, C, Hp, Wp = x.shape\n        x = ops.reshape(x, (B, C, Hp * Wp))\n        x = x.transpose(0, 2, 1)\n\n        return x, (Hp, Wp)\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.LPI</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Local Patch Interaction module that allows explicit communication between tokens in 3x3 windows to augment the implicit communcation performed by the block diagonal scatter attention. Implemented using 2 layers of separable 3x3 convolutions with GeLU and BatchNorm2d</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class LPI(nn.Cell):\n\"\"\"\n    Local Patch Interaction module that allows explicit communication between tokens in 3x3 windows\n    to augment the implicit communcation performed by the block diagonal scatter attention.\n    Implemented using 2 layers of separable 3x3 convolutions with GeLU and BatchNorm2d\n    \"\"\"\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,\n                 drop=0., kernel_size=3) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n\n        padding = kernel_size // 2\n\n        self.conv1 = nn.Conv2d(in_features, out_features, kernel_size=kernel_size,\n                               padding=padding, pad_mode='pad', group=out_features, has_bias=True)\n        self.act = act_layer()\n        self.bn = nn.BatchNorm2d(in_features, use_batch_statistics=True)\n        self.conv2 = nn.Conv2d(in_features, out_features, kernel_size=kernel_size,\n                               padding=padding, pad_mode='pad', group=out_features, has_bias=True)\n\n    def construct(self, x, H, W) -&gt; Tensor:\n        B, N, C = x.shape\n        x = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n        x = self.conv1(x)\n        x = self.act(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = ops.transpose(ops.reshape(x, (B, C, N)), (0, 2, 1))\n\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.PositionalEncodingFourier</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Positional encoding relying on a fourier kernel matching the one used in the \"Attention is all of Need\" paper. The implementation builds on DeTR code https://github.com/facebookresearch/detr/blob/master/models/position_encoding.py</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class PositionalEncodingFourier(nn.Cell):\n\"\"\"\n    Positional encoding relying on a fourier kernel matching the one used in the\n    \"Attention is all of Need\" paper. The implementation builds on DeTR code\n    https://github.com/facebookresearch/detr/blob/master/models/position_encoding.py\n    \"\"\"\n\n    def __init__(self,\n                 hidden_dim: int = 32,\n                 dim: int = 768,\n                 temperature=10000\n                 ) -&gt; None:\n        super().__init__()\n        self.token_projection = nn.Conv2d(\n            hidden_dim * 2, dim, kernel_size=1, has_bias=True)\n        self.scale = 2 * np.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim\n\n    def construct(self, B, H, W) -&gt; Tensor:\n        mask = Tensor(np.zeros((B, H, W)).astype(bool))\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=mstype.float32)\n        x_embed = not_mask.cumsum(2, dtype=mstype.float32)\n        eps = 1e-6\n        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = numpy.arange(self.hidden_dim, dtype=mstype.float32)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.hidden_dim)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = ops.stack((ops.sin(pos_x[:, :, :, 0::2]),\n                           ops.cos(pos_x[:, :, :, 1::2])), 4)\n        x1, x2, x3, x4, x5 = pos_x.shape\n        pos_x = ops.reshape(pos_x, (x1, x2, x3, x4 * x5))\n        pos_y = ops.stack((ops.sin(pos_y[:, :, :, 0::2]),\n                           ops.cos(pos_y[:, :, :, 1::2])), 4)\n        y1, y2, y3, y4, y5 = pos_y.shape\n        pos_y = ops.reshape(pos_y, (y1, y2, y3, y4 * y5))\n        pos = ops.transpose(ops.concat((pos_y, pos_x), 3), (0, 3, 1, 2))\n        pos = self.token_projection(pos)\n        return pos\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.XCA</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Cross-Covariance Attention (XCA) operation where the channels are updated using a weighted  sum. The weights are obtained from the (softmax normalized) Cross-covariance matrix (Q^T K \\in d_h \\times d_h)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class XCA(nn.Cell):\n\n\"\"\" Cross-Covariance Attention (XCA) operation where the channels are updated using a weighted\n     sum. The weights are obtained from the (softmax normalized) Cross-covariance\n    matrix (Q^T K \\\\in d_h \\\\times d_h)\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = Parameter(\n            ops.Ones()((num_heads, 1, 1), mstype.float32))\n        self.qkv = nn.Dense(\n            in_channels=dim, out_channels=dim * 3, has_bias=qkv_bias)\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n        self.softmax = nn.Softmax(axis=-1)\n        self.attn_drop = nn.Dropout(keep_prob=1.0 - attn_drop)\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim)\n        self.proj_drop = nn.Dropout(keep_prob=1.0 - proj_drop)\n\n    def construct(self, x):\n        B, N, C = x.shape\n\n        qkv = ops.reshape(\n            self.qkv(x), (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n\n        q = ops.transpose(q, (0, 1, 3, 2))\n        k = ops.transpose(k, (0, 1, 3, 2))\n        v = ops.transpose(v, (0, 1, 3, 2))\n\n        attn = self.q_matmul_k(q, k) * self.temperature\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = self.attn_matmul_v(attn, v)\n        x = ops.transpose(x, (0, 3, 1, 2))\n        x = ops.reshape(x, (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.XCiT</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>XCiT model class, based on <code>\"XCiT: Cross-Covariance Image Transformers\" &lt;https://arxiv.org/abs/2106.09681&gt;</code>_</p> PARAMETER DESCRIPTION <code>img_size</code> <p>input image size</p> <p> TYPE: <code>int, tuple</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>patch size</p> <p> TYPE: <code>int, tuple</code> DEFAULT: <code>16</code> </p> <code>in_chans</code> <p>number of input channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classes for classification head</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>embedding dimension</p> <p> TYPE: <code>int</code> DEFAULT: <code>768</code> </p> <code>depth</code> <p>depth of transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>num_heads</code> <p>number of attention heads</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>mlp_ratio</code> <p>ratio of mlp hidden dim to embedding dim</p> <p> TYPE: <code>int</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>enable bias for qkv if True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>override default qk scale of head_dim ** -0.5 if set</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>drop_rate</code> <p>dropout rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>attention dropout rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>stochastic depth rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>norm_layer</code> <p>(nn.Module): normalization layer</p> <p> TYPE: <code>nn.Cell</code> DEFAULT: <code>None</code> </p> <code>cls_attn_layers</code> <p>(int) Depth of Class attention layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>use_pos</code> <p>(bool) whether to use positional encoding</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>eta</code> <p>(float) layerscale initialization value</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>tokens_norm</code> <p>(bool) Whether to normalize all tokens or just the cls_token in the CA</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>class XCiT(nn.Cell):\nr\"\"\"XCiT model class, based on\n    `\"XCiT: Cross-Covariance Image Transformers\" &lt;https://arxiv.org/abs/2106.09681&gt;`_\n    Args:\n        img_size (int, tuple): input image size\n        patch_size (int, tuple): patch size\n        in_chans (int): number of input channels\n        num_classes (int): number of classes for classification head\n        embed_dim (int): embedding dimension\n        depth (int): depth of transformer\n        num_heads (int): number of attention heads\n        mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n        qkv_bias (bool): enable bias for qkv if True\n        qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n        drop_rate (float): dropout rate\n        attn_drop_rate (float): attention dropout rate\n        drop_path_rate (float): stochastic depth rate\n        norm_layer: (nn.Module): normalization layer\n        cls_attn_layers: (int) Depth of Class attention layers\n        use_pos: (bool) whether to use positional encoding\n        eta: (float) layerscale initialization value\n        tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA\n    \"\"\"\n\n    def __init__(self,\n                 img_size: int = 224,\n                 patch_size: int = 16,\n                 in_chans: int = 3,\n                 num_classes: int = 1000,\n                 embed_dim: int = 768,\n                 depth: int = 12,\n                 num_heads: int = 12,\n                 mlp_ratio: int = 4.,\n                 qkv_bias: bool = True,\n                 qk_scale: float = None,\n                 drop_rate: float = 0.,\n                 attn_drop_rate: float = 0.,\n                 drop_path_rate: float = 0.,\n                 norm_layer: nn.Cell = None,\n                 cls_attn_layers: int = 2,\n                 use_pos: bool = True,\n                 patch_proj: str = 'linear',\n                 eta: float = None,\n                 tokens_norm: bool = False):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        norm_layer = norm_layer or partial(nn.LayerNorm, epsilon=1e-6)\n\n        self.patch_embed = ConvPatchEmbed(img_size=img_size, embed_dim=embed_dim,\n                                          patch_size=patch_size)\n\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = Parameter(\n            ops.zeros((1, 1, embed_dim), mstype.float32))\n        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n\n        dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.CellList([\n            XCABlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                norm_layer=norm_layer, num_tokens=num_patches, eta=eta)\n            for i in range(depth)])\n\n        self.cls_attn_blocks = nn.CellList([\n            ClassAttentionBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer,\n                eta=eta, tokens_norm=tokens_norm)\n            for i in range(cls_attn_layers)])\n        self.norm = norm_layer([embed_dim])\n        self.head = nn.Dense(\n            in_channels=embed_dim, out_channels=num_classes) if num_classes &gt; 0 else ops.Identity()\n\n        self.pos_embeder = PositionalEncodingFourier(dim=embed_dim)\n        self.use_pos = use_pos\n\n        # Classifier head\n        self.cls_token.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                        self.cls_token.shape,\n                                                        self.cls_token.dtype))\n        self._init_weights()\n\n    def _init_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Dense):\n                m.weight = weight_init.initializer(weight_init.TruncatedNormal(\n                    sigma=0.02), m.weight.shape, mindspore.float32)\n                if m.bias is not None:\n                    m.bias.set_data(weight_init.initializer(\n                        weight_init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.LayerNorm):\n                m.beta.set_data(weight_init.initializer(\n                    weight_init.Constant(0), m.beta.shape))\n                m.gamma.set_data(weight_init.initializer(\n                    weight_init.Constant(1), m.gamma.shape))\n\n    def forward_features(self, x):\n        B, C, H, W = x.shape\n        x, (Hp, Wp) = self.patch_embed(x)\n        if self.use_pos:\n            pos_encoding = self.pos_embeder(B, Hp, Wp).reshape(\n                B, -1, x.shape[1]).transpose(0, 2, 1)\n            x = x + pos_encoding\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x, Hp, Wp)\n        cls_tokens = ops.broadcast_to(self.cls_token, (B, -1, -1))\n        cls_tokens = ops.cast(cls_tokens, x.dtype)\n        x = ops.concat((cls_tokens, x), 1)\n\n        for blk in self.cls_attn_blocks:\n            x = blk(x, Hp, Wp)\n        return self.norm(x)[:, 0]\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.conv3x3(in_planes, out_planes, stride=1)</code> \u00b6 <p>3x3 convolution with padding</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>def conv3x3(in_planes, out_planes, stride=1):\n\"\"\"3x3 convolution with padding\"\"\"\n    return nn.SequentialCell([\n        nn.Conv2d(\n            in_planes, out_planes, kernel_size=3, stride=stride, padding=1, pad_mode='pad', has_bias=False\n        ),\n        nn.BatchNorm2d(out_planes, use_batch_statistics=True)\n    ])\n</code></pre> <code>mindocr.models.backbones.mindcv_models.xcit.xcit_tiny_12_p16(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code> \u00b6 <p>Get xcit_tiny_12_p16 model. Refer to the base class 'models.XCiT' for more details.</p> Source code in <code>mindocr\\models\\backbones\\mindcv_models\\xcit.py</code> <pre><code>@register_model\ndef xcit_tiny_12_p16(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; XCiT:\n\"\"\"Get xcit_tiny_12_p16 model.\n    Refer to the base class 'models.XCiT' for more details.\n    \"\"\"\n    default_cfg = default_cfgs['xcit_tiny_12_p16']\n    model = XCiT(\n        patch_size=16, num_classes=num_classes, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), eta=1.0, tokens_norm=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.backbones.mindcv_wrapper","title":"<code>mindocr.models.backbones.mindcv_wrapper</code>","text":"<code>mindocr.models.backbones.mindcv_wrapper.MindCVBackboneWrapper</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>It reuses the forward_features interface in mindcv models. Please check where the features are extracted.</p> <p>Note: text recognition models like CRNN expects output feature in shape [bs, c, h, w]. but some models in mindcv like ViT output features in shape [bs, c]. please check and pick accordingly.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether the model backbone is pretrained. Default; True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>checkpoint_path</code> <p>The path of checkpoint files. Default: \"\".</p> <p> TYPE: <code>str</code> </p> <code>features_only</code> <p>Output the features at different strides instead. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>out_indices</code> <p>The indicies of the output features when <code>features_only</code> is <code>True</code>.  Default: [0, 1, 2, 3, 4]</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[0, 1, 2, 3, 4]</code> </p> Example <p>network = MindCVBackboneWrapper('resnet50', pretrained=True)</p> Source code in <code>mindocr\\models\\backbones\\mindcv_wrapper.py</code> <pre><code>class MindCVBackboneWrapper(nn.Cell):\n'''\n    It reuses the forward_features interface in mindcv models. Please check where the features are extracted.\n\n    Note: text recognition models like CRNN expects output feature in shape [bs, c, h, w]. but some models in mindcv\n    like ViT output features in shape [bs, c]. please check and pick accordingly.\n\n    Args:\n        pretrained (bool): Whether the model backbone is pretrained. Default; True\n        checkpoint_path (str): The path of checkpoint files. Default: \"\".\n        features_only (bool): Output the features at different strides instead. Default: False\n        out_indices (list[int]): The indicies of the output features when `features_only` is `True`.\n             Default: [0, 1, 2, 3, 4]\n\n    Example:\n        network = MindCVBackboneWrapper('resnet50', pretrained=True)\n    '''\n\n    def __init__(self, name, pretrained=True, ckpt_path=None, features_only: bool = False,\n                 out_indices: List[int] = [0, 1, 2, 3, 4], **kwargs):\n        super().__init__()\n        self.features_only = features_only\n\n        model_name = name.replace('@mindcv', \"\").replace(\"mindcv.\", \"\")\n        network = mindcv_models.create_model(model_name, pretrained=pretrained, features_only=features_only,\n                                             out_indices=out_indices)\n        # for local checkpoint\n        if ckpt_path is not None:\n            checkpoint_param = load_checkpoint(ckpt_path)\n            load_param_into_net(network, checkpoint_param)\n\n        if not self.features_only:\n            if hasattr(network, 'classifier'):\n                del network.classifier  # remove the original header to avoid confusion\n\n            self.network = network\n            # probe to get out_channels\n            # network.eval()\n            # TODO: get image input size from default cfg\n            x = ms.Tensor(np.random.rand(2, 3, 224, 224), dtype=ms.float32)\n            h = network.forward_features(x)\n            h = ops.stop_gradient(h)\n            self.out_channels = h.shape[1]\n\n            print(f'INFO: Load MindCV Backbone {model_name}, the output features shape for input 224x224 is {h.shape}. '\n                  f'\\n\\tProbed out_channels : ', self.out_channels)\n        else:\n            self.network = network\n            self.out_channels = self.network.out_channels\n            print(f'INFO: Load MindCV Backbone {model_name} with feature index {out_indices}, '\n                  f'output channels: {self.out_channels}')\n\n    def construct(self, x):\n        if self.features_only:\n            features = self.network(x)\n            return features\n        else:\n            features = self.network.forward_features(x)\n            return [features]\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.backbones.rec_vgg","title":"<code>mindocr.models.backbones.rec_vgg</code>","text":"<code>mindocr.models.backbones.rec_vgg.RecVGG</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>VGG Network structure</p> Source code in <code>mindocr\\models\\backbones\\rec_vgg.py</code> <pre><code>@register_backbone_class\nclass RecVGG(nn.Cell):\n\"\"\"VGG Network structure\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.conv1 = Conv(3, 64, use_bn=False, padding=1)\n        self.conv2 = Conv(64, 128, use_bn=False, padding=1)\n        self.conv3 = Conv(128, 256, use_bn=True, padding=1)\n        self.conv4 = Conv(256, 256, use_bn=False, padding=1)\n        self.conv5 = Conv(256, 512, use_bn=True, padding=1)\n        self.conv6 = Conv(512, 512, use_bn=False, padding=1)\n        self.conv7 = Conv(512, 512, kernel_size=2,\n                          pad_mode='valid', padding=0, use_bn=True)\n        self.maxpool2d1 = nn.MaxPool2d(\n            kernel_size=2, stride=2, pad_mode='same')\n        self.maxpool2d2 = nn.MaxPool2d(kernel_size=(\n            2, 1), stride=(2, 1), pad_mode='same')\n\n        self.out_channels = 512\n\n    def construct(self, x):\n        x = self.conv1(x)\n        x = self.maxpool2d1(x)\n        x = self.conv2(x)\n        x = self.maxpool2d1(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.maxpool2d2(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.maxpool2d2(x)\n        x = self.conv7(x)\n        return [x]\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.base_model","title":"<code>mindocr.models.base_model</code>","text":""},{"location":"reference/api_doc/#mindocr.models.base_model.BaseModel","title":"<code>mindocr.models.base_model.BaseModel</code>","text":"<p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\base_model.py</code> <pre><code>class BaseModel(nn.Cell):\n    def __init__(self, config: dict):\n\"\"\"\n        Args:\n            config (dict): model config\n\n        Inputs:\n            x (Tensor): The input tensor feeding into the backbone, neck and head sequentially.\n            y (Tensor): The extra input tensor. If it is provided, it will feed into the head. Default: None\n        \"\"\"\n        super(BaseModel, self).__init__()\n\n        config = Dict(config)\n\n        if config.transform:\n            transform_name = config.transform.pop('name')\n            self.transform = build_trans(transform_name, **config.transform)\n        else:\n            self.transform = None\n\n        backbone_name = config.backbone.pop('name')\n        self.backbone = build_backbone(backbone_name, **config.backbone)\n\n        assert hasattr(self.backbone, 'out_channels'), f'Backbones are required to provide out_channels attribute, ' \\\n                                                       f'but not found in {backbone_name}'\n\n        if 'neck' not in config or config.neck is None:\n            neck_name = 'Select'\n        else:\n            neck_name = config.neck.pop('name')\n        self.neck = build_neck(neck_name, in_channels=self.backbone.out_channels, **config.neck)\n\n        assert hasattr(self.neck, 'out_channels'), f'Necks are required to provide out_channels attribute, ' \\\n                                                   f'but not found in {neck_name}'\n\n        head_name = config.head.pop('name')\n        self.head = build_head(head_name, in_channels=self.neck.out_channels, **config.head)\n\n        self.model_name = f'{backbone_name}_{neck_name}_{head_name}'\n\n    def construct(self, x, y=None):\n        if self.transform is not None:\n            x = self.transform(x)\n\n        # TODO: return bout, hout for debugging, using a dict.\n        bout = self.backbone(x)\n\n        nout = self.neck(bout)\n\n        if y is not None:\n            hout = self.head(nout, y)\n        else:\n            hout = self.head(nout)\n\n        # resize back for postprocess\n        # y = F.interpolate(y, size=(H, W), mode='bilinear', align_corners=True)\n\n        return hout\n</code></pre> <code>mindocr.models.base_model.BaseModel.__init__(config)</code> \u00b6 PARAMETER DESCRIPTION <code>config</code> <p>model config</p> <p> TYPE: <code>dict</code> </p> Inputs <p>x (Tensor): The input tensor feeding into the backbone, neck and head sequentially. y (Tensor): The extra input tensor. If it is provided, it will feed into the head. Default: None</p> Source code in <code>mindocr\\models\\base_model.py</code> <pre><code>def __init__(self, config: dict):\n\"\"\"\n    Args:\n        config (dict): model config\n\n    Inputs:\n        x (Tensor): The input tensor feeding into the backbone, neck and head sequentially.\n        y (Tensor): The extra input tensor. If it is provided, it will feed into the head. Default: None\n    \"\"\"\n    super(BaseModel, self).__init__()\n\n    config = Dict(config)\n\n    if config.transform:\n        transform_name = config.transform.pop('name')\n        self.transform = build_trans(transform_name, **config.transform)\n    else:\n        self.transform = None\n\n    backbone_name = config.backbone.pop('name')\n    self.backbone = build_backbone(backbone_name, **config.backbone)\n\n    assert hasattr(self.backbone, 'out_channels'), f'Backbones are required to provide out_channels attribute, ' \\\n                                                   f'but not found in {backbone_name}'\n\n    if 'neck' not in config or config.neck is None:\n        neck_name = 'Select'\n    else:\n        neck_name = config.neck.pop('name')\n    self.neck = build_neck(neck_name, in_channels=self.backbone.out_channels, **config.neck)\n\n    assert hasattr(self.neck, 'out_channels'), f'Necks are required to provide out_channels attribute, ' \\\n                                               f'but not found in {neck_name}'\n\n    head_name = config.head.pop('name')\n    self.head = build_head(head_name, in_channels=self.neck.out_channels, **config.head)\n\n    self.model_name = f'{backbone_name}_{neck_name}_{head_name}'\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.builder","title":"<code>mindocr.models.builder</code>","text":"<p>build models</p>"},{"location":"reference/api_doc/#mindocr.models.builder.build_model","title":"<code>mindocr.models.builder.build_model(name_or_config, **kwargs)</code>","text":"<p>There are two ways to build a model.     1. load a predefined model according the given model name.     2. build the model according to the detailed configuration of the each module (transform, backbone, neck and     head), for lower-level architecture customization.</p> PARAMETER DESCRIPTION <code>name_or_config</code> <p>model name or config if it's a string, it should be a model name (which can be found by mindocr.list_models()) if it's a dict, it should be an architecture configuration defining the backbone/neck/head components (e.g., parsed from yaml config).</p> <p> TYPE: <code>Union[dict, str]</code> </p> <code>kwargs</code> <p>options if name_or_config is a model name, supported args in kwargs are:     - pretrained (bool): if True, pretrained checkpoint will be downloaded and loaded into the network.     - ckpt_load_path (str): path to checkpoint file. if a non-empty string given, the local checkpoint will       loaded into the network. if name_or_config is an architecture definition dict, supported args are:     - ckpt_load_path (str): path to checkpoint file.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell</p> <p>from mindocr.models import build_model  net = build_model(cfg['model'])  net = build_model(cfg['model'], ckpt_load_path='./r50_fpn_dbhead.ckpt') # build network and load checkpoint  net = build_model('dbnet_resnet50', pretrained=True)</p> Source code in <code>mindocr\\models\\builder.py</code> <pre><code>def build_model(name_or_config: Union[str, dict], **kwargs):\n\"\"\"\n    There are two ways to build a model.\n        1. load a predefined model according the given model name.\n        2. build the model according to the detailed configuration of the each module (transform, backbone, neck and\n        head), for lower-level architecture customization.\n\n    Args:\n        name_or_config (Union[dict, str]): model name or config\n            if it's a string, it should be a model name (which can be found by mindocr.list_models())\n            if it's a dict, it should be an architecture configuration defining the backbone/neck/head components\n            (e.g., parsed from yaml config).\n\n        kwargs (dict): options\n            if name_or_config is a model name, supported args in kwargs are:\n                - pretrained (bool): if True, pretrained checkpoint will be downloaded and loaded into the network.\n                - ckpt_load_path (str): path to checkpoint file. if a non-empty string given, the local checkpoint will\n                  loaded into the network.\n            if name_or_config is an architecture definition dict, supported args are:\n                - ckpt_load_path (str): path to checkpoint file.\n\n    Return:\n        nn.Cell\n\n    Example:\n    &gt;&gt;&gt;  from mindocr.models import build_model\n    &gt;&gt;&gt;  net = build_model(cfg['model'])\n    &gt;&gt;&gt;  net = build_model(cfg['model'], ckpt_load_path='./r50_fpn_dbhead.ckpt') # build network and load checkpoint\n    &gt;&gt;&gt;  net = build_model('dbnet_resnet50', pretrained=True)\n\n    \"\"\"\n    is_customized_model = True\n    if isinstance(name_or_config, str):\n        # build model by specific model name\n        model_name = name_or_config\n        if is_model(model_name):\n            create_fn = model_entrypoint(model_name)\n            network = create_fn(**kwargs)\n        else:\n            raise ValueError(\n                f\"Invalid model name: {model_name}. Supported models are {list_models()}\"\n            )\n        is_customized_model = False\n    elif isinstance(name_or_config, dict):\n        network = BaseModel(name_or_config)\n    else:\n        raise ValueError(\"Type error for config\")\n\n    # load checkpoint\n    if \"ckpt_load_path\" in kwargs:\n        load_from = kwargs[\"ckpt_load_path\"]\n        if isinstance(load_from, bool) and is_customized_model:\n            raise ValueError(\n                \"Cannot find the pretrained checkpoint for a customized model without giving the url or local path \"\n                \"to the checkpoint.\\nPlease specify the url or local path by setting `model-pretrained` (if training) \"\n                \"or `eval-ckpt_load_path` (if evaluation) in the yaml config\"\n            )\n\n        load_model(network, load_from)\n\n    if \"amp_level\" in kwargs:\n        auto_mixed_precision(network, amp_level=kwargs[\"amp_level\"])\n\n    return network\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.cls_mv3","title":"<code>mindocr.models.cls_mv3</code>","text":""},{"location":"reference/api_doc/#mindocr.models.det_dbnet","title":"<code>mindocr.models.det_dbnet</code>","text":""},{"location":"reference/api_doc/#mindocr.models.det_east","title":"<code>mindocr.models.det_east</code>","text":""},{"location":"reference/api_doc/#mindocr.models.det_psenet","title":"<code>mindocr.models.det_psenet</code>","text":""},{"location":"reference/api_doc/#mindocr.models.heads","title":"<code>mindocr.models.heads</code>","text":""},{"location":"reference/api_doc/#mindocr.models.heads.build_head","title":"<code>mindocr.models.heads.build_head(head_name, **kwargs)</code>","text":"<p>Build Head network.</p> PARAMETER DESCRIPTION <code>head_name</code> <p>the head layer(s) name, which shoule be one of the supported_heads.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>input args for the head network</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell for head module</p> Construct Example Source code in <code>mindocr\\models\\heads\\builder.py</code> <pre><code>def build_head(head_name, **kwargs):\n\"\"\"\n    Build Head network.\n\n    Args:\n        head_name (str): the head layer(s) name, which shoule be one of the supported_heads.\n        kwargs (dict): input args for the head network\n\n    Return:\n        nn.Cell for head module\n\n    Construct:\n        Input: Tensor\n        Output: Dict[Tensor]\n\n    Example:\n        &gt;&gt;&gt; # build CTCHead\n        &gt;&gt;&gt; from mindocr.models.heads import build_head\n        &gt;&gt;&gt; config = dict(head_name='CTCHead', in_channels=256, out_channels=37)\n        &gt;&gt;&gt; head = build_head(**config)\n        &gt;&gt;&gt; print(head)\n    \"\"\"\n    assert head_name in supported_heads, f'Invalid head {head_name}. Supported heads are {supported_heads}'\n    head = eval(head_name)(**kwargs)\n    return head\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.heads.build_head--build-ctchead","title":"build CTCHead","text":"<p>from mindocr.models.heads import build_head config = dict(head_name='CTCHead', in_channels=256, out_channels=37) head = build_head(**config) print(head)</p>"},{"location":"reference/api_doc/#mindocr.models.heads.builder","title":"<code>mindocr.models.heads.builder</code>","text":"<code>mindocr.models.heads.builder.build_head(head_name, **kwargs)</code> \u00b6 <p>Build Head network.</p> PARAMETER DESCRIPTION <code>head_name</code> <p>the head layer(s) name, which shoule be one of the supported_heads.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>input args for the head network</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell for head module</p> Construct Example Source code in <code>mindocr\\models\\heads\\builder.py</code> <pre><code>def build_head(head_name, **kwargs):\n\"\"\"\n    Build Head network.\n\n    Args:\n        head_name (str): the head layer(s) name, which shoule be one of the supported_heads.\n        kwargs (dict): input args for the head network\n\n    Return:\n        nn.Cell for head module\n\n    Construct:\n        Input: Tensor\n        Output: Dict[Tensor]\n\n    Example:\n        &gt;&gt;&gt; # build CTCHead\n        &gt;&gt;&gt; from mindocr.models.heads import build_head\n        &gt;&gt;&gt; config = dict(head_name='CTCHead', in_channels=256, out_channels=37)\n        &gt;&gt;&gt; head = build_head(**config)\n        &gt;&gt;&gt; print(head)\n    \"\"\"\n    assert head_name in supported_heads, f'Invalid head {head_name}. Supported heads are {supported_heads}'\n    head = eval(head_name)(**kwargs)\n    return head\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.heads.builder.build_head--build-ctchead","title":"build CTCHead","text":"<p>from mindocr.models.heads import build_head config = dict(head_name='CTCHead', in_channels=256, out_channels=37) head = build_head(**config) print(head)</p>"},{"location":"reference/api_doc/#mindocr.models.heads.cls_mv3_head","title":"<code>mindocr.models.heads.cls_mv3_head</code>","text":"<code>mindocr.models.heads.cls_mv3_head.ClsHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Text direction classification head.</p> Source code in <code>mindocr\\models\\heads\\cls_mv3_head.py</code> <pre><code>class ClsHead(nn.Cell):\n\"\"\"\n    Text direction classification head.\n    \"\"\"\n    def __init__(self, in_channels: int, hidden_channels: int, num_classes: int):\n        super().__init__()\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.SequentialCell([\n            nn.Dense(in_channels, hidden_channels),\n            nn.HSwish(),\n            nn.Dropout(keep_prob=0.8),\n            nn.Dense(hidden_channels, num_classes),\n        ])\n        self.softmax = nn.Softmax(axis=-1)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = x.astype('float32')\n        x = self.classifier(x)\n        x = self.softmax(x)\n        return x\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.heads.det_db_head","title":"<code>mindocr.models.heads.det_db_head</code>","text":"<code>mindocr.models.heads.det_db_head.DBHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\heads\\det_db_head.py</code> <pre><code>class DBHead(nn.Cell):\n    def __init__(self, in_channels: int, k=50, adaptive=False, bias=False, weight_init='HeUniform'):\n        super().__init__()\n        self.adaptive = adaptive\n\n        self.segm = self._init_heatmap(in_channels, in_channels // 4, weight_init, bias)\n        if self.adaptive:\n            self.thresh = self._init_heatmap(in_channels, in_channels // 4, weight_init, bias)\n            self.k = k\n            self.diff_bin = nn.Sigmoid()\n\n    @staticmethod\n    def _init_heatmap(in_channels, inter_channels, weight_init, bias):\n        return nn.SequentialCell([  # `pred` block from the original work\n            nn.Conv2d(in_channels, inter_channels, kernel_size=3, padding=1, pad_mode='pad', has_bias=bias,\n                      weight_init=weight_init),\n            nn.BatchNorm2d(inter_channels),\n            nn.ReLU(),\n            nn.Conv2dTranspose(inter_channels, inter_channels, kernel_size=2, stride=2, pad_mode='valid', has_bias=True,\n                               weight_init=weight_init),\n            nn.BatchNorm2d(inter_channels),\n            nn.ReLU(),\n            nn.Conv2dTranspose(inter_channels, 1, kernel_size=2, stride=2, pad_mode='valid', has_bias=True,\n                               weight_init=weight_init),\n            nn.Sigmoid()\n        ])\n\n    def construct(self, features: ms.Tensor) -&gt; Union[ms.Tensor, Tuple[ms.Tensor, ...]]:\n\"\"\"\n        Args:\n            features (Tensor): encoded features\n        Returns:\n            Union(\n            binary: predicted binary map\n            thresh: predicted threshold map (only return if adaptive is True in training)\n            thresh_binary: differentiable binary map (only if adaptive is True in training)\n        \"\"\"\n        binary = self.segm(features)\n\n        if self.adaptive and self.training:\n            # only use binary map to derive polygons in inference\n            thresh = self.thresh(features)\n            thresh_binary = self.diff_bin(self.k * binary - thresh)  # Differentiable Binarization\n            return binary, thresh, thresh_binary\n\n        return binary\n</code></pre> <code>mindocr.models.heads.det_db_head.DBHead.construct(features)</code> \u00b6 PARAMETER DESCRIPTION <code>features</code> <p>encoded features</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Union[ms.Tensor, Tuple[ms.Tensor, ...]]</code> <p>Union(</p> <code>binary</code> <p>predicted binary map</p> <p> TYPE: <code>Union[ms.Tensor, Tuple[ms.Tensor, ...]]</code> </p> <code>thresh</code> <p>predicted threshold map (only return if adaptive is True in training)</p> <p> TYPE: <code>Union[ms.Tensor, Tuple[ms.Tensor, ...]]</code> </p> <code>thresh_binary</code> <p>differentiable binary map (only if adaptive is True in training)</p> <p> TYPE: <code>Union[ms.Tensor, Tuple[ms.Tensor, ...]]</code> </p> Source code in <code>mindocr\\models\\heads\\det_db_head.py</code> <pre><code>def construct(self, features: ms.Tensor) -&gt; Union[ms.Tensor, Tuple[ms.Tensor, ...]]:\n\"\"\"\n    Args:\n        features (Tensor): encoded features\n    Returns:\n        Union(\n        binary: predicted binary map\n        thresh: predicted threshold map (only return if adaptive is True in training)\n        thresh_binary: differentiable binary map (only if adaptive is True in training)\n    \"\"\"\n    binary = self.segm(features)\n\n    if self.adaptive and self.training:\n        # only use binary map to derive polygons in inference\n        thresh = self.thresh(features)\n        thresh_binary = self.diff_bin(self.k * binary - thresh)  # Differentiable Binarization\n        return binary, thresh, thresh_binary\n\n    return binary\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.heads.rec_attn_head","title":"<code>mindocr.models.heads.rec_attn_head</code>","text":"<code>mindocr.models.heads.rec_attn_head.AttentionHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\heads\\rec_attn_head.py</code> <pre><code>class AttentionHead(nn.Cell):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_size: int = 256,\n        batch_max_length: int = 25,\n    ) -&gt; None:\n\"\"\"\n        Inputs:\n            inputs: shape [W, BS, 2*C]\n            label: shape [BS, W]\n        \"\"\"\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_size = hidden_size\n        self.num_classes = out_channels\n        self.batch_max_length = batch_max_length\n\n        self.attention_cell = AttentionCell(\n            self.in_channels, self.hidden_size, self.num_classes\n        )\n        self.generator = nn.Dense(hidden_size, self.num_classes)\n\n        self.one = Tensor(1.0, ms.float32)\n        self.zero = Tensor(0.0, ms.float32)\n\n        self.argmax = ops.Argmax(axis=1)\n\n    def _char_to_onehot(self, input_char: Tensor, onehot_dim: int) -&gt; Tensor:\n        input_one_hot = ops.one_hot(input_char, onehot_dim, self.one, self.zero)\n        return input_one_hot\n\n    def construct(self, inputs: Tensor, targets: Optional[Tensor] = None) -&gt; Tensor:\n        # convert the inputs from [W, BS, C] to [BS, W, C]\n        inputs = ops.transpose(inputs, (1, 0, 2))\n        N = inputs.shape[0]\n        num_steps = self.batch_max_length + 1  # for &lt;STOP&gt; symbol\n\n        hidden = ops.zeros((N, self.hidden_size), inputs.dtype)\n\n        if targets is not None:\n            # training branch\n            output_hiddens = list()\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(targets[:, i], self.num_classes)\n                hidden, _ = self.attention_cell(hidden, inputs, char_onehots)\n                output_hiddens.append(ops.expand_dims(hidden, axis=1))\n            output = ops.concat(output_hiddens, axis=1)\n            probs = self.generator(output)\n        else:\n            # inference branch\n            # &lt;GO&gt; symbol\n            targets = ops.zeros((N,), ms.int32)\n            probs = list()\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(targets, self.num_classes)\n                hidden, _ = self.attention_cell(hidden, inputs, char_onehots)\n                probs_step = self.generator(hidden)\n                probs.append(probs_step)\n                next_input = self.argmax(probs_step)\n                targets = next_input\n            probs = ops.stack(probs, axis=1)\n            probs = ops.softmax(probs, axis=2)\n        return probs\n</code></pre> <code>mindocr.models.heads.rec_attn_head.AttentionHead.__init__(in_channels, out_channels, hidden_size=256, batch_max_length=25)</code> \u00b6 Inputs Source code in <code>mindocr\\models\\heads\\rec_attn_head.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    hidden_size: int = 256,\n    batch_max_length: int = 25,\n) -&gt; None:\n\"\"\"\n    Inputs:\n        inputs: shape [W, BS, 2*C]\n        label: shape [BS, W]\n    \"\"\"\n    super().__init__()\n    self.in_channels = in_channels\n    self.hidden_size = hidden_size\n    self.num_classes = out_channels\n    self.batch_max_length = batch_max_length\n\n    self.attention_cell = AttentionCell(\n        self.in_channels, self.hidden_size, self.num_classes\n    )\n    self.generator = nn.Dense(hidden_size, self.num_classes)\n\n    self.one = Tensor(1.0, ms.float32)\n    self.zero = Tensor(0.0, ms.float32)\n\n    self.argmax = ops.Argmax(axis=1)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.heads.rec_ctc_head","title":"<code>mindocr.models.heads.rec_ctc_head</code>","text":"<code>mindocr.models.heads.rec_ctc_head.CTCHead</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>An MLP module for CTC Loss. For CRNN, the input should be in shape [W, BS, 2*C], which is output by RNNEncoder. The MLP encodes and classifies the features, then return a logit tensor in shape [W, BS, num_classes] For chinese words, num_classes can be over 60,000, so weight regulaization may matter.</p> Source code in <code>mindocr\\models\\heads\\rec_ctc_head.py</code> <pre><code>class CTCHead(nn.Cell):\n\"\"\"\n    An MLP module for CTC Loss.\n    For CRNN, the input should be in shape [W, BS, 2*C], which is output by RNNEncoder.\n    The MLP encodes and classifies the features, then return a logit tensor in shape [W, BS, num_classes]\n    For chinese words, num_classes can be over 60,000, so weight regulaization may matter.\n\n    Args:\n\n    Example:\n\n    \"\"\"\n\n    # TODO: add dropout regularization.\n    #  I think it will benefit the performance of 2-layer MLP for chinese text recoginition.\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 # fc_decay: float=0.0004,\n                 mid_channels: int = None,\n                 return_feats: bool = False,\n                 weight_init: str = 'normal',  # 'xavier_uniform',\n                 bias_init: str = 'zeros',  # 'xavier_uniform',\n                 dropout: float = 0.):\n        super().__init__()\n        # TODO:\n        #  Diff:\n        #    1. paddle initialize weight and bias with a Xaivier Uniform variant.\n        #    2. paddle uses L2 decay on FC weight and bias with specified decay factor fc_decay 0.00002.\n\n        self.out_channels = out_channels\n        self.mid_channels = mid_channels\n        self.return_feats = return_feats\n\n        if weight_init == \"crnn_customised\":\n            weight_init = crnn_head_initialization(in_channels)\n\n        if bias_init == \"crnn_customised\":\n            bias_init = crnn_head_initialization(in_channels)\n\n        # TODO: paddle is not using the exact XavierUniform. It uses check which is better.\n        # w_init = 'xavier_uniform'\n        # b_init = 'xavier_uniform'\n        if mid_channels is None:\n            self.dense1 = nn.Dense(in_channels, out_channels, weight_init=weight_init, bias_init=bias_init)\n        else:\n            # TODO: paddle did not use activation after linear, why no activation?\n            self.dense1 = nn.Dense(in_channels, mid_channels, weight_init=weight_init, bias_init=bias_init)\n            # self.activation = nn.GeLU()\n            self.dropout = nn.Dropout(keep_prob=1 - dropout)\n            self.dense2 = nn.Dense(mid_channels, out_channels, weight_init=weight_init, bias_init=bias_init)\n            # self.dropout = nn.Dropout(keep_prob)\n\n    def construct(self, x: ms.Tensor) -&gt; ms.Tensor:\n\"\"\"Feed Forward construct.\n        Args:\n            x (Tensor): feature in shape [W, BS, 2*C]\n        Returns:\n            h (Tensor): if training, h is logits in shape [W, BS, num_classes], where W - sequence len, fixed.\n                (dim order required by ms.ctcloss)\n                        if not training, h is class probabilites in shape [BS, W, num_classes].\n        \"\"\"\n        h = self.dense1(x)\n        # x = self.dropout(x)\n        if self.mid_channels is not None:\n            h = self.dropout(h)\n            h = self.dense2(h)\n\n        if not self.training:\n            # h = ops.softmax(h, axis=2) # not support on ms 1.8.1\n            h = ops.Softmax(axis=2)(h)\n            h = h.transpose((1, 0, 2))\n\n        return h\n</code></pre> <code>mindocr.models.heads.rec_ctc_head.CTCHead.construct(x)</code> \u00b6 <p>Feed Forward construct.</p> PARAMETER DESCRIPTION <code>x</code> <p>feature in shape [W, BS, 2*C]</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>h</code> <p>if training, h is logits in shape [W, BS, num_classes], where W - sequence len, fixed. (dim order required by ms.ctcloss)         if not training, h is class probabilites in shape [BS, W, num_classes].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindocr\\models\\heads\\rec_ctc_head.py</code> <pre><code>def construct(self, x: ms.Tensor) -&gt; ms.Tensor:\n\"\"\"Feed Forward construct.\n    Args:\n        x (Tensor): feature in shape [W, BS, 2*C]\n    Returns:\n        h (Tensor): if training, h is logits in shape [W, BS, num_classes], where W - sequence len, fixed.\n            (dim order required by ms.ctcloss)\n                    if not training, h is class probabilites in shape [BS, W, num_classes].\n    \"\"\"\n    h = self.dense1(x)\n    # x = self.dropout(x)\n    if self.mid_channels is not None:\n        h = self.dropout(h)\n        h = self.dense2(h)\n\n    if not self.training:\n        # h = ops.softmax(h, axis=2) # not support on ms 1.8.1\n        h = ops.Softmax(axis=2)(h)\n        h = h.transpose((1, 0, 2))\n\n    return h\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks","title":"<code>mindocr.models.necks</code>","text":""},{"location":"reference/api_doc/#mindocr.models.necks.build_neck","title":"<code>mindocr.models.necks.build_neck(neck_name, **kwargs)</code>","text":"<p>Build Neck network.</p> PARAMETER DESCRIPTION <code>neck_name</code> <p>the neck name, which shoule be one of the supported_necks.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>input args for the neck network</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell for neck module</p> Construct Example Source code in <code>mindocr\\models\\necks\\builder.py</code> <pre><code>def build_neck(neck_name, **kwargs):\n\"\"\"\n    Build Neck network.\n\n    Args:\n        neck_name (str): the neck name, which shoule be one of the supported_necks.\n        kwargs (dict): input args for the neck network\n\n    Return:\n        nn.Cell for neck module\n\n    Construct:\n        Input: Tensor\n        Output: Dict[Tensor]\n\n    Example:\n        &gt;&gt;&gt; # build RNNEncoder\n        &gt;&gt;&gt; from mindocr.models.necks import build_neck\n        &gt;&gt;&gt; config = dict(neck_name='RNNEncoder', in_channels=128, hidden_size=256)\n        &gt;&gt;&gt; neck = build_neck(**config)\n        &gt;&gt;&gt; print(neck)\n    \"\"\"\n    assert neck_name in supported_necks, f'Invalid neck: {neck_name}, Support necks are {supported_necks}'\n    neck = eval(neck_name)(**kwargs)\n    return neck\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.build_neck--build-rnnencoder","title":"build RNNEncoder","text":"<p>from mindocr.models.necks import build_neck config = dict(neck_name='RNNEncoder', in_channels=128, hidden_size=256) neck = build_neck(**config) print(neck)</p>"},{"location":"reference/api_doc/#mindocr.models.necks.asf","title":"<code>mindocr.models.necks.asf</code>","text":"<code>mindocr.models.necks.asf.AdaptiveScaleFusion</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>Adaptive Scale Fusion module from the <code>DBNet++ &lt;https://arxiv.org/abs/2202.10304&gt;</code>__ paper.</p> PARAMETER DESCRIPTION <code>channels</code> <p>number of input to and output channels from ASF</p> <p> </p> <code>channel_attention</code> <p>use channel attention</p> <p> DEFAULT: <code>True</code> </p> Source code in <code>mindocr\\models\\necks\\asf.py</code> <pre><code>class AdaptiveScaleFusion(nn.Cell):\n\"\"\"\n    Adaptive Scale Fusion module from the `DBNet++ &lt;https://arxiv.org/abs/2202.10304&gt;`__ paper.\n    Args:\n        channels: number of input to and output channels from ASF\n        channel_attention: use channel attention\n    \"\"\"\n    def __init__(self, channels, channel_attention=True, weight_init='HeUniform'):\n        super().__init__()\n        out_channels = channels // 4\n        self.conv = nn.Conv2d(channels, out_channels, kernel_size=3, padding=1, pad_mode='pad', has_bias=True,\n                              weight_init=weight_init)\n\n        self.chan_att = nn.SequentialCell([\n            nn.Conv2d(out_channels, out_channels // 4, kernel_size=1, pad_mode='valid', weight_init=weight_init),\n            nn.ReLU(),\n            nn.Conv2d(out_channels // 4, out_channels, kernel_size=1, pad_mode='valid', weight_init=weight_init),\n            nn.Sigmoid()\n        ]) if channel_attention else None\n\n        self.spat_att = nn.SequentialCell([\n            nn.Conv2d(1, 1, kernel_size=3, padding=1, pad_mode='pad', weight_init=weight_init),\n            nn.ReLU(),\n            nn.Conv2d(1, 1, kernel_size=1, pad_mode='valid', weight_init=weight_init),\n            nn.Sigmoid()\n        ])\n\n        self.scale_att = nn.SequentialCell([\n            nn.Conv2d(out_channels, 4, kernel_size=1, pad_mode='valid', weight_init=weight_init),\n            nn.Sigmoid()\n        ])\n\n    def construct(self, x):\n        reduced = self.conv(ops.concat(x, axis=1))\n\n        if self.chan_att is not None:\n            ada_pool = ops.mean(reduced, axis=(-2, -1), keep_dims=True)  # equivalent to nn.AdaptiveAvgPool2d(1)\n            reduced = self.chan_att(ada_pool) + reduced\n\n        spatial = ops.mean(reduced, axis=1, keep_dims=True)\n        spat_att = self.spat_att(spatial) + reduced\n\n        scale_att = self.scale_att(spat_att)\n        return ops.concat([scale_att[:, i:i + 1] * x[i] for i in range(len(x))], axis=1)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.builder","title":"<code>mindocr.models.necks.builder</code>","text":"<code>mindocr.models.necks.builder.build_neck(neck_name, **kwargs)</code> \u00b6 <p>Build Neck network.</p> PARAMETER DESCRIPTION <code>neck_name</code> <p>the neck name, which shoule be one of the supported_necks.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>input args for the neck network</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Return <p>nn.Cell for neck module</p> Construct Example Source code in <code>mindocr\\models\\necks\\builder.py</code> <pre><code>def build_neck(neck_name, **kwargs):\n\"\"\"\n    Build Neck network.\n\n    Args:\n        neck_name (str): the neck name, which shoule be one of the supported_necks.\n        kwargs (dict): input args for the neck network\n\n    Return:\n        nn.Cell for neck module\n\n    Construct:\n        Input: Tensor\n        Output: Dict[Tensor]\n\n    Example:\n        &gt;&gt;&gt; # build RNNEncoder\n        &gt;&gt;&gt; from mindocr.models.necks import build_neck\n        &gt;&gt;&gt; config = dict(neck_name='RNNEncoder', in_channels=128, hidden_size=256)\n        &gt;&gt;&gt; neck = build_neck(**config)\n        &gt;&gt;&gt; print(neck)\n    \"\"\"\n    assert neck_name in supported_necks, f'Invalid neck: {neck_name}, Support necks are {supported_necks}'\n    neck = eval(neck_name)(**kwargs)\n    return neck\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.builder.build_neck--build-rnnencoder","title":"build RNNEncoder","text":"<p>from mindocr.models.necks import build_neck config = dict(neck_name='RNNEncoder', in_channels=128, hidden_size=256) neck = build_neck(**config) print(neck)</p>"},{"location":"reference/api_doc/#mindocr.models.necks.fpn","title":"<code>mindocr.models.necks.fpn</code>","text":"<code>mindocr.models.necks.fpn.DBFPN</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\necks\\fpn.py</code> <pre><code>class DBFPN(nn.Cell):\n    def __init__(self, in_channels, out_channels=256, weight_init='HeUniform',\n                 bias=False, use_asf=False, channel_attention=True):\n\"\"\"\n        in_channels: resnet18=[64, 128, 256, 512]\n                    resnet50=[2048,1024,512,256]\n        out_channels: Inner channels in Conv2d\n\n        bias: Whether conv layers have bias or not.\n        use_asf: use ASF module for multi-scale feature aggregation (DBNet++ only)\n        channel_attention: use channel attention in ASF module\n        \"\"\"\n        super().__init__()\n        self.out_channels = out_channels\n\n        self.unify_channels = nn.CellList(\n            [nn.Conv2d(ch, out_channels, 1, pad_mode='valid', has_bias=bias, weight_init=weight_init)\n             for ch in in_channels]\n        )\n\n        self.out = nn.CellList(\n            [nn.Conv2d(out_channels, out_channels // 4, 3, padding=1, pad_mode='pad', has_bias=bias,\n                       weight_init=weight_init) for _ in range(len(in_channels))]\n        )\n\n        self.fuse = AdaptiveScaleFusion(out_channels, channel_attention, weight_init) if use_asf else ops.Concat(axis=1)\n\n    def construct(self, features):\n        for i, uc_op in enumerate(self.unify_channels):\n            features[i] = uc_op(features[i])\n\n        for i in range(2, -1, -1):\n            features[i] += _resize_nn(features[i + 1], shape=features[i].shape[2:])\n\n        for i, out in enumerate(self.out):\n            features[i] = _resize_nn(out(features[i]), shape=features[0].shape[2:])\n\n        return self.fuse(features[::-1])   # matching the reverse order of the original work\n</code></pre> <code>mindocr.models.necks.fpn.DBFPN.__init__(in_channels, out_channels=256, weight_init='HeUniform', bias=False, use_asf=False, channel_attention=True)</code> \u00b6 resnet18=[64, 128, 256, 512] <p>resnet50=[2048,1024,512,256]</p> <p>bias: Whether conv layers have bias or not. use_asf: use ASF module for multi-scale feature aggregation (DBNet++ only) channel_attention: use channel attention in ASF module</p> Source code in <code>mindocr\\models\\necks\\fpn.py</code> <pre><code>def __init__(self, in_channels, out_channels=256, weight_init='HeUniform',\n             bias=False, use_asf=False, channel_attention=True):\n\"\"\"\n    in_channels: resnet18=[64, 128, 256, 512]\n                resnet50=[2048,1024,512,256]\n    out_channels: Inner channels in Conv2d\n\n    bias: Whether conv layers have bias or not.\n    use_asf: use ASF module for multi-scale feature aggregation (DBNet++ only)\n    channel_attention: use channel attention in ASF module\n    \"\"\"\n    super().__init__()\n    self.out_channels = out_channels\n\n    self.unify_channels = nn.CellList(\n        [nn.Conv2d(ch, out_channels, 1, pad_mode='valid', has_bias=bias, weight_init=weight_init)\n         for ch in in_channels]\n    )\n\n    self.out = nn.CellList(\n        [nn.Conv2d(out_channels, out_channels // 4, 3, padding=1, pad_mode='pad', has_bias=bias,\n                   weight_init=weight_init) for _ in range(len(in_channels))]\n    )\n\n    self.fuse = AdaptiveScaleFusion(out_channels, channel_attention, weight_init) if use_asf else ops.Concat(axis=1)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.img2seq","title":"<code>mindocr.models.necks.img2seq</code>","text":"<code>mindocr.models.necks.img2seq.Img2Seq</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> Source code in <code>mindocr\\models\\necks\\img2seq.py</code> <pre><code>class Img2Seq(nn.Cell):\n\"\"\"\n    Inputs: feature list with shape [N, C, 1, W]\n    Outputs: first feature with shape [W, N, C]\n    \"\"\"\n\n    def __init__(self, in_channels: int) -&gt; None:\n        super().__init__()\n        self.out_channels = in_channels\n\n    def construct(self, features: List[Tensor]) -&gt; Tensor:\n        x = features[0]\n        x = ops.squeeze(x, axis=2)\n        x = ops.transpose(x, (2, 0, 1))\n        return x\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.rnn","title":"<code>mindocr.models.necks.rnn</code>","text":"<code>mindocr.models.necks.rnn.RNNEncoder</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>CRNN sequence encoder which contains reshape and bidirectional LSTM layers. Receive visual features [N, C, 1, W] Reshape features to shape [W, N, C] Use Bi-LSTM to encode into new features in shape [W, N, 2*C]. where W - seq len, N - batch size, C - feature len</p> PARAMETER DESCRIPTION <code>input_channels</code> <p>C, number of input channels, corresponding to feature length</p> <p> TYPE: <code>int</code> </p> <code>hidden_size(int)</code> <p>the hidden size in LSTM layers, default is 512</p> <p> </p> Source code in <code>mindocr\\models\\necks\\rnn.py</code> <pre><code>class RNNEncoder(nn.Cell):\n\"\"\"\n     CRNN sequence encoder which contains reshape and bidirectional LSTM layers.\n     Receive visual features [N, C, 1, W]\n     Reshape features to shape [W, N, C]\n     Use Bi-LSTM to encode into new features in shape [W, N, 2*C].\n     where W - seq len, N - batch size, C - feature len\n\n     Args:\n        input_channels (int):  C, number of input channels, corresponding to feature length\n        hidden_size(int): the hidden size in LSTM layers, default is 512\n     \"\"\"\n\n    def __init__(self, in_channels, hidden_size=512, batch_size=None):\n        super().__init__()\n        self.out_channels = 2 * hidden_size\n\n        self.seq_encoder = nn.LSTM(input_size=in_channels,\n                                   hidden_size=hidden_size,\n                                   num_layers=2,\n                                   has_bias=True,\n                                   dropout=0.,\n                                   bidirectional=True)\n\n        # TODO: do we need to add batch size to compute hx menioned in MindSpore LSTM doc\n        self.hx = None\n        if batch_size is not None:\n            h0 = Tensor(np.zeros([2 * 2, batch_size, hidden_size]).astype(np.float32))\n            c0 = Tensor(np.zeros([2 * 2, batch_size, hidden_size]).astype(np.float32))\n            self.hx = (h0, c0)\n\n    def construct(self, features):\n\"\"\"\n        Args:\n            x (Tensor): feature, a Tensor of shape :math:`(N, C, 1, W)`.\n                Note that H must be 1. Width W can be viewed as time length in CRNN algorithm.\n                C - input channels can be viewed as feature length for each time step.  N is batch size.\n\n        Returns:\n            Tensor: Encoded features . Shape :math:`(W, N, 2*C)` where\n        \"\"\"\n        x = features[0]\n        assert x.shape[2] == 1, f'Feature height must be 1, but got {x.shape[2]} from x.shape {x.shape}'\n        x = ops.squeeze(x, axis=2)  # [N, C, W]\n        x = ops.transpose(x, (2, 0, 1))  # [W, N, C]\n\n        if self.hx is None:\n            x, hx_n = self.seq_encoder(x)\n        else:\n            print('using self.hx')\n            x, hx_n = self.seq_encoder(x, self.hx)  # the results are the same\n\n        return x\n</code></pre> <code>mindocr.models.necks.rnn.RNNEncoder.construct(features)</code> \u00b6 PARAMETER DESCRIPTION <code>x</code> <p>feature, a Tensor of shape :math:<code>(N, C, 1, W)</code>. Note that H must be 1. Width W can be viewed as time length in CRNN algorithm. C - input channels can be viewed as feature length for each time step.  N is batch size.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Encoded features . Shape :math:<code>(W, N, 2*C)</code> where</p> Source code in <code>mindocr\\models\\necks\\rnn.py</code> <pre><code>def construct(self, features):\n\"\"\"\n    Args:\n        x (Tensor): feature, a Tensor of shape :math:`(N, C, 1, W)`.\n            Note that H must be 1. Width W can be viewed as time length in CRNN algorithm.\n            C - input channels can be viewed as feature length for each time step.  N is batch size.\n\n    Returns:\n        Tensor: Encoded features . Shape :math:`(W, N, 2*C)` where\n    \"\"\"\n    x = features[0]\n    assert x.shape[2] == 1, f'Feature height must be 1, but got {x.shape[2]} from x.shape {x.shape}'\n    x = ops.squeeze(x, axis=2)  # [N, C, W]\n    x = ops.transpose(x, (2, 0, 1))  # [W, N, C]\n\n    if self.hx is None:\n        x, hx_n = self.seq_encoder(x)\n    else:\n        print('using self.hx')\n        x, hx_n = self.seq_encoder(x, self.hx)  # the results are the same\n\n    return x\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.necks.select","title":"<code>mindocr.models.necks.select</code>","text":"<code>mindocr.models.necks.select.Select</code> \u00b6 <p>         Bases: <code>nn.Cell</code></p> <p>select feature from the backbone output.</p> Source code in <code>mindocr\\models\\necks\\select.py</code> <pre><code>class Select(nn.Cell):\n'''\n    select feature from the backbone output.\n    '''\n    def __init__(self, in_channels, index=-1):\n        super().__init__()\n        self.index = index\n        self.out_channels = in_channels[index]\n\n    def construct(self, x):\n        if isinstance(x, list) or isinstance(x, tuple):\n            return x[self.index]\n        else:\n            return x\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.rec_crnn","title":"<code>mindocr.models.rec_crnn</code>","text":""},{"location":"reference/api_doc/#mindocr.models.rec_rare","title":"<code>mindocr.models.rec_rare</code>","text":""},{"location":"reference/api_doc/#mindocr.models.rec_svtr","title":"<code>mindocr.models.rec_svtr</code>","text":""},{"location":"reference/api_doc/#mindocr.models.utils","title":"<code>mindocr.models.utils</code>","text":""},{"location":"reference/api_doc/#mindocr.models.utils.load_model","title":"<code>mindocr.models.utils.load_model</code>","text":"<code>mindocr.models.utils.load_model.load_model(network, load_from=None, filter_fn=None, auto_mapping=False, strict=False)</code> \u00b6 <p>Load the checkpoint into the model</p> PARAMETER DESCRIPTION <code>network</code> <p>network</p> <p> </p> <code>load_from</code> <p>a string that can be url or local path to a checkpoint, that will be loaded to the network.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>filter_fn</code> <p>a function filtering the parameters that will be loading into the network. If it is None, all parameters will be loaded.</p> <p> TYPE: <code>Optional[Callable[[Dict], Dict]]</code> DEFAULT: <code>None</code> </p> <code>auto_mapping</code> <p>when it is True, then load the paramters even if the names are slightly different</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strict</code> <p>If it is true, then the shape and the type of the parameters in the checkpoint and the network should be consistent raise exception if they do not match.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\models\\utils\\load_model.py</code> <pre><code>def load_model(\n    network,\n    load_from: Optional[str] = None,\n    filter_fn: Optional[Callable[[Dict], Dict]] = None,\n    auto_mapping: bool = False,\n    strict: bool = False,\n):\n\"\"\"\n    Load the checkpoint into the model\n\n    Args:\n        network: network\n        load_from: a string that can be url or local path to a checkpoint, that will be loaded to the network.\n        filter_fn: a function filtering the parameters that will be loading into the network. If it is None,\n            all parameters will be loaded.\n        auto_mapping: when it is True, then load the paramters even if the names are slightly different\n        strict: If it is true, then the shape and the type of the parameters in the checkpoint and the network\n            should be consistent\n            raise exception if they do not match.\n    \"\"\"\n    if load_from is None:\n        return\n\n    if load_from[:4] == \"http\":\n        url_cfg = {\"url\": load_from}\n        local_ckpt_path = download_pretrained(url_cfg)\n    else:\n        local_ckpt_path = load_from\n\n    assert local_ckpt_path and os.path.exists(local_ckpt_path), (\n        f\"Failed to load checkpoint. `{local_ckpt_path}` NOT exist. \\n\"\n        \"Please check the path and set it in `eval-ckpt_load_path` or `model-pretrained` in the yaml config file \"\n    )\n\n    params = load_checkpoint(local_ckpt_path)\n\n    if filter_fn is not None:\n        params = filter_fn(params)\n\n    if auto_mapping:\n        params = auto_map(network, params)\n\n    if not strict:\n        params = drop_inconsistent_shape_parameters(network, params)\n\n    load_param_into_net(network, params, strict_load=strict)\n\n    print(\n        f\"INFO: Finish loading model checkoint from {load_from}. \"\n        \"If no parameter fail-load warning displayed, all checkpoint params have been successfully loaded.\"\n    )\n</code></pre>"},{"location":"reference/api_doc/#mindocr.models.utils.rnn_cells","title":"<code>mindocr.models.utils.rnn_cells</code>","text":"<p>RNN Cells that supports FP16 inputs</p> <code>mindocr.models.utils.rnn_cells.GRUCell</code> \u00b6 <p>         Bases: <code>RNNCellBase</code></p> <p>A GRU(Gated Recurrent Unit) cell.</p> <p>.. math::</p> <pre><code>\\begin{array}{ll}\nr = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\nz = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\nn = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\nh' = (1 - z) * n + z * h\n\\end{array}\n</code></pre> <p>Here :math:<code>\\sigma</code> is the sigmoid function, and :math:<code>*</code> is the Hadamard product. :math:<code>W, b</code> are learnable weights between the output and the input in the formula. For instance, :math:<code>W_{ir}, b_{ir}</code> are the weight and bias used to transform from input :math:<code>x</code> to :math:<code>r</code>. Details can be found in paper <code>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation &lt;https://aclanthology.org/D14-1179.pdf&gt;</code>_.</p> <p>The LSTMCell can be simplified in NN layer, the following formula:</p> <p>.. math::     h{'},c = LSTMCell(x, (h_0, c_0))</p> PARAMETER DESCRIPTION <code>input_size</code> <p>Number of features of input.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>Number of features of hidden layer.</p> <p> TYPE: <code>int</code> </p> <code>has_bias</code> <p>Whether the cell has bias <code>b_in</code> and <code>b_hn</code>. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape (batch_size, <code>input_size</code>).</li> <li>hx (Tensor) - Tensor of data type mindspore.float32 and shape (batch_size, <code>hidden_size</code>).   Data type of <code>hx</code> must be the same as <code>x</code>.</li> </ul> Outputs <ul> <li>hx' (Tensor) - Tensor of shape (batch_size, <code>hidden_size</code>).</li> </ul> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>input_size</code>, <code>hidden_size</code> is not an int.</p> <code>TypeError</code> <p>If <code>has_bias</code> is not a bool.</p> Supported Platforms <p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = nn.GRUCell(10, 16)\n&gt;&gt;&gt; x = Tensor(np.ones([5, 3, 10]).astype(np.float32))\n&gt;&gt;&gt; hx = Tensor(np.ones([3, 16]).astype(np.float32))\n&gt;&gt;&gt; output = []\n&gt;&gt;&gt; for i in range(5):\n...     hx = net(x[i], hx)\n...     output.append(hx)\n&gt;&gt;&gt; print(output[0].shape)\n(3, 16)\n</code></pre> Source code in <code>mindocr\\models\\utils\\rnn_cells.py</code> <pre><code>class GRUCell(RNNCellBase):\nr\"\"\"\n    A GRU(Gated Recurrent Unit) cell.\n\n    .. math::\n\n        \\begin{array}{ll}\n        r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\n        z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\n        n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n        h' = (1 - z) * n + z * h\n        \\end{array}\n\n    Here :math:`\\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`\n    are learnable weights between the output and the input in the formula. For instance,\n    :math:`W_{ir}, b_{ir}` are the weight and bias used to transform from input :math:`x` to :math:`r`.\n    Details can be found in paper\n    `Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n    &lt;https://aclanthology.org/D14-1179.pdf&gt;`_.\n\n    The LSTMCell can be simplified in NN layer, the following formula:\n\n    .. math::\n        h^{'},c^{'} = LSTMCell(x, (h_0, c_0))\n\n    Args:\n        input_size (int): Number of features of input.\n        hidden_size (int):  Number of features of hidden layer.\n        has_bias (bool): Whether the cell has bias `b_in` and `b_hn`. Default: True.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape (batch_size, `input_size`).\n        - **hx** (Tensor) - Tensor of data type mindspore.float32 and shape (batch_size, `hidden_size`).\n          Data type of `hx` must be the same as `x`.\n\n    Outputs:\n        - **hx'** (Tensor) - Tensor of shape (batch_size, `hidden_size`).\n\n    Raises:\n        TypeError: If `input_size`, `hidden_size` is not an int.\n        TypeError: If `has_bias` is not a bool.\n\n    Supported Platforms:\n        ``Ascend`` ``GPU`` ``CPU``\n\n    Examples:\n        &gt;&gt;&gt; net = nn.GRUCell(10, 16)\n        &gt;&gt;&gt; x = Tensor(np.ones([5, 3, 10]).astype(np.float32))\n        &gt;&gt;&gt; hx = Tensor(np.ones([3, 16]).astype(np.float32))\n        &gt;&gt;&gt; output = []\n        &gt;&gt;&gt; for i in range(5):\n        ...     hx = net(x[i], hx)\n        ...     output.append(hx)\n        &gt;&gt;&gt; print(output[0].shape)\n        (3, 16)\n    \"\"\"\n    def __init__(self, input_size: int, hidden_size: int, has_bias: bool = True):\n        super().__init__(input_size, hidden_size, has_bias, num_chunks=3)\n\n    def construct(self, x, hx):\n        _check_batch_size_equal(x.shape[0], hx.shape[0], self.cls_name)\n\n        # FIX: make sure the weight and bias dtype is same as the data type from x\n        # prevent the input type inconsistent error from P.MatMul operator\n        weight_ih = P.cast(self.weight_ih, x.dtype)\n        weight_hh = P.cast(self.weight_hh, x.dtype)\n        bias_ih = P.cast(self.bias_ih, x.dtype)\n        bias_hh = P.cast(self.bias_hh, x.dtype)\n\n        return _gru_cell(x, hx, weight_ih, weight_hh, bias_ih, bias_hh)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim","title":"<code>mindocr.optim</code>","text":"<p>optim init</p>"},{"location":"reference/api_doc/#mindocr.optim.adamw","title":"<code>mindocr.optim.adamw</code>","text":"<p>Gradient clipping wrapper for optimizers.</p>"},{"location":"reference/api_doc/#mindocr.optim.adamw.AdamW","title":"<code>mindocr.optim.adamw.AdamW</code>","text":"<p>         Bases: <code>Optimizer</code></p> <p>Implements the gradient clipping by norm for a AdamWeightDecay optimizer.</p> Source code in <code>mindocr\\optim\\adamw.py</code> <pre><code>class AdamW(Optimizer):\n\"\"\"\n    Implements the gradient clipping by norm for a AdamWeightDecay optimizer.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"adam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"adam_v\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(\n                    _adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr, self.weight_decay\n                ),\n                self.parameters,\n                self.moments1,\n                self.moments2,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.adamw.tensor_grad_scale","title":"<code>mindocr.optim.adamw.tensor_grad_scale(scale, grad)</code>","text":"<p>Get grad with scale.</p> Source code in <code>mindocr\\optim\\adamw.py</code> <pre><code>@_grad_scale.register(\"Number\", \"Tensor\")\ndef tensor_grad_scale(scale, grad):\n\"\"\"Get grad with scale.\"\"\"\n    if scale == 1.0:\n        return grad\n    return ops.mul(grad, ops.cast(scale, grad.dtype))\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.adamw.tensor_grad_scale_with_tensor","title":"<code>mindocr.optim.adamw.tensor_grad_scale_with_tensor(scale, grad)</code>","text":"<p>Get grad with scale.</p> Source code in <code>mindocr\\optim\\adamw.py</code> <pre><code>@_grad_scale.register(\"Tensor\", \"Tensor\")\ndef tensor_grad_scale_with_tensor(scale, grad):\n\"\"\"Get grad with scale.\"\"\"\n    return ops.mul(grad, ops.cast(scale, grad.dtype))\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.adan","title":"<code>mindocr.optim.adan</code>","text":"<p>adan</p>"},{"location":"reference/api_doc/#mindocr.optim.adan.Adan","title":"<code>mindocr.optim.adan.Adan</code>","text":"<p>         Bases: <code>Optimizer</code></p> <p>The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677</p> <p>Note: it is an experimental version.</p> Source code in <code>mindocr\\optim\\adan.py</code> <pre><code>class Adan(Optimizer):\n\"\"\"\n    The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677\n\n    Note: it is an experimental version.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.98,\n        beta2=0.92,\n        beta3=0.99,\n        eps=1e-8,\n        use_locking=False,\n        weight_decay=0.0,\n        loss_scale=1.0,\n    ):\n        super().__init__(\n            learning_rate, params, weight_decay=weight_decay, loss_scale=loss_scale\n        )  # Optimized inherit weight decay is bloaked. weight decay is computed in this py.\n\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        assert isinstance(use_locking, bool), f\"For {self.cls_name}, use_looking should be bool\"\n\n        self.beta1 = Tensor(beta1, mstype.float32)\n        self.beta2 = Tensor(beta2, mstype.float32)\n        self.beta3 = Tensor(beta3, mstype.float32)\n\n        self.eps = Tensor(eps, mstype.float32)\n        self.use_locking = use_locking\n        self.moment1 = self._parameters.clone(prefix=\"moment1\", init=\"zeros\")  # m\n        self.moment2 = self._parameters.clone(prefix=\"moment2\", init=\"zeros\")  # v\n        self.moment3 = self._parameters.clone(prefix=\"moment3\", init=\"zeros\")  # n\n        self.prev_gradient = self._parameters.clone(prefix=\"prev_gradient\", init=\"zeros\")\n\n        self.weight_decay = Tensor(weight_decay, mstype.float32)\n\n    @ms_function\n    def construct(self, gradients):\n        params = self._parameters\n        moment1 = self.moment1\n        moment2 = self.moment2\n        moment3 = self.moment3\n\n        gradients = self.flatten_gradients(gradients)\n        gradients = self.gradients_centralization(gradients)\n        gradients = self.scale_grad(gradients)\n        gradients = self._grad_sparse_indices_deduplicate(gradients)\n        lr = self.get_lr()\n\n        # TODO: currently not support dist\n        success = self.map_(\n            ops.partial(_adan_opt, self.beta1, self.beta2, self.beta3, self.eps, lr, self.weight_decay),\n            params,\n            moment1,\n            moment2,\n            moment3,\n            gradients,\n            self.prev_gradient,\n        )\n\n        return success\n\n    @Optimizer.target.setter\n    def target(self, value):\n\"\"\"\n        If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n        optimizer operation.\n        \"\"\"\n        self._set_base_target(value)\n</code></pre> <code>mindocr.optim.adan.Adan.target(value)</code> \u00b6 <p>If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused optimizer operation.</p> Source code in <code>mindocr\\optim\\adan.py</code> <pre><code>@Optimizer.target.setter\ndef target(self, value):\n\"\"\"\n    If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n    optimizer operation.\n    \"\"\"\n    self._set_base_target(value)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.lion","title":"<code>mindocr.optim.lion</code>","text":""},{"location":"reference/api_doc/#mindocr.optim.lion.Lion","title":"<code>mindocr.optim.lion.Lion</code>","text":"<p>         Bases: <code>Optimizer</code></p> <p>Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'. Additionally, this implementation is with gradient clipping.</p> <p>Notes: lr is usually 3-10x smaller than adamw. weight decay is usually 3-10x larger than adamw.</p> Source code in <code>mindocr\\optim\\lion.py</code> <pre><code>class Lion(Optimizer):\n\"\"\"\n    Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'.\n    Additionally, this implementation is with gradient clipping.\n\n    Notes:\n    lr is usually 3-10x smaller than adamw.\n    weight decay is usually 3-10x larger than adamw.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-4,\n        beta1=0.9,\n        beta2=0.99,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"lion_m\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr, self.weight_decay),\n                self.parameters,\n                self.moments1,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.lion.tensor_grad_scale","title":"<code>mindocr.optim.lion.tensor_grad_scale(scale, grad)</code>","text":"<p>Get grad with scale.</p> Source code in <code>mindocr\\optim\\lion.py</code> <pre><code>@_grad_scale.register(\"Number\", \"Tensor\")\ndef tensor_grad_scale(scale, grad):\n\"\"\"Get grad with scale.\"\"\"\n    if scale == 1.0:\n        return grad\n    return ops.mul(grad, ops.cast(scale, grad.dtype))\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.lion.tensor_grad_scale_with_tensor","title":"<code>mindocr.optim.lion.tensor_grad_scale_with_tensor(scale, grad)</code>","text":"<p>Get grad with scale.</p> Source code in <code>mindocr\\optim\\lion.py</code> <pre><code>@_grad_scale.register(\"Tensor\", \"Tensor\")\ndef tensor_grad_scale_with_tensor(scale, grad):\n\"\"\"Get grad with scale.\"\"\"\n    return ops.mul(grad, ops.cast(scale, grad.dtype))\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.nadam","title":"<code>mindocr.optim.nadam</code>","text":"<p>nadam</p>"},{"location":"reference/api_doc/#mindocr.optim.nadam.NAdam","title":"<code>mindocr.optim.nadam.NAdam</code>","text":"<p>         Bases: <code>Optimizer</code></p> <p>Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).</p> Source code in <code>mindocr\\optim\\nadam.py</code> <pre><code>class NAdam(Optimizer):\n\"\"\"\n    Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        schedule_decay=4e-3,\n    ):\n        super().__init__(learning_rate, params, weight_decay, loss_scale)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"nadam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"nadam_v\", init=\"zeros\")\n        self.schedule_decay = Tensor(np.array([schedule_decay]).astype(np.float32))\n        self.mu_schedule = Parameter(initializer(1, [1], ms.float32), name=\"mu_schedule\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n    @ms_function\n    def construct(self, gradients):\n        lr = self.get_lr()\n        params = self.parameters\n        step = self.global_step + _scaler_one\n        gradients = self.decay_weight(gradients)\n        mu = self.beta1 * (\n            _scaler_one - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), step * self.schedule_decay)\n        )\n        mu_next = self.beta1 * (\n            _scaler_one\n            - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), (step + _scaler_one) * self.schedule_decay)\n        )\n        mu_schedule = self.mu_schedule * mu\n        mu_schedule_next = self.mu_schedule * mu * mu_next\n        self.mu_schedule = mu_schedule\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        num_params = len(params)\n        for i in range(num_params):\n            ops.assign(self.moments1[i], self.beta1 * self.moments1[i] + (_scaler_one - self.beta1) * gradients[i])\n            ops.assign(\n                self.moments2[i], self.beta2 * self.moments2[i] + (_scaler_one - self.beta2) * ops.square(gradients[i])\n            )\n\n            regulate_m = mu_next * self.moments1[i] / (_scaler_one - mu_schedule_next) + (_scaler_one - mu) * gradients[\n                i\n            ] / (_scaler_one - mu_schedule)\n            regulate_v = self.moments2[i] / (_scaler_one - beta2_power)\n\n            update = params[i] - lr * regulate_m / (self.eps + ops.sqrt(regulate_v))\n            ops.assign(params[i], update)\n\n        return params\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.optim_factory","title":"<code>mindocr.optim.optim_factory</code>","text":"<p>optim factory</p>"},{"location":"reference/api_doc/#mindocr.optim.optim_factory.create_optimizer","title":"<code>mindocr.optim.optim_factory.create_optimizer(params, opt='adam', lr=0.001, weight_decay=0, momentum=0.9, nesterov=False, filter_bias_and_bn=True, loss_scale=1.0, schedule_decay=0.004, checkpoint_path='', eps=1e-10, **kwargs)</code>","text":"<p>Creates optimizer by name.</p> PARAMETER DESCRIPTION <code>params</code> <p>network parameters. Union[list[Parameter],list[dict]], which must be the list of parameters or list of dicts. When the list element is a dictionary, the key of the dictionary can be \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".</p> <p> </p> <code>opt</code> <p>wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks. 'adamw' is recommended for ViT-based networks. Default: 'adam'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adam'</code> </p> <code>lr</code> <p>learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.001</code> </p> <code>weight_decay</code> <p>weight decay factor. It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>momentum</code> <p>momentum if the optimizer supports. Default: 0.9.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>nesterov</code> <p>Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filter_bias_and_bn</code> <p>whether to filter batch norm parameters and bias from weight decay. If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>loss_scale</code> <p>A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Optimizer object</p> Source code in <code>mindocr\\optim\\optim_factory.py</code> <pre><code>def create_optimizer(\n    params,\n    opt: str = \"adam\",\n    lr: Optional[float] = 1e-3,\n    weight_decay: float = 0,\n    momentum: float = 0.9,\n    nesterov: bool = False,\n    filter_bias_and_bn: bool = True,\n    loss_scale: float = 1.0,\n    schedule_decay: float = 4e-3,\n    checkpoint_path: str = \"\",\n    eps: float = 1e-10,\n    **kwargs,\n):\nr\"\"\"Creates optimizer by name.\n\n    Args:\n        params: network parameters. Union[list[Parameter],list[dict]], which must be the list of parameters\n            or list of dicts. When the list element is a dictionary, the key of the dictionary can be\n            \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".\n        opt: wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion',\n            'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks.\n            'adamw' is recommended for ViT-based networks. Default: 'adam'.\n        lr: learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.\n        weight_decay: weight decay factor. It should be noted that weight decay can be a constant value or a Cell.\n            It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to\n            dynamic learning rate, users need to customize a weight decay schedule only with global step as input,\n            and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value\n            of current step. Default: 0.\n        momentum: momentum if the optimizer supports. Default: 0.9.\n        nesterov: Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.\n        filter_bias_and_bn: whether to filter batch norm parameters and bias from weight decay.\n            If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. Default: True.\n        loss_scale: A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.\n\n    Returns:\n        Optimizer object\n    \"\"\"\n    opt = opt.lower()\n\n    if weight_decay and filter_bias_and_bn:\n        if not isinstance(params[0], dict):  # check whether param grouping strategy is encoded in `params`\n            params = init_group_params(params, weight_decay)\n        else:\n            print(\n                \"WARNING: Customized param grouping strategy detected in `params`. \"\n                \"filter_bias_and_bn (default=True) will be disabled\"\n            )\n\n    # opt_args = dict(**kwargs)\n    # if lr is not None:\n    #    opt_args.setdefault('lr', lr)\n\n    assert (\n        loss_scale == 1.0\n    ), \"loss scale must be 1.0 in optimizer due to gradients are already scaled previously in TrainStepWrapper.\"\n\n    # non-adaptive: SGD, momentum, and nesterov\n    if opt == \"sgd\":\n        # note: nn.Momentum may perform better if momentum &gt; 0.\n        opt_args = _collect_args(kwargs, nn.SGD)\n\n        optimizer = nn.SGD(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt in [\"momentum\", \"nesterov\"]:\n        opt_args = _collect_args(kwargs, nn.Momentum)\n        optimizer = nn.Momentum(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            use_nesterov=nesterov,\n            loss_scale=loss_scale,\n        )\n    # adaptive\n    elif opt == \"adam\":\n        opt_args = _collect_args(kwargs, nn.Adam)\n        optimizer = nn.Adam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            use_nesterov=nesterov,\n            **opt_args,\n        )\n    elif opt == \"adamw\":\n        opt_args = _collect_args(kwargs, AdamW)\n        optimizer = AdamW(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lion\":\n        opt_args = _collect_args(kwargs, Lion)\n        optimizer = Lion(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"nadam\":\n        opt_args = _collect_args(kwargs, NAdam)\n        optimizer = NAdam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            schedule_decay=schedule_decay,\n            **opt_args,\n        )\n    elif opt == \"adan\":\n        opt_args = _collect_args(kwargs, Adan)\n        optimizer = Adan(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"rmsprop\":\n        opt_args = _collect_args(kwargs, nn.RMSProp)\n        optimizer = nn.RMSProp(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            epsilon=eps,\n            **opt_args,\n        )\n    elif opt == \"adagrad\":\n        opt_args = _collect_args(kwargs, nn.Adagrad)\n        optimizer = nn.Adagrad(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lamb\":\n        assert loss_scale == 1.0, \"Loss scaler is not supported by Lamb optimizer\"\n        opt_args = _collect_args(kwargs, nn.Lamb)\n        optimizer = nn.Lamb(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            **opt_args,\n        )\n    else:\n        raise ValueError(f\"Invalid optimizer: {opt}\")\n\n    if os.path.exists(checkpoint_path):\n        param_dict = load_checkpoint(checkpoint_path)\n        load_param_into_net(optimizer, param_dict)\n\n    return optimizer\n</code></pre>"},{"location":"reference/api_doc/#mindocr.optim.param_grouping","title":"<code>mindocr.optim.param_grouping</code>","text":"<p>group parameters for setting different weight decay or learning rate for different layers in the network.</p>"},{"location":"reference/api_doc/#mindocr.optim.param_grouping.create_group_params","title":"<code>mindocr.optim.param_grouping.create_group_params(params, weight_decay=0, grouping_strategy=None, no_weight_decay_params=[], **kwargs)</code>","text":"<p>create group parameters for setting different weight decay or learning rate for different layers in the network.</p> PARAMETER DESCRIPTION <code>params</code> <p>network params</p> <p> </p> <code>weight_decay</code> <p>weight decay value</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>grouping_strategy</code> <p>name of the hard-coded grouping strategy. If not None, group parameters according to the predefined function and <code>no_weight_decay_params</code> will not make effect.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>no_weight_decay_params</code> <p>list of the param name substrings that will be picked to exclude from weight decay. If a parameter containing one of the substrings in the list, the paramter will not be applied with weigt decay. (Tips: param names can be checked by <code>[p.name for p in network.trainable_params()]</code></p> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> Return <p>list[dict], grouped parameters</p> Source code in <code>mindocr\\optim\\param_grouping.py</code> <pre><code>def create_group_params(params, weight_decay=0, grouping_strategy=None, no_weight_decay_params=[], **kwargs):\n\"\"\"\n    create group parameters for setting different weight decay or learning rate for different layers in the network.\n\n    Args:\n        params: network params\n        weight_decay (float): weight decay value\n        grouping_strategy (str): name of the hard-coded grouping strategy. If not None, group parameters according to\n            the predefined function and `no_weight_decay_params` will not make effect.\n        no_weight_decay_params (list): list of the param name substrings that will be picked to exclude from\n            weight decay. If a parameter containing one of the substrings in the list, the paramter will not be applied\n            with weigt decay. (Tips: param names can be checked by `[p.name for p in network.trainable_params()]`\n\n    Return:\n        list[dict], grouped parameters\n    \"\"\"\n\n    # TODO: assert valid arg names\n    gp = grouping_strategy\n\n    # print(f'INFO: param grouping startegy: {grouping_strategy}, no_weight_decay_params: ', no_weight_decay_params)\n    if gp is not None:\n        if weight_decay == 0:\n            print(\"WARNING: weight decay is 0 in param grouping, which is meaningless. Please check config setting.\")\n        if len(no_weight_decay_params) &gt; 0:\n            print(\n                \"WARNING: Both grouping_strategy and no_weight_decay_params are set, but grouping_strategy is of prior.\"\n                \" no_weight_decay_params={no_weight_decay_params} will not make effect.\"\n            )\n\n        if gp == \"svtr\":\n            return grouping_svtr(params, weight_decay)\n        elif gp == \"filter_norm_and_bias\":\n            return grouping_default(params, weight_decay)\n        else:\n            raise ValueError(\n                f\"The grouping function for {gp} is not defined. \"\n                f\"Valid grouping strategies are {supported_grouping_strategies}\"\n            )\n\n    elif len(no_weight_decay_params) &gt; 0:\n        assert weight_decay &gt; 0, f\"Invalid weight decay value {weight_decay} for param grouping.\"\n        decay_params = []\n        no_decay_params = []\n        for param in params:\n            filter_param = False\n            for k in no_weight_decay_params:\n                if k in param.name:\n                    filter_param = True\n\n            if filter_param:\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        return [\n            {\"params\": decay_params, \"weight_decay\": weight_decay},\n            {\"params\": no_decay_params},\n            {\"order_params\": params},\n        ]\n    else:\n        print(\"INFO: no parameter grouping is applied.\")\n        return params\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess","title":"<code>mindocr.postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.build_postprocess","title":"<code>mindocr.postprocess.build_postprocess(config)</code>","text":"<p>Create postprocess function.</p> PARAMETER DESCRIPTION <code>config</code> <p>configuration for postprocess including postprocess <code>name</code> and also the kwargs specifically</p> <p> TYPE: <code>dict</code> </p> Return <p>Object</p> Example Source code in <code>mindocr\\postprocess\\builder.py</code> <pre><code>def build_postprocess(config: dict):\n\"\"\"\n    Create postprocess function.\n\n    Args:\n        config (dict): configuration for postprocess including postprocess `name` and also the kwargs specifically\n        for each postprocessor.\n            - name (str): metric function name, exactly the same as one of the supported postprocess class names\n\n    Return:\n        Object\n\n    Example:\n        &gt;&gt;&gt; # Create postprocess function\n        &gt;&gt;&gt; from mindocr.postprocess import build_postprocess\n        &gt;&gt;&gt; config = dict(name=\"RecCTCLabelDecode\", use_space_char=False)\n        &gt;&gt;&gt; postprocess = build_postprocess(config)\n        &gt;&gt;&gt; postprocess\n    \"\"\"\n    proc = config.pop(\"name\")\n    if proc in supported_postprocess:\n        postprocessor = eval(proc)(**config)\n    elif proc is None:\n        return None\n    else:\n        raise ValueError(f\"Invalid postprocess name {proc}, support postprocess are {supported_postprocess}\")\n\n    return postprocessor\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.build_postprocess--create-postprocess-function","title":"Create postprocess function","text":"<p>from mindocr.postprocess import build_postprocess config = dict(name=\"RecCTCLabelDecode\", use_space_char=False) postprocess = build_postprocess(config) postprocess</p>"},{"location":"reference/api_doc/#mindocr.postprocess.builder","title":"<code>mindocr.postprocess.builder</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.builder.build_postprocess","title":"<code>mindocr.postprocess.builder.build_postprocess(config)</code>","text":"<p>Create postprocess function.</p> PARAMETER DESCRIPTION <code>config</code> <p>configuration for postprocess including postprocess <code>name</code> and also the kwargs specifically</p> <p> TYPE: <code>dict</code> </p> Return <p>Object</p> Example Source code in <code>mindocr\\postprocess\\builder.py</code> <pre><code>def build_postprocess(config: dict):\n\"\"\"\n    Create postprocess function.\n\n    Args:\n        config (dict): configuration for postprocess including postprocess `name` and also the kwargs specifically\n        for each postprocessor.\n            - name (str): metric function name, exactly the same as one of the supported postprocess class names\n\n    Return:\n        Object\n\n    Example:\n        &gt;&gt;&gt; # Create postprocess function\n        &gt;&gt;&gt; from mindocr.postprocess import build_postprocess\n        &gt;&gt;&gt; config = dict(name=\"RecCTCLabelDecode\", use_space_char=False)\n        &gt;&gt;&gt; postprocess = build_postprocess(config)\n        &gt;&gt;&gt; postprocess\n    \"\"\"\n    proc = config.pop(\"name\")\n    if proc in supported_postprocess:\n        postprocessor = eval(proc)(**config)\n    elif proc is None:\n        return None\n    else:\n        raise ValueError(f\"Invalid postprocess name {proc}, support postprocess are {supported_postprocess}\")\n\n    return postprocessor\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.builder.build_postprocess--create-postprocess-function","title":"Create postprocess function","text":"<p>from mindocr.postprocess import build_postprocess config = dict(name=\"RecCTCLabelDecode\", use_space_char=False) postprocess = build_postprocess(config) postprocess</p>"},{"location":"reference/api_doc/#mindocr.postprocess.cls_postprocess","title":"<code>mindocr.postprocess.cls_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.cls_postprocess.ClsPostprocess","title":"<code>mindocr.postprocess.cls_postprocess.ClsPostprocess</code>","text":"<p>         Bases: <code>object</code></p> <p>Map the predicted index back to orignal format (angle).</p> Source code in <code>mindocr\\postprocess\\cls_postprocess.py</code> <pre><code>class ClsPostprocess(object):\n\"\"\"Map the predicted index back to orignal format (angle).\"\"\"\n\n    def __init__(self, label_list=None, **kwargs):\n        assert (\n            label_list is not None\n        ), \"`label_list` should not be None. Please set it in 'postprocess' section in yaml config file.\"\n        self.label_list = label_list\n\n    def __call__(self, preds, **kwargs):\n        if isinstance(preds, Tensor):\n            preds = preds.asnumpy()\n\n        pred_idxs = preds.argmax(axis=1)\n\n        angles, scores = [], []\n        for i, idx in enumerate(pred_idxs):\n            angles.append(self.label_list[idx])\n            scores.append(preds[i, idx])\n        decode_preds = {\"angles\": angles, \"scores\": scores}\n\n        return decode_preds\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.det_base_postprocess","title":"<code>mindocr.postprocess.det_base_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.det_base_postprocess.DetBasePostprocess","title":"<code>mindocr.postprocess.det_base_postprocess.DetBasePostprocess</code>","text":"<p>Base class for all text detection postprocessings.</p> PARAMETER DESCRIPTION <code>box_type</code> <p>text region representation type after postprocessing, options: ['quad', 'poly']</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quad'</code> </p> <code>rescale_fields</code> <p>names of fields to rescale back to the shape of the original image.</p> <p> TYPE: <code>list</code> DEFAULT: <code>['polys']</code> </p> Source code in <code>mindocr\\postprocess\\det_base_postprocess.py</code> <pre><code>class DetBasePostprocess:\n\"\"\"\n    Base class for all text detection postprocessings.\n\n    Args:\n        box_type (str): text region representation type after postprocessing, options: ['quad', 'poly']\n        rescale_fields (list): names of fields to rescale back to the shape of the original image.\n    \"\"\"\n\n    def __init__(self, box_type=\"quad\", rescale_fields: List[str] = [\"polys\"]):\n        assert box_type in [\"quad\", \"poly\"], f\"box_type must be `quad` or `poly`, but found {box_type}\"\n\n        self._rescale_fields = rescale_fields\n        self.warned = False\n        if self._rescale_fields is None:\n            print(\"WARNING: `rescale_filed` is None. Cannot rescale the predicted polygons to original image space\")\n\n    def _postprocess(self, pred: Union[ms.Tensor, Tuple[ms.Tensor], np.ndarray], **kwargs) -&gt; dict:\n\"\"\"\n        Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to\n        original image space in __call__ function)\n\n        Args:\n            pred: network prediction for input batch data, shape [batch_size, ...]\n\n        Return:\n            postprocessing result as a dict with keys:\n                - polys (List[List[np.ndarray]): predicted polygons on the **transformed**\n                (i.e. resized normally) image space, of shape (batch_size, num_polygons, num_points, 2).\n                If `box_type` is 'quad', num_points=4.\n                - scores (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)\n\n        Notes:\n            - Please cast `pred` to the type you need in your implementation. Some postprocesssing steps use ops from\n              mindspore.nn and prefer Tensor type, while some steps prefer np.ndarray type required in other libraries.\n            - `_postprocess()` should **NOT round** the text box `polys` to integer in return, because they will be\n              recaled and then rounded in the end. Rounding early will cause larger error in polygon rescaling and\n              results in **evaluation performance degradation**, especially on small datasets.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(\n        self,\n        pred: Union[ms.Tensor, Tuple[ms.Tensor], np.ndarray],\n        shape_list: Union[np.ndarray, ms.Tensor] = None,\n        **kwargs,\n    ) -&gt; dict:\n\"\"\"\n        Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get\n        text boxes and then rescale them back to the original image space.\n\n        Args:\n            pred (Union[Tensor, Tuple[Tensor], np.ndarray]): network prediction for input batch data,\n                shape [batch_size, ...]\n            shape_list (Union[np.ndarray, ms.Tensor]): shape and scale info for each image in the batch,\n                shape [batch_size, 4]. Each internal array is [src_h, src_w, scale_h, scale_w],\n                where src_h and src_w are height and width of the original image, and scale_h and scale_w\n                are their scale ratio during image resizing.\n\n        Returns:\n            detection result as a dict with keys:\n                - polys (List[List[np.ndarray]): predicted polygons mapped on the **original** image space,\n                  shape [batch_size, num_polygons, num_points, 2]. If `box_type` is 'quad', num_points=4,\n                  and the internal np.ndarray is of shape [4, 2]\n                - scores (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)\n        \"\"\"\n\n        # 1. Check input type. Covert shape_list to np.ndarray\n        if isinstance(shape_list, Tensor):\n            shape_list = shape_list.asnumpy()\n\n        if shape_list is not None:\n            assert shape_list.shape[0] and shape_list.shape[1] == 4, (\n                \"The shape of each item in shape_list must be 4: [raw_img_h, raw_img_w, scale_h, scale_w]. \"\n                f\"But got shape_list of shape {shape_list.shape}\"\n            )\n        else:\n            # shape_list = [[pred.shape[2], pred.shape[3], 1.0, 1.0] for i in range(pred.shape[0])] # H, W\n            # shape_list = np.array(shape_list, dtype='float32')\n\n            print(\n                \"WARNING: `shape_list` is None in postprocessing. Cannot rescale the prediction result to original \"\n                \"image space, which can lead to inaccurate evaluation. You may add `shape_list` to `output_columns` \"\n                \"list under eval section in yaml config file, or directly parse `shape_list` to postprocess callable \"\n                \"function.\"\n            )\n            self.warned = True\n\n        # 2. Core process\n        result = self._postprocess(pred, **kwargs)\n\n        # 3. Rescale processing results\n        if shape_list is not None and self._rescale_fields is not None:\n            result = self.rescale(result, shape_list)\n\n        return result\n\n    @staticmethod\n    def _rescale_polygons(polygons: Union[List[np.ndarray], np.ndarray], shape_list: np.ndarray):\n\"\"\"\n        polygons (Union[List[np.ndarray], np.ndarray]): polygons for an image, shape [num_polygons, num_points, 2],\n            value: xy coordinates for all polygon points\n        shape_list (np.ndarray): shape and scale info for the image, shape [4,], value: [src_h, src_w, scale_h, scale_w]\n        \"\"\"\n        scale = shape_list[:1:-1]\n        size = shape_list[1::-1] - 1\n\n        if isinstance(polygons, np.ndarray):\n            polygons = np.clip(np.round(polygons / scale), 0, size)\n        else:  # if polygons have different number of vertices and stored as a list\n            polygons = [np.clip(np.round(poly / scale), 0, size) for poly in polygons]\n\n        return polygons\n\n    def rescale(self, result: Dict, shape_list: np.ndarray) -&gt; dict:\n\"\"\"\n        rescale result back to original image shape\n\n        Args:\n            result (dict) with keys for the input data batch\n                polys (np.ndarray): polygons for a batch of images, shape [batch_size, num_polygons, num_points, 2].\n            shape_list (np.ndarray): image shape and scale info, shape [batch_size, 4]\n\n        Return:\n            rescaled result specified by rescale_field\n        \"\"\"\n\n        for field in self._rescale_fields:\n            assert (\n                field in result\n            ), f\"Invalid field {field}. Found fields in intermediate postprocess result are {list(result.keys())}\"\n\n            for i, sample in enumerate(result[field]):\n                if len(sample) &gt; 0:\n                    result[field][i] = self._rescale_polygons(sample, shape_list[i])\n\n        return result\n</code></pre> <code>mindocr.postprocess.det_base_postprocess.DetBasePostprocess.__call__(pred, shape_list=None, **kwargs)</code> \u00b6 <p>Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get text boxes and then rescale them back to the original image space.</p> PARAMETER DESCRIPTION <code>pred</code> <p>network prediction for input batch data, shape [batch_size, ...]</p> <p> TYPE: <code>Union[Tensor, Tuple[Tensor], np.ndarray]</code> </p> <code>shape_list</code> <p>shape and scale info for each image in the batch, shape [batch_size, 4]. Each internal array is [src_h, src_w, scale_h, scale_w], where src_h and src_w are height and width of the original image, and scale_h and scale_w are their scale ratio during image resizing.</p> <p> TYPE: <code>Union[np.ndarray, ms.Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>detection result as a dict with keys: - polys (List[List[np.ndarray]): predicted polygons mapped on the original image space,   shape [batch_size, num_polygons, num_points, 2]. If <code>box_type</code> is 'quad', num_points=4,   and the internal np.ndarray is of shape [4, 2] - scores (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</p> Source code in <code>mindocr\\postprocess\\det_base_postprocess.py</code> <pre><code>def __call__(\n    self,\n    pred: Union[ms.Tensor, Tuple[ms.Tensor], np.ndarray],\n    shape_list: Union[np.ndarray, ms.Tensor] = None,\n    **kwargs,\n) -&gt; dict:\n\"\"\"\n    Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get\n    text boxes and then rescale them back to the original image space.\n\n    Args:\n        pred (Union[Tensor, Tuple[Tensor], np.ndarray]): network prediction for input batch data,\n            shape [batch_size, ...]\n        shape_list (Union[np.ndarray, ms.Tensor]): shape and scale info for each image in the batch,\n            shape [batch_size, 4]. Each internal array is [src_h, src_w, scale_h, scale_w],\n            where src_h and src_w are height and width of the original image, and scale_h and scale_w\n            are their scale ratio during image resizing.\n\n    Returns:\n        detection result as a dict with keys:\n            - polys (List[List[np.ndarray]): predicted polygons mapped on the **original** image space,\n              shape [batch_size, num_polygons, num_points, 2]. If `box_type` is 'quad', num_points=4,\n              and the internal np.ndarray is of shape [4, 2]\n            - scores (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)\n    \"\"\"\n\n    # 1. Check input type. Covert shape_list to np.ndarray\n    if isinstance(shape_list, Tensor):\n        shape_list = shape_list.asnumpy()\n\n    if shape_list is not None:\n        assert shape_list.shape[0] and shape_list.shape[1] == 4, (\n            \"The shape of each item in shape_list must be 4: [raw_img_h, raw_img_w, scale_h, scale_w]. \"\n            f\"But got shape_list of shape {shape_list.shape}\"\n        )\n    else:\n        # shape_list = [[pred.shape[2], pred.shape[3], 1.0, 1.0] for i in range(pred.shape[0])] # H, W\n        # shape_list = np.array(shape_list, dtype='float32')\n\n        print(\n            \"WARNING: `shape_list` is None in postprocessing. Cannot rescale the prediction result to original \"\n            \"image space, which can lead to inaccurate evaluation. You may add `shape_list` to `output_columns` \"\n            \"list under eval section in yaml config file, or directly parse `shape_list` to postprocess callable \"\n            \"function.\"\n        )\n        self.warned = True\n\n    # 2. Core process\n    result = self._postprocess(pred, **kwargs)\n\n    # 3. Rescale processing results\n    if shape_list is not None and self._rescale_fields is not None:\n        result = self.rescale(result, shape_list)\n\n    return result\n</code></pre> <code>mindocr.postprocess.det_base_postprocess.DetBasePostprocess.rescale(result, shape_list)</code> \u00b6 <p>rescale result back to original image shape</p> PARAMETER DESCRIPTION <code>shape_list</code> <p>image shape and scale info, shape [batch_size, 4]</p> <p> TYPE: <code>np.ndarray</code> </p> Return <p>rescaled result specified by rescale_field</p> Source code in <code>mindocr\\postprocess\\det_base_postprocess.py</code> <pre><code>def rescale(self, result: Dict, shape_list: np.ndarray) -&gt; dict:\n\"\"\"\n    rescale result back to original image shape\n\n    Args:\n        result (dict) with keys for the input data batch\n            polys (np.ndarray): polygons for a batch of images, shape [batch_size, num_polygons, num_points, 2].\n        shape_list (np.ndarray): image shape and scale info, shape [batch_size, 4]\n\n    Return:\n        rescaled result specified by rescale_field\n    \"\"\"\n\n    for field in self._rescale_fields:\n        assert (\n            field in result\n        ), f\"Invalid field {field}. Found fields in intermediate postprocess result are {list(result.keys())}\"\n\n        for i, sample in enumerate(result[field]):\n            if len(sample) &gt; 0:\n                result[field][i] = self._rescale_polygons(sample, shape_list[i])\n\n    return result\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.det_db_postprocess","title":"<code>mindocr.postprocess.det_db_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.det_db_postprocess.DBPostprocess","title":"<code>mindocr.postprocess.det_db_postprocess.DBPostprocess</code>","text":"<p>         Bases: <code>DetBasePostprocess</code></p> <p>DBNet &amp; DBNet++ postprocessing pipeline: extracts polygons / rectangles from a binary map (heatmap) and returns     their coordinates.</p> PARAMETER DESCRIPTION <code>binary_thresh</code> <p>binarization threshold applied to the heatmap output of DBNet.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>box_thresh</code> <p>polygon confidence threshold. Polygons with scores lower than this threshold are filtered out.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>max_candidates</code> <p>maximum number of proposed polygons.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>expand_ratio</code> <p>controls by how much polygons need to be expanded to recover the original text shape (DBNet predicts shrunken text masks).</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>box_type</code> <p>output polygons ('polys') or rectangles ('quad') as the network's predictions.</p> <p> DEFAULT: <code>'quad'</code> </p> <code>pred_name</code> <p>heatmap's name used for polygons extraction.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'binary'</code> </p> <code>rescale_fields</code> <p>name of fields to scale back to the shape of the original image.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['polys']</code> </p> Source code in <code>mindocr\\postprocess\\det_db_postprocess.py</code> <pre><code>class DBPostprocess(DetBasePostprocess):\n\"\"\"\n    DBNet &amp; DBNet++ postprocessing pipeline: extracts polygons / rectangles from a binary map (heatmap) and returns\n        their coordinates.\n    Args:\n        binary_thresh: binarization threshold applied to the heatmap output of DBNet.\n        box_thresh: polygon confidence threshold. Polygons with scores lower than this threshold are filtered out.\n        max_candidates: maximum number of proposed polygons.\n        expand_ratio: controls by how much polygons need to be expanded to recover the original text shape\n            (DBNet predicts shrunken text masks).\n        box_type: output polygons ('polys') or rectangles ('quad') as the network's predictions.\n        pred_name: heatmap's name used for polygons extraction.\n        rescale_fields: name of fields to scale back to the shape of the original image.\n    \"\"\"\n\n    def __init__(\n        self,\n        binary_thresh: float = 0.3,\n        box_thresh: float = 0.7,\n        max_candidates: int = 1000,\n        expand_ratio: float = 1.5,\n        box_type=\"quad\",\n        pred_name: str = \"binary\",\n        rescale_fields: List[str] = [\"polys\"],\n    ):\n        super().__init__(box_type, rescale_fields)\n\n        self._min_size = 3\n        self._binary_thresh = binary_thresh\n        self._box_thresh = box_thresh\n        self._max_candidates = max_candidates\n        self._expand_ratio = expand_ratio\n        self._out_poly = box_type == \"poly\"\n        self._name = pred_name\n        self._names = {\"binary\": 0, \"thresh\": 1, \"thresh_binary\": 2}\n\n    def _postprocess(self, pred: Union[Tensor, Tuple[Tensor], np.ndarray], **kwargs) -&gt; dict:\n\"\"\"\n        Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to\n        original image space in __call__ function)\n\n        Args:\n            pred (Union[Tensor, Tuple[Tensor], np.ndarray]): network prediction consists of\n                binary: text region segmentation map, with shape (N, 1, H, W)\n                thresh: [if exists] threshold prediction with shape (N, 1, H, W) (optional)\n                thresh_binary: [if exists] binarized with threshold, (N, 1, H, W) (optional)\n\n        Returns:\n            postprocessing result as a dict with keys:\n                - polys (List[np.ndarray]): predicted polygons on the **transformed** (i.e. resized normally) image\n                space, of shape (batch_size, num_polygons, num_points, 2). If `box_type` is 'quad', num_points=4.\n                - scores (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)\n        \"\"\"\n        if isinstance(pred, tuple):\n            pred = pred[self._names[self._name]]\n        if isinstance(pred, Tensor):\n            pred = pred.asnumpy()\n        if len(pred.shape) == 4 and pred.shape[1] != 1:  # pred shape (N, 3, H, W)\n            pred = pred[:, :1, :, :]  # only need the first output\n        if len(pred.shape) == 4:  # handle pred shape: (N, H, W) skip\n            pred = pred.squeeze(1)\n\n        segmentation = pred &gt;= self._binary_thresh\n\n        polys, scores = [], []\n        for pr, segm in zip(pred, segmentation):\n            sample_polys, sample_scores = self._extract_preds(pr, segm)\n            polys.append(sample_polys)\n            scores.append(sample_scores)\n\n        return {\"polys\": polys, \"scores\": scores}\n\n    def _extract_preds(self, pred: np.ndarray, bitmap: np.ndarray):\n        outs = cv2.findContours(bitmap.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n        if len(outs) == 3:  # FIXME: update to OpenCV 4.x and delete this\n            _, contours, _ = outs[0], outs[1], outs[2]\n        elif len(outs) == 2:\n            contours, _ = outs[0], outs[1]\n\n        polys, scores = [], []\n        for contour in contours[: self._max_candidates]:\n            contour = contour.squeeze(1)\n            score = self._calc_score(pred, bitmap, contour)\n            if score &lt; self._box_thresh:\n                continue\n\n            if self._out_poly:\n                epsilon = 0.005 * cv2.arcLength(contour, closed=True)\n                points = cv2.approxPolyDP(contour, epsilon, closed=True).squeeze(1)\n                if points.shape[0] &lt; 4:\n                    continue\n            else:\n                points, min_side = self._fit_box(contour)\n                if min_side &lt; self._min_size:\n                    continue\n\n            poly = Polygon(points)\n            poly = np.array(expand_poly(points, distance=poly.area * self._expand_ratio / poly.length))\n            if self._out_poly and len(poly) &gt; 1:\n                continue\n            poly = poly.reshape(-1, 2)\n\n            _box, min_side = self._fit_box(poly)\n            if min_side &lt; self._min_size + 2:\n                continue\n            if not self._out_poly:\n                poly = _box\n\n            # TODO: an alternative solution to avoid calling self._fit_box twice:\n            # box = Polygon(points)\n            # box = np.array(\n            # expand_poly(points, distance=box.area * self._expand_ratio / box.length, joint_type=pyclipper.JT_MITER))\n            # assert box.shape[0] == 4, print(f'box shape is {box.shape}')\n\n            polys.append(poly)\n            scores.append(score)\n\n        if self._out_poly:\n            return polys, scores\n        return np.array(polys), np.array(scores).astype(np.float32)\n\n    @staticmethod\n    def _fit_box(contour):\n\"\"\"\n        Finds a minimum rotated rectangle enclosing the contour.\n        \"\"\"\n        # box = cv2.minAreaRect(contour)  # returns center of a rect, size, and angle\n        # # TODO: does the starting point really matter?\n        # points = np.roll(cv2.boxPoints(box), -1, axis=0)  # extract box points from a rotated rectangle\n        # return points, min(box[1])\n        # box = cv2.minAreaRect(contour)  # returns center of a rect, size, and angle\n        # # TODO: does the starting point really matter?\n        # points = np.roll(cv2.boxPoints(box), -1, axis=0)  # extract box points from a rotated rectangle\n        # return points, min(box[1])\n\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        # index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] &gt; points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] &gt; points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2], points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    @staticmethod\n    def _calc_score(pred, mask, contour):\n        # calculates score (mean value) of a prediction inside a given contour.\n        min_vals = np.clip(np.floor(np.min(contour, axis=0)), 0, np.array(pred.shape[::-1]) - 1).astype(np.int32)\n        max_vals = np.clip(np.ceil(np.max(contour, axis=0)), 0, np.array(pred.shape[::-1]) - 1).astype(np.int32)\n        return cv2.mean(\n            pred[min_vals[1] : max_vals[1] + 1, min_vals[0] : max_vals[0] + 1],\n            mask[min_vals[1] : max_vals[1] + 1, min_vals[0] : max_vals[0] + 1].astype(np.uint8),\n        )[0]\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.det_east_postprocess","title":"<code>mindocr.postprocess.det_east_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.det_east_postprocess.EASTPostprocess","title":"<code>mindocr.postprocess.det_east_postprocess.EASTPostprocess</code>","text":"<p>         Bases: <code>DetBasePostprocess</code></p> Source code in <code>mindocr\\postprocess\\det_east_postprocess.py</code> <pre><code>class EASTPostprocess(DetBasePostprocess):\n    def __init__(self, score_thresh=0.8, nms_thresh=0.2, box_type=\"quad\", rescale_fields=[\"polys\"]):\n        super().__init__(box_type, rescale_fields)\n        self._score_thresh = score_thresh\n        self._nms_thresh = nms_thresh\n        if rescale_fields is None:\n            rescale_fields = []\n        self._rescale_fields = rescale_fields\n\n    def _postprocess(self, pred, **kwargs):\n\"\"\"\n        get boxes from feature map\n        Input:\n                pred (tuple) - (score, geo)\n                    'score'       : score map from model &lt;Tensor, (bs,1,row,col)&gt;\n                    'geo'         : geo map from model &lt;Tensor, (bs,5,row,col)&gt;\n        Output:\n                boxes       : dict of polys and scores {'polys': &lt;numpy.ndarray, (bs,n,4,2)&gt;, 'scores': numpy.ndarray,\n                (bs,n,1)&gt;)}\n        \"\"\"\n        score, geo = pred\n        if isinstance(score, Tensor):\n            score = score.asnumpy()\n        if isinstance(geo, Tensor):\n            geo = geo.asnumpy()\n        img_num = score.shape[0]\n        polys_list = []\n        scores_list = []\n        for i in range(img_num):\n            score, geo = score[i], geo[i]\n            score = score[0, :, :]\n            xy_text = np.argwhere(score &gt; self._score_thresh)\n            if xy_text.size == 0:\n                polys = np.array([[[1, 2], [3, 4], [5, 6], [7, 8]]], \"float32\")\n                scores = np.array([[0.0]])\n                polys_list.append(polys)\n                scores_list.append(scores)\n                continue\n\n            xy_text = xy_text[np.argsort(xy_text[:, 0])]\n            valid_pos = xy_text[:, ::-1].copy()  # n x 2, [x, y]\n            valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]]  # 5 x n\n            polys_restored, index = self._restore_polys(valid_pos, valid_geo, score.shape)\n\n            if polys_restored.size == 0:\n                polys = np.array([[[1, 2], [3, 4], [5, 6], [7, 8]]], \"float32\")\n                scores = np.array([[0.0]])\n                polys_list.append(polys)\n                scores_list.append(scores)\n                continue\n\n            boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n            boxes[:, :8] = polys_restored\n            boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n            boxes = lanms.merge_quadrangle_n9(boxes.astype(\"float32\"), self._nms_thresh)\n            polys = boxes[:, :8].reshape(-1, 4, 2)\n            scores = boxes[:, 8].reshape(-1, 1)\n            polys_list.append(polys)\n            scores_list.append(scores)\n        return {\"polys\": np.array(polys_list), \"scores\": np.array(scores_list)}\n\n    def _restore_polys(self, valid_pos, valid_geo, score_shape, scale=4):\n\"\"\"\n        restore polys from feature maps in given positions\n        Input:\n                valid_pos  : potential text positions &lt;numpy.ndarray, (n,2)&gt;\n                valid_geo  : geometry in valid_pos &lt;numpy.ndarray, (5,n)&gt;\n                score_shape: shape of score map\n                scale      : image / feature map\n        Output:\n                restored polys &lt;numpy.ndarray, (n,8)&gt;, index\n        \"\"\"\n        polys = []\n        index = []\n        valid_pos *= scale\n        d = valid_geo[:4, :]  # 4 x N\n        angle = valid_geo[4, :]  # N,\n\n        for i in range(valid_pos.shape[0]):\n            x = valid_pos[i, 0]\n            y = valid_pos[i, 1]\n            y_min = y - d[0, i]\n            y_max = y + d[1, i]\n            x_min = x - d[2, i]\n            x_max = x + d[3, i]\n            rotate_mat = self._get_rotate_mat(-angle[i])\n\n            temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\n            temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\n            coordidates = np.concatenate((temp_x, temp_y), axis=0)\n            res = np.dot(rotate_mat, coordidates)\n            res[0, :] += x\n            res[1, :] += y\n\n            if self._is_valid_poly(res, score_shape, scale):\n                index.append(i)\n                polys.append([res[0, 0], res[1, 0], res[0, 1], res[1, 1], res[0, 2], res[1, 2], res[0, 3], res[1, 3]])\n        return np.array(polys), index\n\n    def _get_rotate_mat(self, theta):\n\"\"\"positive theta value means rotate clockwise\"\"\"\n        return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n\n    def _is_valid_poly(self, res, score_shape, scale):\n\"\"\"\n        check if the poly in image scope\n        Input:\n                res        : restored poly in original image\n                score_shape: score map shape\n                scale      : feature map -&gt; image\n        Output:\n                True if valid\n        \"\"\"\n        cnt = 0\n        for i in range(res.shape[1]):\n            if (\n                res[0, i] &lt; 0\n                or res[0, i] &gt;= score_shape[1] * scale\n                or res[1, i] &lt; 0\n                or res[1, i] &gt;= score_shape[0] * scale\n            ):\n                cnt += 1\n        return cnt &lt;= 1\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.det_pse_postprocess","title":"<code>mindocr.postprocess.det_pse_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.det_pse_postprocess.PSEPostprocess","title":"<code>mindocr.postprocess.det_pse_postprocess.PSEPostprocess</code>","text":"<p>         Bases: <code>DetBasePostprocess</code></p> Source code in <code>mindocr\\postprocess\\det_pse_postprocess.py</code> <pre><code>class PSEPostprocess(DetBasePostprocess):\n    def __init__(\n        self,\n        binary_thresh=0.5,\n        box_thresh=0.85,\n        min_area=16,\n        box_type=\"quad\",\n        scale=4,\n        rescale_fields=[\"polys\"],\n    ):\n        super().__init__(box_type, rescale_fields)\n\n        from .pse import pse\n\n        self._binary_thresh = binary_thresh\n        self._box_thresh = box_thresh\n        self._min_area = min_area\n        self._box_type = box_type\n        self._scale = scale\n        self._interpolate = nn.ResizeBilinear()\n        self._sigmoid = nn.Sigmoid()\n        if rescale_fields is None:\n            rescale_fields = []\n        self._rescale_fields = rescale_fields\n        self._pse = pse\n\n    def _postprocess(self, pred, **kwargs):  # pred: N 7 H W\n\"\"\"\n        Args:\n            pred (Tensor): network prediction with shape [BS, C, H, W]\n        \"\"\"\n        if isinstance(pred, tuple):  # used when inference, only need the first output\n            pred = pred[0]\n        if not isinstance(pred, Tensor):\n            pred = Tensor(pred)\n\n        pred = self._interpolate(pred, scale_factor=4 // self._scale)\n        score = self._sigmoid(pred[:, 0, :, :])\n\n        kernels = (pred &gt; self._binary_thresh).astype(ms.float32)\n        text_mask = kernels[:, :1, :, :]\n        text_mask = text_mask.astype(ms.int8)\n\n        kernels[:, 1:, :, :] = kernels[:, 1:, :, :] * text_mask\n        score = score.asnumpy()\n        kernels = kernels.asnumpy().astype(np.uint8)\n        poly_list, score_list = [], []\n        for batch_idx in range(pred.shape[0]):\n            boxes, scores = self._boxes_from_bitmap(score[batch_idx], kernels[batch_idx])\n            poly_list.append(boxes)\n            score_list.append(scores)\n\n        return {\"polys\": poly_list, \"scores\": score_list}\n\n    def _boxes_from_bitmap(self, score, kernels):\n        label = self._pse(kernels, self._min_area)\n        return self._generate_box(score, label)\n\n    def _generate_box(self, score, label):\n        label_num = np.max(label) + 1\n        boxes = []\n        scores = []\n        for i in range(1, label_num):\n            ind = label == i\n            points = np.array(np.where(ind)).transpose((1, 0))[:, ::-1]\n            if points.shape[0] &lt; self._min_area:\n                label[ind] = 0\n                continue\n\n            score_i = np.mean(score[ind])\n            if score_i &lt; self._box_thresh:\n                label[ind] = 0\n                continue\n\n            if self._box_type == \"quad\":\n                rect = cv2.minAreaRect(points)\n                bbox = cv2.boxPoints(rect)\n            elif self._box_type == \"poly\":\n                box_height = np.max(points[:, 1]) + 10\n                box_width = np.max(points[:, 0]) + 10\n                mask = np.zeros((box_height, box_width), np.uint8)\n                mask[points[:, 1], points[:, 0]] = 255\n                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                bbox = np.squeeze(contours[0], 1)\n            else:\n                raise NotImplementedError(\n                    f\"The value of param 'box_type' can only be 'quad', but got '{self._box_type}'.\"\n                )\n            boxes.append(bbox)\n            scores.append(score_i)\n\n        return boxes, scores\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.rec_postprocess","title":"<code>mindocr.postprocess.rec_postprocess</code>","text":""},{"location":"reference/api_doc/#mindocr.postprocess.rec_postprocess.RecAttnLabelDecode","title":"<code>mindocr.postprocess.rec_postprocess.RecAttnLabelDecode</code>","text":"Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>class RecAttnLabelDecode:\n    def __init__(\n        self, character_dict_path: Optional[str] = None, use_space_char: bool = False, lower: bool = False\n    ) -&gt; None:\n\"\"\"\n        Convert text label (str) to a sequence of character indices according to the char dictionary\n\n        Args:\n            character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n                (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n            use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n            lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n                Set to be True if dictionary only contains lower-case chars.\n                Set to be False if not and want to recognition both upper-case and lower-case.\n\n        Attributes:\n            go_idx: the index of the GO token\n            stop_idx: the index of the STOP token\n            num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n            num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n                so num_classes = num_valid_chars + 1\n        \"\"\"\n        self.lower = lower\n\n        # read dict\n        if character_dict_path is None:\n            char_list = list(\"0123456789abcdefghijklmnopqrstuvwxyz\")\n\n            self.lower = True\n            print(\"INFO: The character_dict_path is None, model can only recognize number and lower letters\")\n        else:\n            # parse char dictionary\n            char_list = []\n            with open(character_dict_path, \"r\") as f:\n                for line in f:\n                    c = line.rstrip(\"\\n\\r\")\n                    char_list.append(c)\n\n        # add space char if set\n        if use_space_char:\n            if \" \" not in char_list:\n                char_list.append(\" \")\n            self.space_idx = len(char_list) + 1\n        else:\n            if \" \" in char_list:\n                print(\n                    \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                    \"because the space char is coded in the dictionary file \",\n                    character_dict_path,\n                )\n\n        self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n        special_token = [\"&lt;GO&gt;\", \"&lt;STOP&gt;\"]\n        char_list = special_token + char_list\n\n        self.go_idx = 0\n        self.stop_idx = 1\n\n        self.character = {idx: c for idx, c in enumerate(char_list)}\n\n        self.num_classes = len(self.character)\n\n    def decode(self, char_indices: np.ndarray, probs: np.ndarray) -&gt; Tuple[List[str], List[float]]:\n        texts = list()\n        confs = list()\n\n        batch_size = len(char_indices)\n        for batch_idx in range(batch_size):\n            char_list = [self.character[i] for i in char_indices[batch_idx]]\n\n            try:\n                pred_EOS = char_list.index(\"&lt;STOP&gt;\")\n            except ValueError:\n                pred_EOS = -1\n\n            if self.lower:\n                char_list = [x.lower() for x in char_list]\n\n            if pred_EOS != -1:\n                char_list = char_list[:pred_EOS]\n                text = \"\".join(char_list)\n            else:\n                text = \"\"\n\n            if probs is not None and pred_EOS != -1:\n                conf_list = probs[batch_idx][:pred_EOS]\n            else:\n                conf_list = [0]\n\n            texts.append(text)\n            confs.append(np.mean(conf_list))\n        return texts, confs\n\n    def __call__(self, preds: Union[Tensor, np.ndarray], labels=None, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Args:\n            preds (dict or tuple): containing prediction tensor in shape [BS, W, num_classes]\n        Return:\n            texts (List[Tuple]): list of string\n        \"\"\"\n        if isinstance(preds, tuple):\n            preds = preds[-1]\n\n        if isinstance(preds, Tensor):\n            preds = preds.asnumpy()\n\n        pred_indices = preds.argmax(axis=-1)\n        pred_probs = preds.max(axis=-1)\n\n        raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n\n        texts, confs = self.decode(pred_indices, pred_probs)\n\n        return {\"texts\": texts, \"confs\": confs, \"raw_chars\": raw_chars}\n</code></pre> <code>mindocr.postprocess.rec_postprocess.RecAttnLabelDecode.__call__(preds, labels=None, **kwargs)</code> \u00b6 PARAMETER DESCRIPTION <code>preds</code> <p>containing prediction tensor in shape [BS, W, num_classes]</p> <p> TYPE: <code>dict or tuple</code> </p> Return <p>texts (List[Tuple]): list of string</p> Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>def __call__(self, preds: Union[Tensor, np.ndarray], labels=None, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Args:\n        preds (dict or tuple): containing prediction tensor in shape [BS, W, num_classes]\n    Return:\n        texts (List[Tuple]): list of string\n    \"\"\"\n    if isinstance(preds, tuple):\n        preds = preds[-1]\n\n    if isinstance(preds, Tensor):\n        preds = preds.asnumpy()\n\n    pred_indices = preds.argmax(axis=-1)\n    pred_probs = preds.max(axis=-1)\n\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n\n    texts, confs = self.decode(pred_indices, pred_probs)\n\n    return {\"texts\": texts, \"confs\": confs, \"raw_chars\": raw_chars}\n</code></pre> <code>mindocr.postprocess.rec_postprocess.RecAttnLabelDecode.__init__(character_dict_path=None, use_space_char=False, lower=False)</code> \u00b6 <p>Convert text label (str) to a sequence of character indices according to the char dictionary</p> PARAMETER DESCRIPTION <code>character_dict_path</code> <p>path to dictionary, if None, a dictionary containing 36 chars (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>use_space_char(bool)</code> <p>if True, add space char to the dict to recognize the space in between two words</p> <p> </p> <code>lower</code> <p>if True, all upper-case chars in the label text will be converted to lower case. Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to recognition both upper-case and lower-case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> ATTRIBUTE DESCRIPTION <code>go_idx</code> <p>the index of the GO token</p> <p> </p> <code>stop_idx</code> <p>the index of the STOP token</p> <p> </p> <code>num_valid_chars</code> <p>the number of valid characters (including space char if used) in the dictionary</p> <p> </p> <code>num_classes</code> <p>the number of classes (which valid characters char and the speical token for blank padding). so num_classes = num_valid_chars + 1</p> <p> </p> Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>def __init__(\n    self, character_dict_path: Optional[str] = None, use_space_char: bool = False, lower: bool = False\n) -&gt; None:\n\"\"\"\n    Convert text label (str) to a sequence of character indices according to the char dictionary\n\n    Args:\n        character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n            (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n        use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n        lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n            Set to be True if dictionary only contains lower-case chars.\n            Set to be False if not and want to recognition both upper-case and lower-case.\n\n    Attributes:\n        go_idx: the index of the GO token\n        stop_idx: the index of the STOP token\n        num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n        num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n            so num_classes = num_valid_chars + 1\n    \"\"\"\n    self.lower = lower\n\n    # read dict\n    if character_dict_path is None:\n        char_list = list(\"0123456789abcdefghijklmnopqrstuvwxyz\")\n\n        self.lower = True\n        print(\"INFO: The character_dict_path is None, model can only recognize number and lower letters\")\n    else:\n        # parse char dictionary\n        char_list = []\n        with open(character_dict_path, \"r\") as f:\n            for line in f:\n                c = line.rstrip(\"\\n\\r\")\n                char_list.append(c)\n\n    # add space char if set\n    if use_space_char:\n        if \" \" not in char_list:\n            char_list.append(\" \")\n        self.space_idx = len(char_list) + 1\n    else:\n        if \" \" in char_list:\n            print(\n                \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                \"because the space char is coded in the dictionary file \",\n                character_dict_path,\n            )\n\n    self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n    special_token = [\"&lt;GO&gt;\", \"&lt;STOP&gt;\"]\n    char_list = special_token + char_list\n\n    self.go_idx = 0\n    self.stop_idx = 1\n\n    self.character = {idx: c for idx, c in enumerate(char_list)}\n\n    self.num_classes = len(self.character)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.postprocess.rec_postprocess.RecCTCLabelDecode","title":"<code>mindocr.postprocess.rec_postprocess.RecCTCLabelDecode</code>","text":"<p>         Bases: <code>object</code></p> <p>Convert text label (str) to a sequence of character indices according to the char dictionary</p> PARAMETER DESCRIPTION <code>character_dict_path</code> <p>path to dictionary, if None, a dictionary containing 36 chars (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.</p> <p> DEFAULT: <code>None</code> </p> <code>use_space_char(bool)</code> <p>if True, add space char to the dict to recognize the space in between two words</p> <p> </p> <code>blank_at_last(bool)</code> <p>padding with blank index (not the space index). If True, a blank/padding token will be appended to the end of the dictionary, so that blank_index = num_chars, where num_chars is the number of character in the dictionary including space char if used. If False, blank token will be inserted in the beginning of the dictionary, so blank_index=0.</p> <p> </p> <code>lower</code> <p>if True, all upper-case chars in the label text will be converted to lower case. Set to be True if dictionary only contains lower-case chars. Set to be False if not and want to recognition both upper-case and lower-case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> ATTRIBUTE DESCRIPTION <code>blank_idx</code> <p>the index of the blank token for padding</p> <p> </p> <code>num_valid_chars</code> <p>the number of valid characters (including space char if used) in the dictionary</p> <p> </p> <code>num_classes</code> <p>the number of classes (which valid characters char and the speical token for blank padding). so num_classes = num_valid_chars + 1</p> <p> </p> Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>class RecCTCLabelDecode(object):\n\"\"\"Convert text label (str) to a sequence of character indices according to the char dictionary\n\n    Args:\n        character_dict_path: path to dictionary, if None, a dictionary containing 36 chars\n            (i.e., \"0123456789abcdefghijklmnopqrstuvwxyz\") will be used.\n        use_space_char(bool): if True, add space char to the dict to recognize the space in between two words\n        blank_at_last(bool): padding with blank index (not the space index).\n            If True, a blank/padding token will be appended to the end of the dictionary, so that\n            blank_index = num_chars, where num_chars is the number of character in the dictionary including space char\n            if used. If False, blank token will be inserted in the beginning of the dictionary, so blank_index=0.\n        lower (bool): if True, all upper-case chars in the label text will be converted to lower case.\n            Set to be True if dictionary only contains lower-case chars.\n            Set to be False if not and want to recognition both upper-case and lower-case.\n\n    Attributes:\n        blank_idx: the index of the blank token for padding\n        num_valid_chars: the number of valid characters (including space char if used) in the dictionary\n        num_classes: the number of classes (which valid characters char and the speical token for blank padding).\n            so num_classes = num_valid_chars + 1\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path=None,\n        use_space_char=False,\n        blank_at_last=True,\n        lower=False,\n    ):\n        self.space_idx = None\n        self.lower = lower\n\n        # read dict\n        if character_dict_path is None:\n            char_list = [c for c in \"0123456789abcdefghijklmnopqrstuvwxyz\"]\n            self.lower = True\n            print(\n                \"INFO: `character_dict_path` for RecCTCLabelDecode is not given. \"\n                'Default dict \"0123456789abcdefghijklmnopqrstuvwxyz\" is applied. Only number and English letters '\n                \"(regardless of lower/upper case) will be recognized and evaluated.\"\n            )\n        else:\n            # parse char dictionary\n            char_list = []\n            with open(character_dict_path, \"r\") as f:\n                for line in f:\n                    c = line.rstrip(\"\\n\\r\")\n                    char_list.append(c)\n        # add space char if set\n        if use_space_char:\n            if \" \" not in char_list:\n                char_list.append(\" \")\n            self.space_idx = len(char_list) - 1\n        else:\n            if \" \" in char_list:\n                print(\n                    \"WARNING: The dict still contains space char in dict although use_space_char is set to be False, \"\n                    \"because the space char is coded in the dictionary file \",\n                    character_dict_path,\n                )\n\n        self.num_valid_chars = len(char_list)  # the number of valid chars (including space char if used)\n\n        # add blank token for padding\n        if blank_at_last:\n            # the index of a char in dict is [0, num_chars-1], blank index is set to num_chars\n            char_list.append(\"&lt;PAD&gt;\")\n            self.blank_idx = self.num_valid_chars\n        else:\n            char_list = [\"&lt;PAD&gt;\"] + char_list\n            self.blank_idx = 0\n\n        self.ignore_indices = [self.blank_idx]\n\n        self.character = {idx: c for idx, c in enumerate(char_list)}\n\n        self.num_classes = len(self.character)\n\n    def decode(self, char_indices, prob=None, remove_duplicate=False):\n\"\"\"\n        Convert to a squence of char indices to text string\n        Args:\n            char_indices (np.ndarray): in shape [BS, W]\n        Returns:\n            text\n        \"\"\"\n\n\"\"\" convert text-index into text-label. \"\"\"\n        texts = []\n        confs = []\n        batch_size = len(char_indices)\n        for batch_idx in range(batch_size):\n            selection = np.ones(len(char_indices[batch_idx]), dtype=bool)\n            if remove_duplicate:\n                selection[1:] = char_indices[batch_idx][1:] != char_indices[batch_idx][:-1]\n            for ignored_token in self.ignore_indices:\n                selection &amp;= char_indices[batch_idx] != ignored_token\n\n            char_list = [self.character[text_id] for text_id in char_indices[batch_idx][selection]]\n            if prob is not None:\n                conf_list = prob[batch_idx][selection]\n            else:\n                conf_list = [1] * len(selection)\n            if len(conf_list) == 0:\n                conf_list = [0]\n\n            if self.lower:\n                char_list = [x.lower() for x in char_list]\n\n            text = \"\".join(char_list)\n\n            # result_list.append((text, np.mean(conf_list).tolist()))\n            texts.append(text)\n            confs.append(np.mean(conf_list))\n        return texts, confs\n\n    def __call__(self, preds: Union[Tensor, np.ndarray], labels=None, **kwargs):\n\"\"\"\n        Args:\n            preds (Union[Tensor, np.ndarray]): network prediction, class probabilities in shape [BS, W, num_classes],\n                where W is the sequence length.\n            labels: optional\n        Return:\n            texts (List[Tuple]): list of string\n\n        \"\"\"\n        if isinstance(preds, tuple):\n            preds = preds[-1]\n\n        if isinstance(preds, Tensor):\n            preds = preds.asnumpy()\n\n        # preds = preds.transpose([1, 0, 2]) # [W, BS, C] -&gt; [BS, W, C]. already did in model head.\n        pred_indices = preds.argmax(axis=-1)\n        pred_prob = preds.max(axis=-1)\n\n        # print('pred indices: ', pred_indices)\n        # print('pred prob: ', pred_prob.shape)\n\n        # TODO: for debug only\n        raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n\n        texts, confs = self.decode(pred_indices, pred_prob, remove_duplicate=True)\n\n        return {\"texts\": texts, \"confs\": confs, \"raw_chars\": raw_chars}\n</code></pre> <code>mindocr.postprocess.rec_postprocess.RecCTCLabelDecode.__call__(preds, labels=None, **kwargs)</code> \u00b6 PARAMETER DESCRIPTION <code>preds</code> <p>network prediction, class probabilities in shape [BS, W, num_classes], where W is the sequence length.</p> <p> TYPE: <code>Union[Tensor, np.ndarray]</code> </p> <code>labels</code> <p>optional</p> <p> DEFAULT: <code>None</code> </p> Return <p>texts (List[Tuple]): list of string</p> Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>def __call__(self, preds: Union[Tensor, np.ndarray], labels=None, **kwargs):\n\"\"\"\n    Args:\n        preds (Union[Tensor, np.ndarray]): network prediction, class probabilities in shape [BS, W, num_classes],\n            where W is the sequence length.\n        labels: optional\n    Return:\n        texts (List[Tuple]): list of string\n\n    \"\"\"\n    if isinstance(preds, tuple):\n        preds = preds[-1]\n\n    if isinstance(preds, Tensor):\n        preds = preds.asnumpy()\n\n    # preds = preds.transpose([1, 0, 2]) # [W, BS, C] -&gt; [BS, W, C]. already did in model head.\n    pred_indices = preds.argmax(axis=-1)\n    pred_prob = preds.max(axis=-1)\n\n    # print('pred indices: ', pred_indices)\n    # print('pred prob: ', pred_prob.shape)\n\n    # TODO: for debug only\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n\n    texts, confs = self.decode(pred_indices, pred_prob, remove_duplicate=True)\n\n    return {\"texts\": texts, \"confs\": confs, \"raw_chars\": raw_chars}\n</code></pre> <code>mindocr.postprocess.rec_postprocess.RecCTCLabelDecode.decode(char_indices, prob=None, remove_duplicate=False)</code> \u00b6 <p>Convert to a squence of char indices to text string</p> PARAMETER DESCRIPTION <code>char_indices</code> <p>in shape [BS, W]</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <p>text</p> Source code in <code>mindocr\\postprocess\\rec_postprocess.py</code> <pre><code>def decode(self, char_indices, prob=None, remove_duplicate=False):\n\"\"\"\n    Convert to a squence of char indices to text string\n    Args:\n        char_indices (np.ndarray): in shape [BS, W]\n    Returns:\n        text\n    \"\"\"\n\n\"\"\" convert text-index into text-label. \"\"\"\n    texts = []\n    confs = []\n    batch_size = len(char_indices)\n    for batch_idx in range(batch_size):\n        selection = np.ones(len(char_indices[batch_idx]), dtype=bool)\n        if remove_duplicate:\n            selection[1:] = char_indices[batch_idx][1:] != char_indices[batch_idx][:-1]\n        for ignored_token in self.ignore_indices:\n            selection &amp;= char_indices[batch_idx] != ignored_token\n\n        char_list = [self.character[text_id] for text_id in char_indices[batch_idx][selection]]\n        if prob is not None:\n            conf_list = prob[batch_idx][selection]\n        else:\n            conf_list = [1] * len(selection)\n        if len(conf_list) == 0:\n            conf_list = [0]\n\n        if self.lower:\n            char_list = [x.lower() for x in char_list]\n\n        text = \"\".join(char_list)\n\n        # result_list.append((text, np.mean(conf_list).tolist()))\n        texts.append(text)\n        confs.append(np.mean(conf_list))\n    return texts, confs\n</code></pre>"},{"location":"reference/api_doc/#mindocr.scheduler","title":"<code>mindocr.scheduler</code>","text":"<p>Learning Rate Scheduler</p>"},{"location":"reference/api_doc/#mindocr.scheduler.dynamic_lr","title":"<code>mindocr.scheduler.dynamic_lr</code>","text":"<p>Meta learning rate scheduler.</p> <p>This module implements exactly the same learning rate scheduler as native PyTorch, see <code>\"torch.optim.lr_scheduler\" &lt;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&gt;</code>_. At present, only <code>constant_lr</code>, <code>linear_lr</code>, <code>polynomial_lr</code>, <code>exponential_lr</code>, <code>step_lr</code>, <code>multi_step_lr</code>, <code>cosine_annealing_lr</code>, <code>cosine_annealing_warm_restarts_lr</code> are implemented. The number, name and usage of the Positional Arguments are exactly the same as those of native PyTorch.</p> <p>However, due to the constraint of having to explicitly return the learning rate at each step, we have to introduce additional Keyword Arguments. There are only three Keyword Arguments introduced, namely <code>lr</code>, <code>steps_per_epoch</code> and <code>epochs</code>, explained as follows: <code>lr</code>: the basic learning rate when creating optim in torch. <code>steps_per_epoch</code>: the number of steps(iterations) of each epoch. <code>epochs</code>: the number of epoch. It and <code>steps_per_epoch</code> determine the length of the returned lrs.</p> <p>Since most scheduler in PyTorch are coarse-grained, that is the learning rate is constant within a single epoch. For non-stepwise scheduler, we introduce several fine-grained variation, that is the learning rate is also changed within a single epoch. The function name of these variants have the <code>refined</code> keyword. The implemented fine-grained variation are list as follows: <code>linear_refined_lr</code>, <code>polynomial_refined_lr</code>, etc.</p>"},{"location":"reference/api_doc/#mindocr.scheduler.dynamic_lr.cosine_decay_lr","title":"<code>mindocr.scheduler.dynamic_lr.cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every epoch</p> Source code in <code>mindocr\\scheduler\\dynamic_lr.py</code> <pre><code>def cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n\"\"\"update every epoch\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = math.floor(i / steps_per_epoch)\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/api_doc/#mindocr.scheduler.dynamic_lr.cosine_decay_refined_lr","title":"<code>mindocr.scheduler.dynamic_lr.cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every step</p> Source code in <code>mindocr\\scheduler\\dynamic_lr.py</code> <pre><code>def cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n\"\"\"update every step\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = i / steps_per_epoch\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/api_doc/#mindocr.scheduler.multi_step_decay_lr","title":"<code>mindocr.scheduler.multi_step_decay_lr</code>","text":"<p>MultiStep Decay Learning Rate Scheduler</p>"},{"location":"reference/api_doc/#mindocr.scheduler.multi_step_decay_lr.MultiStepDecayLR","title":"<code>mindocr.scheduler.multi_step_decay_lr.MultiStepDecayLR</code>","text":"<p>         Bases: <code>LearningRateSchedule</code></p> <p>Multiple step learning rate The learning rate will decay once the number of step reaches one of the milestones.</p> Source code in <code>mindocr\\scheduler\\multi_step_decay_lr.py</code> <pre><code>class MultiStepDecayLR(LearningRateSchedule):\n\"\"\"Multiple step learning rate\n    The learning rate will decay once the number of step reaches one of the milestones.\n    \"\"\"\n\n    def __init__(self, lr, warmup_epochs, decay_rate, milestones, steps_per_epoch, num_epochs):\n        super().__init__()\n        self.warmup_steps = warmup_epochs * steps_per_epoch\n        num_steps = num_epochs * steps_per_epoch\n        step_lrs = []\n        cur_lr = lr\n        k = 0\n        for step in range(num_steps):\n            if step == milestones[k] * steps_per_epoch:\n                cur_lr = cur_lr * decay_rate\n                k = min(k + 1, len(milestones) - 1)\n            step_lrs.append(cur_lr)\n        if self.warmup_steps &gt; 0:\n            self.warmup_lr = nn.WarmUpLR(lr, self.warmup_steps)\n        self.step_lrs = ms.Tensor(step_lrs, ms.float32)\n\n    def construct(self, global_step):\n        if self.warmup_steps &gt; 0 and global_step &lt; self.warmup_steps:\n            lr = self.warmup_lr(global_step)\n        elif global_step &lt; self.step_lrs.shape[0]:\n            lr = self.step_lrs[global_step]\n        else:\n            lr = self.step_lrs[-1]\n        return lr\n</code></pre>"},{"location":"reference/api_doc/#mindocr.scheduler.scheduler_factory","title":"<code>mindocr.scheduler.scheduler_factory</code>","text":"<p>Scheduler Factory</p>"},{"location":"reference/api_doc/#mindocr.scheduler.scheduler_factory.create_scheduler","title":"<code>mindocr.scheduler.scheduler_factory.create_scheduler(steps_per_epoch, scheduler='constant', lr=0.01, min_lr=1e-06, warmup_epochs=3, warmup_factor=0.0, decay_epochs=10, decay_rate=0.9, milestones=None, num_epochs=200, num_cycles=1, cycle_decay=1.0, lr_epoch_stair=False)</code>","text":"<p>Creates learning rate scheduler by name.</p> PARAMETER DESCRIPTION <code>steps_per_epoch</code> <p>number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>scheduler</code> <p>scheduler name like 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'constant'</code> </p> <code>lr</code> <p>learning rate value. Default: 0.01.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>min_lr</code> <p>lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>warmup_epochs</code> <p>epochs to warmup LR, if scheduler supports. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>warmup_factor</code> <p>the warmup phase of scheduler is a linearly increasing lr, the beginning factor is <code>warmup_factor</code>, i.e., the lr of the first step/epoch is lr*warmup_factor, and the ending lr in the warmup phase is lr. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>decay_epochs</code> <p>for 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>. Default: 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>decay_rate</code> <p>LR decay rate (default: 0.9)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>milestones</code> <p>list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>num_epochs</code> <p>number of total epochs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>lr_epoch_stair</code> <p>If True, LR will be updated in the beginning of each new epoch and the LR will be consistent for each batch in one epoch. Otherwise, learning rate will be updated dynamically in each step. (default=False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>Cell object for computing LR with input of current global steps</p> Source code in <code>mindocr\\scheduler\\scheduler_factory.py</code> <pre><code>def create_scheduler(\n    steps_per_epoch: int,\n    scheduler: str = \"constant\",\n    lr: float = 0.01,\n    min_lr: float = 1e-6,\n    warmup_epochs: int = 3,\n    warmup_factor: float = 0.0,\n    decay_epochs: int = 10,\n    decay_rate: float = 0.9,\n    milestones: list = None,\n    num_epochs: int = 200,\n    num_cycles: int = 1,\n    cycle_decay: float = 1.0,\n    lr_epoch_stair: bool = False,\n):\nr\"\"\"Creates learning rate scheduler by name.\n\n    Args:\n        steps_per_epoch: number of steps per epoch.\n        scheduler: scheduler name like 'constant', 'cosine_decay', 'step_decay',\n            'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.\n        lr: learning rate value. Default: 0.01.\n        min_lr: lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.\n        warmup_epochs: epochs to warmup LR, if scheduler supports. Default: 3.\n        warmup_factor: the warmup phase of scheduler is a linearly increasing lr,\n            the beginning factor is `warmup_factor`, i.e., the lr of the first step/epoch is lr*warmup_factor,\n            and the ending lr in the warmup phase is lr. Default: 0.0\n        decay_epochs: for 'cosine_decay' schedulers, decay LR to min_lr in `decay_epochs`.\n            For 'step_decay' scheduler, decay LR by a factor of `decay_rate` every `decay_epochs`. Default: 10.\n        decay_rate: LR decay rate (default: 0.9)\n        milestones: list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing.\n        num_epochs: number of total epochs.\n        lr_epoch_stair: If True, LR will be updated in the beginning of each new epoch\n            and the LR will be consistent for each batch in one epoch.\n            Otherwise, learning rate will be updated dynamically in each step. (default=False)\n    Returns:\n        Cell object for computing LR with input of current global steps\n    \"\"\"\n    # check params\n    if milestones is None:\n        milestones = []\n\n    if warmup_epochs + decay_epochs &gt; num_epochs:\n        print(\"[WARNING]: warmup_epochs + decay_epochs &gt; num_epochs. Please check and reduce decay_epochs!\")\n\n    # lr warmup phase\n    warmup_lr_scheduler = []\n    if warmup_epochs &gt; 0:\n        if warmup_factor == 0 and lr_epoch_stair:\n            print(\n                \"[WARNING]: The warmup factor is set to 0, lr of 0-th epoch is always zero! \" \"Recommend value is 0.01.\"\n            )\n        warmup_func = linear_lr if lr_epoch_stair else linear_refined_lr\n        warmup_lr_scheduler = warmup_func(\n            start_factor=warmup_factor,\n            end_factor=1.0,\n            total_iters=warmup_epochs,\n            lr=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=warmup_epochs,\n        )\n\n    # lr decay phase\n    main_epochs = num_epochs - warmup_epochs\n    if scheduler in [\"cosine_decay\", \"warmup_cosine_decay\"]:\n        cosine_func = cosine_decay_lr if lr_epoch_stair else cosine_decay_refined_lr\n        main_lr_scheduler = cosine_func(\n            decay_epochs=decay_epochs,\n            eta_min=min_lr,\n            eta_max=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n            num_cycles=num_cycles,\n            cycle_decay=cycle_decay,\n        )\n    elif scheduler == \"exponential_decay\":\n        exponential_func = exponential_lr if lr_epoch_stair else exponential_refined_lr\n        main_lr_scheduler = exponential_func(\n            gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"polynomial_decay\":\n        polynomial_func = polynomial_lr if lr_epoch_stair else polynomial_refined_lr\n        main_lr_scheduler = polynomial_func(\n            total_iters=main_epochs, power=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"step_decay\":\n        main_lr_scheduler = step_lr(\n            step_size=decay_epochs, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"multi_step_decay\":\n        main_lr_scheduler = multi_step_lr(\n            milestones=milestones, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"constant\":\n        main_lr_scheduler = [lr for _ in range(steps_per_epoch * main_epochs)]\n    else:\n        raise ValueError(f\"Invalid scheduler: {scheduler}\")\n\n    # combine\n    lr_scheduler = warmup_lr_scheduler + main_lr_scheduler\n\n    return lr_scheduler\n</code></pre>"},{"location":"reference/api_doc/#mindocr.scheduler.warmup_cosine_decay_lr","title":"<code>mindocr.scheduler.warmup_cosine_decay_lr</code>","text":"<p>Cosine Decay with Warmup Learning Rate Scheduler</p>"},{"location":"reference/api_doc/#mindocr.scheduler.warmup_cosine_decay_lr.WarmupCosineDecayLR","title":"<code>mindocr.scheduler.warmup_cosine_decay_lr.WarmupCosineDecayLR</code>","text":"<p>         Bases: <code>LearningRateSchedule</code></p> <p>CosineDecayLR with warmup</p> PARAMETER DESCRIPTION <code>min_lr</code> <p>(float) lower lr bound for 'WarmupCosineDecayLR' schedulers.</p> <p> </p> <code>max_lr</code> <p>(float) upper lr bound for 'WarmupCosineDecayLR' schedulers.</p> <p> </p> <code>warmup_epochs</code> <p>(int) the number of warm up epochs of learning rate.</p> <p> </p> <code>decay_epochs</code> <p>(int) the number of decay epochs of learning rate.</p> <p> </p> <code>steps_per_epoch</code> <p>(int) the number of steps per epoch.</p> <p> </p> <code>step_mode</code> <p>(bool) determine decay along steps or epochs. True for steps, False for epochs.</p> <p> DEFAULT: <code>True</code> </p> <p>The learning rate will increase from 0 to max_lr in <code>warmup_epochs</code> epochs, then decay to min_lr in <code>decay_epochs</code> epochs</p> Source code in <code>mindocr\\scheduler\\warmup_cosine_decay_lr.py</code> <pre><code>class WarmupCosineDecayLR(LearningRateSchedule):\n\"\"\"CosineDecayLR with warmup\n    Args:\n\n        min_lr: (float) lower lr bound for 'WarmupCosineDecayLR' schedulers.\n        max_lr: (float) upper lr bound for 'WarmupCosineDecayLR' schedulers.\n        warmup_epochs: (int) the number of warm up epochs of learning rate.\n        decay_epochs: (int) the number of decay epochs of learning rate.\n        steps_per_epoch: (int) the number of steps per epoch.\n        step_mode: (bool) determine decay along steps or epochs. True for steps, False for epochs.\n\n    The learning rate will increase from 0 to max_lr in `warmup_epochs` epochs,\n    then decay to min_lr in `decay_epochs` epochs\n    \"\"\"\n\n    def __init__(\n        self,\n        min_lr,\n        max_lr,\n        warmup_epochs,\n        decay_epochs,\n        steps_per_epoch,\n        step_mode=True,\n    ):\n        super().__init__()\n        self.warmup_steps = warmup_epochs * steps_per_epoch\n        self.decay_steps = decay_epochs * steps_per_epoch\n        self.decay_epochs = decay_epochs\n        self.warmup_epochs = warmup_epochs\n        self.steps_per_epoch = steps_per_epoch\n        self.step_mode = step_mode\n        if self.warmup_steps &gt; 0:\n            self.warmup_lr = nn.WarmUpLR(max_lr, self.warmup_steps if step_mode else warmup_epochs)\n        self.cosine_decay_lr = nn.CosineDecayLR(min_lr, max_lr, self.decay_steps if step_mode else decay_epochs)\n\n    def step_lr(self, global_step):\n        if self.warmup_steps &gt; 0:\n            if global_step &gt; self.warmup_steps:\n                lr = self.cosine_decay_lr(global_step - self.warmup_steps)\n            else:\n                lr = self.warmup_lr(global_step)\n        else:\n            lr = self.cosine_decay_lr(global_step)\n        return lr\n\n    def epoch_lr(self, global_step):\n        cur_epoch = global_step // self.steps_per_epoch\n        if self.warmup_steps &gt; 0:\n            if global_step &gt; self.warmup_steps:\n                lr = self.cosine_decay_lr(cur_epoch - self.warmup_epochs)\n            else:\n                lr = self.warmup_lr(cur_epoch)\n        else:\n            lr = self.cosine_decay_lr(cur_epoch)\n        return lr\n\n    def construct(self, global_step):\n        if self.step_mode:\n            lr = self.step_lr(global_step)\n        else:\n            lr = self.epoch_lr(global_step)\n\n        return lr\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils","title":"<code>mindocr.utils</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.callbacks","title":"<code>mindocr.utils.callbacks</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.callbacks.EvalSaveCallback","title":"<code>mindocr.utils.callbacks.EvalSaveCallback</code>","text":"<p>         Bases: <code>Callback</code></p> <p>Callbacks for evaluation while training</p> PARAMETER DESCRIPTION <code>network</code> <p>network (without loss)</p> <p> TYPE: <code>nn.Cell</code> </p> <code>loader</code> <p>dataloader</p> <p> TYPE: <code>Dataset</code> DEFAULT: <code>None</code> </p> <code>ema</code> <p>if not None, the ema params will be loaded to the network for evaluation.</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\utils\\callbacks.py</code> <pre><code>class EvalSaveCallback(Callback):\n\"\"\"\n    Callbacks for evaluation while training\n\n    Args:\n        network (nn.Cell): network (without loss)\n        loader (Dataset): dataloader\n        ema: if not None, the ema params will be loaded to the network for evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        network,\n        loader=None,\n        loss_fn=None,\n        postprocessor=None,\n        metrics=None,\n        pred_cast_fp32=False,\n        rank_id=0,\n        device_num=None,\n        logger=None,\n        batch_size=20,\n        ckpt_save_dir=\"./\",\n        main_indicator=\"hmean\",\n        ema=None,\n        loader_output_columns=[],\n        input_indices=None,\n        label_indices=None,\n        meta_data_indices=None,\n        val_interval=1,\n        val_start_epoch=1,\n        log_interval=1,\n        ckpt_save_policy=\"top_k\",\n        ckpt_max_keep=10,\n        start_epoch=0,\n    ):\n        self.rank_id = rank_id\n        self.is_main_device = rank_id in [0, None]\n        self.loader_eval = loader\n        self.network = network\n        self.ema = ema\n        self.logger = print if logger is None else logger.info\n        self.val_interval = val_interval\n        self.val_start_epoch = val_start_epoch\n        self.log_interval = log_interval\n        self.batch_size = batch_size\n        if self.loader_eval is not None:\n            self.net_evaluator = Evaluator(\n                network,\n                loader,\n                loss_fn,\n                postprocessor,\n                metrics,\n                pred_cast_fp32=pred_cast_fp32,\n                loader_output_columns=loader_output_columns,\n                input_indices=input_indices,\n                label_indices=label_indices,\n                meta_data_indices=meta_data_indices,\n            )\n            self.main_indicator = main_indicator\n            self.best_perf = -1e8\n        else:\n            self.main_indicator = \"train_loss\"\n            self.best_perf = 1e8\n\n        self.ckpt_save_dir = ckpt_save_dir\n        if not os.path.exists(ckpt_save_dir):\n            os.makedirs(ckpt_save_dir)\n\n        self.last_epoch_end_time = time.time()\n        self.epoch_start_time = time.time()\n        self.step_start_time = time.time()\n\n        self._loss_avg_meter = AverageMeter()\n\n        self._reduce_sum = ms.ops.AllReduce()\n        self._device_num = device_num\n        # lamda expression is not supported in jit\n        self._loss_reduce = self._reduce if device_num is not None else lambda x: x\n\n        if self.is_main_device:\n            self.ckpt_save_policy = ckpt_save_policy\n            self.ckpt_manager = CheckpointManager(\n                ckpt_save_dir,\n                ckpt_save_policy,\n                k=ckpt_max_keep,\n                prefer_low_perf=(self.main_indicator == \"train_loss\"),\n            )\n        self.start_epoch = start_epoch\n\n    @jit\n    def _reduce(self, x):\n        return self._reduce_sum(x) / self._device_num  # average value across all devices\n\n    def on_train_step_end(self, run_context):\n\"\"\"\n        Print training loss at the end of step.\n\n        Args:\n            run_context (RunContext): Context of the train running.\n        \"\"\"\n        cb_params = run_context.original_args()\n        loss = _handle_loss(cb_params.net_outputs)\n        cur_epoch = cb_params.cur_epoch_num\n        data_sink_mode = cb_params.dataset_sink_mode\n        cur_step_in_epoch = (cb_params.cur_step_num - 1) % cb_params.batch_num + 1\n\n        self._loss_avg_meter.update(self._loss_reduce(loss))\n\n        if not data_sink_mode and cur_step_in_epoch % self.log_interval == 0:\n            opt = cb_params.train_network.optimizer\n            learning_rate = opt.learning_rate\n            cur_lr = learning_rate(opt.global_step - 1).asnumpy().squeeze()\n            per_step_time = (time.time() - self.step_start_time) * 1000 / self.log_interval\n            fps = self.batch_size * 1000 / per_step_time\n            loss = self._loss_avg_meter.val.asnumpy()\n            msg = (\n                f\"epoch: [{cur_epoch}/{cb_params.epoch_num}] step: [{cur_step_in_epoch}/{cb_params.batch_num}], \"\n                f\"loss: {loss:.6f}, lr: {cur_lr:.6f}, per step time: {per_step_time:.3f} ms, fps: {fps:.2f} img/s\"\n            )\n            self.logger(msg)\n            self.step_start_time = time.time()\n\n    def on_train_epoch_begin(self, run_context):\n\"\"\"\n        Called before each epoch beginning.\n        Args:\n            run_context (RunContext): Include some information of the model.\n        \"\"\"\n        self._loss_avg_meter.reset()\n        self.epoch_start_time = time.time()\n        self.step_start_time = time.time()\n\n    def on_train_epoch_end(self, run_context):\n\"\"\"\n        Called after each training epoch end.\n\n        Args:\n            run_context (RunContext): Include some information of the model.\n        \"\"\"\n        cb_params = run_context.original_args()\n        cur_epoch = cb_params.cur_epoch_num\n        train_time = time.time() - self.epoch_start_time\n        train_loss = self._loss_avg_meter.avg.asnumpy()\n\n        data_sink_mode = cb_params.dataset_sink_mode\n        if data_sink_mode:\n            loss_scale_manager = cb_params.train_network.network.loss_scaling_manager\n        else:\n            loss_scale_manager = cb_params.train_network.loss_scaling_manager\n\n        epoch_time = time.time() - self.epoch_start_time\n        per_step_time = epoch_time * 1000 / cb_params.batch_num\n        fps = 1000 * self.batch_size / per_step_time\n        msg = (\n            f\"epoch: [{cur_epoch}/{cb_params.epoch_num}], loss: {train_loss:.6f}, \"\n            f\"epoch time: {epoch_time:.3f} s, per step time: {per_step_time:.3f} ms, fps: {fps:.2f} img/s\"\n        )\n        self.logger(msg)\n\n        eval_done = False\n        if self.loader_eval is not None:\n            if cur_epoch &gt;= self.val_start_epoch and (cur_epoch - self.val_start_epoch) % self.val_interval == 0:\n                eval_start = time.time()\n                if self.ema is not None:\n                    # swap ema weight and network weight\n                    self.ema.swap_before_eval()\n                measures = self.net_evaluator.eval()\n\n                eval_done = True\n                if self.is_main_device:\n                    perf = measures[self.main_indicator]\n                    eval_time = time.time() - eval_start\n                    self.logger(f\"Performance: {measures}, eval time: {eval_time}\")\n            else:\n                measures = {m_name: None for m_name in self.net_evaluator.metric_names}\n                eval_time = 0\n                perf = 1e-8\n        else:\n            perf = train_loss\n\n        # save best models and results using card 0\n        if self.is_main_device:\n            # save best models\n            if (self.main_indicator == \"train_loss\" and perf &lt; self.best_perf) or (\n                self.main_indicator != \"train_loss\" and eval_done and perf &gt; self.best_perf\n            ):  # when val_while_train enabled, only find best checkpoint after eval done.\n                self.best_perf = perf\n                # ema weight will be saved if enabled.\n                save_checkpoint(self.network, os.path.join(self.ckpt_save_dir, \"best.ckpt\"))\n\n                self.logger(f\"=&gt; Best {self.main_indicator}: {self.best_perf}, checkpoint saved.\")\n\n            # save history checkpoints\n            self.ckpt_manager.save(self.network, perf, ckpt_name=f\"e{cur_epoch}.ckpt\")\n            ms.save_checkpoint(\n                cb_params.train_network,\n                os.path.join(self.ckpt_save_dir, \"train_resume.ckpt\"),\n                append_dict={\"epoch_num\": cur_epoch, \"loss_scale\": loss_scale_manager.get_loss_scale()},\n            )\n            # record results\n            if cur_epoch == 1:\n                if self.loader_eval is not None:\n                    perf_columns = [\"loss\"] + list(measures.keys()) + [\"train_time\", \"eval_time\"]\n                else:\n                    perf_columns = [\"loss\", \"train_time\"]\n                self.rec = PerfRecorder(self.ckpt_save_dir, metric_names=perf_columns)  # record column names\n            elif cur_epoch == self.start_epoch + 1:\n                self.rec = PerfRecorder(self.ckpt_save_dir, resume=True)\n\n            if self.loader_eval is not None:\n                epoch_perf_values = [cur_epoch, train_loss] + list(measures.values()) + [train_time, eval_time]\n            else:\n                epoch_perf_values = [cur_epoch, train_loss, train_time]\n            self.rec.add(*epoch_perf_values)  # record column values\n\n        # swap back network weight and ema weight. MUST execute after model saving and before next-step training\n        if (self.ema is not None) and eval_done:\n            self.ema.swap_after_eval()\n\n        # tot_time = time.time() - self.last_epoch_end_time\n        self.last_epoch_end_time = time.time()\n\n    def on_train_end(self, run_context):\n        if self.is_main_device:\n            self.rec.save_curves()  # save performance curve figure\n            self.logger(f\"=&gt; Best {self.main_indicator}: {self.best_perf} \\nTraining completed!\")\n\n            if self.ckpt_save_policy == \"top_k\":\n                log_str = f\"Top K checkpoints:\\n{self.main_indicator}\\tcheckpoint\\n\"\n                for p, ckpt_name in self.ckpt_manager.get_ckpt_queue():\n                    log_str += f\"{p:.4f}\\t{os.path.join(self.ckpt_save_dir, ckpt_name)}\\n\"\n                self.logger(log_str)\n</code></pre> <code>mindocr.utils.callbacks.EvalSaveCallback.on_train_epoch_begin(run_context)</code> \u00b6 <p>Called before each epoch beginning.</p> PARAMETER DESCRIPTION <code>run_context</code> <p>Include some information of the model.</p> <p> TYPE: <code>RunContext</code> </p> Source code in <code>mindocr\\utils\\callbacks.py</code> <pre><code>def on_train_epoch_begin(self, run_context):\n\"\"\"\n    Called before each epoch beginning.\n    Args:\n        run_context (RunContext): Include some information of the model.\n    \"\"\"\n    self._loss_avg_meter.reset()\n    self.epoch_start_time = time.time()\n    self.step_start_time = time.time()\n</code></pre> <code>mindocr.utils.callbacks.EvalSaveCallback.on_train_epoch_end(run_context)</code> \u00b6 <p>Called after each training epoch end.</p> PARAMETER DESCRIPTION <code>run_context</code> <p>Include some information of the model.</p> <p> TYPE: <code>RunContext</code> </p> Source code in <code>mindocr\\utils\\callbacks.py</code> <pre><code>def on_train_epoch_end(self, run_context):\n\"\"\"\n    Called after each training epoch end.\n\n    Args:\n        run_context (RunContext): Include some information of the model.\n    \"\"\"\n    cb_params = run_context.original_args()\n    cur_epoch = cb_params.cur_epoch_num\n    train_time = time.time() - self.epoch_start_time\n    train_loss = self._loss_avg_meter.avg.asnumpy()\n\n    data_sink_mode = cb_params.dataset_sink_mode\n    if data_sink_mode:\n        loss_scale_manager = cb_params.train_network.network.loss_scaling_manager\n    else:\n        loss_scale_manager = cb_params.train_network.loss_scaling_manager\n\n    epoch_time = time.time() - self.epoch_start_time\n    per_step_time = epoch_time * 1000 / cb_params.batch_num\n    fps = 1000 * self.batch_size / per_step_time\n    msg = (\n        f\"epoch: [{cur_epoch}/{cb_params.epoch_num}], loss: {train_loss:.6f}, \"\n        f\"epoch time: {epoch_time:.3f} s, per step time: {per_step_time:.3f} ms, fps: {fps:.2f} img/s\"\n    )\n    self.logger(msg)\n\n    eval_done = False\n    if self.loader_eval is not None:\n        if cur_epoch &gt;= self.val_start_epoch and (cur_epoch - self.val_start_epoch) % self.val_interval == 0:\n            eval_start = time.time()\n            if self.ema is not None:\n                # swap ema weight and network weight\n                self.ema.swap_before_eval()\n            measures = self.net_evaluator.eval()\n\n            eval_done = True\n            if self.is_main_device:\n                perf = measures[self.main_indicator]\n                eval_time = time.time() - eval_start\n                self.logger(f\"Performance: {measures}, eval time: {eval_time}\")\n        else:\n            measures = {m_name: None for m_name in self.net_evaluator.metric_names}\n            eval_time = 0\n            perf = 1e-8\n    else:\n        perf = train_loss\n\n    # save best models and results using card 0\n    if self.is_main_device:\n        # save best models\n        if (self.main_indicator == \"train_loss\" and perf &lt; self.best_perf) or (\n            self.main_indicator != \"train_loss\" and eval_done and perf &gt; self.best_perf\n        ):  # when val_while_train enabled, only find best checkpoint after eval done.\n            self.best_perf = perf\n            # ema weight will be saved if enabled.\n            save_checkpoint(self.network, os.path.join(self.ckpt_save_dir, \"best.ckpt\"))\n\n            self.logger(f\"=&gt; Best {self.main_indicator}: {self.best_perf}, checkpoint saved.\")\n\n        # save history checkpoints\n        self.ckpt_manager.save(self.network, perf, ckpt_name=f\"e{cur_epoch}.ckpt\")\n        ms.save_checkpoint(\n            cb_params.train_network,\n            os.path.join(self.ckpt_save_dir, \"train_resume.ckpt\"),\n            append_dict={\"epoch_num\": cur_epoch, \"loss_scale\": loss_scale_manager.get_loss_scale()},\n        )\n        # record results\n        if cur_epoch == 1:\n            if self.loader_eval is not None:\n                perf_columns = [\"loss\"] + list(measures.keys()) + [\"train_time\", \"eval_time\"]\n            else:\n                perf_columns = [\"loss\", \"train_time\"]\n            self.rec = PerfRecorder(self.ckpt_save_dir, metric_names=perf_columns)  # record column names\n        elif cur_epoch == self.start_epoch + 1:\n            self.rec = PerfRecorder(self.ckpt_save_dir, resume=True)\n\n        if self.loader_eval is not None:\n            epoch_perf_values = [cur_epoch, train_loss] + list(measures.values()) + [train_time, eval_time]\n        else:\n            epoch_perf_values = [cur_epoch, train_loss, train_time]\n        self.rec.add(*epoch_perf_values)  # record column values\n\n    # swap back network weight and ema weight. MUST execute after model saving and before next-step training\n    if (self.ema is not None) and eval_done:\n        self.ema.swap_after_eval()\n\n    # tot_time = time.time() - self.last_epoch_end_time\n    self.last_epoch_end_time = time.time()\n</code></pre> <code>mindocr.utils.callbacks.EvalSaveCallback.on_train_step_end(run_context)</code> \u00b6 <p>Print training loss at the end of step.</p> PARAMETER DESCRIPTION <code>run_context</code> <p>Context of the train running.</p> <p> TYPE: <code>RunContext</code> </p> Source code in <code>mindocr\\utils\\callbacks.py</code> <pre><code>def on_train_step_end(self, run_context):\n\"\"\"\n    Print training loss at the end of step.\n\n    Args:\n        run_context (RunContext): Context of the train running.\n    \"\"\"\n    cb_params = run_context.original_args()\n    loss = _handle_loss(cb_params.net_outputs)\n    cur_epoch = cb_params.cur_epoch_num\n    data_sink_mode = cb_params.dataset_sink_mode\n    cur_step_in_epoch = (cb_params.cur_step_num - 1) % cb_params.batch_num + 1\n\n    self._loss_avg_meter.update(self._loss_reduce(loss))\n\n    if not data_sink_mode and cur_step_in_epoch % self.log_interval == 0:\n        opt = cb_params.train_network.optimizer\n        learning_rate = opt.learning_rate\n        cur_lr = learning_rate(opt.global_step - 1).asnumpy().squeeze()\n        per_step_time = (time.time() - self.step_start_time) * 1000 / self.log_interval\n        fps = self.batch_size * 1000 / per_step_time\n        loss = self._loss_avg_meter.val.asnumpy()\n        msg = (\n            f\"epoch: [{cur_epoch}/{cb_params.epoch_num}] step: [{cur_step_in_epoch}/{cb_params.batch_num}], \"\n            f\"loss: {loss:.6f}, lr: {cur_lr:.6f}, per step time: {per_step_time:.3f} ms, fps: {fps:.2f} img/s\"\n        )\n        self.logger(msg)\n        self.step_start_time = time.time()\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.checkpoint","title":"<code>mindocr.utils.checkpoint</code>","text":"<p>checkpoint manager</p>"},{"location":"reference/api_doc/#mindocr.utils.checkpoint.CheckpointManager","title":"<code>mindocr.utils.checkpoint.CheckpointManager</code>","text":"<p>Manage checkpoint files according to ckpt_save_policy of checkpoint.</p> PARAMETER DESCRIPTION <code>ckpt_save_dir</code> <p>directory to save the checkpoints</p> <p> TYPE: <code>str</code> </p> <code>ckpt_save_policy</code> <p>Checkpoint saving strategy. Option: None, \"top_k\", or \"latest_k\". None means to save each checkpoint, top_k means to save K checkpoints with the best performance, and latest_k means saving the latest K checkpoint. Default: top_k.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'top_k'</code> </p> <code>k</code> <p>top k value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>prefer_low_perf</code> <p>standard for selecting the top k performance. If False, pick top k checkpoints with highest performance e.g. accuracy. If True, pick top k checkpoints with the lowest performance, e.g. loss.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>class CheckpointManager:\n\"\"\"\n    Manage checkpoint files according to ckpt_save_policy of checkpoint.\n    Args:\n        ckpt_save_dir (str): directory to save the checkpoints\n        ckpt_save_policy (str): Checkpoint saving strategy. Option: None, \"top_k\", or \"latest_k\".\n            None means to save each checkpoint, top_k means to save K checkpoints with the best performance,\n            and latest_k means saving the latest K checkpoint. Default: top_k.\n        k (int): top k value\n        prefer_low_perf (bool): standard for selecting the top k performance. If False, pick top k checkpoints with\n            highest performance e.g. accuracy. If True, pick top k checkpoints with the lowest performance, e.g. loss.\n\n    \"\"\"\n\n    def __init__(self, ckpt_save_dir, ckpt_save_policy=\"top_k\", k=10, prefer_low_perf=False, del_past=True):\n        self.ckpt_save_dir = ckpt_save_dir\n        self._ckpt_filelist = []\n        self.ckpt_save_policy = ckpt_save_policy\n        self.k = k\n\n        self.ckpt_queue = []\n        self.del_past = del_past\n        self.prefer_low_perf = prefer_low_perf\n\n    def get_ckpt_queue(self):\n\"\"\"Get all the related checkpoint files managed here.\"\"\"\n        return self.ckpt_queue\n\n    @property\n    def ckpt_num(self):\n\"\"\"Get the number of the related checkpoint files managed here.\"\"\"\n        return len(self.ckpt_queue)\n\n    def remove_ckpt_file(self, file_name):\n\"\"\"Remove the specified checkpoint file from this checkpoint manager and also from the directory.\"\"\"\n        try:\n            if os.path.exists(file_name):\n                os.chmod(file_name, stat.S_IWRITE)\n                os.remove(file_name)\n        except OSError:\n            logger.warning(\"OSError, failed to remove the older ckpt file %s.\", file_name)\n        except ValueError:\n            logger.warning(\"ValueError, failed to remove the older ckpt file %s.\", file_name)\n\n    def save_top_k(self, network, perf, ckpt_name, verbose=True):\n\"\"\"Save and return Top K checkpoint address and accuracy.\"\"\"\n        self.ckpt_queue.append((perf, ckpt_name))\n        self.ckpt_queue = sorted(\n            self.ckpt_queue, key=lambda x: x[0], reverse=not self.prefer_low_perf\n        )  # by default, reverse is True for descending order\n        if len(self.ckpt_queue) &gt; self.k:\n            to_del = self.ckpt_queue.pop(-1)\n            # save if the perf is better than the minimum in the heap\n            if to_del[1] != ckpt_name:\n                ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n                # del minimum\n                self.remove_ckpt_file(os.path.join(self.ckpt_save_dir, to_del[1]))\n        else:\n            ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n\n    def save_latest_k(self, network, ckpt_name):\n\"\"\"Save latest K checkpoint.\"\"\"\n        ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n        self.ckpt_queue.append(ckpt_name)\n        if len(self.ckpt_queue) &gt; self.k:\n            to_del = self.ckpt_queue.pop(0)\n            if self.del_past:\n                self.remove_ckpt_file(os.path.join(self.ckpt_save_dir, to_del))\n\n    def save_single(self, network, ckpt_path):\n        ms.save_checkpoint(network, ckpt_path)\n\n    def save(self, network, perf=None, ckpt_name=None):\n\"\"\"Save checkpoint according to different save strategy.\"\"\"\n        if self.ckpt_save_policy is None:\n            ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n        elif self.ckpt_save_policy == \"top_k\":\n            if perf is None:\n                raise ValueError(\n                    \"Evaluation performance is None, but `top_k` ckpt save policy requires evaluation performance\"\n                )\n            self.save_top_k(network, perf, ckpt_name)\n            return self.ckpt_queue\n        elif self.ckpt_save_policy == \"latest_k\":\n            self.save_latest_k(network, ckpt_name)\n            return self.ckpt_queue\n        else:\n            raise ValueError(\n                f\"The expected 'ckpt_save_policy' is None, top_k or latest_k, but got: {self.ckpt_save_policy}.\"\n            )\n</code></pre> <code>mindocr.utils.checkpoint.CheckpointManager.ckpt_num</code> <code>property</code> \u00b6 <p>Get the number of the related checkpoint files managed here.</p> <code>mindocr.utils.checkpoint.CheckpointManager.get_ckpt_queue()</code> \u00b6 <p>Get all the related checkpoint files managed here.</p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>def get_ckpt_queue(self):\n\"\"\"Get all the related checkpoint files managed here.\"\"\"\n    return self.ckpt_queue\n</code></pre> <code>mindocr.utils.checkpoint.CheckpointManager.remove_ckpt_file(file_name)</code> \u00b6 <p>Remove the specified checkpoint file from this checkpoint manager and also from the directory.</p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>def remove_ckpt_file(self, file_name):\n\"\"\"Remove the specified checkpoint file from this checkpoint manager and also from the directory.\"\"\"\n    try:\n        if os.path.exists(file_name):\n            os.chmod(file_name, stat.S_IWRITE)\n            os.remove(file_name)\n    except OSError:\n        logger.warning(\"OSError, failed to remove the older ckpt file %s.\", file_name)\n    except ValueError:\n        logger.warning(\"ValueError, failed to remove the older ckpt file %s.\", file_name)\n</code></pre> <code>mindocr.utils.checkpoint.CheckpointManager.save(network, perf=None, ckpt_name=None)</code> \u00b6 <p>Save checkpoint according to different save strategy.</p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>def save(self, network, perf=None, ckpt_name=None):\n\"\"\"Save checkpoint according to different save strategy.\"\"\"\n    if self.ckpt_save_policy is None:\n        ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n    elif self.ckpt_save_policy == \"top_k\":\n        if perf is None:\n            raise ValueError(\n                \"Evaluation performance is None, but `top_k` ckpt save policy requires evaluation performance\"\n            )\n        self.save_top_k(network, perf, ckpt_name)\n        return self.ckpt_queue\n    elif self.ckpt_save_policy == \"latest_k\":\n        self.save_latest_k(network, ckpt_name)\n        return self.ckpt_queue\n    else:\n        raise ValueError(\n            f\"The expected 'ckpt_save_policy' is None, top_k or latest_k, but got: {self.ckpt_save_policy}.\"\n        )\n</code></pre> <code>mindocr.utils.checkpoint.CheckpointManager.save_latest_k(network, ckpt_name)</code> \u00b6 <p>Save latest K checkpoint.</p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>def save_latest_k(self, network, ckpt_name):\n\"\"\"Save latest K checkpoint.\"\"\"\n    ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n    self.ckpt_queue.append(ckpt_name)\n    if len(self.ckpt_queue) &gt; self.k:\n        to_del = self.ckpt_queue.pop(0)\n        if self.del_past:\n            self.remove_ckpt_file(os.path.join(self.ckpt_save_dir, to_del))\n</code></pre> <code>mindocr.utils.checkpoint.CheckpointManager.save_top_k(network, perf, ckpt_name, verbose=True)</code> \u00b6 <p>Save and return Top K checkpoint address and accuracy.</p> Source code in <code>mindocr\\utils\\checkpoint.py</code> <pre><code>def save_top_k(self, network, perf, ckpt_name, verbose=True):\n\"\"\"Save and return Top K checkpoint address and accuracy.\"\"\"\n    self.ckpt_queue.append((perf, ckpt_name))\n    self.ckpt_queue = sorted(\n        self.ckpt_queue, key=lambda x: x[0], reverse=not self.prefer_low_perf\n    )  # by default, reverse is True for descending order\n    if len(self.ckpt_queue) &gt; self.k:\n        to_del = self.ckpt_queue.pop(-1)\n        # save if the perf is better than the minimum in the heap\n        if to_del[1] != ckpt_name:\n            ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n            # del minimum\n            self.remove_ckpt_file(os.path.join(self.ckpt_save_dir, to_del[1]))\n    else:\n        ms.save_checkpoint(network, os.path.join(self.ckpt_save_dir, ckpt_name))\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.ema","title":"<code>mindocr.utils.ema</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.ema.EMA","title":"<code>mindocr.utils.ema.EMA</code>","text":"<p>         Bases: <code>nn.Cell</code></p> PARAMETER DESCRIPTION <code>updates</code> <p>number of ema updates, which can be restored from resumed training.</p> <p> DEFAULT: <code>0</code> </p> Source code in <code>mindocr\\utils\\ema.py</code> <pre><code>class EMA(nn.Cell):\n\"\"\"\n    Args:\n        updates: number of ema updates, which can be restored from resumed training.\n    \"\"\"\n\n    def __init__(self, network, ema_decay=0.9999, updates=0):\n        super().__init__()\n        # TODO: net.trainable_params() is more reasonable?\n        self.net_weight = ms.ParameterTuple(network.get_parameters())\n        self.ema_weight = self.net_weight.clone(prefix=\"ema\", init=\"same\")\n        self.swap_cache = self.net_weight.clone(prefix=\"swap\", init=\"zeros\")\n\n        self.ema_decay = ema_decay\n        self.updates = Parameter(Tensor(updates, ms.float32), requires_grad=False)\n\n        self.hyper_map = C.HyperMap()\n        self.map = ops.HyperMap()\n\n    def ema_update(self):\n\"\"\"Update EMA parameters.\"\"\"\n        self.updates += 1\n        d = self.ema_decay * (1 - F.exp(-self.updates / 2000))\n        # update trainable parameters\n        success = self.hyper_map(F.partial(_ema_op, d), self.ema_weight, self.net_weight)\n        self.updates = F.depend(self.updates, success)\n        return self.updates\n\n    # @ms_function\n    def swap_before_eval(self):\n        # net -&gt; swap\n        success = self.map(ops.assign, self.swap_cache, self.net_weight)\n        # ema -&gt; net\n        success = F.depend(success, self.map(ops.assign, self.net_weight, self.ema_weight))\n        return success\n\n    # @ms_function\n    def swap_after_eval(self):\n        # swap -&gt; net\n        success = self.map(ops.assign, self.net_weight, self.swap_cache)\n        return success\n</code></pre> <code>mindocr.utils.ema.EMA.ema_update()</code> \u00b6 <p>Update EMA parameters.</p> Source code in <code>mindocr\\utils\\ema.py</code> <pre><code>def ema_update(self):\n\"\"\"Update EMA parameters.\"\"\"\n    self.updates += 1\n    d = self.ema_decay * (1 - F.exp(-self.updates / 2000))\n    # update trainable parameters\n    success = self.hyper_map(F.partial(_ema_op, d), self.ema_weight, self.net_weight)\n    self.updates = F.depend(self.updates, success)\n    return self.updates\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.evaluator","title":"<code>mindocr.utils.evaluator</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.evaluator.Evaluator","title":"<code>mindocr.utils.evaluator.Evaluator</code>","text":"PARAMETER DESCRIPTION <code>network</code> <p>network</p> <p> </p> <code>dataloader</code> <p>data loader to generate batch data, where the data columns in a batch are defined by the transform pipeline and <code>output_columns</code>.</p> <p> </p> <code>loss_fn</code> <p>loss function</p> <p> DEFAULT: <code>None</code> </p> <code>postprocessor</code> <p>post-processor</p> <p> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>metrics to evaluate network performance</p> <p> DEFAULT: <code>None</code> </p> <code>pred_cast_fp32</code> <p>whether to cast network prediction to float 32. Set True if AMP is used.</p> <p> DEFAULT: <code>False</code> </p> <code>input_indices</code> <p>The indices of the data tuples which will be fed into the network. If it is None, then the first item will be fed only.</p> <p> DEFAULT: <code>None</code> </p> <code>label_indices</code> <p>The indices of the data tuples which will be marked as label. If it is None, then the remaining items will be marked as label.</p> <p> DEFAULT: <code>None</code> </p> <code>meta_data_indices</code> <p>The indices for the data tuples which will be marked as metadata. If it is None, then the item indices not in input or label indices are marked as meta data.</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\utils\\evaluator.py</code> <pre><code>class Evaluator:\n\"\"\"\n    Args:\n        network: network\n        dataloader : data loader to generate batch data, where the data columns in a batch are defined by the transform\n            pipeline and `output_columns`.\n        loss_fn: loss function\n        postprocessor: post-processor\n        metrics: metrics to evaluate network performance\n        pred_cast_fp32: whether to cast network prediction to float 32. Set True if AMP is used.\n        input_indices: The indices of the data tuples which will be fed into the network.\n            If it is None, then the first item will be fed only.\n        label_indices: The indices of the data tuples which will be marked as label.\n            If it is None, then the remaining items will be marked as label.\n        meta_data_indices: The indices for the data tuples which will be marked as metadata.\n            If it is None, then the item indices not in input or label indices are marked as meta data.\n    \"\"\"\n\n    def __init__(\n        self,\n        network,\n        dataloader,\n        loss_fn=None,\n        postprocessor=None,\n        metrics=None,\n        pred_cast_fp32=False,\n        loader_output_columns=None,\n        input_indices=None,\n        label_indices=None,\n        meta_data_indices=None,\n        num_epochs=-1,\n        visualize=False,\n        verbose=False,\n        **kwargs,\n    ):\n        self.net = network\n        self.postprocessor = postprocessor\n        self.metrics = metrics if isinstance(metrics, List) else [metrics]\n        self.metric_names = []\n        for m in metrics:\n            assert hasattr(m, \"metric_names\") and isinstance(m.metric_names, List), (\n                f\"Metric object must contain `metric_names` attribute to indicate the metric names as a List type, \"\n                f\"but not found in {m.__class__.__name__}\"\n            )\n            self.metric_names += m.metric_names\n\n        self.pred_cast_fp32 = pred_cast_fp32\n        self.visualize = visualize\n        self.verbose = verbose\n        eval_loss = False\n        if loss_fn is not None:\n            eval_loss = True\n            self.loss_fn = loss_fn\n        assert not eval_loss, \"not impl\"\n\n        # create iterator\n        self.reload(\n            dataloader,\n            loader_output_columns,\n            input_indices,\n            label_indices,\n            meta_data_indices,\n            num_epochs,\n        )\n\n    def reload(\n        self,\n        dataloader,\n        loader_output_columns=None,\n        input_indices=None,\n        label_indices=None,\n        meta_data_indices=None,\n        num_epochs=-1,\n    ):\n        # create iterator\n        self.iterator = dataloader.create_tuple_iterator(num_epochs=num_epochs, output_numpy=False, do_copy=False)\n        self.num_batches_eval = dataloader.get_dataset_size()\n\n        # dataset output columns\n        self.loader_output_columns = loader_output_columns or []\n        self.input_indices = input_indices\n        self.label_indices = label_indices\n        self.meta_data_indices = meta_data_indices\n\n    def eval(self):\n\"\"\"\n        Args:\n        \"\"\"\n        eval_res = {}\n\n        self.net.set_train(False)\n        for m in self.metrics:\n            m.clear()\n\n        for i, data in tqdm(enumerate(self.iterator), total=self.num_batches_eval):\n            if self.input_indices is not None:\n                inputs = [data[x] for x in self.input_indices]\n            else:\n                inputs = [data[0]]\n\n            if self.label_indices is not None:\n                gt = [data[x] for x in self.label_indices]\n            else:\n                gt = data[1:]\n\n            preds = self.net(*inputs)\n\n            if self.pred_cast_fp32:\n                if isinstance(preds, ms.Tensor):\n                    preds = F.cast(preds, mstype.float32)\n                else:\n                    preds = [F.cast(p, mstype.float32) for p in preds]\n\n            data_info = {\"labels\": gt, \"img_shape\": inputs[0].shape}\n\n            if self.postprocessor is not None:\n                # additional info such as image path, original image size, pad shape, extracted in data processing\n                if self.meta_data_indices is not None:\n                    meta_info = [data[x] for x in self.meta_data_indices]\n                else:\n                    # assume the indices not in input_indices or label_indices are all meta_data_indices\n                    input_indices = set(self.input_indices) if self.input_indices is not None else {0}\n                    label_indices = (\n                        set(self.label_indices) if self.label_indices is not None else set(range(1, len(data), 1))\n                    )\n                    meta_data_indices = sorted(set(range(len(data))) - input_indices - label_indices)\n                    meta_info = [data[x] for x in meta_data_indices]\n\n                data_info[\"meta_info\"] = meta_info\n\n                # NOTES: add more if new postprocess modules need new keys. shape_list is commonly needed by detection\n                possible_keys_for_postprocess = [\"shape_list\", \"raw_img_shape\"]\n                # TODO: remove raw_img_shape (used in tools/infer/text/parallel).\n                #  shape_list = [h, w, ratio_h, ratio_w] already contain raw image shape.\n                for k in possible_keys_for_postprocess:\n                    if k in self.loader_output_columns:\n                        data_info[k] = data[self.loader_output_columns.index(k)]\n\n                preds = self.postprocessor(preds, **data_info)\n\n            # metric internal update\n            for m in self.metrics:\n                m.update(preds, gt)\n\n            if self.verbose:\n                print(\"Data meta info: \", data_info)\n\n        for m in self.metrics:\n            res_dict = m.eval()\n            eval_res.update(res_dict)\n\n        self.net.set_train(True)\n\n        return eval_res\n</code></pre> <code>mindocr.utils.evaluator.Evaluator.eval()</code> \u00b6 Source code in <code>mindocr\\utils\\evaluator.py</code> <pre><code>def eval(self):\n\"\"\"\n    Args:\n    \"\"\"\n    eval_res = {}\n\n    self.net.set_train(False)\n    for m in self.metrics:\n        m.clear()\n\n    for i, data in tqdm(enumerate(self.iterator), total=self.num_batches_eval):\n        if self.input_indices is not None:\n            inputs = [data[x] for x in self.input_indices]\n        else:\n            inputs = [data[0]]\n\n        if self.label_indices is not None:\n            gt = [data[x] for x in self.label_indices]\n        else:\n            gt = data[1:]\n\n        preds = self.net(*inputs)\n\n        if self.pred_cast_fp32:\n            if isinstance(preds, ms.Tensor):\n                preds = F.cast(preds, mstype.float32)\n            else:\n                preds = [F.cast(p, mstype.float32) for p in preds]\n\n        data_info = {\"labels\": gt, \"img_shape\": inputs[0].shape}\n\n        if self.postprocessor is not None:\n            # additional info such as image path, original image size, pad shape, extracted in data processing\n            if self.meta_data_indices is not None:\n                meta_info = [data[x] for x in self.meta_data_indices]\n            else:\n                # assume the indices not in input_indices or label_indices are all meta_data_indices\n                input_indices = set(self.input_indices) if self.input_indices is not None else {0}\n                label_indices = (\n                    set(self.label_indices) if self.label_indices is not None else set(range(1, len(data), 1))\n                )\n                meta_data_indices = sorted(set(range(len(data))) - input_indices - label_indices)\n                meta_info = [data[x] for x in meta_data_indices]\n\n            data_info[\"meta_info\"] = meta_info\n\n            # NOTES: add more if new postprocess modules need new keys. shape_list is commonly needed by detection\n            possible_keys_for_postprocess = [\"shape_list\", \"raw_img_shape\"]\n            # TODO: remove raw_img_shape (used in tools/infer/text/parallel).\n            #  shape_list = [h, w, ratio_h, ratio_w] already contain raw image shape.\n            for k in possible_keys_for_postprocess:\n                if k in self.loader_output_columns:\n                    data_info[k] = data[self.loader_output_columns.index(k)]\n\n            preds = self.postprocessor(preds, **data_info)\n\n        # metric internal update\n        for m in self.metrics:\n            m.update(preds, gt)\n\n        if self.verbose:\n            print(\"Data meta info: \", data_info)\n\n    for m in self.metrics:\n        res_dict = m.eval()\n        eval_res.update(res_dict)\n\n    self.net.set_train(True)\n\n    return eval_res\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.logger","title":"<code>mindocr.utils.logger</code>","text":"<p>Custom Logger.</p>"},{"location":"reference/api_doc/#mindocr.utils.logger.Logger","title":"<code>mindocr.utils.logger.Logger</code>","text":"<p>         Bases: <code>logging.Logger</code></p> <p>Logger.</p> PARAMETER DESCRIPTION <code>logger_name</code> <p>String. Logger name.</p> <p> </p> <code>rank</code> <p>Integer. Rank id.</p> <p> DEFAULT: <code>0</code> </p> Source code in <code>mindocr\\utils\\logger.py</code> <pre><code>class Logger(logging.Logger):\n\"\"\"\n    Logger.\n\n    Args:\n         logger_name: String. Logger name.\n         rank: Integer. Rank id.\n    \"\"\"\n\n    def __init__(self, logger_name, rank=0, log_fn=None):\n        super(Logger, self).__init__(logger_name)\n        self.rank = rank or 0\n        self.log_fn = log_fn\n        is_main_device = not rank\n\n        if is_main_device:\n            console = logging.StreamHandler(sys.stdout)\n            console.setLevel(logging.INFO)\n            formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\")\n            console.setFormatter(formatter)\n            self.addHandler(console)\n\n    def setup_logging_file(self, log_dir):\n\"\"\"Setup logging file.\"\"\"\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir, exist_ok=True)\n        if self.log_fn is None:\n            log_name = \"log_%s.txt\" % self.rank\n            self.log_save_path = os.path.join(log_dir, log_name)\n        else:\n            self.log_save_path = os.path.join(log_dir, self.log_fn)\n        fh = logging.FileHandler(self.log_save_path)\n        fh.setLevel(logging.INFO)\n        formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\")\n        fh.setFormatter(formatter)\n        self.addHandler(fh)\n\n    def info(self, msg, *args, **kwargs):\n        if self.isEnabledFor(logging.INFO):\n            self._log(logging.INFO, msg, args, **kwargs)\n\n    def save_args(self, args):\n        self.info(\"Args:\")\n        args_dict = vars(args)\n        for key in args_dict.keys():\n            self.info(\"--&gt; %s: %s\", key, args_dict[key])\n        self.info(\"\")\n\n    def important_info(self, msg, *args, **kwargs):\n        if self.isEnabledFor(logging.INFO) and self.rank == 0:\n            line_width = 2\n            important_msg = \"\\n\"\n            important_msg += (\"*\" * 70 + \"\\n\") * line_width\n            important_msg += (\"*\" * line_width + \"\\n\") * 2\n            important_msg += \"*\" * line_width + \" \" * 8 + msg + \"\\n\"\n            important_msg += (\"*\" * line_width + \"\\n\") * 2\n            important_msg += (\"*\" * 70 + \"\\n\") * line_width\n            self.info(important_msg, *args, **kwargs)\n</code></pre> <code>mindocr.utils.logger.Logger.setup_logging_file(log_dir)</code> \u00b6 <p>Setup logging file.</p> Source code in <code>mindocr\\utils\\logger.py</code> <pre><code>def setup_logging_file(self, log_dir):\n\"\"\"Setup logging file.\"\"\"\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir, exist_ok=True)\n    if self.log_fn is None:\n        log_name = \"log_%s.txt\" % self.rank\n        self.log_save_path = os.path.join(log_dir, log_name)\n    else:\n        self.log_save_path = os.path.join(log_dir, self.log_fn)\n    fh = logging.FileHandler(self.log_save_path)\n    fh.setLevel(logging.INFO)\n    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\")\n    fh.setFormatter(formatter)\n    self.addHandler(fh)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.logger.get_logger","title":"<code>mindocr.utils.logger.get_logger(log_dir, rank, log_fn=None)</code>","text":"<p>Get Logger.</p> Source code in <code>mindocr\\utils\\logger.py</code> <pre><code>def get_logger(log_dir, rank, log_fn=None):\n\"\"\"Get Logger.\"\"\"\n    logger = Logger(\"mindocr\", rank, log_fn=log_fn)\n    logger.setup_logging_file(log_dir)\n\n    return logger\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.loss_scaler","title":"<code>mindocr.utils.loss_scaler</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.loss_scaler.get_loss_scales","title":"<code>mindocr.utils.loss_scaler.get_loss_scales(cfg)</code>","text":"PARAMETER DESCRIPTION <code>cfg</code> <p>configure dict of loss scaler</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>nn.Cell: scale_sens used to scale gradient</p> <code>float</code> <p>loss_scale used in optimizer (only used when loss scaler type is static and drop_overflow update is False)</p> Source code in <code>mindocr\\utils\\loss_scaler.py</code> <pre><code>def get_loss_scales(cfg):\n\"\"\"\n    Args:\n        cfg (dict): configure dict of loss scaler\n\n    Returns:\n        nn.Cell: scale_sens used to scale gradient\n        float: loss_scale used in optimizer\n            (only used when loss scaler type is static and drop_overflow update is False)\n    \"\"\"\n    # loss scale is 1.0 by default\n    loss_scale_manager = nn.FixedLossScaleUpdateCell(loss_scale_value=1.0)\n\n    # Only when `FixedLossScaleManager` is used for training and the `drop_overflow_update` in\n    # `FixedLossScaleManager` is set to False, then this value needs to be the same as the `loss_scale` in\n    # `FixedLossScaleManager`\n    # But we never use FixedLossScaleManager, so optimizer_loss_scale is always 1.\n    optimizer_loss_scale = 1.0\n\n    if \"loss_scaler\" in cfg:\n        assert (\n            \"loss_scale\" in cfg.loss_scaler\n        ), \"Must specify the value for `loss_scale` in the config if `loss_scaler` is used.\"\n        if cfg.loss_scaler.type == \"dynamic\":\n            # TODO: scale_window can be related to num_batches, e.g., scale_window = num_batches * 2\n            scale_factor = cfg.loss_scaler.get(\"scale_factor\", 2.0)\n            scale_window = cfg.loss_scaler.get(\"scale_window\", 2000)\n            # adjust by gradient_accumulation_steps so that the scaling process is the same as that of\n            # batch_size=batch_size*gradient_accumulation_steps\n            grad_accu_steps = cfg.train.get(\"gradient_accumulation_steps\", 1)\n            if grad_accu_steps &gt; 1:\n                scale_factor = scale_factor ** (1 / grad_accu_steps)\n                scale_window = scale_window * grad_accu_steps\n                print(\n                    \"INFO: gradient_accumulation_steps &gt; 1, scale_factor and scale_window are adjusted accordingly for \"\n                    \"dynamic loss scaler\"\n                )\n            loss_scale_manager = nn.DynamicLossScaleUpdateCell(\n                loss_scale_value=cfg.loss_scaler.get(\"loss_scale\", 2**16),\n                scale_factor=scale_factor,\n                scale_window=scale_window,\n            )\n        elif cfg.loss_scaler.type == \"static\":\n            loss_scale = cfg.loss_scaler.get(\"loss_scale\", 1.0)\n            loss_scale_manager = nn.FixedLossScaleUpdateCell(loss_scale)\n        else:\n            raise ValueError(f\"Available loss scaler types are `static` and `dynamic`, but got {cfg.loss_scaler}\")\n\n    return loss_scale_manager, optimizer_loss_scale\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.misc","title":"<code>mindocr.utils.misc</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.misc.AverageMeter","title":"<code>mindocr.utils.misc.AverageMeter</code>","text":"<p>Computes and stores the average and current value</p> Source code in <code>mindocr\\utils\\misc.py</code> <pre><code>class AverageMeter:\n\"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        self.val = Tensor(0.0, dtype=ms.float32)\n        self.avg = Tensor(0.0, dtype=ms.float32)\n        self.sum = Tensor(0.0, dtype=ms.float32)\n        self.count = Tensor(0.0, dtype=ms.float32)\n\n    def update(self, val: Tensor, n: int = 1) -&gt; None:\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.model_wrapper","title":"<code>mindocr.utils.model_wrapper</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.model_wrapper.NetWithEvalWrapper","title":"<code>mindocr.utils.model_wrapper.NetWithEvalWrapper</code>","text":"<p>         Bases: <code>nn.Cell</code></p> <p>A universal wrapper for any network with any loss for evaluation pipeline. Difference from NetWithLossWrapper: it returns loss_val, pred, and labels.</p> PARAMETER DESCRIPTION <code>net</code> <p>network</p> <p> TYPE: <code>nn.Cell</code> </p> <code>loss_fn</code> <p>loss function, if None, will not compute loss for evaluation dataset</p> <p> DEFAULT: <code>None</code> </p> <code>input_indices</code> <p>The indices of the data tuples which will be fed into the network. If it is None, then the first item will be fed only.</p> <p> DEFAULT: <code>None</code> </p> <code>label_indices</code> <p>The indices of the data tuples which will be fed into the loss function. If it is None, then the remaining items will be fed.</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\utils\\model_wrapper.py</code> <pre><code>class NetWithEvalWrapper(nn.Cell):\n\"\"\"\n    A universal wrapper for any network with any loss for evaluation pipeline.\n    Difference from NetWithLossWrapper: it returns loss_val, pred, and labels.\n\n    Args:\n        net (nn.Cell): network\n        loss_fn: loss function, if None, will not compute loss for evaluation dataset\n        input_indices: The indices of the data tuples which will be fed into the network.\n            If it is None, then the first item will be fed only.\n        label_indices: The indices of the data tuples which will be fed into the loss function.\n            If it is None, then the remaining items will be fed.\n    \"\"\"\n\n    def __init__(self, net, loss_fn=None, input_indices=None, label_indices=None):\n        super().__init__(auto_prefix=False)\n        self._net = net\n        self._loss_fn = loss_fn\n        # TODO: get this automatically from net and loss func\n        self.input_indices = input_indices\n        self.label_indices = label_indices\n\n    def construct(self, *args):\n\"\"\"\n        Args:\n            args (Tuple): contains network inputs, labels (given by data loader)\n        Returns:\n            Tuple: loss value (Tensor), pred (Union[Tensor, Tuple[Tensor]]), labels (Tuple)\n        \"\"\"\n        # TODO: pred is a dict\n        if self.input_indices is None:\n            pred = self._net(args[0])\n        else:\n            pred = self._net(*select_inputs_by_indices(args, self.input_indices))\n\n        if self.label_indices is None:\n            labels = args[1:]\n        else:\n            labels = select_inputs_by_indices(args, self.label_indices)\n\n        if self._loss_fn is not None:\n            loss_val = self._loss_fn(pred, *labels)\n        else:\n            loss_val = None\n\n        return loss_val, pred, labels\n</code></pre> <code>mindocr.utils.model_wrapper.NetWithEvalWrapper.construct(*args)</code> \u00b6 PARAMETER DESCRIPTION <code>args</code> <p>contains network inputs, labels (given by data loader)</p> <p> TYPE: <code>Tuple</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <p>loss value (Tensor), pred (Union[Tensor, Tuple[Tensor]]), labels (Tuple)</p> Source code in <code>mindocr\\utils\\model_wrapper.py</code> <pre><code>def construct(self, *args):\n\"\"\"\n    Args:\n        args (Tuple): contains network inputs, labels (given by data loader)\n    Returns:\n        Tuple: loss value (Tensor), pred (Union[Tensor, Tuple[Tensor]]), labels (Tuple)\n    \"\"\"\n    # TODO: pred is a dict\n    if self.input_indices is None:\n        pred = self._net(args[0])\n    else:\n        pred = self._net(*select_inputs_by_indices(args, self.input_indices))\n\n    if self.label_indices is None:\n        labels = args[1:]\n    else:\n        labels = select_inputs_by_indices(args, self.label_indices)\n\n    if self._loss_fn is not None:\n        loss_val = self._loss_fn(pred, *labels)\n    else:\n        loss_val = None\n\n    return loss_val, pred, labels\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.model_wrapper.NetWithLossWrapper","title":"<code>mindocr.utils.model_wrapper.NetWithLossWrapper</code>","text":"<p>         Bases: <code>nn.Cell</code></p> <p>A universal wrapper for any network with any loss.</p> PARAMETER DESCRIPTION <code>net</code> <p>network</p> <p> TYPE: <code>nn.Cell</code> </p> <code>loss_fn</code> <p>loss function</p> <p> </p> <code>input_indices</code> <p>The indices of the data tuples which will be fed into the network. If it is None, then the first item will be fed only.</p> <p> DEFAULT: <code>None</code> </p> <code>label_indices</code> <p>The indices of the data tuples which will be fed into the loss function. If it is None, then the remaining items will be fed.</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>mindocr\\utils\\model_wrapper.py</code> <pre><code>class NetWithLossWrapper(nn.Cell):\n\"\"\"\n    A universal wrapper for any network with any loss.\n\n    Args:\n        net (nn.Cell): network\n        loss_fn: loss function\n        input_indices: The indices of the data tuples which will be fed into the network.\n            If it is None, then the first item will be fed only.\n        label_indices: The indices of the data tuples which will be fed into the loss function.\n            If it is None, then the remaining items will be fed.\n    \"\"\"\n\n    def __init__(self, net, loss_fn, pred_cast_fp32=False, input_indices=None, label_indices=None):\n        super().__init__(auto_prefix=False)\n        self._net = net\n        self._loss_fn = loss_fn\n        # TODO: get this automatically from net and loss func\n        self.input_indices = input_indices\n        self.label_indices = label_indices\n        self.pred_cast_fp32 = pred_cast_fp32\n        self.cast = ops.Cast()\n\n    def construct(self, *args):\n\"\"\"\n        Args:\n            args (Tuple): contains network inputs, labels (given by data loader)\n        Returns:\n            loss_val (Tensor): loss value\n        \"\"\"\n        if self.input_indices is None:\n            pred = self._net(args[0])\n        else:\n            pred = self._net(*select_inputs_by_indices(args, self.input_indices))\n\n        if self.pred_cast_fp32:\n            if isinstance(pred, list) or isinstance(pred, tuple):\n                pred = [self.cast(p, mstype.float32) for p in pred]\n            else:\n                pred = self.cast(pred, mstype.float32)\n\n        if self.label_indices is None:\n            loss_val = self._loss_fn(pred, *args[1:])\n        else:\n            loss_val = self._loss_fn(pred, *select_inputs_by_indices(args, self.label_indices))\n\n        return loss_val\n</code></pre> <code>mindocr.utils.model_wrapper.NetWithLossWrapper.construct(*args)</code> \u00b6 PARAMETER DESCRIPTION <code>args</code> <p>contains network inputs, labels (given by data loader)</p> <p> TYPE: <code>Tuple</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>loss_val</code> <p>loss value</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindocr\\utils\\model_wrapper.py</code> <pre><code>def construct(self, *args):\n\"\"\"\n    Args:\n        args (Tuple): contains network inputs, labels (given by data loader)\n    Returns:\n        loss_val (Tensor): loss value\n    \"\"\"\n    if self.input_indices is None:\n        pred = self._net(args[0])\n    else:\n        pred = self._net(*select_inputs_by_indices(args, self.input_indices))\n\n    if self.pred_cast_fp32:\n        if isinstance(pred, list) or isinstance(pred, tuple):\n            pred = [self.cast(p, mstype.float32) for p in pred]\n        else:\n            pred = self.cast(pred, mstype.float32)\n\n    if self.label_indices is None:\n        loss_val = self._loss_fn(pred, *args[1:])\n    else:\n        loss_val = self._loss_fn(pred, *select_inputs_by_indices(args, self.label_indices))\n\n    return loss_val\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.recorder","title":"<code>mindocr.utils.recorder</code>","text":""},{"location":"reference/api_doc/#mindocr.utils.recorder.PerfRecorder","title":"<code>mindocr.utils.recorder.PerfRecorder</code>","text":"<p>         Bases: <code>object</code></p> Source code in <code>mindocr\\utils\\recorder.py</code> <pre><code>class PerfRecorder(object):\n    def __init__(\n        self,\n        save_dir,\n        metric_names: List = [\"loss\", \"precision\", \"recall\", \"hmean\", \"s/epoch\"],\n        file_name=\"result.log\",\n        separator=\"\\t\",\n        resume=False,\n    ):\n        self.save_dir = save_dir\n        self.sep = separator\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n            print(f\"{save_dir} not exist. Created.\")\n\n        self.log_txt_fp = os.path.join(save_dir, file_name)\n        if not resume:\n            result_log = separator.join([\"Epoch\"] + metric_names)\n            with open(self.log_txt_fp, \"w\", encoding=\"utf-8\") as fp:\n                fp.write(result_log + \"\\n\")\n\n    def add(self, epoch, *measures):\n\"\"\"\n        measures (Tuple): measurement values corresponding to the metric names\n        \"\"\"\n        sep = self.sep\n        line = f\"{epoch}{sep}\"\n        for i, m in enumerate(measures):\n            if isinstance(m, ms.Tensor):\n                m = m.asnumpy()\n\n            if isinstance(m, float) or isinstance(m, np.float32):\n                line += f\"{m:.4f}\"\n            elif m is None:\n                line += \"NA\"\n            else:\n                line += f\"{m}\"\n\n            if i &lt; len(measures) - 1:\n                line += f\"{sep}\"\n        # line += f\"{epoch_time:.2f}\\n\"\n\n        with open(self.log_txt_fp, \"a\", encoding=\"utf-8\") as fp:\n            fp.write(line + \"\\n\")\n\n    def save_curves(self):\n        plot_result(self.log_txt_fp, save_fig=True, sep=self.sep)\n</code></pre> <code>mindocr.utils.recorder.PerfRecorder.add(epoch, *measures)</code> \u00b6 <p>measures (Tuple): measurement values corresponding to the metric names</p> Source code in <code>mindocr\\utils\\recorder.py</code> <pre><code>def add(self, epoch, *measures):\n\"\"\"\n    measures (Tuple): measurement values corresponding to the metric names\n    \"\"\"\n    sep = self.sep\n    line = f\"{epoch}{sep}\"\n    for i, m in enumerate(measures):\n        if isinstance(m, ms.Tensor):\n            m = m.asnumpy()\n\n        if isinstance(m, float) or isinstance(m, np.float32):\n            line += f\"{m:.4f}\"\n        elif m is None:\n            line += \"NA\"\n        else:\n            line += f\"{m}\"\n\n        if i &lt; len(measures) - 1:\n            line += f\"{sep}\"\n    # line += f\"{epoch_time:.2f}\\n\"\n\n    with open(self.log_txt_fp, \"a\", encoding=\"utf-8\") as fp:\n        fp.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.seed","title":"<code>mindocr.utils.seed</code>","text":"<p>random seed</p>"},{"location":"reference/api_doc/#mindocr.utils.seed.set_seed","title":"<code>mindocr.utils.seed.set_seed(seed=42)</code>","text":"<p>Note: to ensure model init stability, rank_id is removed from seed.</p> Source code in <code>mindocr\\utils\\seed.py</code> <pre><code>def set_seed(seed=42):\n\"\"\"\n    seed: seed int\n\n    Note: to ensure model init stability, rank_id is removed from seed.\n    \"\"\"\n    # if rank is None:\n    #    rank = 0\n    random.seed(seed)\n    ms.set_seed(seed)\n    np.random.seed(seed)\n</code></pre>"},{"location":"reference/api_doc/#mindocr.utils.train_step_wrapper","title":"<code>mindocr.utils.train_step_wrapper</code>","text":"<p>Train step wrapper supporting setting drop overflow update, ema etc</p>"},{"location":"reference/api_doc/#mindocr.utils.train_step_wrapper.TrainOneStepWrapper","title":"<code>mindocr.utils.train_step_wrapper.TrainOneStepWrapper</code>","text":"<p>         Bases: <code>nn.TrainOneStepWithLossScaleCell</code></p> <p>TrainStep with ema and clip grad.</p> PARAMETER DESCRIPTION <code>drop_overflow_update</code> <p>if True, network will not be updated when gradient is overflow.</p> <p> DEFAULT: <code>True</code> </p> <code>scale_sense</code> <p>If this value is a Cell, it will be called to update loss scale. If this value is a Tensor, the loss scale can be modified by <code>set_sense_scale</code>, the shape should be :math:<code>()</code> or :math:<code>(1,)</code>.</p> <p> TYPE: <code>Union[Tensor, Cell]</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Tuple of 3 Tensor, the loss, overflow flag and current loss scale value.</p> <p>loss (Tensor) -  A scalar, the loss value.</p> <p>overflow (Tensor) -  A scalar, whether overflow occur or not, the type is bool.</p> <p>loss scale (Tensor) -  The loss scale value, the shape is :math:<code>()</code> or :math:<code>(1,)</code>.</p> Source code in <code>mindocr\\utils\\train_step_wrapper.py</code> <pre><code>class TrainOneStepWrapper(nn.TrainOneStepWithLossScaleCell):\n\"\"\"TrainStep with ema and clip grad.\n\n    Args:\n        drop_overflow_update: if True, network will not be updated when gradient is overflow.\n        scale_sense (Union[Tensor, Cell]): If this value is a Cell, it will be called\n            to update loss scale. If this value is a Tensor, the loss scale can be modified by `set_sense_scale`,\n            the shape should be :math:`()` or :math:`(1,)`.\n\n    Returns:\n        Tuple of 3 Tensor, the loss, overflow flag and current loss scale value.\n        loss (Tensor) -  A scalar, the loss value.\n        overflow (Tensor) -  A scalar, whether overflow occur or not, the type is bool.\n        loss scale (Tensor) -  The loss scale value, the shape is :math:`()` or :math:`(1,)`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        network,\n        optimizer,\n        scale_sense=1.0,\n        ema=None,\n        updates=0,\n        drop_overflow_update=True,\n        gradient_accumulation_steps=1,\n        clip_grad=False,\n        clip_norm=1.0,\n        verbose=False,\n    ):\n        super().__init__(network, optimizer, scale_sense)\n        self.ema = ema\n        self.drop_overflow_update = drop_overflow_update\n\n        assert isinstance(clip_grad, bool), f\"Invalid type of clip_grad, got {type(clip_grad)}, expected bool\"\n        assert clip_norm &gt; 0.0 and isinstance(clip_norm, float), f\"clip_norm must be float &gt; 1.0, but got {clip_norm}\"\n        self.clip_grad = clip_grad\n        self.clip_norm = clip_norm\n\n        assert gradient_accumulation_steps &gt;= 1\n        self.grad_accu_steps = gradient_accumulation_steps\n        if gradient_accumulation_steps &gt; 1:\n            # additionally caches network trainable parameters. overhead caused.\n            # TODO: try to store it in CPU memory instead of GPU/NPU memory.\n            self.accumulated_grads = optimizer.parameters.clone(prefix=\"grad_accumulated_\", init=\"zeros\")\n            self.zeros = optimizer.parameters.clone(prefix=\"zeros_\", init=\"zeros\")\n            self.cur_accu_step = Parameter(Tensor(0, ms.int32), \"grad_accumulate_step_\", requires_grad=False)\n            self.zero = Tensor(0, ms.int32)\n            for p in self.accumulated_grads:\n                p.requires_grad = False\n            for z in self.zeros:\n                z.requires_grad = False\n\n        self.verbose = verbose\n        self.is_cpu_device = context.get_context(\"device_target\") == \"CPU\"  # to support CPU in CI\n\n        self.map = ops.Map()\n        self.partial = ops.Partial()\n\n    def construct(self, *inputs):\n        # compute loss\n        weights = self.weights\n        loss = self.network(*inputs)  # mini-batch loss\n        scaling_sens = self.scale_sense\n\n        # check loss overflow\n        if not self.is_cpu_device:\n            status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n        else:\n            status = None\n\n        scaling_sens_filled = C.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))  # loss scale value\n\n        # 1. compute gradients (of the up-scaled loss w.r.t. the model weights)\n        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n\n        # 2. down-scale gradients by loss_scale. grads = grads / scaling_sense  / grad_accu_steps\n        # also divide gradients by accumulation steps to avoid taking mean of  the accumulated gradients later\n        grads = self.hyper_map(F.partial(_grad_scale, scaling_sens * self.grad_accu_steps), grads)\n\n        # 3. check gradient overflow\n        if not self.is_cpu_device:\n            cond = self.get_overflow_status(status, grads)\n            overflow = self.process_loss_scale(cond)\n        else:\n            overflow = ms.Tensor(False)\n            cond = ms.Tensor(False)\n\n        # accumulate gradients and update model weights if no overflow or allow to update even when overflow\n        if (not self.drop_overflow_update) or (not overflow):\n            # 4. gradient accumulation if enabled\n            if self.grad_accu_steps &gt; 1:\n                # self.accumulated_grads += grads\n                loss = F.depend(loss, self.map(self.partial(ops.assign_add), self.accumulated_grads, grads))\n                # self.cur_accu_step += 1\n                loss = F.depend(loss, ops.assign_add(self.cur_accu_step, Tensor(1, ms.int32)))\n\n                if self.cur_accu_step % self.grad_accu_steps == 0:\n                    # 5. gradient reduction on distributed GPUs/NPUs\n                    grads = self.grad_reducer(self.accumulated_grads)\n\n                    # 6. clip grad\n                    if self.clip_grad:\n                        grads = ops.clip_by_global_norm(grads, self.clip_norm)\n                    # 7. optimize\n                    loss = F.depend(loss, self.optimizer(grads))\n\n                    # clear gradient accumulation states\n                    loss = F.depend(\n                        loss, self.map(self.partial(ops.assign), self.accumulated_grads, self.zeros)\n                    )  # self.accumulated_grads = 0\n                    loss = F.depend(loss, ops.assign(self.cur_accu_step, self.zero))  # self.cur_accu_step = 0\n                else:\n                    # update LR in each gradient step but not optimize net parameter to ensure the LR curve is\n                    # consistent\n                    loss = F.depend(loss, self.optimizer.get_lr())  # .get_lr() will make lr step increased by 1\n            else:\n                # 5. gradient reduction on distributed GPUs/NPUs\n                grads = self.grad_reducer(grads)\n                # 6. clip grad\n                if self.clip_grad:\n                    grads = ops.clip_by_global_norm(grads, self.clip_norm)\n                # 7. optimize\n                loss = F.depend(loss, self.optimizer(grads))\n\n            # 8.ema\n            if self.ema is not None:\n                self.ema.ema_update()\n        else:\n            # print(\"WARNING: Gradient overflow! update skipped.\")\n            pass\n\n        return loss, cond, scaling_sens\n</code></pre>"},{"location":"reference/api_doc/#mindocr.version","title":"<code>mindocr.version</code>","text":"<p>version init</p>"},{"location":"tutorials/training_recognition_custom_dataset/","title":"Text Recognition","text":"<p>English | \u4e2d\u6587</p>"},{"location":"tutorials/training_recognition_custom_dataset/#training-recognition-network-with-custom-datasets","title":"Training Recognition Network with Custom Datasets","text":"<p>This document provides tutorials on how to train recognition networks using custom datasets, including the training of recognition networks in Chinese and English languages.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#dataset-preperation","title":"Dataset preperation","text":"<p>Currently, MindOCR recognition network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images and text files. It is read by RecDataset. - <code>LMDB Dataset</code>: A file format provided by LMDB. It is read by LMDBDataset.</p> <p>The following tutorials take the use of the <code>Common Dataset</code> file format as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-training-data","title":"Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # Corresponding label\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-validation-data","title":"Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#dictionary-preperation","title":"Dictionary Preperation","text":"<p>To train recognition networks for different languages, users need to configure corresponding dictionaries. Only characters that exist in the dictionary will be correctly predicted by the model. MindOCR currently provides three dictionaries, corresponding to Default, Chinese and English respectively. - <code>Default Dictionary</code>\uff1aincludes lowercase English letters and numbers only. If users do not configure the dictionay, this one will be used by default. - <code>English Dictionary</code>\uff1aincludes uppercase and lowercase English letters, numbers and punctuation marks, it is place at <code>mindocr/utils/dict/en_dict.txt</code>. - <code>Chinese Dictionary</code>\uff1aincludes commonly used Chinese characters, uppercase and lowercase English letters, numbers, and punctuation marks, it is placed at <code>mindocr/utils/dict/ch_dict.txt</code>.</p> <p>Currently, MindOCR does not provide a dictionary configuration for other languages. This feature will be released in a upcoming version.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configuration-file-preperation","title":"Configuration File Preperation","text":"<p>To configure the corresponding configuration file for a specific network architecture, users need to provide the necessary settings. As an example, we can take CRNN (with backbone Resnet34) as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-english-model","title":"Configure an English Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n...\n</code></pre> <p>And also modify the corresponding dictionary location to the the English dictionary path.</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>To use the complete English dictionary, users need to modify the <code>common:num_classes</code> attribute in the corresponding configuration file, as the initial configuration file\u2019s dictionary only includes lowercase English and numbers.</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 95                                        # The number is equal to the number of dictionary characters plus 1\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 96                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\nuse_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-english-dictionary","title":"Configuring a custom English dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-chinese-model","title":"Configure an Chinese Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34_ch.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n...\n</code></pre> <p>And also modify the corresponding dictionary location to the the Chinese dictionary path.</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 6625                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\nuse_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-chinese-dictionary","title":"Configuring a custom Chinese dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-training-and-evaluation","title":"Model Training And Evaluation","text":"<p>When all datasets and configuration files have been prepared, users can start training their custom models. Since the training methods for different models are different, users can refer to the <code>Model Training</code> and <code>Model Evaluation</code> sections of the documentation for the corresponding model.</p>"},{"location":"tutorials/transform_tutorial/","title":"Transformation Tutorial","text":""},{"location":"tutorials/transform_tutorial/#mechanism","title":"Mechanism","text":"<ol> <li>Each transformation is a class with a callable function. An example is as follows</li> </ol> <pre><code>class ToCHWImage(object):\n\"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>The input for transformation is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>The transformation api should have clarify the required keys in input and the modified or/and added keys in output the data dict.</p> </li> </ol> <p>Available transformations can be checked in <code>mindocr/data/transforms/*_transform.py</code></p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#text-detection","title":"Text detection","text":""},{"location":"tutorials/transform_tutorial/#1-load-image-and-annotations","title":"1. Load image and annotations","text":""},{"location":"tutorials/transform_tutorial/#preparation","title":"Preparation","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"tutorials/transform_tutorial/#decode-the-image-decodeimage","title":"Decode the image  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"tutorials/transform_tutorial/#detlabelencode","title":"DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#2-image-and-annotation-processingaugmentation","title":"2. Image and annotation processing/augmentation","text":""},{"location":"tutorials/transform_tutorial/#randomcrop-eastrandomcropdata","title":"RandomCrop - EastRandomCropData","text":"<pre><code>from mindocr.data.transforms.general_transforms import RandomCropWithBBox\nimport copy\n\n#crop_data = det_transforms.EastRandomCropData(size=(640, 640))\ncrop_data = RandomCropWithBBox(crop_size=(640, 640))\n\nshow_img(data['image'])\nfor i in range(2):\n    data_cache = copy.deepcopy(data)\n    data_cropped = crop_data(data_cache)\n\n    res_crop = draw_boxes(data_cropped['image'], data_cropped['polys'])\n    show_img(res_crop)\n</code></pre>"},{"location":"tutorials/transform_tutorial/#colorjitter","title":"ColorJitter","text":"<pre><code>random_color_adj = general_transforms.RandomColorAdjust(brightness=0.4, saturation=0.5)\n\ndata_cache = copy.deepcopy(data)\n#data_cache['image'] = data_cache['image'][:,:, ::-1]\ndata_adj = random_color_adj(data_cache)\n#print(data_adj)\nshow_img(data_adj['image'], is_bgr_img=True)\n</code></pre>"},{"location":"cn/","title":"\u4e3b\u9875","text":""},{"location":"cn/#mindocr","title":"MindOCR","text":"<p>English | \u4e2d\u6587</p> <p>\u6982\u8ff0 | \u5b89\u88c5 | \u5feb\u901f\u4e0a\u624b | \u6a21\u578b\u5217\u8868 | \u91cd\u8981\u4fe1\u606f</p>"},{"location":"cn/#_1","title":"\u6982\u8ff0","text":"<p>MindOCR\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore\u6846\u67b6\u7684OCR\u5f00\u53d1\u53ca\u5e94\u7528\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u8bad\u7ec3\u3001\u5e94\u7528\u4e1a\u754c\u6700\u6709\u4f18\u7684\u6587\u672c\u68c0\u6d4b\u3001\u6587\u672c\u8bc6\u522b\u6a21\u578b\uff0c\u4f8b\u5982DBNet/DBNet++\u548cCRNN/SVTR\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u6587\u672c\u7406\u89e3\u7684\u9700\u6c42\u3002</p>  \u4e3b\u8981\u7279\u6027   - **\u6a21\u5757\u5316\u8bbe\u8ba1**: MindOCR\u5c06OCR\u4efb\u52a1\u89e3\u8026\u6210\u591a\u4e2a\u53ef\u914d\u7f6e\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u4fee\u6539\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u5b9a\u5236\u5316\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0a\u914d\u7f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u7684\u5168\u6d41\u7a0b\uff1b - **\u9ad8\u6027\u80fd**: MindOCR\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u5176\u8fbe\u5230OCR\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\uff1b - **\u6613\u7528\u6027**: MindOCR\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u5e2e\u52a9\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u8fdb\u884c\u6587\u672c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\uff08\u656c\u8bf7\u671f\u5f85\uff09\u3002"},{"location":"cn/#_2","title":"\u5b89\u88c5","text":""},{"location":"cn/#_3","title":"\u4f9d\u8d56","text":"<p>\u8bf7\u8fd0\u884c\u5982\u4e0b\u4ee3\u7801\u5b89\u88c5\u4f9d\u8d56\u5305\uff1a <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>\u6b64\u5916\uff0c\u8bf7\u6309\u5b98\u65b9\u6307\u5f15\u5b89\u88c5MindSpore(&gt;=1.9) \u6765\u9002\u914d\u60a8\u7684\u673a\u5668\u3002\u5982\u679c\u9700\u8981\u5728\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd8\u8bf7\u5b89\u88c5openmpi\u3002</p> \u73af\u5883 \u7248\u672c MindSpore &gt;=1.9 Python &gt;=3.7 <p>\u6ce8\u610f\uff1a - \u5982\u679c\u4f7f\u7528ACL\u63a8\u7406\uff0cPython\u7248\u672c\u9700\u4e3a3.9\u3002 - \u5982\u679c\u9047\u5230scikit_image\u5bfc\u5165\u9519\u8bef\uff0c\u53c2\u8003\u6b64\u5904\uff0c\u4f60\u9700\u8981\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>$LD_PRELOAD</code>\uff0c\u547d\u4ee4\u5982\u4e0b\u3002\u66ff\u6362<code>path/to</code>\u4e3a\u4f60\u7684\u76ee\u5f55\u3002   <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"cn/#pypi","title":"\u901a\u8fc7PyPI\u5b89\u88c5","text":"<p>\u656c\u8bf7\u671f\u5f85</p>"},{"location":"cn/#_4","title":"\u901a\u8fc7\u6e90\u6587\u4ef6\u5b89\u88c5","text":"<p>\u6700\u65b0\u7248\u7684MindOCR\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a <pre><code>pip install git+https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>\u6ce8\u610f\uff1aMindOCR\u76ee\u524d\u6682\u65f6\u53ea\u5728MindSpore&gt;=1.9\u7248\u672c\uff0cLinux\u7cfb\u7edf\uff0cGPU/Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u8fc7\u6d4b\u8bd5\u3002</p>"},{"location":"cn/#_5","title":"\u5feb\u901f\u4e0a\u624b","text":""},{"location":"cn/#1","title":"1. \u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30","text":""},{"location":"cn/#11","title":"1.1 \u6587\u672c\u68c0\u6d4b","text":"<p>MindOCR\u652f\u6301\u591a\u79cd\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u53ca\u6570\u636e\u96c6\uff0c\u5728\u6b64\u6211\u4eec\u4f7f\u7528**DBNet**\u6a21\u578b\u548c**ICDAR2015**\u6570\u636e\u96c6\u8fdb\u884c\u6f14\u793a\u3002\u8bf7\u53c2\u8003DBNet\u6a21\u578b\u6587\u6863\u3002</p>"},{"location":"cn/#12","title":"1.2 \u6587\u672c\u8bc6\u522b","text":"<p>MindOCR\u652f\u6301\u591a\u79cd\u6587\u672c\u8bc6\u522b\u6a21\u578b\u53ca\u6570\u636e\u96c6\uff0c\u5728\u6b64\u6211\u4eec\u4f7f\u7528**CRNN**\u6a21\u578b\u548c**LMDB**\u6570\u636e\u96c6\u8fdb\u884c\u6f14\u793a\u3002\u8bf7\u53c2\u8003CRNN\u6a21\u578b\u6587\u6863\u3002</p>"},{"location":"cn/#2","title":"2. \u63a8\u7406\u4e0e\u90e8\u7f72","text":""},{"location":"cn/#21-mindspore-liteaclascend-310","title":"2.1 \u4f7f\u7528MindSpore Lite\u548cACL\u63a8\u7406(Ascend 310)","text":"<p>MindOCR\u96c6\u6210\u4e86MindSpore Lite\u548cACL\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8bc6\u522b\u4e32\u8054\u63a8\u7406\u3002</p> <p>\u5177\u4f53\u8bf4\u660e\u8bf7\u53c2\u8003MindOCR 310\u63a8\u7406\u3002</p>"},{"location":"cn/#22-mindsporecpugpuascend-910","title":"2.2 \u4f7f\u7528\u539f\u751fMindSpore\u5728\u7ebf\u63a8\u7406(CPU/GPU/Ascend 910)","text":"<p>MindOCR\u63d0\u4f9b\u6613\u7528\u7684\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u63a8\u7406\u5de5\u5177\uff0c\u652f\u6301CPU/GPU/Ascend 910\u786c\u4ef6\u5e73\u53f0\u3002\u5728\u7ebf\u63a8\u7406\u57fa\u4e8e\u4f7f\u7528MindOCR\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002</p> <p>\u5177\u4f53\u7528\u6cd5\u548c\u6548\u679c\u8bf7\u53c2\u8003 MindOCR\u5728\u7ebf\u63a8\u7406\u3002</p>"},{"location":"cn/#_6","title":"\u6a21\u578b\u5217\u8868","text":"\u6587\u672c\u68c0\u6d4b  - [x] [DBNet](../../configs/det/dbnet/README.md) (AAAI'2020) - [x] [DBNet++](../../configs/det/dbnet/README.md) (TPAMI'2022) - [x] [PSENet](../../configs/det/psenet/README.md) (CVPR'2019) - [x] [EAST](../../configs/det/east/README.md)(CVPR'2017) - [ ] [FCENet](https://arxiv.org/abs/2104.10442) (CVPR'2021) [coming soon]   \u6587\u672c\u8bc6\u522b   - [x] [CRNN](../../configs/rec/crnn/README.md) (TPAMI'2016) - [x] [CRNN-Seq2Seq/RARE](../../configs/rec/rare/README.md) (CVPR'2016) - [x] [SVTR](../../configs/rec/svtr/README.md) (IJCAI'2022) - [ ] [ABINet](https://arxiv.org/abs/2103.06495) (CVPR'2021) [coming soon]   \u6a21\u578b\u8bad\u7ec3\u7684\u914d\u7f6e\u53ca\u6027\u80fd\u7ed3\u679c\u8bf7\u89c1[configs](../../configs).  MindSpore Lite\u548cACL\u6a21\u578b\u63a8\u7406\u7684\u652f\u6301\u5217\u8868\uff0c\u8bf7\u89c1[MindOCR\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868](inference/models_list_cn.md)\u548c[\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868](inference/models_list_thirdparty_cn.md).  ## \u6570\u636e\u96c6 ### \u4e0b\u8f7d  \u6211\u4eec\u63d0\u4f9b\u4ee5\u4e0b\u6570\u636e\u96c6\u7684\u4e0b\u8f7d\u8bf4\u660e\u3002   \u6587\u672c\u68c0\u6d4b  - [x] ICDAR2015 [\u8bba\u6587](../../(https:/rrc.cvc.uab.es/files/short_rrc_2015.pdf)) [\u4e3b\u9875](../../(https:/rrc.cvc.uab.es/?ch=4)) [\u4e0b\u8f7d\u8bf4\u660e](datasets/icdar2015_CN.md)  - [x] Total-Text [\u8bba\u6587](https://arxiv.org/abs/1710.10400) [\u4e3b\u9875](https://github.com/cs-chan/Total-Text-Dataset/tree/master/Dataset) [\u4e0b\u8f7d\u8bf4\u660e](datasets/totaltext_CN.md)  - [x] Syntext150k [\u8bba\u6587](https://arxiv.org/abs/2002.10200) [\u4e3b\u9875](https://github.com/aim-uofa/AdelaiDet) [\u4e0b\u8f7d\u8bf4\u660e](datasets/syntext150k_CN.md)  - [x] MLT2017 [\u8bba\u6587](https://ieeexplore.ieee.org/abstract/document/8270168) [\u4e3b\u9875](https://rrc.cvc.uab.es/?ch=8&amp;com=introduction) [\u4e0b\u8f7d\u8bf4\u660e](datasets/mlt2017_CN.md)  - [x] MSRA-TD500 [\u8bba\u6587](https://ieeexplore.ieee.org/abstract/document/6247787) [\u4e3b\u9875](http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500)) [\u4e0b\u8f7d\u8bf4\u660e](datasets/td500_CN.md)  - [x] SCUT-CTW1500 [\u8bba\u6587](https://www.sciencedirect.com/science/article/pii/S0031320319300664) [\u4e3b\u9875](https://github.com/Yuliang-Liu/Curve-Text-Detector) [\u4e0b\u8f7d\u8bf4\u660e](datasets/ctw1500_CN.md)    ### \u8f6c\u6362  \u5728 `DATASETS_DIR` \u6587\u4ef6\u5939\u4e2d\u4e0b\u8f7d\u8fd9\u4e9b\u6570\u636e\u96c6\u540e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c `bash tools/convert_datasets.sh` \u5c06\u6240\u6709\u4e0b\u8f7d\u7684\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u76ee\u6807\u683c\u5f0f\u3002[\u8fd9\u91cc](../../tools/dataset_converters/README_CN.md)\u6709\u4e00\u4e2a icdar2015 \u6570\u636e\u96c6\u8f6c\u6362\u7684\u4f8b\u5b50\u3002   ## \u91cd\u8981\u4fe1\u606f  ### \u53d8\u66f4\u65e5\u5fd7  - 2023/06/07 1. \u589e\u52a0\u65b0\u6a21\u578b     - \u6587\u672c\u68c0\u6d4b[PSENet](../../configs/det/psenet)     - \u6587\u672c\u68c0\u6d4b[EAST](../../configs/det/east)     - \u6587\u672c\u8bc6\u522b[SVTR](../../configs/rec/svtr) 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c     - [totaltext](datasets/totaltext_CN.md)     - [mlt2017](datasets/mlt2017_CN.md)     - [chinese_text_recognition](datasets/chinese_text_recognition_CN.md) 3. \u589e\u52a0\u65ad\u70b9\u91cd\u8bad(resume training)\u529f\u80fd\uff0c\u53ef\u5728\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\u4f7f\u7528\u3002\u5982\u9700\u4f7f\u7528\uff0c\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d`model`\u5b57\u6bb5\u4e0b\u589e\u52a0`resume`\u53c2\u6570\uff0c\u5141\u8bb8\u4f20\u5165\u5177\u4f53\u8def\u5f84`resume: /path/to/train_resume.ckpt`\u6216\u8005\u901a\u8fc7\u8bbe\u7f6e`resume: True`\u6765\u52a0\u8f7d\u5728ckpt_save_dir\u4e0b\u4fdd\u5b58\u7684trian_resume.ckpt   - 2023/05/15 1. \u589e\u52a0\u65b0\u6a21\u578b     - \u6587\u672c\u68c0\u6d4b[DBNet++](../../configs/det/dbnet)     - \u6587\u672c\u8bc6\u522b[CRNN-Seq2Seq](../../configs/rec/rare)     - \u5728SynthText\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684[DBNet](https://download.mindspore.cn/toolkits/mindocr/dbnet/dbnet_resnet50_synthtext-40655acb.ckpt) 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c     - [SynthText](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c), [MSRA-TD500](datasets/td500_CN.md), [CTW1500](datasets/ctw1500_CN.md)     - DBNet\u7684\u66f4\u591a\u57fa\u51c6\u7ed3\u679c\u53ef\u4ee5[\u5728\u6b64\u627e\u5230](../../configs/det/dbnet/README_CN.md). 3. \u6dfb\u52a0\u7528\u4e8e\u4fdd\u5b58\u524dk\u4e2acheckpoint\u7684checkpoint manager\u5e76\u6539\u8fdb\u65e5\u5fd7\u3002 4. Python\u63a8\u7406\u4ee3\u7801\u91cd\u6784\u3002 5. Bug\u4fee\u590d\uff1a\u5bf9\u5927\u578b\u6570\u636e\u96c6\u4f7f\u7528\u5e73\u5747\u635f\u5931meter\uff0c\u5728AMP\u8bad\u7ec3\u4e2d\u5bf9ctcloss\u7981\u7528`pred_cast_fp32`\uff0c\u4fee\u590d\u5b58\u5728\u65e0\u6548\u591a\u8fb9\u5f62\u7684\u9519\u8bef\u3002  - 2023/05/04 1. \u652f\u6301\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3checkpoint\uff0c \u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06`model-pretrained`\u8bbe\u7f6e\u4e3acheckpoint url\u6216\u672c\u5730\u8def\u5f84\u6765\u4f7f\u7528\u3002 2. \u652f\u6301\u8bbe\u7f6e\u6267\u884c\u5305\u62ec\u65cb\u8f6c\u548c\u7ffb\u8f6c\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u6982\u7387\u3002 3. \u4e3a\u6a21\u578b\u8bad\u7ec3\u6dfb\u52a0EMA\u529f\u80fd\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e`train-ema`\uff08\u9ed8\u8ba4\u503c\uff1aFalse\uff09\u548c`train-ema_decay`\u6765\u542f\u7528\u3002 4. \u53c2\u6570\u4fee\u6539\uff1a`num_columns_to_net` -&gt; `net_input_column_index`: \u8f93\u5165\u7f51\u7edc\u7684columns\u6570\u91cf\u6539\u4e3a\u8f93\u5165\u7f51\u7edc\u7684columns\u7d22\u5f15 5. \u53c2\u6570\u4fee\u6539\uff1a`num_columns_of_labels` -&gt; `label_column_index`: \u7528\u7d22\u5f15\u66ff\u6362\u6570\u91cf\uff0c\u4ee5\u8868\u793alebel\u7684\u4f4d\u7f6e\u3002  - 2023/04/21 1. \u6dfb\u52a0\u53c2\u6570\u5206\u7ec4\u4ee5\u652f\u6301\u8bad\u7ec3\u4e2d\u7684\u6b63\u5219\u5316\u3002\u7528\u6cd5\uff1a\u5728yaml config\u4e2d\u6dfb\u52a0`grouping_strategy`\u53c2\u6570\u4ee5\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6216\u4f7f\u7528`no_weight_decay_params`\u53c2\u6570\u9009\u62e9\u8981\u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u7684\u5c42\uff08\u4f8b\u5982\uff0cbias\u3001norm\uff09\u3002\u793a\u4f8b\u53ef\u53c2\u8003`configs/rec/crn/crnn_icdar15.yaml` 2. \u6dfb\u52a0\u68af\u5ea6\u79ef\u7d2f\uff0c\u652f\u6301\u5927\u6279\u91cf\u8bad\u7ec3\u3002\u7528\u6cd5\uff1a\u5728yaml\u914d\u7f6e\u4e2d\u6dfb\u52a0`gradient_accumulation_steps`\uff0c\u5168\u5c40\u6279\u91cf\u5927\u5c0f=batch_size * devices * gradient_aaccumulation_steps\u3002\u793a\u4f8b\u53ef\u53c2\u8003`configs/rec/crn/crnn_icdar15.yaml` 3. \u6dfb\u52a0\u68af\u5ea6\u88c1\u526a\uff0c\u652f\u6301\u8bad\u7ec3\u7a33\u5b9a\u3002\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06`grad_clip`\u8bbe\u7f6e\u4e3aTrue\u6765\u542f\u7528\u3002  - 2023/03/23 1. \u589e\u52a0dynamic loss scaler\u652f\u6301, \u4e14\u4e0edrop overflow update\u517c\u5bb9\u3002\u5982\u9700\u4f7f\u7528, \u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0`loss_scale`\u5b57\u6bb5\u5e76\u5c06`type`\u53c2\u6570\u8bbe\u4e3a`dynamic`\uff0c\u53c2\u8003\u4f8b\u5b50\u8bf7\u89c1`configs/rec/crnn/crnn_icdar15.yaml`  - 2023/03/20 1. \u53c2\u6570\u540d\u4fee\u6539\uff1a`output_keys` -&gt; `output_columns`\uff1b`num_keys_to_net` -&gt; `num_columns_to_net`\uff1b 2. \u66f4\u65b0\u6570\u636e\u6d41\u7a0b\u3002  - 2023/03/13 1. \u589e\u52a0\u7cfb\u7edf\u6d4b\u8bd5\u548cCI\u5de5\u4f5c\u6d41\uff1b 2. \u589e\u52a0modelarts\u5e73\u53f0\u9002\u914d\u5668\uff0c\u4f7f\u5f97\u652f\u6301\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff0c\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a   <pre><code>  i)   \u5728OpenI\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u4efb\u52a1\uff1b\n  ii)  \u5728\u7f51\u9875\u4e0a\u5173\u8054\u6570\u636e\u96c6\uff0c\u5982ic15_mindocr\uff1b\n  iii) \u589e\u52a0 `config` \u53c2\u6570\uff0c\u5728\u7f51\u9875\u7684UI\u754c\u9762\u914d\u7f6eyaml\u6587\u4ef6\u8def\u5f84\uff0c\u5982'/home/work/user-job-dir/V0001/configs/rec/test.yaml'\uff1b\n  iv)  \u5728\u7f51\u9875\u7684UI\u754c\u9762\u589e\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u5c06\u5176\u8bbe\u7f6e\u4e3aTrue\uff1b\n  v)   \u586b\u5199\u5176\u4ed6\u9879\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre>  - 2023/03/08 1. \u589e\u52a0\u8bc4\u4f30\u811a\u672c with  arg `ckpt_load_path`\uff1b 2. Yaml\u6587\u4ef6\u4e2d\u7684`ckpt_save_dir`\u53c2\u6570\u4ece`system` \u79fb\u52a8\u5230 `train`\uff1b 3. \u589e\u52a0rop_overflow_update\u63a7\u5236\u3002  ### \u5982\u4f55\u8d21\u732e  \u6211\u4eec\u6b22\u8fce\u5305\u62ec\u95ee\u9898\u5355\u548cPR\u5728\u5185\u7684\u6240\u6709\u8d21\u732e\uff0c\u6765\u8ba9MindOCR\u53d8\u5f97\u66f4\u597d\u3002  \u8bf7\u53c2\u8003[CONTRIBUTING.md](../../CONTRIBUTING.md)\u4f5c\u4e3a\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u6309\u7167[Model Template and Guideline](../../mindocr/models/README.md)\u7684\u6307\u5f15\u8d21\u732e\u4e00\u4e2a\u9002\u914d\u6240\u6709\u63a5\u53e3\u7684\u6a21\u578b\uff0c\u591a\u8c22\u5408\u4f5c\u3002  ### \u8bb8\u53ef  \u672c\u9879\u76ee\u9075\u4ece[Apache License 2.0](../../LICENSE)\u5f00\u6e90\u8bb8\u53ef\u3002  ### \u5f15\u7528  \u5982\u679c\u672c\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a  <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"cn/index_mainpage/#_1","title":"\u6982\u8ff0","text":"<p>MindOCR\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore\u6846\u67b6\u7684OCR\u5f00\u53d1\u53ca\u5e94\u7528\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u8bad\u7ec3\u3001\u5e94\u7528\u4e1a\u754c\u6700\u6709\u4f18\u7684\u6587\u672c\u68c0\u6d4b\u3001\u6587\u672c\u8bc6\u522b\u6a21\u578b\uff0c\u4f8b\u5982DBNet/DBNet++\u548cCRNN/SVTR\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u6587\u672c\u7406\u89e3\u7684\u9700\u6c42\u3002</p>  \u4e3b\u8981\u7279\u6027   - **\u6a21\u5757\u5316\u8bbe\u8ba1**: MindOCR\u5c06OCR\u4efb\u52a1\u89e3\u8026\u6210\u591a\u4e2a\u53ef\u914d\u7f6e\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u4fee\u6539\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u5b9a\u5236\u5316\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0a\u914d\u7f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u7684\u5168\u6d41\u7a0b\uff1b - **\u9ad8\u6027\u80fd**: MindOCR\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u5176\u8fbe\u5230OCR\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\uff1b - **\u6613\u7528\u6027**: MindOCR\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u5e2e\u52a9\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u8fdb\u884c\u6587\u672c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\uff08\u656c\u8bf7\u671f\u5f85\uff09\u3002"},{"location":"cn/index_mainpage/#_2","title":"\u5b89\u88c5","text":""},{"location":"cn/index_mainpage/#_3","title":"\u4f9d\u8d56","text":"<p>\u8bf7\u8fd0\u884c\u5982\u4e0b\u4ee3\u7801\u5b89\u88c5\u4f9d\u8d56\u5305\uff1a <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>\u6b64\u5916\uff0c\u8bf7\u6309\u5b98\u65b9\u6307\u5f15\u5b89\u88c5MindSpore(&gt;=1.9) \u6765\u9002\u914d\u60a8\u7684\u673a\u5668\u3002\u5982\u679c\u9700\u8981\u5728\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd8\u8bf7\u5b89\u88c5openmpi\u3002</p> \u73af\u5883 \u7248\u672c MindSpore &gt;=1.9 Python &gt;=3.7 <p>\u6ce8\u610f\uff1a - \u5982\u679c\u4f7f\u7528ACL\u63a8\u7406(\u8bf7\u89c1: 2.1 \u4f7f\u7528mindspore lite\u548cacl\u63a8\u7406)\uff0cPython\u7248\u672c\u9700\u4e3a3.9\u3002 - \u5982\u679c\u9047\u5230scikit_image\u5bfc\u5165\u9519\u8bef\uff0c\u53c2\u8003\u6b64\u5904\uff0c\u4f60\u9700\u8981\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>$LD_PRELOAD</code>\uff0c\u547d\u4ee4\u5982\u4e0b\u3002\u66ff\u6362<code>path/to</code>\u4e3a\u4f60\u7684\u76ee\u5f55\u3002   <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"cn/index_mainpage/#pypi","title":"\u901a\u8fc7PyPI\u5b89\u88c5","text":"<p>\u656c\u8bf7\u671f\u5f85</p>"},{"location":"cn/index_mainpage/#_4","title":"\u901a\u8fc7\u6e90\u6587\u4ef6\u5b89\u88c5","text":"<p>\u6700\u65b0\u7248\u7684MindOCR\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a <pre><code>pip install git+https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>\u6ce8\u610f\uff1aMindOCR\u76ee\u524d\u6682\u65f6\u53ea\u5728MindSpore&gt;=1.9\u7248\u672c\uff0cLinux\u7cfb\u7edf\uff0cGPU/Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u8fc7\u6d4b\u8bd5\u3002</p>"},{"location":"cn/index_mainpage/#_5","title":"\u5feb\u901f\u4e0a\u624b","text":""},{"location":"cn/index_mainpage/#1","title":"1. \u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30","text":""},{"location":"cn/index_mainpage/#11","title":"1.1 \u6587\u672c\u68c0\u6d4b","text":"<p>MindOCR\u652f\u6301\u591a\u79cd\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u53ca\u6570\u636e\u96c6\uff0c\u5728\u6b64\u6211\u4eec\u4f7f\u7528**DBNet**\u6a21\u578b\u548c**ICDAR2015**\u6570\u636e\u96c6\u8fdb\u884c\u6f14\u793a\u3002\u8bf7\u53c2\u8003<code>configs/det/dbnet/README_CN.md</code>\u3002</p>"},{"location":"cn/index_mainpage/#12","title":"1.2 \u6587\u672c\u8bc6\u522b","text":"<p>MindOCR\u652f\u6301\u591a\u79cd\u6587\u672c\u8bc6\u522b\u6a21\u578b\u53ca\u6570\u636e\u96c6\uff0c\u5728\u6b64\u6211\u4eec\u4f7f\u7528**CRNN**\u6a21\u578b\u548c**LMDB**\u6570\u636e\u96c6\u8fdb\u884c\u6f14\u793a\u3002\u8bf7\u53c2\u8003<code>configs/rec/crnn/README_CN.md</code>\u3002</p>"},{"location":"cn/index_mainpage/#2","title":"2. \u63a8\u7406\u4e0e\u90e8\u7f72","text":""},{"location":"cn/index_mainpage/#21-mindspore-liteaclascend-310","title":"2.1 \u4f7f\u7528MindSpore Lite\u548cACL\u63a8\u7406(Ascend 310)","text":"<p>MindOCR\u96c6\u6210\u4e86MindSpore Lite\u548cACL\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8bc6\u522b\u4e32\u8054\u63a8\u7406\u3002</p> <p>\u5177\u4f53\u8bf4\u660e\u8bf7\u53c2\u8003MindOCR 310\u63a8\u7406\u3002</p>"},{"location":"cn/index_mainpage/#22-mindsporecpugpuascend-910","title":"2.2 \u4f7f\u7528\u539f\u751fMindSpore\u5728\u7ebf\u63a8\u7406(CPU/GPU/Ascend 910)","text":"<p>MindOCR\u63d0\u4f9b\u6613\u7528\u7684\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u63a8\u7406\u5de5\u5177\uff0c\u652f\u6301CPU/GPU/Ascend 910\u786c\u4ef6\u5e73\u53f0\u3002\u5728\u7ebf\u63a8\u7406\u57fa\u4e8e\u4f7f\u7528MindOCR\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002</p> <p>\u5177\u4f53\u7528\u6cd5\u548c\u6548\u679c\u8bf7\u53c2\u8003 MindOCR\u5728\u7ebf\u63a8\u7406\u3002</p>"},{"location":"cn/index_mainpage/#_6","title":"\u6a21\u578b\u5217\u8868","text":"\u6587\u672c\u68c0\u6d4b  - [x] DBNet (AAAI'2020) - [x] DBNet++ (TPAMI'2022) - [x] PSENet (CVPR'2019) - [x] EAST (CVPR'2017) - [ ] FCENet (CVPR'2021) [coming soon]   \u6587\u672c\u8bc6\u522b   - [x] CRNN (TPAMI'2016) - [x] CRNN-Seq2Seq/RARE (CVPR'2016) - [x] SVTR (IJCAI'2022) - [ ] ABINet (CVPR'2021) [coming soon]   \u6a21\u578b\u8bad\u7ec3\u7684\u914d\u7f6e\u53ca\u6027\u80fd\u7ed3\u679c\u8bf7\u89c1`configs/`\u76ee\u5f55\u3002  MindSpore Lite\u548cACL\u6a21\u578b\u63a8\u7406\u7684\u652f\u6301\u5217\u8868\uff0c\u8bf7\u89c1[MindOCR\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868](mkdocs/inference_models_list.md)\u548c[\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868](mkdocs/inference_models_list_thirdparty.md).  ## \u6570\u636e\u96c6 ### \u4e0b\u8f7d  \u6211\u4eec\u63d0\u4f9b\u4ee5\u4e0b\u6570\u636e\u96c6\u7684\u4e0b\u8f7d\u8bf4\u660e\u3002   \u6587\u672c\u68c0\u6d4b  - [x] ICDAR2015 [\u8bba\u6587]((https://rrc.cvc.uab.es/files/short_rrc_2015.pdf)) [\u4e3b\u9875]((https://rrc.cvc.uab.es/?ch=4))  - [x] Total-Text [\u8bba\u6587](https://arxiv.org/abs/1710.10400) [\u4e3b\u9875](https://github.com/cs-chan/Total-Text-Dataset/tree/master/Dataset)  - [x] Syntext150k [\u8bba\u6587](https://arxiv.org/abs/2002.10200) [\u4e3b\u9875](https://github.com/aim-uofa/AdelaiDet)  - [x] MLT2017 [\u8bba\u6587](https://ieeexplore.ieee.org/abstract/document/8270168) [\u4e3b\u9875](https://rrc.cvc.uab.es/?ch=8&amp;com=introduction)  - [x] MSRA-TD500 [\u8bba\u6587](https://ieeexplore.ieee.org/abstract/document/6247787) [\u4e3b\u9875](http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500))  - [x] SCUT-CTW1500 [\u8bba\u6587](https://www.sciencedirect.com/science/article/pii/S0031320319300664) [\u4e3b\u9875](https://github.com/Yuliang-Liu/Curve-Text-Detector)    ### \u8f6c\u6362  \u5728 `DATASETS_DIR` \u6587\u4ef6\u5939\u4e2d\u4e0b\u8f7d\u8fd9\u4e9b\u6570\u636e\u96c6\u540e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c `bash tools/convert_datasets.sh` \u5c06\u6240\u6709\u4e0b\u8f7d\u7684\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u76ee\u6807\u683c\u5f0f\u3002[\u8fd9\u91cc](mkdocs/dataset_converters.md)\u6709\u4e00\u4e2a icdar2015 \u6570\u636e\u96c6\u8f6c\u6362\u7684\u4f8b\u5b50\u3002   ## \u91cd\u8981\u4fe1\u606f  ### \u53d8\u66f4\u65e5\u5fd7  - 2023/06/07 1. \u589e\u52a0\u65b0\u6a21\u578b     - \u6587\u672c\u68c0\u6d4bPSENet     - \u6587\u672c\u68c0\u6d4bEAST     - \u6587\u672c\u8bc6\u522bSVTR 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c     - totaltext     - mlt2017     - chinese_text_recognition 3. \u589e\u52a0\u65ad\u70b9\u91cd\u8bad(resume training)\u529f\u80fd\uff0c\u53ef\u5728\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\u4f7f\u7528\u3002\u5982\u9700\u4f7f\u7528\uff0c\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d`model`\u5b57\u6bb5\u4e0b\u589e\u52a0`resume`\u53c2\u6570\uff0c\u5141\u8bb8\u4f20\u5165\u5177\u4f53\u8def\u5f84`resume: /path/to/train_resume.ckpt`\u6216\u8005\u901a\u8fc7\u8bbe\u7f6e`resume: True`\u6765\u52a0\u8f7d\u5728ckpt_save_dir\u4e0b\u4fdd\u5b58\u7684trian_resume.ckpt   - 2023/05/15 1. \u589e\u52a0\u65b0\u6a21\u578b     - \u6587\u672c\u68c0\u6d4bDBNet++     - \u6587\u672c\u8bc6\u522bCRNN-Seq2Seq     - \u5728SynthText\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684DBNet\u7684[\u6743\u91cd\u6587\u4ef6](https://download.mindspore.cn/toolkits/mindocr/dbnet/dbnet_resnet50_synthtext-40655acb.ckpt) 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c     - [SynthText](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c), MSRA-TD500, CTW1500     - DBNet\u7684\u66f4\u591a\u57fa\u51c6\u7ed3\u679c\u8bf7\u89c1: `configs/det/dbnet/README_CN.md`. 3. \u6dfb\u52a0\u7528\u4e8e\u4fdd\u5b58\u524dk\u4e2acheckpoint\u7684checkpoint manager\u5e76\u6539\u8fdb\u65e5\u5fd7\u3002 4. Python\u63a8\u7406\u4ee3\u7801\u91cd\u6784\u3002 5. Bug\u4fee\u590d\uff1a\u5bf9\u5927\u578b\u6570\u636e\u96c6\u4f7f\u7528\u5e73\u5747\u635f\u5931meter\uff0c\u5728AMP\u8bad\u7ec3\u4e2d\u5bf9ctcloss\u7981\u7528`pred_cast_fp32`\uff0c\u4fee\u590d\u5b58\u5728\u65e0\u6548\u591a\u8fb9\u5f62\u7684\u9519\u8bef\u3002  - 2023/05/04 1. \u652f\u6301\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3checkpoint\uff0c \u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06`model-pretrained`\u8bbe\u7f6e\u4e3acheckpoint url\u6216\u672c\u5730\u8def\u5f84\u6765\u4f7f\u7528\u3002 2. \u652f\u6301\u8bbe\u7f6e\u6267\u884c\u5305\u62ec\u65cb\u8f6c\u548c\u7ffb\u8f6c\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u6982\u7387\u3002 3. \u4e3a\u6a21\u578b\u8bad\u7ec3\u6dfb\u52a0EMA\u529f\u80fd\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e`train-ema`\uff08\u9ed8\u8ba4\u503c\uff1aFalse\uff09\u548c`train-ema_decay`\u6765\u542f\u7528\u3002 4. \u53c2\u6570\u4fee\u6539\uff1a`num_columns_to_net` -&gt; `net_input_column_index`: \u8f93\u5165\u7f51\u7edc\u7684columns\u6570\u91cf\u6539\u4e3a\u8f93\u5165\u7f51\u7edc\u7684columns\u7d22\u5f15 5. \u53c2\u6570\u4fee\u6539\uff1a`num_columns_of_labels` -&gt; `label_column_index`: \u7528\u7d22\u5f15\u66ff\u6362\u6570\u91cf\uff0c\u4ee5\u8868\u793alebel\u7684\u4f4d\u7f6e\u3002  - 2023/04/21 1. \u6dfb\u52a0\u53c2\u6570\u5206\u7ec4\u4ee5\u652f\u6301\u8bad\u7ec3\u4e2d\u7684\u6b63\u5219\u5316\u3002\u7528\u6cd5\uff1a\u5728yaml config\u4e2d\u6dfb\u52a0`grouping_strategy`\u53c2\u6570\u4ee5\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6216\u4f7f\u7528`no_weight_decay_params`\u53c2\u6570\u9009\u62e9\u8981\u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u7684\u5c42\uff08\u4f8b\u5982\uff0cbias\u3001norm\uff09\u3002\u793a\u4f8b\u53ef\u53c2\u8003`configs/rec/crn/crnn_icdar15.yaml` 2. \u6dfb\u52a0\u68af\u5ea6\u79ef\u7d2f\uff0c\u652f\u6301\u5927\u6279\u91cf\u8bad\u7ec3\u3002\u7528\u6cd5\uff1a\u5728yaml\u914d\u7f6e\u4e2d\u6dfb\u52a0`gradient_accumulation_steps`\uff0c\u5168\u5c40\u6279\u91cf\u5927\u5c0f=batch_size * devices * gradient_aaccumulation_steps\u3002\u793a\u4f8b\u53ef\u53c2\u8003`configs/rec/crn/crnn_icdar15.yaml` 3. \u6dfb\u52a0\u68af\u5ea6\u88c1\u526a\uff0c\u652f\u6301\u8bad\u7ec3\u7a33\u5b9a\u3002\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06`grad_clip`\u8bbe\u7f6e\u4e3aTrue\u6765\u542f\u7528\u3002  - 2023/03/23 1. \u589e\u52a0dynamic loss scaler\u652f\u6301, \u4e14\u4e0edrop overflow update\u517c\u5bb9\u3002\u5982\u9700\u4f7f\u7528, \u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0`loss_scale`\u5b57\u6bb5\u5e76\u5c06`type`\u53c2\u6570\u8bbe\u4e3a`dynamic`\uff0c\u53c2\u8003\u4f8b\u5b50\u8bf7\u89c1`configs/rec/crnn/crnn_icdar15.yaml`  - 2023/03/20 1. \u53c2\u6570\u540d\u4fee\u6539\uff1a`output_keys` -&gt; `output_columns`\uff1b`num_keys_to_net` -&gt; `num_columns_to_net`\uff1b 2. \u66f4\u65b0\u6570\u636e\u6d41\u7a0b\u3002  - 2023/03/13 1. \u589e\u52a0\u7cfb\u7edf\u6d4b\u8bd5\u548cCI\u5de5\u4f5c\u6d41\uff1b 2. \u589e\u52a0modelarts\u5e73\u53f0\u9002\u914d\u5668\uff0c\u4f7f\u5f97\u652f\u6301\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff0c\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a   <pre><code>  i)   \u5728OpenI\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u4efb\u52a1\uff1b\n  ii)  \u5728\u7f51\u9875\u4e0a\u5173\u8054\u6570\u636e\u96c6\uff0c\u5982ic15_mindocr\uff1b\n  iii) \u589e\u52a0 `config` \u53c2\u6570\uff0c\u5728\u7f51\u9875\u7684UI\u754c\u9762\u914d\u7f6eyaml\u6587\u4ef6\u8def\u5f84\uff0c\u5982'/home/work/user-job-dir/V0001/configs/rec/test.yaml'\uff1b\n  iv)  \u5728\u7f51\u9875\u7684UI\u754c\u9762\u589e\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u5c06\u5176\u8bbe\u7f6e\u4e3aTrue\uff1b\n  v)   \u586b\u5199\u5176\u4ed6\u9879\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre>  - 2023/03/08 1. \u589e\u52a0\u8bc4\u4f30\u811a\u672c with  arg `ckpt_load_path`\uff1b 2. Yaml\u6587\u4ef6\u4e2d\u7684`ckpt_save_dir`\u53c2\u6570\u4ece`system` \u79fb\u52a8\u5230 `train`\uff1b 3. \u589e\u52a0rop_overflow_update\u63a7\u5236\u3002  ### \u5982\u4f55\u8d21\u732e  \u6211\u4eec\u6b22\u8fce\u5305\u62ec\u95ee\u9898\u5355\u548cPR\u5728\u5185\u7684\u6240\u6709\u8d21\u732e\uff0c\u6765\u8ba9MindOCR\u53d8\u5f97\u66f4\u597d\u3002  \u8bf7\u53c2\u8003[CONTRIBUTING.md](mkdocs/contributing.md)\u4f5c\u4e3a\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u6309\u7167[Model Template and Guideline](mkdocs/customize_model.md)\u7684\u6307\u5f15\u8d21\u732e\u4e00\u4e2a\u9002\u914d\u6240\u6709\u63a5\u53e3\u7684\u6a21\u578b\uff0c\u591a\u8c22\u5408\u4f5c\u3002  ### \u8bb8\u53ef  \u672c\u9879\u76ee\u9075\u4ece[Apache License 2.0](mkdocs/license.md)\u5f00\u6e90\u8bb8\u53ef\u3002  ### \u5f15\u7528  \u5982\u679c\u672c\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a  <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition_CN/","title":"chinese text recognition CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_1","title":"\u4e2d\u6587\u6587\u5b57\u8bc6\u522b\u6570\u636e\u96c6","text":"<p>\u672c\u6587\u6863\u4ecb\u7ecd\u4e2d\u6587\u6587\u672c\u8bc6\u522b\u7684\u6570\u636e\u96c6\u51c6\u5907\u3002</p>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_2","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6309\u7167 Benchmarking-Chinese-Text-Recognition \u4e2d\u7684\u8bbe\u7f6e\uff0c\u6211\u4eec\u4f7f\u7528\u4e0e Datasets \u4e2d\u63cf\u8ff0\u7684\u76f8\u540c\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6570\u636e\u3002</p> <p>\u8bf7\u4e0b\u8f7dDownload\u4e2d\u4ecb\u7ecd\u7684\u4ee5\u4e0bLMDB\u6587\u4ef6\uff1a</p> <ul> <li>\u573a\u666f\u6570\u636e\u96c6\uff1a\u8054\u5408\u6570\u636e\u96c6\u5305\u542b RCTW, ReCTS, LSVT, ArT, CTW</li> <li>\u7f51\u9875\uff1aMTWI</li> <li>\u6587\u6863\uff1a\u4f7f\u7528 Text Render \u751f\u6210</li> <li>\u624b\u5199\u6570\u636e\u96c6\uff1aSCUT-HCCDoc</li> </ul>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_3","title":"\u6570\u636e\u7ed3\u6784\u6574\u7406","text":"<p>\u4e0b\u8f7d\u6587\u4ef6\u540e\uff0c\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u6587\u4ef6\u653e\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939 <code>training</code> \u4e0b\uff0c\u6240\u6709\u9a8c\u8bc1\u6570\u636e\u653e\u5728 <code>validation</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u6240\u6709\u8bc4\u4f30\u6570\u636e\u653e\u5728<code>evaluation</code>\u4e0b\u3002</p> <p>\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_4","title":"\u6570\u636e\u96c6\u914d\u7f6e","text":"<p>\u8981\u4f7f\u7528\u6570\u636e\u96c6\uff0c\u60a8\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u6570\u636e\u96c6\uff0c\u5982\u4e0b\u6240\u793a\u3002</p>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_5","title":"\u6a21\u578b\u8bad\u7ec3","text":"<pre><code>...\ntrain:\n...\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\ndata_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\ndata_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n...\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition_CN/#_6","title":"\u6a21\u578b\u8bc4\u4f30","text":"<pre><code>...\ntrain:\n# \u8bad\u7ec3\u90e8\u5206\u4e0d\u9700\u8981\u4fee\u6539\uff0c\u56e0\u4e0d\u4f1a\u8c03\u7528\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\ndata_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n...\n</code></pre> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/ctw1500_CN/","title":"ctw1500 CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/ctw1500_CN/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08SCUT-CTW1500\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/ctw1500_CN/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ctw1500_CN/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/train_images/ \\\n--label_dir path/to/ctw1500/ctw_1500_train_labels \\\n--output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/test_images/ \\\n--label_dir path/to/ctw1500/gt_ctw_1500 \\\n--output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>ctw1500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/icdar2015_CN/","title":"icdar2015 CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/icdar2015_CN/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>ICDAR 2015 \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u5730\u5740: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> \u4ece\u4f55\u5904\u4e0b\u8f7d ICDAR 2015  ICDAR 2015 \u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u4efb\u52a1\u3002\u4efb\u52a11\u662f\u6587\u672c\u5b9a\u4f4d\u3002\u4efb\u52a13\u662f\u5355\u8bcd\u8bc6\u522b\u3002\u4efb\u52a14\u662f\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u3002\u4efb\u52a12\u6587\u672c\u5206\u5272\u7684\u6570\u636e\u4e0d\u53ef\u7528\u3002  ### Text Localization  \u6709\u56db\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre>  ### Word Recognition  \u6709\u4e09\u4e2a\u4e0e\u4efb\u52a13\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre>  \u8fd9\u4e09\u4e2a\u6587\u4ef6\u4ec5\u7528\u4e8e\u8bad\u7ec3\u5355\u8bcd\u8bc6\u522b\u6a21\u578b\u3002\u8bad\u7ec3\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u4e0d\u9700\u8981\u8fd9\u4e09\u4e2a\u6587\u4ef6\u3002  ### E2E  \u6709\u4e5d\u4e2a\u4e0e\u4efb\u52a14\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\u3002\u5176\u4e2d\u5305\u62ec\u4efb\u52a11\u4e2d\u7684\u56db\u4e2a\u6587\u4ef6\uff0c \u8fd8\u6709\u4e94\u4e2a\u8bcd\u6c47\u6587\u4ef6\u3002  <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre>  \u5982\u679c\u60a8\u4e0b\u8f7d\u4e86\u4e00\u4e2a\u540d\u4e3a `Challenge4_Test_Task4_GT.zip` \u7684\u6587\u4ef6\uff0c\u8bf7\u6ce8\u610f\u5b83\u4e0e `Challenge4_Test_Task1_GT.zip` \u662f\u76f8\u540c\u7684\u6587\u4ef6\uff0c\u9664\u4e86\u540d\u79f0\u4e0d\u540c\u3002\u5728\u8fd9\u4e2arepo\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 `Challenge4_Test_Task4_GT.zip` \u6765\u4ee3\u66ff ICDAR2015 \u6570\u636e\u96c6\u7684\u6587\u4ef6 `Challenge4_Test_Task1_GT.zip`\u3002    <p>\u5728 icdar2015 \u4e0b\u8f7d\u5b8c\u6210\u4ee5\u540e, \u8bf7\u628a\u6240\u6709\u7684\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/mlt2017_CN/","title":"mlt2017 CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/mlt2017_CN/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>MLT (Multi-Lingual) 2017 \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u5730\u5740: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> \u4ece\u4f55\u5904\u4e0b\u8f7d MLT 2017  MLT 2017 \u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u4efb\u52a1. \u4efb\u52a1 1 \u662f\u6587\u672c\u68c0\u6d4b (\u591a\u8bed\u8a00\u6587\u672c)\u3002 \u4efb\u52a12\u662f\u6587\u672c\u8bc6\u522b\u3002  ### \u6587\u672c\u68c0\u6d4b  \u670911\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre>  \u6d4b\u8bd5\u96c6\u4e0d\u9700\u8981\u4e0b\u8f7d\u3002  ### \u6587\u672c\u8bc6\u522b  \u67096\u4e2a\u4e0e\u4efb\u52a12\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n&lt;/details&gt;\n\n\n\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e, \u5c06\u6587\u4ef6\u653e\u4e8e `[path-to-data-dir]` \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip   ```  [\u8fd4\u56de](../../../tools/dataset_converters/README_CN.md)"},{"location":"cn/datasets/svt_CN/","title":"svt CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/svt_CN/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u8857\u666f\u6587\u672c\u6570\u636e\u96c6\uff08SVT\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"cn/datasets/svt_CN/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/svt_CN/#_3","title":"\u8bc6\u522b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u8bc6\u522b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name  svt --task rec \\\n--image_dir path/to/svt1/ \\\n--label_dir path/to/svt1/train.xml \\\n--output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>svt1/</code> \u4e0b\u6709\u4e00\u4e2a\u6587\u4ef6\u5939 <code>cropped_images/</code> \u548c\u4e00\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>rec_train_gt.txt</code>\u3002</p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/syntext150k_CN/","title":"syntext150k CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/syntext150k_CN/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>SynText150k \u6587\u7ae0</p> <p>\u4e0b\u8f7d Syntext-150k (Part1: 54,327 [imgs][annos]. Part2: 94,723 [imgs][annos].)</p> <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/synthtext_CN/","title":"synthtext CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/synthtext_CN/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>SynthText\u662f\u4e00\u4e2a\u5408\u6210\u751f\u6210\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5355\u8bcd\u5b9e\u4f8b\u88ab\u653e\u7f6e\u5728\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\uff0c\u5e76\u8003\u8651\u4e86\u573a\u666f\u5e03\u5c40\u3002</p> <p>\u8bba\u6587 | \u4e0b\u8f7dSynthText</p> <p>\u4e0b\u8f7d<code>SynthText.zip</code>\u6587\u4ef6\u5e76\u89e3\u538b\u7f29\u5230<code>[path-to-data-dir]</code>\u6587\u4ef6\u5939\u4e2d\uff1a <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: \u53e6\u5916, \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u5728\u4f7f\u7528 <code>SynthText</code> \u6570\u636e\u96c6\u4e4b\u524d\u5148\u8fdb\u884c\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e00\u4e9b\u9519\u8bef\u7684\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u4e0b\u5217\u7684\u65b9\u5f0f\u8fdb\u884c\u6821\u6b63: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat\n</code></pre> \u4ee5\u4e0a\u7684\u64cd\u4f5c\u4f1a\u4ea7\u751f\u4e0e<code>SynthText</code>\u539f\u59cb\u6807\u6ce8\u683c\u5f0f\u76f8\u540c\u4f46\u662f\u662f\u7ecf\u8fc7\u8fc7\u6ee4\u540e\u7684\u6807\u6ce8\u6570\u636e.</p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/td500_CN/","title":"td500 CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/td500_CN/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08MSRA-TD500\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/td500_CN/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/td500_CN/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/train/ \\\n--label_dir path/to/MSRA-TD500/train \\\n--output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/test/ \\\n--label_dir path/to/MSRA-TD500/test \\\n--output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>MSRA-TD500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56de</p>"},{"location":"cn/datasets/totaltext_CN/","title":"totaltext CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/datasets/totaltext_CN/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>Total-Text \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6\u56fe\u50cf \u5730\u5740  (size = 441Mb).</p> \u56fe\u50cf\u4e0b\u8f7d\u94fe\u63a5     [\u56fe\u50cf\u4e0b\u8f7d\u94fe\u63a5](https://drive.google.com/file/d/1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2/view?usp=sharing) (size = 441Mb).   <p>\u4e0b\u8f7d\u6570\u636e\u96c6\u6807\u6ce8 \u5730\u5740.</p> \u6807\u6ce8\u4e0b\u8f7d\u94fe\u63a5     [\u6807\u6ce8\u4e0b\u8f7d\u94fe\u63a5](https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view?usp=sharing) for text file format('.txt').   <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>\u8fd4\u56de</p>"},{"location":"cn/inference/convert_dynamic_cn/","title":"Convert dynamic cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/convert_dynamic_cn/#-shape","title":"\u63a8\u7406 - \u6a21\u578bShape\u5206\u6863","text":""},{"location":"cn/inference/convert_dynamic_cn/#1","title":"1 \u7b80\u4ecb","text":"<p>\u6839\u636e\u63d0\u4f9b\u7684\u6570\u636e\u96c6\uff0c\u7edf\u8ba1\u56fe\u50cf<code>height</code>\u548c<code>width</code>\u7684\u5206\u5e03\u8303\u56f4\uff0c\u79bb\u6563\u7684\u9009\u62e9<code>batch size</code>\u3001<code>height</code>\u3001<code>width</code>\u7ec4\u5408\uff0c\u5b9e\u73b0\u5206\u6863\uff0c\u518d\u4f7f\u7528 ATC \u6216 MindSpore Lite \u8fdb\u884c\u6a21\u578b\u8f6c\u6362\u3002</p>"},{"location":"cn/inference/convert_dynamic_cn/#2","title":"2 \u73af\u5883\u51c6\u5907","text":"\u73af\u5883 \u7248\u672c Device Ascend310/310P Python &gt;= 3.7"},{"location":"cn/inference/convert_dynamic_cn/#3","title":"3 \u6a21\u578b\u51c6\u5907","text":"<p>\u4f8b\u5982\uff0c\u9700\u8981\u5148\u4e0b\u8f7d\u63a8\u7406\u6a21\u578b\uff08 \u68c0\u6d4b \uff0c \u8bc6\u522b \uff0c \u5206\u7c7b \uff09\uff0c\u4f7f\u7528 paddle2onnx \u5de5\u5177\u8f6c\u6362\u5f97\u5230\u4ee5\u4e0bONNX\u6a21\u578b\u3002</p> \u6a21\u578b\u7c7b\u522b \u6a21\u578b\u540d\u79f0 \u8f93\u5165shape \u68c0\u6d4b ch_PP-OCRv3_det_infer.onnx -1,3,-1,-1 \u8bc6\u522b ch_PP-OCRv3_rec_infer.onnx -1,3,48,-1 \u5206\u7c7b ch_ppocr_mobile_v2.0_cls_infer.onnx -1,3,48,192"},{"location":"cn/inference/convert_dynamic_cn/#4","title":"4 \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u4f8b\u5982\uff0c\u6570\u636e\u96c6ICDAR 2015\uff1a<code>Text Localization</code> \uff0c\u4e0b\u8f7d\u9700\u8981\u5148\u6ce8\u518c\u8d26\u53f7\u3002</p> <p>\u6570\u636e\u96c6\u51c6\u5907\u8bf7\u53c2\u8003tools/dataset_converters/converter.py \u6570\u636e\u683c\u5f0f\u5316\u8f6c\u6362\u811a\u672c\uff0c\u5e76\u6309\u7167README_CN<code>\u6587\u672c\u68c0\u6d4b/\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b</code> \u90e8\u5206\u6267\u884c\u811a\u672c\u3002\u6700\u7ec8\u5f97\u5230\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u6807\u6ce8\u6587\u4ef6\u3002</p>"},{"location":"cn/inference/convert_dynamic_cn/#5","title":"5 \u5206\u6863\u5de5\u5177\u4f7f\u7528","text":""},{"location":"cn/inference/convert_dynamic_cn/#51","title":"5.1 \u81ea\u52a8\u5206\u6863\u8c03\u7528\u793a\u4f8b","text":"<p>\u53c2\u8003<code>deploy/models_utils/auto_scaling/converter.py</code>\u5c06\u6a21\u578b\u8f6cOM\u6a21\u578b\u3002   <pre><code># git clone https://github.com/mindspore-lab/mindocr\n# cd mindocr/deploy/models_utils/auto_scaling\n\n# \u793a\u4f8b1\uff1a\u5bf9batch size\u8fdb\u884c\u5206\u6863\npython converter.py --model_path=/path/to/ch_PP-OCRv3_rec_infer.onnx \\\n                    --dataset_path=/path/to/det_gt.txt\n                    --input_shape=-1,3,48,192 \\\n                    --output_path=output\n\n\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1ach_PP-OCRv3_rec_infer_dynamic_bs.om\u3002\n</code></pre> <pre><code># \u793a\u4f8b2\uff1a\u5bf9height\u548cwidth\u8fdb\u884c\u5206\u6863\npython converter.py --model_path=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n                    --dataset_path=/path/to/images \\\n                    --input_shape=1,3,-1,-1 \\\n                    --output_path=output\n\n\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1ach_PP-OCRv3_det_infer_dynamic_hw.om\u3002\n</code></pre> <pre><code># \u793a\u4f8b3\uff1a\u5bf9batch szie\u3001height\u548cwidth\u8fdb\u884c\u5206\u6863\npython converter.py --model_path=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n                    --dataset_path=/path/to/images \\\n                    --input_shape=-1,3,-1,-1 \\\n                    --output_path=output\n\n\u8f93\u51fa\u7ed3\u679c\u4e3a\u591a\u4e2aOM\u6a21\u578b\uff1ach_PP-OCRv3_det_infer_dynamic_bs1_hw.om\uff0cch_PP-OCRv3_det_infer_dynamic_bs4_hw.om\uff0c\u2026\u2026\uff0cch_PP-OCRv3_det_infer_dynamic_bs64_hw.om\u3002\n</code></pre> <pre><code># \u793a\u4f8b4\uff1a\u4e0d\u505a\u5206\u6863\npython converter.py --model_path=/path/to/ch_ppocr_mobile_v2.0_cls_infer.onnx \\\n                    --input_shape=4,3,48,192 \\\n                    --output_path=output\n\n\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1ach_ppocr_mobile_v2.0_cls_infer_static.om\u3002\n</code></pre></p> <p>\u9700\u8981\u9002\u914d\u811a\u672c\u5bf9\u5e94\u6570\u636e\u548c\u6a21\u578b\u53c2\u6570\uff1a</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 model_path \u5fc5\u9009\uff0c\u9700\u8981\u8f6c\u6362\u7684\u6a21\u578b\u6587\u4ef6\u8def\u5f84\u3002 data_path \u975e\u5fc5\u9009\uff0c\u68c0\u6d4b\u6a21\u578b\u4e3a\u6570\u636e\u96c6\u56fe\u7247\u8def\u5f84\uff0c\u8bc6\u522b\u6a21\u578b\u4e3a\u6807\u6ce8\u6587\u4ef6\u8def\u5f84\uff0c\u7528\u6237\u4e0d\u4f20\u6b64\u53c2\u6570\u4f1a\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u4e2d\u5206\u6863\u6570\u636e\u3002 input_name \u975e\u5fc5\u9009\uff0c\u6a21\u578b\u8f93\u5165\u53d8\u91cf\u540d\uff0c\u9ed8\u8ba4\uff1ax\u3002 input_shape \u5fc5\u9009\uff0c\u6a21\u578b\u8f93\u5165shape\uff1aNCHW\uff0cN\u3001H\u3001W\u652f\u6301\u5206\u6863\u3002 backend \u975e\u5fc5\u9009\uff0c\u8f6c\u6362\u5de5\u5177\uff1aatc\u6216lite\uff0c\u9ed8\u8ba4\uff1aatc\u3002 output_path \u975e\u5fc5\u9009\uff0c\u8f93\u51fa\u6a21\u578b\u4fdd\u5b58\u8def\u5f84\uff0c\u9ed8\u8ba4\uff1a./output\u3002 soc_version \u975e\u5fc5\u9009\uff0cAscend310P3\u6216Ascend310P\uff0c\u9ed8\u8ba4\uff1aAscend310P3\u3002"},{"location":"cn/inference/convert_dynamic_cn/#52-atcmindspore-lite","title":"5.2 <code>ATC</code>\u6216<code>MindSpore Lite</code>\u5355\u72ec\u4f7f\u7528\u793a\u4f8b","text":"<p>\u5728<code>deploy/models_utils/auto_scaling/example</code>\u4e0b\u7ed9\u51fa\u4e86\u5355\u72ec\u8c03\u7528<code>ATC</code>\u6216<code>MindSpore Lite</code>\u8f6c\u6362\u7684\u793a\u4f8b\u3002</p> <p><pre><code># ATC\natc --model=/path/to/ch_ppocr_mobile_v2.0_cls_infer.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:-1,3,48,192\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"1;4;8;16;32\" \\\n    --soc_version=Ascend310P3 \\\n    --output=output \\\n    --log=error\n</code></pre>   \u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1aoutput.om\u3002\u66f4\u591a\u8bf7\u53c2\u8003\uff1aATC\u793a\u4f8b</p> <p><pre><code># MindSpore Lite\nconverter_lite  --modelFile=/path/to/ch_PP-OCRv3_det_infer.onnx \\\n    --fmk=ONNX \\\n    --configFile=lite_config.txt \\\n    --saveType=MINDIR \\\n    --NoFusion=false \\\n    --device=Ascend \\\n    --outputFile=output\n</code></pre>   \u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1aoutput.om\u3002\u66f4\u591a\u8bf7\u53c2\u8003\uff1aMindSpore Lite\u793a\u4f8b</p> <p>\u6ce8\u610f\uff1a<code>MindSpore Lite</code>\u8f6c\u6362\u9700\u8981\u914d\u7f6e\u4e00\u4e2a<code>lite_config.txt</code>\u6587\u4ef6, \u5982\u4e0b\u6240\u793a\uff1a   <pre><code>[ascend_context]\ninput_format = NCHW\ninput_shape = x:[1,3,-1,-1]\ndynamic_dims = [1248,640],[1248,672],...,[1280,768],[1280,800]\n</code></pre></p>"},{"location":"cn/inference/convert_dynamic_cn/#53","title":"5.3 \u81ea\u52a8\u5206\u6863\u914d\u7f6e\u6587\u4ef6","text":"<p><code>limit_side_len</code>\uff1a \u539f\u59cb\u8f93\u5165\u6570\u636e\u7684width\u548cheight\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u8303\u56f4\u6309\u7167\u6bd4\u4f8b\u8fdb\u884c\u538b\u7f29\uff0c\u53ef\u4ee5\u8c03\u6574\u6570\u636e\u7684\u79bb\u6563\u7a0b\u5ea6\u3002</p> <p><code>strategy</code>\uff1a\u6570\u636e\u7edf\u8ba1\u7b97\u6cd5\u7b56\u7565\uff0c\u652f\u6301mean_std\u548cmax_min\u4e24\u79cd\u7b97\u6cd5\uff0c\u9ed8\u8ba4\uff1amean_std\u3002</p> <pre><code>\u5047\u8bbe\u6570\u636e\u5e73\u5747\u503c\uff1amean\uff0c\u6807\u51c6\u5dee\uff1asigma\uff0c\u6700\u5927\u503c\uff1amax\uff0c\u6700\u5c0f\u503c\uff1amin\u3002\n\nmean_std\uff1a\u8ba1\u7b97\u65b9\u5f0f\uff1a[mean - n_std * sigma\uff0cmean + n_std * sigma]\uff0cn_std\uff1a3\u3002\n\nmax_min\uff1a\u8ba1\u7b97\u65b9\u5f0f\uff1a[min - (max - min)*expand_ratio/2\uff0cmax + (max - min)*expand_ratio/2]\uff0c expand_ratio\uff1a0.2\u3002\n</code></pre> <p><code>width_range/height_range</code>\uff1a \u5bf9\u79bb\u6563\u7edf\u8ba1\u4e4b\u540e\u7684width/height\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u5c06\u88ab\u8fc7\u6ee4\u3002</p> <p><code>interval</code>\uff1a\u5206\u6863\u95f4\u9694\u5927\u5c0f\u3002</p> <p><code>max_scaling_num</code>\uff1a\u5206\u6863\u7ec4\u5408\u6570\u4e0a\u9650\u3002</p> <p><code>batch_choices</code>\uff1a\u9ed8\u8ba4\u7684batch size\u8303\u56f4\u3002</p> <p><code>default_scaling</code>\uff1a\u7528\u6237\u4e0d\u4f20\u5165\u6570\u636e\u65f6\uff0c\u63d0\u4f9b\u9ed8\u8ba4\u7684\u5206\u6863\u6570\u636e\u3002</p>"},{"location":"cn/inference/convert_dynamic_cn/#54","title":"5.4 \u81ea\u52a8\u5206\u6863\u5de5\u5177\u4ee3\u7801\u76ee\u5f55\u7ed3\u6784","text":"<pre><code>auto_scaling\n\u251c\u2500\u2500 configs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 auto_scaling.yaml\n\u251c\u2500\u2500 converter.py\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 atc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_dynamic_bs.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_dynamic_hw.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 atc_static.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mindspore_lite\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_bs.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_bs.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_hw.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_dynamic_hw.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 lite_static.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 lite_static.txt\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 auto_scaling.py\n    \u251c\u2500\u2500 backend\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 atc_converter.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 lite_converter.py\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 scale_analyzer\n        \u251c\u2500\u2500 dataset_analyzer.py\n        \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"cn/inference/convert_tutorial_cn/","title":"Convert tutorial cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/convert_tutorial_cn/#-","title":"\u63a8\u7406 - \u6a21\u578b\u8f6c\u6362\u6559\u7a0b","text":""},{"location":"cn/inference/convert_tutorial_cn/#1-mindocr","title":"1. MindOCR\u6a21\u578b","text":"<p>MindOCR\u6a21\u578b\u7684\u63a8\u7406\u4f7f\u7528MindSpore Lite\u540e\u7aef\u3002</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(\u79bb\u7ebf\u8f6c\u6362)\"| o[MindIR];\n</code></pre>"},{"location":"cn/inference/convert_tutorial_cn/#11","title":"1.1 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u5728\u63a8\u7406\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u8bad\u7ec3\u7aef\u7684ckpt\u6587\u4ef6\u5bfc\u51fa\u4e3aMindIR\u6587\u4ef6\uff0c\u5b83\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u6743\u91cd\u53c2\u6570\u3002</p> <p>\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u4e86MIndIR\u5bfc\u51fa\u6587\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5\uff0c\u89c1\u6a21\u578b\u5217\u8868\uff0c\u53ef\u8df3\u8f6c\u5230\u5bf9\u5e94\u6a21\u578b\u7684\u4ecb\u7ecd\u9875\u9762\u8fdb\u884c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#12","title":"1.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u9700\u8981\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\uff0c\u5c06\u4e0a\u8ff0\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\u8fdb\u884c\u79bb\u7ebf\u8f6c\u6362\uff0c\u4ece\u800c\u7528\u4e8eMindSpore Lite\u7684\u63a8\u7406\u3002</p> <p><code>converter_lite</code>\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1\u63a8\u7406\u6a21\u578b\u79bb\u7ebf\u8f6c\u6362\u3002</p> <p>\u5047\u8bbe\u8f93\u5165\u6a21\u578b\u4e3ainput.mindir\uff0c\u7ecf\u8fc7<code>converter_lite</code>\u5de5\u5177\u8f6c\u6362\u540e\u7684\u8f93\u51fa\u6a21\u578b\u4e3aoutput.mindir\uff0c\u5219\u6a21\u578b\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>\u5176\u4e2d\uff0c<code>config.txt</code>\u53ef\u4ee5\u8bbe\u7f6e\u8f6c\u6362\u6a21\u578b\u7684Shape\u548c\u63a8\u7406\u7cbe\u5ea6\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#121-shape","title":"1.2.1 \u6a21\u578bShape\u914d\u7f6e","text":"<ul> <li>\u9759\u6001Shape</li> </ul> <p>\u5982\u679c\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u540d\u4e3a<code>x</code>\uff0c\u8f93\u5165Shape\u4e3a<code>(1,3,736,1280)</code>\uff0c\u5219config.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>\u8f6c\u6362\u751f\u6210\u7684output.mindir\u4e3a\u9759\u6001shape\u7248\uff0c\u63a8\u7406\u65f6\u7684\u8f93\u5165\u56fe\u50cf\u9700\u8981Resize\u5230\u8be5input_shape\u4ee5\u6ee1\u8db3\u8f93\u5165\u8981\u6c42\u3002</p> <p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff0c\u5982\u68c0\u6d4b\u51fa\u76ee\u6807\u540e\u518d\u6267\u884c\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\uff0c\u7531\u4e8e\u76ee\u6807\u4e2a\u6570\u548c\u5927\u5c0f\u4e0d\u56fa\u5b9a\uff0c\u5982\u679c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u5047\u8bbe\u5bfc\u51fa\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3, -1, -1)\uff0cNHW\u8fd93\u4e2a\u8f74\u662f\u52a8\u6001\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\u3002</p> <p><code>converter_lite</code>\u901a\u8fc7<code>configFile</code>\u914d\u7f6e<code>[ascend_context]</code>\u4e2d<code>dynamic_dims</code>\u53c2\u6570\u6765\u5b9e\u73b0\uff0c\u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003\u52a8\u6001shape\u914d\u7f6e\uff0c\u4e0b\u6587\u7b80\u79f0\u201d\u5206\u6863\u201c\u3002</p> <p>\u6240\u4ee5\uff0c\u8f6c\u6362\u65f6\u67092\u79cd\u9009\u62e9\uff0c\u901a\u8fc7\u8bbe\u7f6e\u4e0d\u540c\u7684config.txt\u5b9e\u73b0\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u9009\u62e9\u52a8\u6001\u503c\u548c\u6a21\u578b\u8f6c\u6362\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#122","title":"1.2.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>converter_lite</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003Ascend\u8f6c\u6362\u5de5\u5177\u529f\u80fd\u8bf4\u660e\uff0c\u5728\u914d\u7f6e\u6587\u4ef6\u7684\u8868\u683c\u4e2d\u63cf\u8ff0\u4e86<code>precision_mode</code>\u53c2\u6570\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u53ef\u9009\u62e9<code>enforce_fp16</code>\u3001<code>enforce_fp32</code>\u3001<code>preferred_fp32</code>\u548c<code>enforce_origin</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>config.txt</code>\u7684<code>[ascend_context]</code>\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>enforce_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#2-paddleocr","title":"2. PaddleOCR\u6a21\u578b","text":"<p>PaddleOCR\u6a21\u578b\u7684\u63a8\u7406\u53ef\u4ee5\u4f7f\u7528ACL\u548cMindSpore Lite\u4e24\u79cd\u540e\u7aef\uff0c\u5206\u522b\u5bf9\u5e94OM\u6a21\u578b\u548cMindIR\u6a21\u578b\u3002</p> <pre><code>graph LR;\n    \u8bad\u7ec3\u6a21\u578b -- export --&gt; \u63a8\u7406\u6a21\u578b -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"cn/inference/convert_tutorial_cn/#21-","title":"2.1 \u8bad\u7ec3\u6a21\u578b -&gt; \u63a8\u7406\u6a21\u578b","text":"<p>\u5728PaddleOCR\u6a21\u578b\u7684\u4e0b\u8f7d\u94fe\u63a5\u4e2d\uff0c\u6709\u8bad\u7ec3\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u4e24\u79cd\u683c\u5f0f\uff0c\u5982\u679c\u63d0\u4f9b\u7684\u662f\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u683c\u5f0f\u3002</p> <p>\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u539fPaddleOCR\u4ecb\u7ecd\u9875\u9762\uff0c\u4e00\u822c\u4f1a\u6709\u8f6c\u6362\u811a\u672c\u6837\u4f8b\uff0c\u53ea\u9700\u8981\u4f20\u5165\u8bad\u7ec3\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u3001\u6a21\u578b\u6587\u4ef6\u548c\u4fdd\u5b58\u8def\u5f84\u5373\u53ef\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"cn/inference/convert_tutorial_cn/#22-onnx","title":"2.2 \u63a8\u7406\u6a21\u578b -&gt; ONNX","text":"<p>\u5b89\u88c5\u6a21\u578b\u8f6c\u6362\u5de5\u5177paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>\u8be6\u7ec6\u4f7f\u7528\u6559\u7a0b\u8bf7\u53c2\u8003Paddle2ONNX\u6a21\u578b\u8f6c\u5316\u4e0e\u9884\u6d4b\u3002</p> <p>\u6267\u884c\u8f6c\u6362\u547d\u4ee4\uff0c\u751f\u6210onnx\u6a21\u578b\uff1a</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>\u53c2\u6570\u4e2dinput_shape_dict\u7684\u503c\uff0c\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7Netron\u5de5\u5177\u6253\u5f00\u63a8\u7406\u6a21\u578b\u67e5\u770b\uff0c\u6216\u8005\u5728\u4e0a\u8ff0tools/export_model.py\u7684\u4ee3\u7801\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>\u4f7f\u7528ATC\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aOM\u6a21\u578b\u3002</p> <p>\u6607\u817e\u5f20\u91cf\u7f16\u8bd1\u5668\uff08Ascend Tensor Compiler\uff0c\u7b80\u79f0ATC\uff09\u662f\u5f02\u6784\u8ba1\u7b97\u67b6\u6784CANN\u4f53\u7cfb\u4e0b\u7684\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u5b83\u53ef\u4ee5\u5c06\u5f00\u6e90\u6846\u67b6\u7684\u7f51\u7edc\u6a21\u578b\u8f6c\u6362\u4e3a\u6607\u817eAI\u5904\u7406\u5668\u652f\u6301\u7684.om\u683c\u5f0f\u79bb\u7ebf\u6a21\u578b\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1ATC\u6a21\u578b\u8f6c\u6362\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#231-shape","title":"2.3.1 \u6a21\u578bShape\u914d\u7f6e","text":"<p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3,-1,-1)\u3002</p> <ul> <li>\u9759\u6001Shape</li> </ul> <p>\u53ef\u4ee5\u8f6c\u6362\u4e3a\u9759\u6001Shape\u7248\u7684\u6a21\u578b\uff0cNHW\u90fd\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>ATC\u5de5\u5177\u901a\u8fc7\u8bbe\u7f6e\u53c2\u6570 dynamic_dims\u6765\u652f\u6301Shape\u7684**\u5206\u6863**\uff0c\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u5982\u4e0b\u4e24\u79cd\u9009\u62e9\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4e00\u952e\u5f0f\u5b8c\u6210\u52a8\u6001\u503c\u9009\u62e9\u548c\u6a21\u578b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#232","title":"2.3.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>ATC</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003\u53c2\u6570precision_mode\u7684\u8bf4\u660e\uff0c\u53ef\u9009\u62e9<code>force_fp16</code>\u3001<code>force_fp32</code>\u3001<code>allow_fp32_to_fp16</code>\u3001<code>must_keep_origin_dtype</code>\u548c<code>allow_mix_precision</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>atc</code>\u547d\u4ee4\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>force_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>\u4f7f\u7528converter_lite\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aMindIR\u6a21\u578b\u3002\u5de5\u5177\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1MindSpore Lite\u4e91\u4fa7\u63a8\u7406\u79bb\u7ebf\u6a21\u578b\u8f6c\u6362\u3002</p> <p>\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>\u8f6c\u6362\u6d41\u7a0b\u548cMindOCR\u6a21\u578b\u5b8c\u5168\u76f8\u540c\uff0c\u4ec5\u6709\u533a\u522b\u662f<code>--fmk</code>\u9700\u6307\u5b9a\u8f93\u5165\u662fONNX\u6a21\u578b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#3-mmocr","title":"3. MMOCR\u6a21\u578b","text":"<p>MMOCR\u4f7f\u7528Pytorch\uff0c\u5176\u6a21\u578b\u6587\u4ef6\u4e00\u822c\u662fpth\u683c\u5f0f\u3002</p> <p>\u9700\u8981\u5148\u628a\u5b83\u5bfc\u51fa\u4e3aONNX\u683c\u5f0f\uff0c\u518d\u8f6c\u6362\u4e3aACL/MindSpore Lite\u652f\u6301\u7684OM/MindIR\u683c\u5f0f\u3002</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"cn/inference/convert_tutorial_cn/#31-mmocr-onnx","title":"3.1 MMOCR\u6a21\u578b -&gt; ONNX","text":"<p>MMDeploy\u63d0\u4f9b\u4e86MMOCR\u6a21\u578b\u5bfc\u51faONNX\u7684\u547d\u4ee4\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1\u5982\u4f55\u8f6c\u6362\u6a21\u578b\u3002</p> <p>\u5bf9\u4e8e\u53c2\u6570<code>deploy_cfg</code>\u9700\u9009\u62e9\u76ee\u5f55mmdeploy/configs/mmocr\u4e0b\u7684<code>*_onnxruntime_dynamic.py</code>\u6587\u4ef6\uff0c\u4ece\u800c\u5bfc\u51fa\u4e3a\u52a8\u6001Shape\u7248ONNX\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; OM\u3002</p>"},{"location":"cn/inference/convert_tutorial_cn/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; MIndIR\u3002</p>"},{"location":"cn/inference/environment_cn/","title":"Environment cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/environment_cn/#-","title":"\u63a8\u7406 - \u8fd0\u884c\u73af\u5883\u5b89\u88c5","text":"<p>MindOCR\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\u7684\u63a8\u7406\u3002</p> <p>\u8bf7\u786e\u4fdd\u7cfb\u7edf\u6b63\u786e\u5b89\u88c5\u4e86\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\uff0c\u5982\u679c\u6ca1\u6709\u5b89\u88c5\uff0c\u8bf7\u5148\u53c2\u8003\u5b89\u88c5\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\u5c0f\u8282\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>MindOCR\u540e\u7aef\u652f\u6301ACL\u548cMindSpore Lite\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u4f7f\u7528ACL\u6a21\u5f0f\u63a8\u7406\u524d\u9700\u4f7f\u7528ATC\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210om\u683c\u5f0f\uff0c\u4f7f\u7528MindSpore Lite\u63a8\u7406\u524d\u9700\u4f7f\u7528converter_lite\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210MindIR\u683c\u5f0f\uff0c\u5177\u4f53\u533a\u522b\u5982\u4e0b\uff1a</p> ACL Mindspore Lite \u8f6c\u6362\u5de5\u5177 ATC converter_lite \u63a8\u7406\u6a21\u578b\u683c\u5f0f om MindIR"},{"location":"cn/inference/environment_cn/#1-acl","title":"1. ACL\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684ACL\u65b9\u5f0f\u63a8\u7406\uff0c\u76ee\u524dPython\u4fa7\u4f9d\u8d56\u4e8eMindX\u7684Python API\u63a5\u53e3\uff0c\u8be5\u63a5\u53e3\u6682\u53ea\u652f\u6301Python3.9\u3002</p> \u73af\u5883 \u7248\u672c Python 3.9 MindX 3.0.0 <p>\u5728Python3.9\u73af\u5883\u57fa\u7840\u4e0a\uff0c\u4e0b\u8f7dMindX\u7684mxVision SDK\u5b89\u88c5\u5305\uff0c\u53c2\u8003\u6307\u5bfc\u6559\u7a0b\u8fdb\u884c\u5b89\u88c5\uff0c\u4e3b\u8981\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <p><pre><code># \u589e\u52a0\u53ef\u6267\u884c\u6743\u9650\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# \u6267\u884c\u5b89\u88c5\u547d\u4ee4\uff0c\u5982\u679c\u63d0\u793a\u9700\u6307\u5b9acann\u5305\u8def\u5f84\uff0c\u5219\u589e\u52a0\u53c2\u6570\u5982:--cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nsource mxVision/set_env.sh\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c \u5b89\u88c5\u5b8c\u6bd5\u4e4b\u540e\u6d4b\u8bd5\u4e00\u4e0bmindx\u662f\u5426\u53ef\u4ee5\u6b63\u5e38\u5bfc\u5165\uff1a<code>python -c \"import mindx\"</code></p> <p>\u5982\u679c\u63d0\u793a\u627e\u4e0d\u5230mindx\uff0c\u5219\u8f6c\u5230mxVision/python\u76ee\u5f55\u4e0b\uff0c\u5b89\u88c5\u5bf9\u5e94\u7684whl\u5305\uff1a</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> \u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\u5219\u65e0\u9700\u6267\u884c\u4e0a\u8ff0\u6b65\u9aa4\u3002</p>"},{"location":"cn/inference/environment_cn/#2-mindspore-lite","title":"2. MindSpore Lite\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684MindSpore Lite\u63a8\u7406\uff0c\u9700\u8981\u5b89\u88c52.0.0-rc1\u6216\u4ee5\u4e0a\u7248\u672c\u7684MindSpore Lite\u7684**\u4e91\u4fa7**\u63a8\u7406\u5de5\u5177\u5305\u3002</p> <p>\u5148\u4e0b\u8f7dAscend\u7248\u7684\u4e91\u4fa7\u7248\u672c\u7684\u63a8\u7406\u5de5\u5177\u5305tar.gz\u6587\u4ef6\uff0c\u4ee5\u53caPython\u63a5\u53e3Wheel\u5305\u3002</p> <p>\u4e0b\u8f7d\u5730\u5740\u4e2d\u63d0\u4f9b\u4e863.7\u7248\u672c\u7684Python\u5305\uff0c\u5982\u9700\u5176\u5b83\u7248\u672c\u53ef\u53c2\u8003\u7f16\u8bd1\u6559\u7a0b\u3002</p> <p>\u63a8\u7406\u5de5\u5177\u5305\u5b89\u88c5\u65f6\u76f4\u63a5\u89e3\u538b\u5373\u53ef\uff0c\u5e76\u6ce8\u610f\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff1a</p> <p><pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c\u4f7f\u7528pip\u5b89\u88c5\u6240\u9700\u7684whl\u5305 <pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre></p> <p>\u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\uff0c\u5219\u65e0\u9700\u5b89\u88c5\u3002</p>"},{"location":"cn/inference/inference_tutorial_cn/","title":"Inference tutorial cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/inference_tutorial_cn/#-","title":"\u63a8\u7406 - \u4f7f\u7528\u6559\u7a0b","text":""},{"location":"cn/inference/inference_tutorial_cn/#1","title":"1. \u7b80\u4ecb","text":"<p>MindOCR\u7684\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u91c7\u7528MindSpore Lite\u548cACL\u4e24\u79cd\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u89d2\u5ea6\u5206\u7c7b\u548c\u6587\u5b57\u8bc6\u522b\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684OCR\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u5e76\u884c\u5316\u65b9\u5f0f\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002</p>"},{"location":"cn/inference/inference_tutorial_cn/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u8fd0\u884c\u73af\u5883\u51c6\u5907\uff0c\u914d\u7f6eMindOCR\u7684\u63a8\u7406\u8fd0\u884c\u73af\u5883\uff0c\u6ce8\u610f\u7ed3\u5408\u6a21\u578b\u7684\u652f\u6301\u60c5\u51b5\u6765\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/inference/inference_tutorial_cn/#3","title":"3. \u6a21\u578b\u8f6c\u6362","text":"<p>MindOCR\u9664\u4e86\u652f\u6301\u81ea\u8eab\u8bad\u7ec3\u7aef\u5bfc\u51fa\u6a21\u578b\u7684\u63a8\u7406\u5916\uff0c\u8fd8\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5217\u8868\u89c1MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u3002</p> <p>\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u5c06\u5176\u8f6c\u6362\u4e3aMindOCR\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u683c\u5f0f\u3002</p>"},{"location":"cn/inference/inference_tutorial_cn/#4-python","title":"4. \u63a8\u7406 (Python)","text":"<p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u4fa7\u76ee\u5f55\u4e0b\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"cn/inference/inference_tutorial_cn/#41","title":"4.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--res_save_dir=det\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>\u8bc6\u522b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u5b57\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728rec/rec_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"cn/inference/inference_tutorial_cn/#42","title":"4.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str lite \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 precision_mode str \u65e0 \u63a8\u7406\u7684\u7cbe\u5ea6\u6a21\u5f0f\uff0c\u6682\u53ea\u652f\u6301\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\uff0c\u6b64\u5904\u4e0d\u751f\u6548 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 vis_det_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_pipeline_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u548c\u6587\u672c\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_font_path str \u65e0 \u7ed8\u5236\u6587\u5b57\u65f6\u7684\u5b57\u4f53\u8def\u5f84 crop_save_dir str \u65e0 \u6587\u672c\u68c0\u6d4b\u540e\u88c1\u526a\u56fe\u7247\u7684\u4fdd\u5b58\u8def\u5f84 show_log bool False \u662f\u5426\u6253\u5370\u65e5\u5fd7 save_log_dir str \u65e0 \u65e5\u5fd7\u4fdd\u5b58\u6587\u4ef6\u5939 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 det_model_name str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0 det_config_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 cls_model_name str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u540d\u79f0 cls_config_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 rec_model_name str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0 rec_config_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199 <p>\u8bf4\u660e\uff1a</p> <ol> <li> <p>\u5bf9\u4e8e\u5df2\u9002\u914d\u7684\u6a21\u578b\uff0c<code>*_model_path</code>\u3001<code>*_model_name</code>\u548c<code>*_config_path</code>\u662f\u5bf9\u5e94\u7ed1\u5b9a\u8d77\u6765\u7684\uff0c\u53ef\u53c2\u8003MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\uff0c    \u5176\u4e2d<code>*_model_name</code>\u548c<code>*_config_path</code>\u90fd\u662f\u7528\u6765\u786e\u5b9a\u9884/\u540e\u5904\u7406\u53c2\u6570\u7684\uff0c\u5728\u4f7f\u7528\u65f6\u9009\u62e9\u5176\u4e00\u5373\u53ef\uff1b</p> </li> <li> <p>\u5982\u679c\u9700\u8981\u9002\u914d\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u5219*_config_path\u4f20\u5165\u81ea\u5b9a\u4e49\u7684yaml\u6587\u4ef6\u5373\u53ef\uff0c\u683c\u5f0f\u8bf7\u53c2\u8003MindOCR\u6a21\u578b\u7684configs\u6216\u7b2c\u4e09\u65b9\u6a21\u578b\u7684configs\u3002</p> </li> </ol>"},{"location":"cn/inference/inference_tutorial_cn/#5-c","title":"5. \u63a8\u7406 (C++)","text":"<p>\u76ee\u524d\u6682\u65f6\u53ea\u652f\u6301pp-ocr\u7cfb\u5217\u7684\u4e2d\u6587DBNET\u3001CRNN\u3001SVTR\u6a21\u578b\u3002</p> <p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u6d4b\u76ee\u5f55\u4e0b <code>cd deploy/cpp_infer</code>,\u6267\u884c\u7f16\u8bd1\u811a\u672c <code>bash build.sh</code>, \u6784\u5efa\u5b8c\u6210\u4e4b\u540e\u5728\u5f53\u524d\u8def\u5f84dist\u76ee\u5f55\u4e0b\u751f\u6210\u53ef\u6267\u884c\u6587\u4ef6infer\u3002</p>"},{"location":"cn/inference/inference_tutorial_cn/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"cn/inference/inference_tutorial_cn/#52","title":"5.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str acl \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199"},{"location":"cn/inference/model_evaluation_cn/","title":"Model evaluation cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/model_evaluation_cn/#_1","title":"\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u8bc4\u4f30","text":""},{"location":"cn/inference/model_evaluation_cn/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u68c0\u6d4b\u7ed3\u679c\uff1a <pre><code>python deploy/eval_utils/eval_det.py \\\n--gt_path=/path/to/det_gt.txt \\\n--pred_path=/path/to/prediction/det_results.txt\n</code></pre></p>"},{"location":"cn/inference/model_evaluation_cn/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u8bc6\u522b\u7ed3\u679c\uff1a</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n--gt_path=/path/to/rec_gt.txt \\\n--pred_path=/path/to/prediction/rec_results.txt \\\n--character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0ccharacter_dict_path\u662f\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u5b57\u5178\u4ec5\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199\u3002</p> <p>\u5728\u8bc4\u4f30PaddleOCR\u6216MMOCR\u7cfb\u5217\u6a21\u578b\u65f6\uff0c\u8bf7\u53c2\u7167\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4f7f\u7528\u5bf9\u5e94\u5b57\u5178\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty_cn/","title":"Model perf thirdparty cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/model_perf_thirdparty_cn/#_1","title":"\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6027\u80fd\u5217\u8868","text":"<p>\u672c\u6587\u6863\u5c06\u7ed9\u51fa\u7b2c\u4e09\u65b9\u63a8\u7406\u6a21\u578b\u5728\u8fdb\u884c\u6a21\u578b\u8f6c\u6362\u540e\uff0c\u4f7f\u7528MindIR\u683c\u5f0f\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty_cn/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6d4b\u8bd5\u6570\u636e recall precision f-score \u6765\u6e90 ch_pp_server_det_v2.0 DB ResNet18_vd MLT17 0.3637 0.6340 0.4622 PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 MLT17 0.2557 0.5021 0.3389 PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 MLT17 0.3258 0.6318 0.4299 PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 MLT17 0.2346 0.4868 0.3166 PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 MLT17 0.2403 0.4597 0.3156 PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 IC15 0.3866 0.4630 0.4214 PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 MLT17 0.5992 0.7348 0.6601 PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 0.7463 0.9043 0.8177 PaddleOCR en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 0.8387 0.7900 0.8136 MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 0.8681 0.8074 0.8367 MMOCR"},{"location":"cn/inference/model_perf_thirdparty_cn/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6d4b\u8bd5\u6570\u636e accuracy norm edit distance \u6765\u6e90 ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (only Chinese) 0.4991 0.7411 PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only Chinese) 0.4991 0.7535 PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (only Chinese) 0.4459 0.7036 PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (only Chinese) 0.2459 0.4878 PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only English) 0.7964 0.8854 PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 MLT17 (only English) 0.0164 0.0657 PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 MLT17 (only English) 0.4304 0.5944 PaddleOCR <p>\u8bf7\u6ce8\u610f\uff0c\u4e0a\u8ff0\u6a21\u578b\u91c7\u7528\u4e86shape\u5206\u6863\uff0c\u56e0\u6b64\u8be5\u6027\u80fd\u4ec5\u8868\u793a\u5728\u67d0\u4e9bshape\u4e0b\u7684\u6027\u80fd\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty_cn/#3","title":"3. \u8bc4\u4f30\u65b9\u6cd5","text":"<p>\u8bf7\u53c2\u8003\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u8bc4\u4f30\u6587\u6863\u3002</p>"},{"location":"cn/inference/models_list_cn/","title":"Models list cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/models_list_cn/#-mindocr","title":"\u63a8\u7406 - MindOCR\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u63a8\u7406\u652f\u6301\u8bad\u7ec3\u7aefckpt\u5bfc\u51fa\u7684\u6a21\u578b\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u8bf7\u81ea\u884c\u5bfc\u51fa\u6216\u4e0b\u8f7d\u5df2\u9884\u5148\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\uff0c\u5e76\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u518d\u8fdb\u884c\u63a8\u7406\u3002</p>"},{"location":"cn/inference/models_list_cn/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"cn/inference/models_list_cn/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 CRNN VGG7 \u9ed8\u8ba4 en crnn_vgg7.yaml ResNet34_vd \u9ed8\u8ba4 en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"cn/inference/models_list_thirdparty_cn/","title":"Models list thirdparty cn.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/inference/models_list_thirdparty_cn/#-","title":"\u63a8\u7406 - \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u53ef\u4ee5\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u5728\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\u540e\uff0c\u9700\u8981\u628a\u5b83\u8f6c\u6362\u4e3aACL/MindSpore Lite\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u6587\u4ef6\uff08MindIR\u6216OM\uff09\uff0c\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\u3002</p> <p>\u5176\u4e2d\uff0c\u6d89\u53ca\u7684\u539f\u59cb\u6a21\u578b\u6587\u4ef6\u5982\u4e0b\u8868\uff1a</p> \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u683c\u5f0f \u7b80\u4ecb pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle\u63a8\u7406\u6a21\u578b\uff0c\u53ef\u7531\u5176\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa\u5f97\u5230\uff0c\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u53c2\u6570 pth .pth Pytorch\u6a21\u578b\u6587\u4ef6\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u7ed3\u6784\u3001\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f"},{"location":"cn/inference/models_list_thirdparty_cn/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR"},{"location":"cn/inference/models_list_thirdparty_cn/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR"},{"location":"cn/inference/models_list_thirdparty_cn/#3","title":"3. \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"\u540d\u79f0 \u6a21\u578b \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"cn/inference/models_list_thirdparty_cn/#4","title":"4. \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6027\u80fd","text":"<p>\u8bf7\u53c2\u8003\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6d4b\u8bd5\u6027\u80fd\u8868\u683c\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#-","title":"\u63a8\u7406 - \u6a21\u578b\u8f6c\u6362\u6559\u7a0b","text":""},{"location":"cn/mkdocs/convert_tutorial/#1-mindocr","title":"1. MindOCR\u6a21\u578b","text":"<p>MindOCR\u6a21\u578b\u7684\u63a8\u7406\u4f7f\u7528MindSpore Lite\u540e\u7aef\u3002</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(\u79bb\u7ebf\u8f6c\u6362)\"| o[MindIR];\n</code></pre>"},{"location":"cn/mkdocs/convert_tutorial/#11","title":"1.1 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u5728\u63a8\u7406\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u8bad\u7ec3\u7aef\u7684ckpt\u6587\u4ef6\u5bfc\u51fa\u4e3aMindIR\u6587\u4ef6\uff0c\u5b83\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u6743\u91cd\u53c2\u6570\u3002</p> <p>\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u4e86MIndIR\u5bfc\u51fa\u6587\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5\uff0c\u89c1\u6a21\u578b\u5217\u8868\uff0c\u53ef\u8df3\u8f6c\u5230\u5bf9\u5e94\u6a21\u578b\u7684\u4ecb\u7ecd\u9875\u9762\u8fdb\u884c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#12","title":"1.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u9700\u8981\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\uff0c\u5c06\u4e0a\u8ff0\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\u8fdb\u884c\u79bb\u7ebf\u8f6c\u6362\uff0c\u4ece\u800c\u7528\u4e8eMindSpore Lite\u7684\u63a8\u7406\u3002</p> <p><code>converter_lite</code>\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1\u63a8\u7406\u6a21\u578b\u79bb\u7ebf\u8f6c\u6362\u3002</p> <p>\u5047\u8bbe\u8f93\u5165\u6a21\u578b\u4e3ainput.mindir\uff0c\u7ecf\u8fc7<code>converter_lite</code>\u5de5\u5177\u8f6c\u6362\u540e\u7684\u8f93\u51fa\u6a21\u578b\u4e3aoutput.mindir\uff0c\u5219\u6a21\u578b\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>\u5176\u4e2d\uff0c<code>config.txt</code>\u53ef\u4ee5\u8bbe\u7f6e\u8f6c\u6362\u6a21\u578b\u7684Shape\u548c\u63a8\u7406\u7cbe\u5ea6\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#121-shape","title":"1.2.1 \u6a21\u578bShape\u914d\u7f6e","text":"<ul> <li>\u9759\u6001Shape</li> </ul> <p>\u5982\u679c\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u540d\u4e3a<code>x</code>\uff0c\u8f93\u5165Shape\u4e3a<code>(1,3,736,1280)</code>\uff0c\u5219config.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>\u8f6c\u6362\u751f\u6210\u7684output.mindir\u4e3a\u9759\u6001shape\u7248\uff0c\u63a8\u7406\u65f6\u7684\u8f93\u5165\u56fe\u50cf\u9700\u8981Resize\u5230\u8be5input_shape\u4ee5\u6ee1\u8db3\u8f93\u5165\u8981\u6c42\u3002</p> <p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff0c\u5982\u68c0\u6d4b\u51fa\u76ee\u6807\u540e\u518d\u6267\u884c\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\uff0c\u7531\u4e8e\u76ee\u6807\u4e2a\u6570\u548c\u5927\u5c0f\u4e0d\u56fa\u5b9a\uff0c\u5982\u679c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u5047\u8bbe\u5bfc\u51fa\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3, -1, -1)\uff0cNHW\u8fd93\u4e2a\u8f74\u662f\u52a8\u6001\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\u3002</p> <p><code>converter_lite</code>\u901a\u8fc7<code>configFile</code>\u914d\u7f6e<code>[ascend_context]</code>\u4e2d<code>dynamic_dims</code>\u53c2\u6570\u6765\u5b9e\u73b0\uff0c\u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003\u52a8\u6001shape\u914d\u7f6e\uff0c\u4e0b\u6587\u7b80\u79f0\u201d\u5206\u6863\u201c\u3002</p> <p>\u6240\u4ee5\uff0c\u8f6c\u6362\u65f6\u67092\u79cd\u9009\u62e9\uff0c\u901a\u8fc7\u8bbe\u7f6e\u4e0d\u540c\u7684config.txt\u5b9e\u73b0\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u9009\u62e9\u52a8\u6001\u503c\u548c\u6a21\u578b\u8f6c\u6362\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#122","title":"1.2.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>converter_lite</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003Ascend\u8f6c\u6362\u5de5\u5177\u529f\u80fd\u8bf4\u660e\uff0c\u5728\u914d\u7f6e\u6587\u4ef6\u7684\u8868\u683c\u4e2d\u63cf\u8ff0\u4e86<code>precision_mode</code>\u53c2\u6570\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u53ef\u9009\u62e9<code>enforce_fp16</code>\u3001<code>enforce_fp32</code>\u3001<code>preferred_fp32</code>\u548c<code>enforce_origin</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>config.txt</code>\u7684<code>[ascend_context]</code>\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>enforce_fp16</code>\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#2-paddleocr","title":"2. PaddleOCR\u6a21\u578b","text":"<p>PaddleOCR\u6a21\u578b\u7684\u63a8\u7406\u53ef\u4ee5\u4f7f\u7528ACL\u548cMindSpore Lite\u4e24\u79cd\u540e\u7aef\uff0c\u5206\u522b\u5bf9\u5e94OM\u6a21\u578b\u548cMindIR\u6a21\u578b\u3002</p> <pre><code>graph LR;\n    \u8bad\u7ec3\u6a21\u578b -- export --&gt; \u63a8\u7406\u6a21\u578b -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"cn/mkdocs/convert_tutorial/#21-","title":"2.1 \u8bad\u7ec3\u6a21\u578b -&gt; \u63a8\u7406\u6a21\u578b","text":"<p>\u5728PaddleOCR\u6a21\u578b\u7684\u4e0b\u8f7d\u94fe\u63a5\u4e2d\uff0c\u6709\u8bad\u7ec3\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u4e24\u79cd\u683c\u5f0f\uff0c\u5982\u679c\u63d0\u4f9b\u7684\u662f\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u683c\u5f0f\u3002</p> <p>\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u539fPaddleOCR\u4ecb\u7ecd\u9875\u9762\uff0c\u4e00\u822c\u4f1a\u6709\u8f6c\u6362\u811a\u672c\u6837\u4f8b\uff0c\u53ea\u9700\u8981\u4f20\u5165\u8bad\u7ec3\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u3001\u6a21\u578b\u6587\u4ef6\u548c\u4fdd\u5b58\u8def\u5f84\u5373\u53ef\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"cn/mkdocs/convert_tutorial/#22-onnx","title":"2.2 \u63a8\u7406\u6a21\u578b -&gt; ONNX","text":"<p>\u5b89\u88c5\u6a21\u578b\u8f6c\u6362\u5de5\u5177paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>\u8be6\u7ec6\u4f7f\u7528\u6559\u7a0b\u8bf7\u53c2\u8003Paddle2ONNX\u6a21\u578b\u8f6c\u5316\u4e0e\u9884\u6d4b\u3002</p> <p>\u6267\u884c\u8f6c\u6362\u547d\u4ee4\uff0c\u751f\u6210onnx\u6a21\u578b\uff1a</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>\u53c2\u6570\u4e2dinput_shape_dict\u7684\u503c\uff0c\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7Netron\u5de5\u5177\u6253\u5f00\u63a8\u7406\u6a21\u578b\u67e5\u770b\uff0c\u6216\u8005\u5728\u4e0a\u8ff0tools/export_model.py\u7684\u4ee3\u7801\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>\u4f7f\u7528ATC\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aOM\u6a21\u578b\u3002</p> <p>\u6607\u817e\u5f20\u91cf\u7f16\u8bd1\u5668\uff08Ascend Tensor Compiler\uff0c\u7b80\u79f0ATC\uff09\u662f\u5f02\u6784\u8ba1\u7b97\u67b6\u6784CANN\u4f53\u7cfb\u4e0b\u7684\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u5b83\u53ef\u4ee5\u5c06\u5f00\u6e90\u6846\u67b6\u7684\u7f51\u7edc\u6a21\u578b\u8f6c\u6362\u4e3a\u6607\u817eAI\u5904\u7406\u5668\u652f\u6301\u7684.om\u683c\u5f0f\u79bb\u7ebf\u6a21\u578b\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1ATC\u6a21\u578b\u8f6c\u6362\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#231-shape","title":"2.3.1 \u6a21\u578bShape\u914d\u7f6e","text":"<p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3,-1,-1)\u3002</p> <ul> <li>\u9759\u6001Shape</li> </ul> <p>\u53ef\u4ee5\u8f6c\u6362\u4e3a\u9759\u6001Shape\u7248\u7684\u6a21\u578b\uff0cNHW\u90fd\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>ATC\u5de5\u5177\u901a\u8fc7\u8bbe\u7f6e\u53c2\u6570 dynamic_dims\u6765\u652f\u6301Shape\u7684**\u5206\u6863**\uff0c\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u5982\u4e0b\u4e24\u79cd\u9009\u62e9\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4e00\u952e\u5f0f\u5b8c\u6210\u52a8\u6001\u503c\u9009\u62e9\u548c\u6a21\u578b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#232","title":"2.3.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>ATC</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003\u53c2\u6570precision_mode\u7684\u8bf4\u660e\uff0c\u53ef\u9009\u62e9<code>force_fp16</code>\u3001<code>force_fp32</code>\u3001<code>allow_fp32_to_fp16</code>\u3001<code>must_keep_origin_dtype</code>\u548c<code>allow_mix_precision</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>atc</code>\u547d\u4ee4\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>force_fp16</code>\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>\u4f7f\u7528converter_lite\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aMindIR\u6a21\u578b\u3002\u5de5\u5177\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1MindSpore Lite\u4e91\u4fa7\u63a8\u7406\u79bb\u7ebf\u6a21\u578b\u8f6c\u6362\u3002</p> <p>\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>\u8f6c\u6362\u6d41\u7a0b\u548cMindOCR\u6a21\u578b\u5b8c\u5168\u76f8\u540c\uff0c\u4ec5\u6709\u533a\u522b\u662f<code>--fmk</code>\u9700\u6307\u5b9a\u8f93\u5165\u662fONNX\u6a21\u578b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#3-mmocr","title":"3. MMOCR\u6a21\u578b","text":"<p>MMOCR\u4f7f\u7528Pytorch\uff0c\u5176\u6a21\u578b\u6587\u4ef6\u4e00\u822c\u662fpth\u683c\u5f0f\u3002</p> <p>\u9700\u8981\u5148\u628a\u5b83\u5bfc\u51fa\u4e3aONNX\u683c\u5f0f\uff0c\u518d\u8f6c\u6362\u4e3aACL/MindSpore Lite\u652f\u6301\u7684OM/MindIR\u683c\u5f0f\u3002</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);\n</code></pre>"},{"location":"cn/mkdocs/convert_tutorial/#31-mmocr-onnx","title":"3.1 MMOCR\u6a21\u578b -&gt; ONNX","text":"<p>MMDeploy\u63d0\u4f9b\u4e86MMOCR\u6a21\u578b\u5bfc\u51faONNX\u7684\u547d\u4ee4\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1\u5982\u4f55\u8f6c\u6362\u6a21\u578b\u3002</p> <p>\u5bf9\u4e8e\u53c2\u6570<code>deploy_cfg</code>\u9700\u9009\u62e9\u76ee\u5f55mmdeploy/configs/mmocr\u4e0b\u7684<code>*_onnxruntime_dynamic.py</code>\u6587\u4ef6\uff0c\u4ece\u800c\u5bfc\u51fa\u4e3a\u52a8\u6001Shape\u7248ONNX\u6a21\u578b\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; OM\u3002</p>"},{"location":"cn/mkdocs/convert_tutorial/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; MIndIR\u3002</p>"},{"location":"cn/mkdocs/dataset_converters/","title":"Dataset Preparation","text":"<p>English | \u4e2d\u6587</p> <p>\u672c\u6587\u6863\u5c55\u793a\u4e86\u5982\u4f55\u5c06OCR\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\uff08\u4e0d\u5305\u62ecLMDB\uff09\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u4ee5\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <p>\u60a8\u4e5f\u53ef\u4ee5\u53c2\u8003 <code>convert_datasets.sh</code>\u3002\u8fd9\u662f\u5c06\u7ed9\u5b9a\u76ee\u5f55\u4e0b\u6240\u6709\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u7684Shell \u811a\u672c\u3002</p> <p>\u8981\u4e0b\u8f7dOCR\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u683c\u5f0f\u8f6c\u6362\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 Chinese text recognition, CTW1500, ICDAR2015, MLT2017, SVT, Syntext 150k, TD500, Total Text, SynthText \u7684\u8bf4\u660e\u3002</p>"},{"location":"cn/mkdocs/dataset_converters/#_1","title":"\u6587\u672c\u68c0\u6d4b/\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b","text":"<p>\u8f6c\u6362\u540e\u7684\u6807\u6ce8\u6587\u4ef6\u683c\u5f0f\u5e94\u4e3a\uff1a <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>\u4ee5ICDAR2015\uff08ic15\uff09\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u8981\u5c06ic15\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6240\u9700\u7684\u683c\u5f0f\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/train/ch4_training_images \\\n--label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n--output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/test/ch4_test_images \\\n--label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n--output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"cn/mkdocs/dataset_converters/#_2","title":"\u6587\u672c\u8bc6\u522b","text":"<p>\u6587\u672c\u8bc6\u522b\u6570\u636e\u96c6\u7684\u6807\u6ce8\u683c\u5f0f\u5982\u4e0b\uff1a</p> <p><pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> \u8bf7\u6ce8\u610f\uff0c\u56fe\u50cf\u540d\u79f0\u548c\u6587\u672c\u6807\u7b7e\u4ee5<code>\\t</code>\u5206\u9694\u3002</p> <p>\u8981\u8f6c\u6362\u6807\u6ce8\u6587\u4ef6\uff0c\u8bf7\u8fd0\u884c\uff1a <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"cn/mkdocs/inference_models_list/#-mindocr","title":"\u63a8\u7406 - MindOCR\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u63a8\u7406\u652f\u6301\u8bad\u7ec3\u7aefckpt\u5bfc\u51fa\u7684\u6a21\u578b\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u8bf7\u81ea\u884c\u5bfc\u51fa\u6216\u4e0b\u8f7d\u5df2\u9884\u5148\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\uff0c\u5e76\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u518d\u8fdb\u884c\u63a8\u7406\u3002</p>"},{"location":"cn/mkdocs/inference_models_list/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"cn/mkdocs/inference_models_list/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 CRNN VGG7 \u9ed8\u8ba4 en crnn_vgg7.yaml ResNet34_vd \u9ed8\u8ba4 en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"cn/mkdocs/inference_models_list_thirdparty/#-","title":"\u63a8\u7406 - \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u53ef\u4ee5\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u5728\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\u540e\uff0c\u9700\u8981\u628a\u5b83\u8f6c\u6362\u4e3aACL/MindSpore Lite\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u6587\u4ef6\uff08MindIR\u6216OM\uff09\uff0c\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\u3002</p> <p>\u5176\u4e2d\uff0c\u6d89\u53ca\u7684\u539f\u59cb\u6a21\u578b\u6587\u4ef6\u5982\u4e0b\u8868\uff1a</p> \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u683c\u5f0f \u7b80\u4ecb pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle\u63a8\u7406\u6a21\u578b\uff0c\u53ef\u7531\u5176\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa\u5f97\u5230\uff0c\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u53c2\u6570 pth .pth Pytorch\u6a21\u578b\u6587\u4ef6\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u7ed3\u6784\u3001\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f"},{"location":"cn/mkdocs/inference_models_list_thirdparty/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR"},{"location":"cn/mkdocs/inference_models_list_thirdparty/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR"},{"location":"cn/mkdocs/inference_models_list_thirdparty/#3","title":"3. \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"\u540d\u79f0 \u6a21\u578b \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"cn/mkdocs/inference_models_list_thirdparty/#4","title":"4. \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6027\u80fd","text":"<p>\u8bf7\u53c2\u8003\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6d4b\u8bd5\u6027\u80fd\u8868\u683c\u3002</p>"},{"location":"cn/mkdocs/inference_tutorial/#-","title":"\u63a8\u7406 - \u4f7f\u7528\u6559\u7a0b","text":""},{"location":"cn/mkdocs/inference_tutorial/#1","title":"1. \u7b80\u4ecb","text":"<p>MindOCR\u7684\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u91c7\u7528MindSpore Lite\u548cACL\u4e24\u79cd\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u89d2\u5ea6\u5206\u7c7b\u548c\u6587\u5b57\u8bc6\u522b\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684OCR\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u5e76\u884c\u5316\u65b9\u5f0f\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002</p>"},{"location":"cn/mkdocs/inference_tutorial/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u8fd0\u884c\u73af\u5883\u51c6\u5907\uff0c\u914d\u7f6eMindOCR\u7684\u63a8\u7406\u8fd0\u884c\u73af\u5883\uff0c\u6ce8\u610f\u7ed3\u5408\u6a21\u578b\u7684\u652f\u6301\u60c5\u51b5\u6765\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/mkdocs/inference_tutorial/#3","title":"3. \u6a21\u578b\u8f6c\u6362","text":"<p>MindOCR\u9664\u4e86\u652f\u6301\u81ea\u8eab\u8bad\u7ec3\u7aef\u5bfc\u51fa\u6a21\u578b\u7684\u63a8\u7406\u5916\uff0c\u8fd8\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5217\u8868\u89c1MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u3002</p> <p>\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u5c06\u5176\u8f6c\u6362\u4e3aMindOCR\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u683c\u5f0f\u3002</p>"},{"location":"cn/mkdocs/inference_tutorial/#4-python","title":"4. \u63a8\u7406 (Python)","text":"<p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u4fa7\u76ee\u5f55\u4e0b\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"cn/mkdocs/inference_tutorial/#41","title":"4.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=det_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name=en_ms_det_dbnet_resnet50 \\\n--res_save_dir=det\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>\u8bc6\u522b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u5b57\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name=en_ms_rec_crnn_resnet34 \\\n--res_save_dir=rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728rec/rec_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"cn/mkdocs/inference_tutorial/#42","title":"4.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str lite \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 precision_mode str \u65e0 \u63a8\u7406\u7684\u7cbe\u5ea6\u6a21\u5f0f\uff0c\u6682\u53ea\u652f\u6301\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\uff0c\u6b64\u5904\u4e0d\u751f\u6548 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 vis_det_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_pipeline_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u548c\u6587\u672c\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_font_path str \u65e0 \u7ed8\u5236\u6587\u5b57\u65f6\u7684\u5b57\u4f53\u8def\u5f84 crop_save_dir str \u65e0 \u6587\u672c\u68c0\u6d4b\u540e\u88c1\u526a\u56fe\u7247\u7684\u4fdd\u5b58\u8def\u5f84 show_log bool False \u662f\u5426\u6253\u5370\u65e5\u5fd7 save_log_dir str \u65e0 \u65e5\u5fd7\u4fdd\u5b58\u6587\u4ef6\u5939 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 det_model_name str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0 det_config_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 cls_model_name str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u540d\u79f0 cls_config_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 rec_model_name str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0 rec_config_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199 <p>\u8bf4\u660e\uff1a</p> <ol> <li> <p>\u5bf9\u4e8e\u5df2\u9002\u914d\u7684\u6a21\u578b\uff0c<code>*_model_path</code>\u3001<code>*_model_name</code>\u548c<code>*_config_path</code>\u662f\u5bf9\u5e94\u7ed1\u5b9a\u8d77\u6765\u7684\uff0c\u53ef\u53c2\u8003MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\uff0c    \u5176\u4e2d<code>*_model_name</code>\u548c<code>*_config_path</code>\u90fd\u662f\u7528\u6765\u786e\u5b9a\u9884/\u540e\u5904\u7406\u53c2\u6570\u7684\uff0c\u5728\u4f7f\u7528\u65f6\u9009\u62e9\u5176\u4e00\u5373\u53ef\uff1b</p> </li> <li> <p>\u5982\u679c\u9700\u8981\u9002\u914d\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u5219*_config_path\u4f20\u5165\u81ea\u5b9a\u4e49\u7684yaml\u6587\u4ef6\u5373\u53ef\uff0c\u683c\u5f0f\u8bf7\u53c2\u8003MindOCR\u6a21\u578b\u7684configs\u6216\u7b2c\u4e09\u65b9\u6a21\u578b\u7684configs\u3002</p> </li> </ol>"},{"location":"cn/mkdocs/inference_tutorial/#5-c","title":"5. \u63a8\u7406 (C++)","text":"<p>\u76ee\u524d\u6682\u65f6\u53ea\u652f\u6301pp-ocr\u7cfb\u5217\u7684\u4e2d\u6587DBNET\u3001CRNN\u3001SVTR\u6a21\u578b\u3002</p> <p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u6d4b\u76ee\u5f55\u4e0b <code>cd deploy/cpp_infer</code>,\u6267\u884c\u7f16\u8bd1\u811a\u672c <code>bash build.sh</code>, \u6784\u5efa\u5b8c\u6210\u4e4b\u540e\u5728\u5f53\u524d\u8def\u5f84dist\u76ee\u5f55\u4e0b\u751f\u6210\u53ef\u6267\u884c\u6587\u4ef6infer\u3002</p>"},{"location":"cn/mkdocs/inference_tutorial/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <p><pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> \u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"cn/mkdocs/inference_tutorial/#52","title":"5.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str acl \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199"},{"location":"cn/mkdocs/modelzoo/","title":"\u6a21\u578b\u5217\u8868","text":"model type dataset fscore(detection)/accuracy(recognition) mindocr recipe vanilla mindspore dbnet_mobilenetv3 detection icdar2015 77.28 config dbnet_resnet18 detection icdar2015 83.71 config dbnet_resnet50 detection icdar2015 84.99 config link dbnet_resnet50 detection msra-td500 85.03 config dbnet++_resnet50 detection icdar2015 86.60 config psenet_resnet152 detection icdar2015 82.06 config link east_resnet50 detection icdar2015 84.87 config link svtr_tiny recognition IC03,13,15,IIIT,etc 89.02 config crnn_vgg7 recognition IC03,13,15,IIIT,etc 82.03 config link crnn_resnet34_vd recognition IC03,13,15,IIIT,etc 84.45 config rare_resnet34_vd recognition IC03,13,15,IIIT,etc 85.19 config"},{"location":"cn/tutorials/distribute_train_CN/","title":"\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u7684\u6559\u7a0b\uff0c\u5728Ascend\u5904\u7406\u5668\u4e0a\u6709\u4e24\u79cd\u65b9\u5f0f\u53ef\u4ee5\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\uff0c\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u6216\u901a\u8fc7\u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002\u5728GPU\u5904\u7406\u5668\u4e0a\u53ef\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002</p>"},{"location":"cn/tutorials/distribute_train_CN/#openmpi","title":"\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u5f53\u524dMindSpore\u5728Ascend\u4e0a\u5df2\u7ecf\u652f\u6301\u4e86\u901a\u8fc7OpenMPI\u7684mpirun\u547d\u4ee4\u8fd0\u884c\u811a\u672c\uff0c\u7528\u6237\u53ef\u53c2\u8003dbnet readme\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4e0b\u4e3a\u547d\u4ee4\u7528\u4f8b\u3002 \u8fd0\u884c\u547d\u4ee4\u524d\u8bf7\u786e\u4fddyaml\u6587\u4ef6\u4e2d\u7684<code>distribute</code>\u53c2\u6570\u4e3aTrue\u3002</p> <pre><code># n is the number of GPUs/NPUs\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre>"},{"location":"cn/tutorials/distribute_train_CN/#rank_table_file","title":"\u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u6b64\u79cd\u65b9\u6cd5\u5728\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u524d\u9700\u8981\u521b\u5efajson\u683c\u5f0f\u7684HCCL\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u751f\u6210RANK_TABLE_FILE\u6587\u4ef6\uff0c\u4ee5\u4e0b\u4e3a\u751f\u62108\u5361\u76f8\u5e94\u914d\u7f6e\u6587\u4ef6\u547d\u4ee4\uff0c\u66f4\u5177\u4f53\u4fe1\u606f\u53ca\u76f8\u5e94\u811a\u672c\u53c2\u89c1hccl_tools\u4e2d\u7684\u8bf4\u660e\uff0c <pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> \u8f93\u51fa\u4e3a\uff1a <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre> \u5176\u4e2d<code>hccl_8p_10234567_127.0.0.1.json</code>\u4e2d\u5185\u5bb9\u793a\u4f8b\u4e3a\uff1a <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"0\",\n                    \"device_ip\": \"192.168.100.101\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"1\",\n                    \"device_ip\": \"192.168.101.101\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"2\",\n                    \"device_ip\": \"192.168.102.101\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"3\",\n                    \"device_ip\": \"192.168.103.101\",\n                    \"rank_id\": \"3\"\n                },\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"4\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"5\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"6\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"7\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre></p> <p>\u4e4b\u540e\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\uff0c\u8fd0\u884c\u547d\u4ee4\u524d\u8bf7\u786e\u4fddyaml\u6587\u4ef6\u4e2d\u7684<code>distribute</code>\u53c2\u6570\u4e3aTrue\u3002 <pre><code>bash ascend8p.sh\n</code></pre></p> <p>\u4ee5CRNN\u8bad\u7ec3\u4e3a\u4f8b\uff0c\u5176<code>ascend8p.sh</code>\u811a\u672c\u4e3a\uff1a <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${DEVICE_NUM}; i++)); do\nexport DEVICE_ID=$i\nexport RANK_ID=$i\necho \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\nif [ $i -eq 0 ]; then\necho 'i am 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\nelse\necho 'not 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\nfi\ndone\n</code></pre></p> <p>\u5f53\u9700\u8981\u8bad\u7ec3\u5176\u4ed6\u6a21\u578b\u65f6\uff0c\u53ea\u8981\u5c06\u811a\u672c\u4e2d\u7684yaml config\u6587\u4ef6\u8def\u5f84\u66ff\u6362\u5373\u53ef\uff0c\u5373<code>python -u tools/train.py --config path/to/model_config.yaml</code></p> <p>\u6b64\u65f6\u8bad\u7ec3\u5df2\u7ecf\u5f00\u59cb\uff0c\u53ef\u5728<code>train.log</code>\u4e2d\u67e5\u770b\u8bad\u7ec3\u65e5\u5fd7\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/","title":"training recognition custom dataset CN.md","text":"<p>English | \u4e2d\u6587</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_1","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u8bc6\u522b\u7f51\u7edc","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8fdb\u884c\u8bc6\u522b\u7f51\u7edc\u8bad\u7ec3\u7684\u6559\u5b66\uff0c\u5305\u62ec\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_2","title":"\u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u76ee\u524dMindOCR\u8bc6\u522b\u7f51\u7edc\u652f\u6301\u4e24\u79cd\u8f93\u5165\u5f62\u5f0f\uff0c\u5206\u522b\u4e3a - <code>\u901a\u7528\u6570\u636e</code>\uff1a\u4f7f\u7528\u56fe\u50cf\u548c\u6587\u672c\u6587\u4ef6\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5RecDataset\u7c7b\u578b\u8bfb\u53d6\u3002 - <code>LMDB\u6570\u636e</code>: \u4f7f\u7528LMDB\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5LMDBDataset\u7c7b\u578b\u8bfb\u53d6\u3002</p> <p>\u4ee5\u4e0b\u6559\u5b66\u4ee5\u4f7f\u7528<code>\u901a\u7528\u6570\u636e</code>\u6587\u4ef6\u683c\u5f0f\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_3","title":"\u8bad\u7ec3\u96c6\u51c6\u5907","text":"<p>\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002txt\u6587\u4ef6\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># \u6587\u4ef6\u540d   # \u5bf9\u5e94\u6807\u7b7e\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> \u6ce8\u610f\uff1a\u8bf7\u5c06\u56fe\u7247\u540d\u548c\u6807\u7b7e\u4ee5 \\tab \u4f5c\u4e3a\u5206\u9694\uff0c\u907f\u514d\u4f7f\u7528\u7a7a\u683c\u6216\u5176\u4ed6\u5206\u9694\u7b26\u3002</p> <p>\u6700\u7ec8\u8bad\u7ec3\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_4","title":"\u9a8c\u8bc1\u96c6\u51c6\u5907","text":"<p>\u540c\u6837\uff0c\u8bf7\u5c06\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002\u6700\u7ec8\u9a8c\u8bc1\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_5","title":"\u5b57\u5178\u51c6\u5907","text":"<p>\u4e3a\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\uff0c\u7528\u6237\u9700\u914d\u7f6e\u5bf9\u5e94\u7684\u5b57\u5178\u3002\u53ea\u6709\u5b58\u5728\u4e8e\u5b57\u5178\u4e2d\u7684\u5b57\u7b26\u4f1a\u88ab\u6a21\u578b\u6b63\u786e\u9884\u6d4b\u3002MindOCR\u73b0\u63d0\u4f9b\u9ed8\u8ba4\u3001\u4e2d\u548c\u82f1\u4e09\u79cd\u5b57\u5178\uff0c\u5176\u4e2d - <code>\u9ed8\u8ba4\u5b57\u5178</code>: \u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\u3002\u5982\u7528\u6237\u4e0d\u914d\u7f6e\u5b57\u5178\uff0c\u8be5\u5b57\u5178\u4f1a\u88ab\u9ed8\u8ba4\u4f7f\u7528\u3002 - <code>\u82f1\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/en_dict.txt</code>\u3002 - <code>\u4e2d\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5e38\u7528\u4e2d\u6587\u5b57\u7b26\u3001\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/ch_dict.txt</code>\u3002</p> <p>\u76ee\u524dMindOCR\u6682\u672a\u63d0\u4f9b\u5176\u4ed6\u8bed\u79cd\u7684\u5b57\u5178\u914d\u7f6e\u3002\u8be5\u529f\u80fd\u5c06\u5728\u65b0\u7248\u672c\u4e2d\u63a8\u51fa\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_6","title":"\u914d\u7f6e\u6587\u4ef6\u51c6\u5907","text":"<p>\u9488\u5bf9\u4e0d\u540c\u7f51\u7edc\u7ed3\u6784\uff0c\u7528\u6237\u9700\u914d\u7f6e\u76f8\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u73b0\u5df2CRNN\uff08\u4ee5Resnet34\u4e3a\u9aa8\u5e72\u6a21\u578b\uff09\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_7","title":"\u914d\u7f6e\u82f1\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u82f1\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>\u7531\u4e8e\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\u7684\u5b57\u5178\u9ed8\u8ba4\u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\uff0c\u4e3a\u4f7f\u7528\u5b8c\u6574\u82f1\u6587\u5b57\u5178\uff0c\u7528\u6237\u9700\u8981\u4fee\u6539\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u7684<code>common: num_classes</code>\u5c5e\u6027\uff1a</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 95                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 96                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\nuse_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_8","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u82f1\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\uff08\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_9","title":"\u914d\u7f6e\u4e2d\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34_ch.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u540c\u6837\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u8bad\u7ec3\u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u9a8c\u8bc1\u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u4e2d\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 6625                                      # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\nuse_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_10","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u4e2d\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\uff08\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset_CN/#_11","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b","text":"<p>\u5f53\u6240\u6709\u6570\u636e\u96c6\u548c\u914d\u7f6e\u6587\u4ef6\u51c6\u5907\u5b8c\u6210\uff0c\u7528\u6237\u53ef\u5f00\u59cb\u8bad\u7ec3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u7684\u6a21\u578b\u3002\u7531\u4e8e\u5404\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u4e0d\u540c\uff0c\u7528\u6237\u53ef\u53c2\u8003\u5bf9\u5e94\u6a21\u578b\u4ecb\u7ecd\u6587\u6863\u4e2d\u7684**\u6a21\u578b\u8bad\u7ec3**\u548c**\u6a21\u578b\u8bc4\u4f30**\u7ae0\u8282\u3002</p>"}]}