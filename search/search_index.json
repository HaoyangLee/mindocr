{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindocr","title":"MindOCR","text":""},{"location":"#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore, which integrates series of mainstream text detection and recognition algorihtms and models and provides easy-to-use training and inference tools. It can accelerate the process of developing and deploying SoTA text detection and recognition models in real-world applications, such as DBNet/DBNet++ and CRNN/SVTR, and help fulfill the need of image-text understanding .</p>  Major Features  <ul> <li>Modular design: We decoupled the OCR task into several configurable modules. Users can setup the training and evaluation pipelines, customize the data processing pipeline and model architectures easily by modifying just few lines of code.</li> <li>High-performance: MindOCR provides a series of pretrained weights trained with optimized configurations that reach competitive performance on OCR tasks.</li> <li>Low-cost-to-apply: Easy-to-use inference tools are provided in MindOCR to perform text detection and recognition tasks.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>MindOCR is built on MindSpore AI framework, which supports CPU/GPU/NPU devices. MindOCR is compatible with the following framework versions. For details and installation guideline, please refer to the installation links shown below.</p> <ul> <li>mindspore &gt;= 1.9  [install]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [install]</li> <li>mindspore lite (for inference)  [install]</li> </ul>"},{"location":"#dependency","title":"Dependency","text":"<p><pre><code>pip install -r requirements.txt\n</code></pre> Tips:</p> <ul> <li> <p>If scikit_image cannot be imported, please set environment variable <code>$LD_PRELOAD</code> as follows, (related opencv issue)</p> <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre> </li> </ul>"},{"location":"#install-from-source-recommend","title":"Install from Source (recommend)","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>Using <code>-e</code> for \"editable\" mode can help resolve potential module import issues.</p>"},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install mindocr\n</code></pre> <p>As this project is under active development, the version installed from PyPI is out-of-date currently. (will update soon).</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#text-detection-and-recognition-demo","title":"Text Detection and Recognition Demo","text":"<p>After installing MindOCR, we can run text detection and recognition on an arbitrary image easily as follows.</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>After running, the results will be saved in <code>./inference_results</code> by default. Here is an example result.</p> <p> </p> <p>  Visualization of text detection and recognition result  </p> <p>We can see that all texts on the image are detected and recognized accurately. For more usage, please refer to the inference section in tutorials.</p>"},{"location":"#model-training-and-evaluation-quick-guideline","title":"Model Training and Evaluation - Quick Guideline","text":"<p>It is easy to train your OCR model with the <code>tools/train.py</code> script, which supports both text detection and recognition model training.</p> <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <p>The <code>--config</code> arg specifies the path to a yaml file that defines the model to be trained and the training strategy including data process pipeline, optimizer, lr scheduler, etc.</p> <p>MindOCR provides SoTA OCR models with their training strategies in <code>configs</code> folder. You may adapt it to your task/dataset, for example, by running</p> <p><pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/db++_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre></p> <p>Similarly, it is simple to evaluate the trained model with the <code>tools/eval.py</code> script. <pre><code>python tools/eval.py \\\n--config {path/to/model_config.yaml} \\\n--opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre></p> <p>For more illustration and usage, please refer to the model training section in Tutorials.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Datasets<ul> <li>Dataset Preparation</li> <li>Data Transformation Mechanism</li> </ul> </li> <li>Model Training<ul> <li>Yaml Configuration</li> <li>Text Detection</li> <li>Text Recognition</li> <li>Distributed Training</li> <li>Advance: Gradient Accumulation, EMA, Resume Training, etc</li> </ul> </li> <li>Inference and Deployment<ul> <li>Python/C++ Inference on Ascend 310</li> <li>Python Online Inference</li> </ul> </li> <li>Developer Guides<ul> <li>Customize Dataset</li> <li>Customize Data Transformation</li> <li>Customize a New Model</li> <li>Customize Postprocessing Method</li> </ul> </li> </ul>"},{"location":"#model-list","title":"Model List","text":"Text Detection <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021) [coming soon]</li> </ul> Text Recognition <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> ABINet (CVPR'2021) [coming soon]</li> </ul> <p>For the detailed performance of the trained models, please refer to configs.</p> <p>For detailed support for MindSpore Lite and ACL inference models, please refer to MindOCR Models Support List and Third-Party Models Support List.</p>"},{"location":"#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li> ICDAR2015 paper(datasets/icdar2015.md)]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k paper(datasets/syntext150k.md)]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"#notes","title":"Notes","text":""},{"location":"#what-is-new","title":"What is New","text":"<ul> <li> <p>2023/06/07 1. Add new trained models</p> <ul> <li>PSENet for text detection</li> <li>EAST for text detection</li> <li>SVTR for text recognition 2. Add more benchmark datasets and their results</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the <code>resume</code> parameter under the <code>model</code> field in the yaml config, e.g.,<code>resume: True</code>, load and resume training from {ckpt_save_dir}/train_resume.ckpt or <code>resume: /path/to/train_resume.ckpt</code>, load and resume training from the given path. 4. Improve postprocessing for detection: re-scale detected text polygons to original image space by default, which can be enabled by add \"shape_list\" to the <code>eval.dataset.output_columns</code> list. 5. Refactor online inference to support more models, see README.md for details.</li> </ul> </li> <li> <p>2023/05/15 1. Add new trained models</p> <ul> <li>DBNet++ for text detection</li> <li>CRNN-Seq2Seq for text recognition</li> <li>DBNet pretrained on SynthText is now available: checkpoint url 2. Add more benchmark datasets and their results</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>More benchmark results for DBNet are reported here. 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refactored. 5. Bug fix: use Meter to average loss for large datasets, disable <code>pred_cast_fp32</code> for ctcloss in AMP training, fix error when invalid polygons exist.</li> </ul> </li> <li> <p>2023/05/04 1. Support loading self-defined pretrained checkpoints via setting <code>model-pretrained</code> with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting <code>train-ema</code> (default: False) and <code>train-ema_decay</code> in the yaml config. 4. Arg parameter changed\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: change the column number corresponds to the label to the column index.</p> </li> <li> <p>2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add <code>grouping_strategy</code> argument in yaml config to select a predefined grouping strategy, or use <code>no_weight_decay_params</code> argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 2. Add gradient accumulation to support large batch size training. Usage: add <code>gradient_accumulation_steps</code> in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 3. Add gradient clip to support training stablization. Enable it by setting <code>grad_clip</code> as True in yaml config.</p> </li> <li> <p>2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set <code>type</code> of <code>loss_scale</code> as <code>dynamic</code>. A YAML example can be viewed in <code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. Arg names changed: <code>output_keys</code> -&gt; <code>output_columns</code>, <code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code> 2. Data pipeline updated</p> </li> <li> <p>2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:   <pre><code>  i)   Create a new training task on the openi cloud platform.\n  ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n  iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n  iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n  v)   Fill in other blanks and launch.\n</code></pre></p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindOCR better.</p> <p>Please refer to CONTRIBUTING.md for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"index_readme/","title":"Index readme.md","text":""},{"location":"index_readme/#mindocr","title":"MindOCR","text":"<p>English | \u4e2d\u6587</p> <p>\ud83d\udcddIntroduction | \ud83d\udd28Installation | \ud83d\ude80Quick Start | \ud83d\udcdaTutorials | \ud83c\udf81Model List | \ud83d\udcf0Dataset List | \ud83c\udf89Notes</p>"},{"location":"index_readme/#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore, which integrates series of mainstream text detection and recognition algorihtms and models and provides easy-to-use training and inference tools. It can accelerate the process of developing and deploying SoTA text detection and recognition models in real-world applications, such as DBNet/DBNet++ and CRNN/SVTR, and help fulfill the need of image-text understanding .</p>  Major Features  <ul> <li>Modular design: We decoupled the OCR task into several configurable modules. Users can setup the training and evaluation pipelines, customize the data processing pipeline and model architectures easily by modifying just few lines of code.</li> <li>High-performance: MindOCR provides a series of pretrained weights trained with optimized configurations that reach competitive performance on OCR tasks.</li> <li>Low-cost-to-apply: Easy-to-use inference tools are provided in MindOCR to perform text detection and recognition tasks.</li> </ul>"},{"location":"index_readme/#installation","title":"Installation","text":""},{"location":"index_readme/#prerequisites","title":"Prerequisites","text":"<p>MindOCR is built on MindSpore AI framework, which supports CPU/GPU/NPU devices. MindOCR is compatible with the following framework versions. For details and installation guideline, please refer to the installation links shown below.</p> <ul> <li>mindspore &gt;= 1.9  [install]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [install]</li> <li>mindspore lite (for inference)  [install]</li> </ul>"},{"location":"index_readme/#dependency","title":"Dependency","text":"<p><pre><code>pip install -r requirements.txt\n</code></pre> Tips:</p> <ul> <li> <p>If scikit_image cannot be imported, please set environment variable <code>$LD_PRELOAD</code> as follows, (related opencv issue)</p> <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre> </li> </ul>"},{"location":"index_readme/#install-from-source-recommend","title":"Install from Source (recommend)","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>Using <code>-e</code> for \"editable\" mode can help resolve potential module import issues.</p>"},{"location":"index_readme/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install mindocr\n</code></pre> <p>As this project is under active development, the version installed from PyPI is out-of-date currently. (will update soon).</p>"},{"location":"index_readme/#quick-start","title":"Quick Start","text":""},{"location":"index_readme/#text-detection-and-recognition-demo","title":"Text Detection and Recognition Demo","text":"<p>After installing MindOCR, we can run text detection and recognition on an arbitrary image easily as follows.</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>After running, the results will be saved in <code>./inference_results</code> by default. Here is an example result.</p> <p> </p> <p>  Visualization of text detection and recognition result  </p> <p>We can see that all texts on the image are detected and recognized accurately. For more usage, please refer to the inference section in tutorials.</p>"},{"location":"index_readme/#model-training-and-evaluation-quick-guideline","title":"Model Training and Evaluation - Quick Guideline","text":"<p>It is easy to train your OCR model with the <code>tools/train.py</code> script, which supports both text detection and recognition model training.</p> <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <p>The <code>--config</code> arg specifies the path to a yaml file that defines the model to be trained and the training strategy including data process pipeline, optimizer, lr scheduler, etc.</p> <p>MindOCR provides SoTA OCR models with their training strategies in <code>configs</code> folder. You may adapt it to your task/dataset, for example, by running</p> <p><pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/db++_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre></p> <p>Similarly, it is simple to evaluate the trained model with the <code>tools/eval.py</code> script. <pre><code>python tools/eval.py \\\n--config {path/to/model_config.yaml} \\\n--opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre></p> <p>For more illustration and usage, please refer to the model training section in Tutorials.</p>"},{"location":"index_readme/#tutorials","title":"Tutorials","text":"<ul> <li>Datasets<ul> <li>Dataset Preparation</li> <li>Data Transformation Mechanism</li> </ul> </li> <li>Model Training<ul> <li>Yaml Configuration</li> <li>Text Detection</li> <li>Text Recognition</li> <li>Distributed Training</li> <li>Advance: Gradient Accumulation, EMA, Resume Training, etc</li> </ul> </li> <li>Inference and Deployment<ul> <li>Python/C++ Inference on Ascend 310</li> <li>Python Online Inference</li> </ul> </li> <li>Developer Guides<ul> <li>Customize Dataset</li> <li>Customize Data Transformation</li> <li>Customize a New Model</li> <li>Customize Postprocessing Method</li> </ul> </li> </ul>"},{"location":"index_readme/#model-list","title":"Model List","text":"Text Detection <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021) [coming soon]</li> </ul> Text Recognition <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> ABINet (CVPR'2021) [coming soon]</li> </ul> <p>For the detailed performance of the trained models, please refer to configs.</p> <p>For detailed support for MindSpore Lite and ACL inference models, please refer to MindOCR Models Support List and Third-Party Models Support List.</p>"},{"location":"index_readme/#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li> ICDAR2015 paper(datasets/icdar2015.md)]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k paper(datasets/syntext150k.md)]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"index_readme/#notes","title":"Notes","text":""},{"location":"index_readme/#what-is-new","title":"What is New","text":"<ul> <li> <p>2023/06/07 1. Add new trained models</p> <ul> <li>PSENet for text detection</li> <li>EAST for text detection</li> <li>SVTR for text recognition 2. Add more benchmark datasets and their results</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the <code>resume</code> parameter under the <code>model</code> field in the yaml config, e.g.,<code>resume: True</code>, load and resume training from {ckpt_save_dir}/train_resume.ckpt or <code>resume: /path/to/train_resume.ckpt</code>, load and resume training from the given path. 4. Improve postprocessing for detection: re-scale detected text polygons to original image space by default, which can be enabled by add \"shape_list\" to the <code>eval.dataset.output_columns</code> list. 5. Refactor online inference to support more models, see README.md for details.</li> </ul> </li> <li> <p>2023/05/15 1. Add new trained models</p> <ul> <li>DBNet++ for text detection</li> <li>CRNN-Seq2Seq for text recognition</li> <li>DBNet pretrained on SynthText is now available: checkpoint url 2. Add more benchmark datasets and their results</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>More benchmark results for DBNet are reported here. 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refactored. 5. Bug fix: use Meter to average loss for large datasets, disable <code>pred_cast_fp32</code> for ctcloss in AMP training, fix error when invalid polygons exist.</li> </ul> </li> <li> <p>2023/05/04 1. Support loading self-defined pretrained checkpoints via setting <code>model-pretrained</code> with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting <code>train-ema</code> (default: False) and <code>train-ema_decay</code> in the yaml config. 4. Arg parameter changed\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: change the column number corresponds to the label to the column index.</p> </li> <li> <p>2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add <code>grouping_strategy</code> argument in yaml config to select a predefined grouping strategy, or use <code>no_weight_decay_params</code> argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 2. Add gradient accumulation to support large batch size training. Usage: add <code>gradient_accumulation_steps</code> in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 3. Add gradient clip to support training stablization. Enable it by setting <code>grad_clip</code> as True in yaml config.</p> </li> <li> <p>2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set <code>type</code> of <code>loss_scale</code> as <code>dynamic</code>. A YAML example can be viewed in <code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. Arg names changed: <code>output_keys</code> -&gt; <code>output_columns</code>, <code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code> 2. Data pipeline updated</p> </li> <li> <p>2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:   <pre><code>  i)   Create a new training task on the openi cloud platform.\n  ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n  iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n  iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n  v)   Fill in other blanks and launch.\n</code></pre></p> </li> </ul>"},{"location":"index_readme/#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindOCR better.</p> <p>Please refer to CONTRIBUTING.md for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"index_readme/#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"index_readme/#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"datasets/chinese_text_recognition/","title":"Chinese-Text-Recognition","text":"<p>This document introduce the dataset preparation for Chinese Text Recognition.</p>"},{"location":"datasets/chinese_text_recognition/#data-downloading","title":"Data Downloading","text":"<p>Following the setup in Benchmarking-Chinese-Text-Recognition, we use the same training, validation and evaliation data as described in Datasets.</p> <p>Please download the following LMDB files as introduced in Downloads:</p> <ul> <li>scene datasets: The union dataset contains RCTW, ReCTS, LSVT, ArT, CTW</li> <li>web: MTWI</li> <li>document: generated with Text Render</li> <li>handwriting dataset: SCUT-HCCDoc</li> </ul>"},{"location":"datasets/chinese_text_recognition/#data-structure","title":"Data Structure","text":"<p>After downloading the files, please put all training files under the same folder <code>training</code>, all validation data under <code>validation</code> folder, and all evaluation data under <code>evaluation</code>.</p> <p>The data structure should be like:</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#data-configuration","title":"Data Configuration","text":"<p>To use the datasets, you can specify the datasets as follow in configuration file.</p>"},{"location":"datasets/chinese_text_recognition/#model-training","title":"Model Training","text":"<pre><code>...\ntrain:\n...\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\ndata_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\ndata_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n...\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#model-evaluation","title":"Model Evaluation","text":"<pre><code>...\ntrain:\n# NO NEED TO CHANGE ANYTHING IN TRAIN SINCE IT IS NOT USED\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\ndata_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n...\n</code></pre> <p>Back to dataset converters</p>"},{"location":"datasets/converters/","title":"Dataset Preparation","text":"<p>This document shows how to convert ocr annotation to the general format (not including LMDB) for model training.</p> <p>You may also refer to <code>convert_datasets.sh</code> which is a quick solution for converting annotation files of all datasets under a given directory.</p> <p>To download and convert OCR datasets to the required data format, please refer to the following instructions: Chinese text recognition, CTW1500, ICDAR2015, MLT2017, SVT, Syntext 150k, TD500, Total Text, SynthText.</p>"},{"location":"datasets/converters/#text-detectionspotting-annotation","title":"Text Detection/Spotting Annotation","text":"<p>The format of the converted annotation file should follow: <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>Taking ICDAR2015 (ic15) dataset as an example, to convert the ic15 dataset to the required format, please run</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/train/ch4_training_images \\\n--label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n--output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/test/ch4_test_images \\\n--label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n--output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"datasets/converters/#text-recognition-annotation","title":"Text Recognition Annotation","text":"<p>The annotation format for text recognition dataset follows <pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> Note that image name and text label are seperated by \\t.</p> <p>To convert, please run: <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"datasets/ctw1500/","title":"SCUT-CTW1500 Datasets","text":""},{"location":"datasets/ctw1500/#data-downloading","title":"Data Downloading","text":"<p>SCUT-CTW1500 Datasets official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/ctw1500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw1500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/train_images/ \\\n--label_dir path/to/ctw1500/ctw_1500_train_labels \\\n--output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/test_images/ \\\n--label_dir path/to/ctw1500/gt_ctw_1500 \\\n--output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>ctw1500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/icdar2015/","title":"Data Downloading","text":"<p>ICDAR 2015 paper</p> <p>download source: one must register an account to download the dataset.</p> Where to Download ICDAR 2015  ICDAR 2015 Challenge has three tasks. Task 1 is Text Localization. Task 3 is Word Recognition. Task 4 is End-to-end Text Spotting. Task 2 Text Segmentation is not available.  ### Text Localization  The four files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 1 are <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre>  ### Word Recognition  The three files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 3 are <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre> The three files are only needed for training word recognition models. Training text detection models does not require the three files.      ### E2E  The nine files downloaded from [web](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads) for task 4 are the union of the four files in the text localization task (task 1) and five vocabulary files <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre> If you download a file named `Challenge4_Test_Task4_GT.zip`, please note that it is the same file as `Challenge4_Test_Task1_GT.zip`, except for its name. In this repository, we will use `Challenge4_Test_Task4_GT.zip` for ICDAR2015 dataset.   <p>After downloading the icdar2015 dataset, place all the files under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/mlt2017/","title":"Data Downloading","text":"<p>MLT (Multi-Lingual) 2017 paper</p> <p>download source. One need to register an account to download this dataset.</p> Where to Download MLT 2017  MLT 2017 dataset consists of two tasks. Task 1 is Text detection (Multi-Language Script) and Task 2 is Word Recognition.  ### Text Detection(Multi-script)  The 11 files downloaded from [web](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads) for task 1 are  <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre>  No need to download the Test Set.   ### Word Identification  The 6 files downloaded from [web](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads) for task 2 are <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n\n&lt;/details&gt;\n\nAfter downloading the files, place them under `[path-to-data-dir]` folder:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip  ```  [Back to dataset converters](converters.md)"},{"location":"datasets/svt/","title":"The Street View Text Dataset (SVT)","text":""},{"location":"datasets/svt/#data-downloading","title":"Data Downloading","text":"<p>The Street View Text Dataset (SVT) official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"datasets/svt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/svt/#for-recognition-task","title":"For Recognition task","text":"<p>To prepare the data for text recognition, you can run the following command:</p> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name  svt --task rec \\\n--image_dir path/to/svt1/ \\\n--label_dir path/to/svt1/train.xml \\\n--output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>Then you can have a folder <code>cropped_images/</code> and an annotation file <code>rec_train_gt.txt</code> under the folder <code>svt1/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/syntext150k/","title":"Data Downloading","text":"<p>SynText150k paper</p> <p>Download Syntext-150k (Part1: 54,327 [imgs][annos]. Part2: 94,723 [imgs][annos].)</p> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/synthtext/","title":"Data Downloading","text":"<p>SynthText is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout. Paper | Download SynthText</p> <p>Download the <code>SynthText.zip</code> file and unzip in <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: Additionally, It is strongly recommended to pre-process the <code>SynthText</code> dataset before using it as it contains some faulty data: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat\n</code></pre> This operation will generate a filtered output in the same format as the original <code>SynthText</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/td500/","title":"MSRA Text Detection 500 Database (MSRA-TD500)","text":""},{"location":"datasets/td500/#data-downloading","title":"Data Downloading","text":"<p>MSRA Text Detection 500 Database\uff08MSRA-TD500\uff09official website</p> <p>download dataset</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/td500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/td500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/train/ \\\n--label_dir path/to/MSRA-TD500/train \\\n--output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/test/ \\\n--label_dir path/to/MSRA-TD500/test \\\n--output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>MSRA-TD500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/totaltext/","title":"Data Downloading","text":"<p>Total-Text paper</p> <p>Download Total-Text Images from source  (size = 441Mb).</p> How to Download Total-Text Images     The Total-Text dataset can be downloaded at [this https URL](https://drive.google.com/file/d/1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2/view?usp=sharing) (size = 441Mb).   <p>Download Total-Text Ground Truth from source.</p> How to Download Total-Text Ground Truth     The groundtruth of the Total-Text dataset can be downloaded through  [this https URL](https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view?usp=sharing) for text file format('.txt').   <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"inference/convert_dynamic/","title":"Convert dynamic.md","text":""},{"location":"inference/convert_dynamic/#inference-model-shape-scaling","title":"Inference - Model Shape Scaling","text":""},{"location":"inference/convert_dynamic/#1-introduction","title":"1. Introduction","text":"<p>In some inference scenarios, such as object recognition after detection, the input batch size and image size of the recognition network are not fixed because the number of object and the size of the object are not fixed. If each inference is calculated according to the maximum batch size or maximum image size, it will cause a waste of computing resources.</p> <p>Therefore, you can set some candidate values during model conversion, and resize to the best matching candidate value during inference, thereby improving performance. Users can manually select these candidate values empirically, or they can be statistically derived from the dataset.</p> <p>This tool integrates the function of dataset statistics, can count the appropriate combination of <code>batch size</code>, <code>height</code> and <code>width</code> as candidate values, and encapsulates the model conversion tool, thus realizing the automatic model shape scaling.</p>"},{"location":"inference/convert_dynamic/#2-environment","title":"2. Environment","text":"<p>Please refer to Environment Installation to install ACL or MindSpore Lite environment.</p>"},{"location":"inference/convert_dynamic/#3-model","title":"3. Model","text":"<p>Currently, ONNX model files are supported, and by selecting different backends, they are automatically shape scaling and converted to OM or MIndIR model files.</p> <p>Please make sure that the input model is the dynamic shape version. For example, if the text detection model needs to shape scaling for H and W, make sure that at least the H and W axes are dynamic, and the shape can be <code>(1,3,-1,-1)</code> and <code>(-1,3,- 1,-1)</code>etc.</p>"},{"location":"inference/convert_dynamic/#4-dataset","title":"4. Dataset","text":"<p>Two types of data are supported:</p> <ol> <li>Image folder</li> </ol> <ul> <li> <p>This tool will read all the images in the folder, record <code>height</code> and <code>width</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text detection and text recognition models</p> </li> </ul> <ol> <li>Annotation file for text detection</li> </ol> <ul> <li> <p>Refer to converter, which is the annotation file output when the      parameter <code>task</code> is <code>det</code></p> </li> <li> <p>This tool will read the coordinates of the text box marked under each image, record <code>height</code> and <code>width</code>, and the      number of boxes as <code>batch size</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text recognition models</p> </li> </ul>"},{"location":"inference/convert_dynamic/#5-usages","title":"5. Usages","text":"<p><code>cd deploy/models_utils/auto_scaling</code></p>"},{"location":"inference/convert_dynamic/#51-command-example","title":"5.1 Command example","text":"<ul> <li>auto shape scaling for batch size</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/det_gt.txt\n    --input_shape=-1,3,48,192 \\\n--output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_dynamic_bs.om</code></p> <ul> <li>auto shape scaling for height and width</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/images \\\n--input_shape=1,3,-1,-1 \\\n--output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_dynamic_hw.om</code></p> <ul> <li>auto shape scaling for batch szie, height and width</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/images \\\n--input_shape=-1,3,-1,-1 \\\n--output_path=output\n</code></pre> <p>The output result is multiple OM models, combining multiple different batch sizes, and each model uses the same dynamic image size\uff1a<code>model_dynamic_bs1_hw.om</code>, <code>model_dynamic_bs4_hw.om</code>, ......</p> <ul> <li>no shape scaling</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--input_shape=4,3,48,192 \\\n--output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_static.om</code></p>"},{"location":"inference/convert_dynamic/#52-parameter-details","title":"5.2 Parameter Details","text":"Name Default Required Description model_path None Y Path to model file input_shape None Y model input shape, NCHW format data_path None N Path to image folder or annotation file input_name x N model input name backend atc N converter backend\uff0catc or lite output_path ./output N Path to output model soc_version Ascend310P3 N soc_version for Ascend\uff0cAscend310P3 or Ascend310"},{"location":"inference/convert_dynamic/#53-configuration-file","title":"5.3 Configuration file","text":"<p>In addition to the above command line parameters, there are some parameters in auto_scaling.yaml to describe the statistics of the dataset, You can modify it yourself if necessary:</p> <ul> <li>limit_side_len</li> </ul> <p>The size limit of <code>height</code> and <code>width</code> of the original input data, if it exceeds the range, it will be compressed   according to the ratio, and the degree of discreteness of the data can be adjusted.</p> <ul> <li>strategy</li> </ul> <p>Data statistics algorithm strategy, supports <code>mean_std</code> and <code>max_min</code> two algorithms, default: <code>mean_std</code>.</p> <ul> <li>mean_std</li> </ul> <pre><code>mean_std = [mean - n_std * sigma\uff0cmean + n_std * sigma]\n</code></pre> <ul> <li>max_min</li> </ul> <pre><code>max_min = [min - (max - min) * expand_ratio / 2\uff0cmax + (max - min) * expand_ratio / 2]\n</code></pre> <ul> <li>width_range/height_range</li> </ul> <p>For the width/height size limit after discrete statistics, exceeding will be filtered.</p> <ul> <li>interval</li> </ul> <p>Interval size, such as some networks may require that the input size must be a multiple of 32.</p> <ul> <li>max_scaling_num</li> </ul> <p>The maximum number of discrete values for shape scaling .</p> <ul> <li>batch_choices</li> </ul> <p>The default batch size value, if the data_path uses an image folder, the batch size information cannot be counted, and   the default value will be used.</p> <ul> <li>default_scaling</li> </ul> <p>If user does not use data_path, provide default <code>height</code> and <code>width</code> discrete values for shape scaling .</p>"},{"location":"inference/convert_tutorial/","title":"Convert tutorial.md","text":""},{"location":"inference/convert_tutorial/#inference-model-conversion-tutorial","title":"Inference - Model Conversion Tutorial","text":""},{"location":"inference/convert_tutorial/#1-mindocr-models","title":"1. MindOCR models","text":"<p>The inference of MindOCR models supports MindSpore Lite backend.</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(offline conversion)\"| o[MindIR];</code></pre>"},{"location":"inference/convert_tutorial/#11-model-export","title":"1.1 Model Export","text":"<p>Before inference, it is necessary to export the trained ckpt to a MindIR file, which stores the model structure and weight parameters.</p> <p>Some models provide download links for MIndIR export files, as shown in Model List. You can jump to the corresponding model introduction page for download.</p>"},{"location":"inference/convert_tutorial/#12-model-conversion","title":"1.2 Model Conversion","text":"<p>You need to use the <code>converter_lite</code> tool to convert the above exported MindIR file offline so that it can be used for MindSpore Lite inference.</p> <p>The tutorial for the <code>converter_lite</code> command can be referred to Offline Conversion of Inference Models.</p> <p>Assuming the input model is input.mindir and the output model after <code>converter_lite</code> conversion is output.mindir, the conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>Among them, <code>config.txt</code> can be used to set the shape and inference precision of the conversion model.</p>"},{"location":"inference/convert_tutorial/#121-model-shape-setting","title":"1.2.1 Model Shape Setting","text":"<ul> <li>Static Shape</li> </ul> <p>If the input name of the exported model is <code>x</code>, and the input shape is <code>(1,3,736,1280)</code>, then the <code>config.txt</code> is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>The generated output.mindir is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number and size of targets is not fixed resulting. If each inference is computed at the maximum Batch Size or maximum Image Size, it will result in wasted computational resources.</p> <p>Assuming the exported model input shape is (-1, 3, -1, -1), and the NHW axes are dynamic. Therefore, some optional values can be set during model conversion to adapt to input images of various size during inference.</p> <p><code>converter_lite</code> achieves this by setting the <code>dynamic_dims</code> parameter in <code>[ascend_context]</code> through <code>--configFile</code>. Please refer to the Dynamic Shape Configuration for details. We will refer to it as Model Shape Scaling for short.</p> <p>So, there are two options for conversion, by setting different config.txt:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the config.txt is as follows:</p> <pre><code> [ascend_context]\ninput_format=NCHW\n input_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial/#122-model-precision-mode-setting","title":"1.2.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>converter_lite</code> when converting the model. Please refer to the Ascend Conversion Tool Description, the usage of <code>precision_mode</code> parameter is described in the table of the configuration file, you can choose <code>enforce_fp16</code>, <code>enforce_fp32</code>, <code>preferred_fp32</code> and <code>enforce_origin</code> etc. So, you can add the <code>precision_mode</code> parameter in the <code>[Ascend_context]</code> of the above config.txt file to set the precision mode:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>If not set, defaults to <code>enforce_fp16</code>.</p>"},{"location":"inference/convert_tutorial/#2-paddleocr-models","title":"2. PaddleOCR models","text":"<p>The PaddleOCR models support two inference backends: ACL and MindSpore Lite, corresponding to the OM model and MindIR model, respectively.</p> <pre><code>graph LR;\n    in1[trained model] -- export --&gt; in2[inference model] -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);</code></pre>"},{"location":"inference/convert_tutorial/#21-trained-inference-model","title":"2.1 Trained -&gt; Inference model","text":"<p>In the download link of PaddleOCR model, there are two formats: trained model and inference model. If a training model is provided, it needs to be converted to the format of inference model.</p> <p>On the original PaddleOCR introduction page of each trained model, there are usually conversion script samples that only need to input the config file, model file, and save path of the trained model. The example is as follows:</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"inference/convert_tutorial/#22-inference-model-onnx","title":"2.2 Inference model -&gt; ONNX","text":"<p>Install model conversion tool paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>For detailed usage tutorials, please refer to Paddle2ONNX model transformation and prediction\u3002</p> <p>Run the conversion command to generate the onnx model:</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>The <code>input_shape_dict</code> in the parameter can generally be viewed by opening the inference model using the Netron, or found in the code in tools/export_model. py above.</p>"},{"location":"inference/convert_tutorial/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>The ONNX model can be converted into an OM model by ATC tools.</p> <p>Ascend Tensor Compiler (ATC) is a model conversion tool built upon the heterogeneous computing architecture CANN. It is designed to convert models of open-source frameworks into .om offline models supported by Ascend AI Processor. A detailed tutorial on the tool can be found in ATC Instructions.</p>"},{"location":"inference/convert_tutorial/#231-model-shape-setting","title":"2.3.1 Model Shape Setting","text":"<p>The exported ONNX in the example has an input shape of (-1, 3, -1, -1).</p> <ul> <li>Static Shape</li> </ul> <p>It can be converted to a static shape version by fixed values for NHW, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>The generated file is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> <p>The ATC tool also supports Model Shape Scaling by parameter dynamic_dims, and some optional values can be set during model conversion to adapt to input images of various shape during inference.</p> <p>So, there are two options for conversion, by setting different command line parameters:</p> <ul> <li>Dynamic Image Size</li> </ul> <p>N uses fixed values, HW uses multiple optional values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>Dynamic Batch Size</li> </ul> <p>N uses multiple optional values, HW uses fixed values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial/#232-model-precision-mode-setting","title":"2.3.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>atc</code> when converting the model. Please refer to the Command-Line Options. Optional values include <code>force_fp16</code>, <code>force_fp32</code>, <code>allow_fp32_to_fp16</code>, <code>must_keep_origin_dtype</code>, <code>allow_mix_precision</code>, etc. So, you can add the <code>precision_mode</code> parameter in the <code>atc</code> command line to set the precision:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>If not set, defaults to <code>force_fp16</code>.</p>"},{"location":"inference/convert_tutorial/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>The <code>converter_lite</code> can be used to convert the ONNX into a MindIR. For detailed usage tutorials, please refer to Offline Conversion of Inference Models\u3002</p> <p>The conversion command is as follows:</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>The conversion process is completely the same as the MindOCR models, except that <code>--fmk</code> needs to specify that the input is the ONNX, which will not be repeated here.</p>"},{"location":"inference/convert_tutorial/#3-mmocr-models","title":"3. MMOCR models","text":"<p>MMOCR uses Pytorch, and its model files typically have a pth format suffix. You need to first export it to ONNX format and then convert to an OM/MindIR format file supported by ACL/MindSpore Lite.</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);</code></pre>"},{"location":"inference/convert_tutorial/#31-mmocr-model-onnx","title":"3.1 MMOCR model -&gt; ONNX","text":"<p>MMDeploy provides the command to export MMOCR models to ONNX. For detailed tutorials, please refer to How to convert model.</p> <p>For parameter <code>deploy_cfg</code>, you need to select the <code>*_onnxruntime_dynamic.py</code> file in directory mmdeploy/configs/mmocr to export as a dynamic shape ONNX model.</p>"},{"location":"inference/convert_tutorial/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>Please refer to ONNX -&gt; OM in the PaddleOCR section above.</p>"},{"location":"inference/convert_tutorial/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>Please refer to ONNX -&gt; MIndIR in the PaddleOCR section above.</p>"},{"location":"inference/environment/","title":"Environment.md","text":""},{"location":"inference/environment/#inference-environment-installation","title":"Inference - Environment Installation","text":"<p>MindOCR supports inference for Ascend310/Ascend310P device.</p> <p>Please make sure that the Ascend AI processor software package is correctly installed on your system. If it is not installed, please refer to the section Installing Ascend AI processor software package to install it.</p> <p>The MindOCR backend supports two types of inference: ACL and MindSpore Lite. Before inference using ACL mode, you need to use ATC tool to convert the model to om format, or to use converter_lite tool to convert the model to MindIR format, the specific differences are as follows:</p> ACL Mindspore Lite Conversion Tool ATC converter_lite Inference Model Format om MindIR"},{"location":"inference/environment/#1-acl-inference","title":"1. ACL inference","text":"<p>For the ACL inference of MindOCR, it currently relies on the Python API interface by MindX, which currently only supports Python 3.9.</p> package version Python 3.9 MindX 3.0.0 <p>On the basis of the Python 3.9 environment, download the mxVision SDK installation package for MindX and refer to the tutorial for installation. The main steps are as follows:</p> <pre><code># add executable permissions\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# execute the installation command\n# if prompted to specify the path to CANN, add parameters such as: --cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# set environment variable\nsource mxVision/set_env.sh\n</code></pre> <p>If use python interface, after installation, test whether mindx can be imported normally\uff1a<code>python -c \"import mindx\"</code></p> <p>If prompted that mindx cannot be found, go to the mxVision/Python directory and install the corresponding Whl package:</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> If use C++ interface, the above steps are not necessary.</p>"},{"location":"inference/environment/#2-mindspore-lite-inference","title":"2. MindSpore Lite inference","text":"<p>For the MindSpore Lite inference of MindOCR, It requires the version 2.0.0-rc1 or higher of the MindSpore Lite cloud-side inference toolkit.</p> <p>Download the Ascend version of the cloud-side inference toolkit tar.gz file, as well as the Python interface Wheel package.</p> <p>The download address provides the Python package for version 3.7. If you need other versions, please refer to the compilation tutorial.</p> <p>Just decompress the inference toolkit, and set environment variables:</p> <pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib::$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> <p>If using python interface, install the required .whl package using pip:</p> <p><pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre> The installation is not necessary if using the C++ interface.</p>"},{"location":"inference/inference_tutorial/","title":"Python/C++ Inference on Ascend 310","text":""},{"location":"inference/inference_tutorial/#inference-tutorial","title":"Inference - Tutorial","text":""},{"location":"inference/inference_tutorial/#1-introduction","title":"1. Introduction","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend, integrates text detection, angle classification, and text recognition, implements end-to-end OCR inference process, and optimizes inference performance using pipeline parallelism.</p>"},{"location":"inference/inference_tutorial/#2-environment","title":"2. Environment","text":"<p>Please refer to the environment installation to configure the inference runtime environment for MindOCR, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"inference/inference_tutorial/#3-model-conversion","title":"3. Model conversion","text":"<p>MindOCR inference not only supports exported models from trained ckpt file, but also supports the third-party models, as listed in the MindOCR Models Support List and Third-Party Models Support List.</p> <p>Please refer to the Conversion Tutorial, to convert it into a model format supported by MindOCR inference.</p>"},{"location":"inference/inference_tutorial/#4-inference-python","title":"4. Inference (Python)","text":"<p>Enter the inference directory\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"inference/inference_tutorial/#41-command-example","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--res_save_dir=det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code># cls_mv3.mindir is converted from ppocr\npython infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>recognition</li> </ul> <p>Run text recognition alone.</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=rec\n</code></pre> <p>The results will be saved in rec/rec_results.txt, with the following format:</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"inference/inference_tutorial/#42-detail-of-inference-parameter","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Image or folder path for inference device str Ascend Device type, support Ascend device_id int 0 Device id backend str lite Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism precision_mode str None Precision mode, only supports setting by Model Conversion currently, and it takes no effect here <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results vis_det_save_dir str None Saving dir for images of with detection boxes vis_pipeline_save_dir str None Saving dir for images of with detection boxes and text vis_font_path str None Font path for drawing text crop_save_dir str None Saving path for cropped images after detection show_log bool False Whether show log when inferring save_log_dir str None Log saving dir <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection det_model_name_or_config str None Model name or YAML config file path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification cls_model_name_or_config str None Model name or YAML config file path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_model_name_or_config str None Model name or YAML config file path for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase <p>Notes\uff1a</p> <p><code>*_model_name_or_config</code> can be the model name or YAML config file path, please refer to MindOCR Models Support List and Third-Party Models Support List.</p>"},{"location":"inference/inference_tutorial/#5-inference-c","title":"5. Inference (C++)","text":"<p>Currently, only the Chinese DBNet, CRNN, and SVTR models in the PP-OCR series are supported.</p> <p>Enter the inference directory\uff1a<code>cd deploy/cpp_infer</code>,then execute the compilation script 'bash build.sh'. Once the build process is complete, an executable file named 'infer' will be generated in the 'dist' directory located in the current path.</p>"},{"location":"inference/inference_tutorial/#41-command-example_1","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"inference/inference_tutorial/#42-detail-of-inference-parameter_1","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Image or folder path for inference device str Ascend Device type, support Ascend device_id int 0 Device id backend str acl Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase"},{"location":"inference/model_evaluation/","title":"Model evaluation.md","text":""},{"location":"inference/model_evaluation/#model-inference-evaluation","title":"Model Inference Evaluation","text":""},{"location":"inference/model_evaluation/#1-text-detection","title":"1. Text detection","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_det.py \\\n--gt_path=/path/to/det_gt.txt \\\n--pred_path=/path/to/prediction/det_results.txt\n</code></pre>"},{"location":"inference/model_evaluation/#2-text-recognition","title":"2. Text recognition","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n--gt_path=/path/to/rec_gt.txt \\\n--pred_path=/path/to/prediction/rec_results.txt \\\n--character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>Please note that character_dict_path is an optional parameter, and the default dictionary only supports numbers and English lowercase.</p> <p>When evaluating the PaddleOCR or MMOCR series models, please refer to Third-Party Model Support List to use the corresponding dictionary.</p>"},{"location":"inference/model_perf_thirdparty/","title":"Model perf thirdparty.md","text":""},{"location":"inference/model_perf_thirdparty/#third-party-model-inference-performance-table","title":"Third-party Model Inference Performance Table","text":"<p>This document will give the performance of the third-party inference model using the MindIR format after performing model conversion.</p>"},{"location":"inference/model_perf_thirdparty/#1-text-detection","title":"1. Text detection","text":"name model backbone test data recall precision f-score source ch_pp_server_det_v2.0 DB ResNet18_vd MLT17 0.3637 0.6340 0.4622 PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 MLT17 0.2557 0.5021 0.3389 PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 MLT17 0.3258 0.6318 0.4299 PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 MLT17 0.2346 0.4868 0.3166 PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 MLT17 0.2403 0.4597 0.3156 PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 IC15 0.3866 0.4630 0.4214 PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 MLT17 0.5992 0.7348 0.6601 PaddleOCR en_pp_det_dbnet_resnet50vd DBNet ResNet50_vd IC15 0.8281 0.7716 0.7989 PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 0.7463 0.9043 0.8177 PaddleOCR en_pp_det_psenet_resnet50vd PSENet ResNet50_vd IC15 0.7664 0.8463 0.8044 PaddleOCR en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 0.8387 0.7900 0.8136 MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 0.8681 0.8074 0.8367 MMOCR"},{"location":"inference/model_perf_thirdparty/#2-text-recognition","title":"2. Text recognition","text":"name model backbone test data accuracy norm edit distance source ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (only Chinese) 0.4991 0.7411 PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only Chinese) 0.4991 0.7535 PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (only Chinese) 0.4459 0.7036 PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (only Chinese) 0.2459 0.4878 PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only English) 0.7964 0.8854 PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 MLT17 (only English) 0.0164 0.0657 PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 MLT17 (only English) 0.4304 0.5944 PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd IC15 0.6635 0.8392 PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta ResNet34_vd IC15 0.6428 0.8321 PaddleOCR en_pp_rec_vitstr_vitstr VITSTR vitstr IC15 0.6842 0.8578 PaddleOCR en_mm_rec_nrtr_resnet31 NRTR ResNet31 IC15 0.6726 0.8574 MMOCR en_mm_rec_satrn_shallowcnn SATRN shallowcnn IC15 0.7352 0.8887 MMOCR <p>Please note that the above models use model shape scaling, so the performance here only represents the performance under certain input shapes.</p>"},{"location":"inference/model_perf_thirdparty/#3-evaluation-method","title":"3. Evaluation method","text":"<p>Please refer to Model Inference Evaluation document.</p>"},{"location":"inference/models_list/","title":"Models list.md","text":""},{"location":"inference/models_list/#inference-mindocr-models-support-list","title":"Inference - MindOCR Models Support List","text":"<p>MindOCR inference supports exported models from trained ckpt file, and this document displays a list of adapted models.</p> <p>Please export or download the pre-exported MindIR file, and refer to the model conversion tutorial before inference.</p>"},{"location":"inference/models_list/#1-text-detection","title":"1. Text detection","text":"model backbone language config DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"inference/models_list/#2-text-recognition","title":"2. Text recognition","text":"model backbone dict file language config CRNN VGG7 Default en crnn_vgg7.yaml ResNet34_vd Default en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"inference/models_list_thirdparty/","title":"Models list thirdparty.md","text":""},{"location":"inference/models_list_thirdparty/#inference-third-party-models-support-list","title":"Inference - Third-Party Models Support List","text":"<p>MindOCR can support the inference of third-party models, and this document displays a list of adapted models.</p> <p>After downloading the model file, it needs to be converted to a model file supported by ACL/MindSpore Lite inference (OM or MindIR). Please refer to the model conversion tutorial.</p> <p>The original model files involved are as follows:</p> type format description pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle trained model, which can store information such as model structure, weights, optimizer status, etc pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle inference model, which can be derived from its trained model, saving the network structure and weights. pth .pth Pytorch model, which can store information such as model structure, weights, optimizer status, etc"},{"location":"inference/models_list_thirdparty/#1-text-detection","title":"1. Text detection","text":"name model backbone config download reference source ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR <p>Notice: When using the en_pp_det_psenet_resnet50vd model for inference, you need to modify the onnx file with the following command</p> <pre><code>python deploy/models_utils/onnx_optim/insert_pse_postprocess.py \\\n--model_path=./pse_r50vd.onnx \\\n--binary_thresh=0.0 \\\n--scale=1.0\n</code></pre>"},{"location":"inference/models_list_thirdparty/#2-text-recognition","title":"2. Text recognition","text":"name model backbone dict file config download reference source ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta ResNet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR en_mm_rec_nrtr_resnet31 NRTR ResNet31 english_digits_symbols.txt yaml pth NRTR MMOCR en_mm_rec_satrn_shallowcnn SATRN ShallowCNN english_digits_symbols.txt yaml pth SATRN MMOCR"},{"location":"inference/models_list_thirdparty/#3-text-angle-classification","title":"3. Text angle classification","text":"name model config download reference source ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"inference/models_list_thirdparty/#4-third-party-model-inference-performance","title":"4. Third-party model inference performance","text":"<p>Please refer to the third-party model inference test performance table.</p>"},{"location":"mkdocs/contributing/","title":"Contributing","text":""},{"location":"mkdocs/contributing/#mindocr-contributing-guidelines","title":"MindOCR Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little helps, and credit will always be given.</p>"},{"location":"mkdocs/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindOCR community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"mkdocs/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"mkdocs/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"mkdocs/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"mkdocs/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to fix it.</p>"},{"location":"mkdocs/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindOCR could always use more documentation, whether as part of the official MindOCR docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"mkdocs/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"mkdocs/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindocr</code> for local development.</p> <ol> <li>Fork the <code>mindocr</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindocr.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindocr\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindocr python=3.8\nconda activate mindocr\ncd mindocr\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you finish making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"mkdocs/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindocr/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"mkdocs/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"mkdocs/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed. Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"mkdocs/customize_data_transform/","title":"Customize Data Transformation","text":""},{"location":"mkdocs/customize_data_transform/#guideline-for-developing-your-transformation","title":"Guideline for Developing Your Transformation","text":""},{"location":"mkdocs/customize_data_transform/#writing-guideline","title":"Writing Guideline","text":"<ol> <li> <p>Each transformation is a class with a callable function. An example is shown below.</p> </li> <li> <p>The input to the transformation function is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>Please write comments for the call function to clarify the required/modified/added keys in the data dict.</p> </li> <li> <p>Add kwargs in the class init function for extension, which is used to parse global config, such as is_train.</p> </li> </ol> <pre><code>class ToCHWImage(object):\n\"\"\" convert hwc image to chw image\n    \"\"\"\n\n    def __init__(self, channel, **kwargs):\n        self.is_train = kwargs.get('is_train', True)\n\n    def __call__(self, data: dict):\n'''\n        required keys:\n            - image\n        modified keys:\n            - image\n        '''\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre>"},{"location":"mkdocs/customize_data_transform/#add-unit-test-and-visualization","title":"Add Unit Test and Visualization","text":"<p>Please add unit test in <code>tests/ut/transforms</code> for the written transformation and try to cover different cases (inputs and settings).</p> <p>Please visually check the correctness of the transformation on image and annotation using the jupyter notebook. See <code>transform_tutorial.ipynb</code>.</p>"},{"location":"mkdocs/customize_data_transform/#important-notes","title":"Important Notes","text":"<ol> <li>For spatial transformation operaions that will be used in text detection inference or evaluation (e.g. determinstic resize, scale), please record the space transformation information in <code>shape_list</code>. Otherwise, the postprocessing method won't be able to map the results back to the orignal image space. On how to record <code>shape_list</code>, please refer to DetResize.</li> </ol>"},{"location":"mkdocs/customize_dataset/","title":"Customize Dataset","text":""},{"location":"mkdocs/customize_dataset/#guideline-for-data-module","title":"Guideline for Data Module","text":""},{"location":"mkdocs/customize_dataset/#code-structure","title":"Code Structure","text":"<pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base_dataset.py                 # base dataset class with __getitem__\n\u251c\u2500\u2500 builder.py                  # API for create dataset and loader\n\u251c\u2500\u2500 det_dataset.py              # general text detection dataset class\n\u251c\u2500\u2500 rec_dataset.py              # general rec detection dataset class\n\u251c\u2500\u2500 rec_lmdb_dataset.py             # LMDB dataset class (To be impl.)\n\u2514\u2500\u2500 transforms\n    \u251c\u2500\u2500 det_transforms.py           # processing and augmentation ops (callabel classes) especially for detection tasks\n    \u251c\u2500\u2500 general_transforms.py           # general processing and augmentation ops (callabel classes)\n    \u251c\u2500\u2500 modelzoo_transforms.py          # transformations adopted from modelzoo\n    \u251c\u2500\u2500 rec_transforms.py           # processing and augmentation ops (callabel classes) especially for recognition tasks\n    \u2514\u2500\u2500 transforms_factory.py           # API for create and run transforms\n</code></pre>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-dataset-class","title":"How to add your own dataset class","text":"<ol> <li> <p>Inherit from BaseDataset class</p> </li> <li> <p>Rewrite the following file and annotation parsing functions in BaseDataset.</p> <p>def load_data_list(self, label_file: Union[str, List[str]], sample_ratio: Union[float, List] = 1.0,  shuffle: bool = False, **kwargs) -&gt; List[dict]</p> <p>def _parse_annotation(self, data_line: str) -&gt; Union[dict, List[dict]]</p> </li> </ol>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-data-transformation","title":"How to add your own data transformation","text":"<p>Please refer to Guideline for Developing Your Transformation</p>"},{"location":"mkdocs/customize_model/","title":"Customize a New Model","text":""},{"location":"mkdocs/customize_model/#guideline-for-model-module","title":"Guideline for Model Module","text":""},{"location":"mkdocs/customize_model/#how-to-add-a-new-model-in-mindocr","title":"How to Add a New Model in MindOCR","text":"<ol> <li> <p>Decompose the model into 3 (or 2) modules: backbone, (neck,) head. Neck is usually not involved in recognition tasks.</p> </li> <li> <p>For each module:</p> <p>a. if it is implemented in MindOCR, skip since you can get the module by the <code>build_{module}</code> function .</p> <p>b. if not, please implement it and follow the module format guideline</p> </li> <li> <p>Define your model in two ways</p> <p>a. Write a model py file, which includes the model class and specification functions. Please follow the model format guideline. It is to allows users to invoke a pre-defined model easily, such as <code>model = build_model('dbnet_resnet50', pretrained=True)</code>  .</p> <p>b. Config the architecture in a yaml file. Please follow the yaml format guideline . It is to allows users to modify a base architecture quickly in yaml file.</p> </li> <li> <p>To verify the correctness of the written model, please add your yaml config file path in <code>test_models.py</code>, modify the main function to build the desired model, and then run <code>test_models.py</code></p> </li> </ol> <pre><code>python tests/ut/test_models.py --config /path/to/yaml_config_file\n</code></pre>"},{"location":"mkdocs/customize_model/#format-guideline-for-writing-a-new-module","title":"Format Guideline for Writing a New Module","text":""},{"location":"mkdocs/customize_model/#backbone","title":"Backbone","text":"<ul> <li>File naming format: <code>models/backbones/{task}_{backbone}.py</code>, e.g, <code>det_resnet.py</code>   (since the same backbone for det and rec may differ, the task prefix is necessary)</li> <li>Class naming format: {Task}{BackboneName}{Variant} e.g. <code>class DetResNet</code></li> <li>Class <code>__init__</code> args: no limitation, define by model need.</li> <li>Class attributes: MUST contain <code>out_channels</code> (List), to describe channels of each output features. e.g. <code>self.out_channels=[256, 512, 1024, 2048]</code></li> <li>Class <code>construct</code> args: x (Tensor)</li> <li>Class <code>construct</code> return: features (List[Tensor]) for features extracted from different layers in the backbone, feature dim order <code>[bs, channels, \u2026]</code>. Expect shape of each feature: <code>[bs, channels, H, W]</code></li> </ul>"},{"location":"mkdocs/customize_model/#neck","title":"Neck","text":"<ul> <li>File naming format: <code>models/necks/{neck_name}.py</code>, e.g, <code>fpn.py</code></li> <li>Class naming format: {NeckName} e.g. <code>class FPN</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=256, **kwargs)</code>.</li> <li>Class attributes: MUST contain <code>out_channels</code> attribute, to describe channel of the output feature. e.g. <code>self.out_channels=256</code></li> <li>Class <code>construct</code> args: features (List(Tensor))</li> <li>Class <code>construct</code> return: feature (Tensor) for output feature, feature dim order <code>[bs, channels, \u2026]</code></li> </ul>"},{"location":"mkdocs/customize_model/#head","title":"Head","text":"<ul> <li>File naming: <code>models/heads/{head_name}.py</code>, e.g., <code>dbhead.py</code></li> <li>Class naming: {HeadName} e.g. <code>class DBHead</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=2, **kwargs)</code>.</li> <li>Class <code>construct</code> args: feature (Tensor), extra_input (Optional[Tensor]). The extra_input tensor is only applicable for head that needs recurrent input (e.g., Attention head), or heads with multiple inputs.</li> <li>Class <code>construct</code> return: prediction (Union(Tensor, Tuple[Tensor])). If there is only one output, return Tensor. If there are multiple outputs, return Tuple of Tensor, e.g., <code>return output1, output2, output_N</code>. Note that the order should match the loss function or the postprocess function.</li> </ul> <p>Note: if there is no neck in the model architecture like crnn, you can skip writing for neck. <code>BaseModel</code> will select the last feature of the features (List(Tensor)) output by Backbone, and forward it to Head module.</p>"},{"location":"mkdocs/customize_model/#format-guideline-for-model-py-file","title":"Format Guideline for Model Py File","text":"<ul> <li>File naming: <code>models/{task}_{model_class_name}.py</code>, e.g., <code>det_dbnet.py</code></li> <li>Class naming: {ModelName}, e.g., <code>class DBNet</code></li> <li>Class MUST inherent from <code>BaseModel</code>, e.g., <code>class DBNet(BaseModel)</code></li> <li>Spec. function naming: <code>{model_class_name}_{specifiation}.py</code>, e.g. <code>def dbnet_resnet50()</code> (Note: no need to add task prefix assuming no one model can solve any two tasks)</li> <li>Spec. function args: (pretrained=False, **kwargs), e.g. <code>def dbnet_resnet50(pretrained=False, **kwargs)</code>.</li> <li>Spec. function return: model (nn.Cell), which is the model instance</li> <li>Spec. function decorator: MUST add @register_model decorator, and import model file in <code>mindocr/models/__init__.py</code>, which is to register the model to the supported model list.</li> </ul> <p>After writing and registration, model can be created via the <code>build_model</code> func.  ``` python</p>"},{"location":"mkdocs/customize_model/#in-a-python-script","title":"in a python script","text":"<p>model = build_model('dbnet_resnet50', pretrained=False) <pre><code>## Format Guideline for Yaml File\n\nTo define/config the model architecture in yaml file, you should follow the keys in the following examples.\n\n\n- For models with a neck.\n\n``` python\nmodel:              # R\n  type: det\n  backbone:             # R\n    name: det_resnet50      # R, backbone specification function name\n    pretrained: False\n  neck:             # R\n    name: FPN           # R, neck class name\n    out_channels: 256       # D, neck class __init__ arg\n    #use_asf: True\n  head:             # R, head class name\n    name: ConvHead      # D, head class __init__ arg\n    out_channels: 2\n    k: 50\n</code></pre></p> <ul> <li>For models without a neck <pre><code>model:              # R\n  type: rec\n  backbone:         # R\n    name: resnet50      # R\n    pretrained: False\n  head:             # R\n    name: ConvHead      # R\n    out_channels: 30        # D\n</code></pre></li> </ul> <p>(R - Required. D - Depends on model)</p>"},{"location":"mkdocs/customize_postprocess/","title":"Customize Postprocessing Method","text":""},{"location":"mkdocs/customize_postprocess/#guideline-for-postprocessing-module","title":"Guideline for Postprocessing Module","text":""},{"location":"mkdocs/customize_postprocess/#common-protocols","title":"Common Protocols","text":"<ol> <li>Each postprocessing module is a class with a callable function.</li> <li>The input to the postprocessing function is network prediction and additional data information if needed.</li> <li>The output of the postprocessing function is a alwasy a dict, where the key is a field name, such as 'polys' for polygons in text detection, 'text' for text detection.</li> </ol>"},{"location":"mkdocs/customize_postprocess/#detection-postprocessing-api-protocols","title":"Detection Postprocessing API Protocols","text":"<ol> <li> <p>class naming: Det{Method}Postprocess</p> </li> <li> <p>class  <code>__init__()</code> args:</p> <ul> <li><code>box_type</code> (string): options are [\"quad', 'polys\"] for quadriateral and polygon text representation.</li> <li><code>rescale_fields</code> (List[str]='polys'): indicates which fields in the output dict will be rescaled to the original image space. Field name: \"polys\" for polygons</li> </ul> </li> <li> <p><code>__call__()</code> method: If inherit from <code>DetBasePostprocess</code>DetBasePostprocess<code>`, you don't need to implement this method in your Postproc. class.     Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get text boxes (by</code>self._postprocess()<code>function) and then rescale them back to the original image space (by</code>self.rescale()` function).</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>shape_list</code> (Union[List, np.ndarray, ms.Tensor]): shape and scale info for each image in the batch, shape [batch_size, 4]. Each internal array of length 4 is [src_h, src_w, scale_h, scale_w], where src_h and src_w are height and width of the original image, and scale_h and scale_w are their scale ratio after image resizing respectively.</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: detection result as a dictionary with the following keys</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons mapped on the original image space, shape [batch_size, num_polygons, num_points, 2]. If <code>box_type</code> is 'quad', num_points=4, and the internal np.ndarray is of shape [4, 2]</li> <li><code>scores</code> (List[float]): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> </ul> </li> <li> <p><code>_postprocess()</code> method: Implement your postprocessing method here if inherit from <code>DetBasePostprocess</code>     Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to original image space in call function)</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: postprocessing result as a dict with keys:</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons on the transformed (i.e. resized normally) image space, of shape (batch_size, num_polygons, num_points, 2). If <code>box_type</code> is 'quad', num_points=4.</li> <li><code>scores</code> (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Please cast <code>pred</code> to the type you need in your implementation. Some postprocesssing steps use ops from mindspore.nn and prefer Tensor type, while some steps prefer np.ndarray type required in other libraries.</li> <li><code>_postprocess()</code> should NOT round the text box <code>polys</code> to integer in return, because they will be recaled and then rounded in the end. Rounding early will cause larger error in polygon rescaling and results in evaluation performance degradation, especially on small datasets.</li> </ul> </li> </ul> </li> <li> <p>About rescaling the polygons back to the original image spcae</p> <ul> <li>The rescaling step is necessary for a fair evaluation and is needed in cropping text regions from the orginal image in inference.</li> <li>To enable rescaling for evaluation<ol> <li>add \"shape_list\" to the <code>eval.dataset.output_columns</code> in the YAML config file of the model.</li> <li>make sure <code>rescale_fields</code> is not None (default is [\"polys\"])</li> </ol> </li> <li>To enable rescaling in inference:<ol> <li>directly parse <code>shape_list</code> (which is got from data[\"shape_list\"] after data loading) to the postprocessing function.  It works with <code>rescale_fields</code> to decide whether to do rescaling and which fields are to be rescaled.</li> </ol> </li> <li><code>shape_list</code> is originally recorded in image resize transformation, such as <code>DetResize</code>.</li> </ul> </li> </ol> <p>Example Code: DetBasePostprocess and DetDBPostprocess</p>"},{"location":"mkdocs/customize_postprocess/#recognition-postprocessing-api-protocols","title":"Recognition Postprocessing API Protocols","text":"<ol> <li> <p>class  <code>__init__()</code> should support the follow args:         - character_dict_path         - use_space_char         - blank_at_last         - lower     Please see the API docs in RecCTCLabelDecode for argument illustration.</p> </li> <li> <p><code>__call__()</code> method:</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: det_res as a dictionary with the following keys</p> <ul> <li><code>texts</code> (List[str]): list of preditected text string</li> <li><code>confs</code> (List[float]): confidence of each prediction</li> </ul> </li> </ul> </li> </ol> <p>Example code: RecCTCLabelDecode</p>"},{"location":"mkdocs/license/","title":"License.md","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"mkdocs/modelzoo/","title":"Model Zoo","text":"model type dataset fscore(detection)/accuracy(recognition) mindocr recipe vanilla mindspore dbnet_mobilenetv3 detection icdar2015 77.28 config dbnet_resnet18 detection icdar2015 83.71 config dbnet_resnet50 detection icdar2015 84.99 config link dbnet_resnet50 detection msra-td500 85.03 config dbnet++_resnet50 detection icdar2015 86.60 config psenet_resnet152 detection icdar2015 82.06 config link east_resnet50 detection icdar2015 84.87 config link svtr_tiny recognition IC03,13,15,IIIT,etc 89.02 config crnn_vgg7 recognition IC03,13,15,IIIT,etc 82.03 config link crnn_resnet34_vd recognition IC03,13,15,IIIT,etc 84.45 config rare_resnet34_vd recognition IC03,13,15,IIIT,etc 85.19 config"},{"location":"mkdocs/online_inference/","title":"Python Online Inference","text":""},{"location":"mkdocs/online_inference/#mindocr-online-inference","title":"MindOCR Online Inference","text":"<p>About Online Inference: Online inference is to infer based on the native MindSpore framework by loading the model checkpoint file then running prediction with MindSpore APIs.</p> <p>Compared to offline inference (which is implemented in <code>deploy/py_infer</code> in MindOCR), online inferece does not require model conversion for target platforms and can run directly on the training devices (e.g. Ascend 910). But it requires installing the heavy AI framework and the model is not optimized for deployment.</p> <p>Thus, online inference is more suitable for demonstration and to visually evaluate model generalization ability on unseen data.</p>"},{"location":"mkdocs/online_inference/#dependency-and-installation","title":"Dependency and Installation","text":"Environment Version MindSpore &gt;=1.9 Python &gt;=3.7 <p>Supported platforms: Linux, MacOS, Windows (Not tested)</p> <p>Supported devices: CPU, GPU, and Ascend.</p> <p>Please clone MindOCR at first <pre><code>git clone https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>Then install the dependency by <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>For MindSpore(&gt;=1.9) installation, please follow the official installation instructions for the best fit of your machine.</p>"},{"location":"mkdocs/online_inference/#text-detection","title":"Text Detection","text":"<p>To run text detection on an input image or a directory containing multiple images, please execute</p> <pre><code>python tools/infer/text/predict_det.py  --image_dir {path_to_img or dir_to_imgs} --det_algorithm DB++\n</code></pre> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection result on img_108.jpg </p> <p>, where the saved txt file is as follows <pre><code>img_108.jpg [[[228, 440], [403, 413], [406, 433], [231, 459]], [[282, 280], [493, 252], [499, 293], [288, 321]], [[500, 253], [636, 232], [641, 269], [505, 289]], ...]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p> Visualization of text detection result on paper_sam.png </p> <p>, where the saved txt file is as follows <pre><code>paper_sam.png   [[[1161, 340], [1277, 340], [1277, 378], [1161, 378]], [[895, 335], [1152, 340], [1152, 382], [894, 378]], ...]\n</code></pre></p> <p>Notes: - For input images with high resolution, please set <code>--det_limit_side_len</code> larger, e.g., 1280. <code>--det_limit_type</code> can be set as \"min\" or \"max\", where \"min \" means limiting the image size to be at least  <code>--det_limit_side_len</code>, \"max\" means limiting the image size to be at most <code>--det_limit_side_len</code>.</p> <ul> <li> <p>For more argument illustrations and usage, please run <code>python tools/infer/text/predict_det.py -h</code> or view <code>tools/infer/text/config.py</code></p> </li> <li> <p>Currently, this script runs serially to avoid dynamic shape issue and achieve better performance.</p> </li> </ul>"},{"location":"mkdocs/online_inference/#supported-detection-algorithms-and-networks","title":"Supported Detection Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | DB  | dbnet_resnet50 | English |   | DB++ | dbnetpp_resnet50 | English |   | DB_MV3 | dbnet_mobilenetv3 | English |   | PSE | psenet_resnet152 | English |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_det.py</code>.</p>"},{"location":"mkdocs/online_inference/#text-recognition","title":"Text Recognition","text":"<p>To run text recognition on an input image or a directory containing multiple images, please execute</p> <p><pre><code>python tools/infer/text/predict_rec.py  --image_dir {path_to_img or dir_to_imgs} --rec_algorithm CRNN\n</code></pre> After running, the inference results will be saved in <code>{args.draw_img_save_dir}/rec_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <ul> <li>English text recognition</li> </ul> <p> </p> <p>  word_1216.png  </p> <p> </p> <p>  word_1217.png  </p> <p>Recognition results: <pre><code>word_1216.png   coffee\nword_1217.png   club\n</code></pre></p> <ul> <li>Chinese text recognition:</li> </ul> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>Recognition results: <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre></p> <p>Notes: - For more argument illustrations and usage, please run <code>python tools/infer/text/predict_rec.py -h</code> or view <code>tools/infer/text/config.py</code> - Both batch-wise and single-mode inference are supported. Batch mode is enabled by default for better speed. You can set the batch size via <code>--rec_batch_size</code>. You can also run in single-mode by set <code>--det_batch_mode</code> False, which may improve accuracy if the text length varies a lot.</p>"},{"location":"mkdocs/online_inference/#supported-recognition-algorithms-and-networks","title":"Supported Recognition Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | CRNN | crnn_resnet34 | English |   | RARE | rare_resnet34 | English |   | SVTR | svtr_tiny | English|   | CRNN_CH | crnn_resnet34_ch | Chinese |   | RARE_CH | rare_resnet34_ch | Chinese |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_rec.py</code></p> <p>Currently, space char recognition is not supported for the listed models. We will support it soon.</p>"},{"location":"mkdocs/online_inference/#text-detection-and-recognition-concatenation","title":"Text Detection and Recognition Concatenation","text":"<p>To run text spoting (i.e., detect all text regions then recognize each of them) on an input image or multiple images in a directory, please run:</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>Note: set <code>--visualize_output True</code> if you want to visualize the detection and recognition results on the input image.</p> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>,  where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection and recognition result on img_10.jpg  </p> <p>, where the saved txt file is as follows <pre><code>img_10.jpg  [{\"transcription\": \"residential\", \"points\": [[43, 88], [149, 78], [151, 101], [44, 111]]}, {\"transcription\": \"areas\", \"points\": [[152, 83], [201, 81], [202, 98], [153, 100]]}, {\"transcription\": \"when\", \"points\": [[36, 56], [101, 56], [101, 78], [36, 78]]}, {\"transcription\": \"you\", \"points\": [[99, 54], [143, 52], [144, 78], [100, 80]]}, {\"transcription\": \"pass\", \"points\": [[140, 54], [186, 50], [188, 74], [142, 78]]}, {\"transcription\": \"by\", \"points\": [[182, 52], [208, 52], [208, 75], [182, 75]]}, {\"transcription\": \"volume\", \"points\": [[199, 30], [254, 30], [254, 46], [199, 46]]}, {\"transcription\": \"your\", \"points\": [[164, 28], [203, 28], [203, 46], [164, 46]]}, {\"transcription\": \"lower\", \"points\": [[109, 25], [162, 25], [162, 46], [109, 46]]}, {\"transcription\": \"please\", \"points\": [[31, 18], [109, 20], [108, 48], [30, 46]]}]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p>  Visualization of text detection and recognition result on web_cvpr.png  </p> <p>, where the saved txt file is as follows</p> <pre><code>web_cvpr.png    [{\"transcription\": \"canada\", \"points\": [[430, 148], [540, 148], [540, 171], [430, 171]]}, {\"transcription\": \"vancouver\", \"points\": [[263, 148], [420, 148], [420, 171], [263, 171]]}, {\"transcription\": \"cvpr\", \"points\": [[32, 69], [251, 63], [254, 174], [35, 180]]}, {\"transcription\": \"2023\", \"points\": [[194, 44], [256, 45], [255, 72], [194, 70]]}, {\"transcription\": \"june\", \"points\": [[36, 45], [110, 44], [110, 70], [37, 71]]}, {\"transcription\": \"1822\", \"points\": [[114, 43], [190, 45], [190, 70], [113, 69]]}]\n</code></pre> <p>Notes: 1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_system.py -h</code> or view <code>tools/infer/text/config.py</code></p>"},{"location":"mkdocs/online_inference/#evaluation-of-the-inference-results","title":"Evaluation of the Inference Results","text":"<p>To infer on the whole ICDAR15 test set, please run: <pre><code>python tools/infer/text/predict_system.py --image_dir /path/to/icdar15/det/test_images  /\n                                          --det_algorithm {DET_ALGO}    /\n                                          --rec_algorithm {REC_ALGO}  /\n                                          --det_limit_type min  /\n                                          --det_limit_side_len 720\n</code></pre></p> <p>Note: Here we set<code>det_limit_type</code> as <code>min</code> for better performance, due to the input image in ICDAR15 is of high resolution (720x1280).</p> <p>After running, the results including image names, bounding boxes (<code>points</code>) and recognized texts (<code>transcription</code>) will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>. The format of prediction results is shown as follows.</p> <pre><code>img_1.jpg   [{\"transcription\": \"hello\", \"points\": [600, 150, 715, 157, 714, 177, 599, 170]}, {\"transcription\": \"world\", \"points\": [622, 126, 695, 129, 694, 154, 621, 151]}, ...]\nimg_2.jpg   [{\"transcription\": \"apple\", \"points\": [553, 338, 706, 318, 709, 342, 556, 362]}, ...]\n   ...\n</code></pre> <p>Prepare the ground truth file (in the same format as above), which can be obtained from the dataset conversion script in <code>tools/dataset_converters</code>, and run the following command to evaluate the prediction results.</p> <pre><code>python deploy/eval_utils/eval_pipeline.py --gt_path path/to/gt.txt --pred_path path/to/system_results.txt\n</code></pre> <p>Evaluation of the text spotting inference results on Ascend 910 with MindSpore 2.0rc1 are shown as follows.</p>   | Det. Algorithm| Rec. Algorithm |  Dataset     | Accuracy(%) | FPS (imgs/s) | |---------|----------|--------------|---------------|-------| | DBNet   | CRNN    | ICDAR15 | 57.82 | 4.86 | | PSENet  | CRNN    | ICDAR15 | 47.91 | 1.65| | PSENet (det_limit_side_len=1472 )  | CRNN    | ICDAR15 | 55.51 | 0.44 | | DBNet++   | RARE | ICDAR15 | 59.17  | 3.47 | | DBNet++   | SVTR | ICDAR15 | 64.42  | 2.49 |  <p>Notes: 1. Currently, online inference pipeline is not optimized for efficiency, thus FPS is only for comparison between models. If FPS is your highest priority, please refer to Inference on Ascend 310, which is much faster. 2. Unless extra inidication, all experiments are run with <code>--det_limit_type</code>=\"min\" and <code>--det_limit_side</code>=720. 3. SVTR is run in mixed precision mode (amp_level=O2) since it is optimized for O2.</p>"},{"location":"mkdocs/online_inference/#argument-list","title":"Argument List","text":"<p>All CLI argument definition can be viewed via <code>python tools/infer/text/predict_system.py -h</code> or reading <code>tools/infer/text/config.py</code>.</p>"},{"location":"mkdocs/online_inference/#developer-guide-how-to-add-a-new-model-for-inference","title":"Developer Guide - How to Add a New Model for Inference","text":""},{"location":"mkdocs/online_inference/#preprocessing","title":"Preprocessing","text":"<p>The optimal preprocessing strategy can vary from model to model, especially for the resize setting (keep_ratio, padding, etc). We define the preprocessing pipeline for each model in <code>tools/infer/text/preprocess.py</code> for different tasks.</p> <p>If you find the default preprocessing pipeline or hyper-params does not meet the network requirement, please extend by changing the if-else conditions or adding a new key-value pair to the <code>optimal_hparam</code> dict in <code>tools/infer/text/preprocess.py</code>, where key is the algorithm name and the value is the suitable hyper-param setting for the target network inference.</p>"},{"location":"mkdocs/online_inference/#network-inference","title":"Network Inference","text":"<p>Supported alogirhtms and their corresponding network names (which can be checked by using the <code>list_model()</code> API) are defined in the <code>algo_to_model_name</code> dict in <code>predict_det.py</code> and <code>predict_rec.py</code>.</p> <p>To add a new detection model for inference, please add a new key-value pair to <code>algo_to_model_name</code> dict, where the key is an algorithm name and the value is the corresponding network name registered in <code>mindocr/models/{your_model}.py</code>.</p> <p>By default, model weights will be loaded from the pro-defined URL in <code>mindocr/models/{your_model}.py</code>. If you want to load a local checkpoint instead, please set <code>--det_model_dir</code> or <code>--rec_model_dir</code> to the path of your local checkpoint or the directory containing a model checkpoint.</p>"},{"location":"mkdocs/online_inference/#postproprocess","title":"Postproprocess","text":"<p>Similar to preprocessing, the postprocessing method for each algorithm can vary. The postprocessing method for each algorithm is defined in <code>tools/infer/text/postprocess.py</code>.</p> <p>If you find the default postprocessing method or hyper-params does not meet the model need, please extend the if-else conditions or add a new key-value pair  to the <code>optimal_hparam</code> dict in <code>tools/infer/text/postprocess.py</code>, where the key is an algorithm name and the value is the hyper-param setting.</p>"},{"location":"reference/api_doc/","title":"API doc","text":"<p>coming soon...</p>"},{"location":"tutorials/advanced_train/","title":"Advanced Training","text":""},{"location":"tutorials/advanced_train/#tricks-gradient-accumulation-gradient-clipping-and-ema","title":"Tricks: Gradient Accumulation, Gradient Clipping, and EMA","text":"<p>All the training tricks can be configured in the model config files. After setting, please run <code>tools/train.py</code> script to initiate training.</p> <p>Example Yaml Config</p> <pre><code>train:\ngradient_accumulation_steps: 2\nclip_grad: True\nclip_norm: 5.0\nema: True\nema_decay: 0.9999\n</code></pre>"},{"location":"tutorials/advanced_train/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradient accumulation is an effective way to address  memory limitation issue and allow training with large global batch size.</p> <p>To enable it, set <code>train.gradient_accumulation_steps</code> to values larger than 1 in yaml config.</p> <p>The equivalent global batch size would be <code>global_batch_size = batch_size * num_devices * gradient_accumulation_steps</code></p>"},{"location":"tutorials/advanced_train/#gradient-clipping","title":"Gradient Clipping","text":"<p>Gradient clipping is a method to address gradient explosion/overflow problem and stabilize model convergence.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code> and optionally adjust the norm value in <code>train.clip_norm</code>.</p>"},{"location":"tutorials/advanced_train/#ema","title":"EMA","text":"<p>Exponential Moving Average (EMA) can be viewed as a model ensemble method that smooths the model weights. It can help stabilize model convergence in training and usually leads to better model performance.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code>. You may also adjust <code>train.ema_decay</code> to control the decay rate.</p>"},{"location":"tutorials/advanced_train/#resume-training","title":"Resume Training","text":"<p>Resuming training is useful when the training was interrupted unexpectedly.</p> <p>To resume training, set <code>model.resume</code> to <code>True</code> in the yaml config as follows: <pre><code>model:\nresume: True\n</code></pre></p> <p>By default, it will resume from the \"train_resume.ckpt\" checkpoint file located in the directory specified in <code>train.ckpt_save_dir</code>.</p> <p>If you want to use another checkpoint to resume from, specify the checkpoint path in <code>resume</code> as follows:</p> <pre><code>model:\nresume: /some/path/to/train_resume.ckpt\n</code></pre>"},{"location":"tutorials/advanced_train/#training-on-openi-cloud-platform","title":"Training on OpenI Cloud Platform","text":"<p>Please refer to the MindOCR OpenI Training Guideline</p>"},{"location":"tutorials/distribute_train/","title":"Distributed parallel training","text":"<p>This document provides a tutorial on distributed parallel training. There are two ways to train on the Ascend AI processor: by running scripts with OpenMPI or configuring <code>RANK_TABLE_FILE</code> for training. On GPU processors, scripts can be run with OpenMPI for training.</p>"},{"location":"tutorials/distribute_train/#run-scripts-with-openmpi","title":"Run scripts with OpenMPI","text":"<p>MindSpore supports scripts execution with OpenMPI's <code>mpirun</code> on Ascend hardware platform. Users can refer to DBNet Readme for more information on training. The following is the command use case:</p> <pre><code># n is the number of GPUs/NPUs used in training\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the command.</p>"},{"location":"tutorials/distribute_train/#configure-rank_table_file-for-training","title":"Configure RANK_TABLE_FILE for training","text":"<p>Before using this method for distributed training, it is necessary to create an HCCL configuration file in json format, i.e. generate RANK_TABLE_FILE. The following is the command to generate the corresponding configuration file for 8 devices (for more information please refer to HCCL tools):</p> <p><pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> This command produces the following output file: <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre></p> <p>An example of the content in <code>hccl_8p_10234567_127.0.0.1.json</code>:</p> <pre><code>{\n\"version\": \"1.0\",\n\"server_count\": \"1\",\n\"server_list\": [\n{\n\"server_id\": \"127.0.0.1\",\n\"device\": [\n{\n\"device_id\": \"0\",\n\"device_ip\": \"192.168.100.101\",\n\"rank_id\": \"0\"\n},\n{\n\"device_id\": \"1\",\n\"device_ip\": \"192.168.101.101\",\n\"rank_id\": \"1\"\n},\n{\n\"device_id\": \"2\",\n\"device_ip\": \"192.168.102.101\",\n\"rank_id\": \"2\"\n},\n{\n\"device_id\": \"3\",\n\"device_ip\": \"192.168.103.101\",\n\"rank_id\": \"3\"\n},\n{\n\"device_id\": \"4\",\n\"device_ip\": \"192.168.100.100\",\n\"rank_id\": \"4\"\n},\n{\n\"device_id\": \"5\",\n\"device_ip\": \"192.168.101.100\",\n\"rank_id\": \"5\"\n},\n{\n\"device_id\": \"6\",\n\"device_ip\": \"192.168.102.100\",\n\"rank_id\": \"6\"\n},\n{\n\"device_id\": \"7\",\n\"device_ip\": \"192.168.103.100\",\n\"rank_id\": \"7\"\n}\n],\n\"host_nic_ip\": \"reserve\"\n}\n],\n\"status\": \"completed\"\n}\n</code></pre> <p>Then start the training by running the following command:</p> <pre><code>bash ascend8p.sh\n</code></pre> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the command.</p> <p>Here is an example of the <code>ascend8p.sh</code> script for CRNN training:</p> <p><pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${DEVICE_NUM}; i++)); do\nexport DEVICE_ID=$i\nexport RANK_ID=$i\necho \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\nif [ $i -eq 0 ]; then\necho 'i am 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\nelse\necho 'not 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\nfi\ndone\n</code></pre> When training other models, simply replace the yaml config file path in the script, i.e. <code>path/to/model_config.yaml</code>.</p> <p>After the training has started, and you can find the training log <code>train.log</code> in the project root directory.</p>"},{"location":"tutorials/training_detection_custom_dataset/","title":"Training Detection Network with Custom Datasets","text":"<p>This document provides tutorials on how to train text detection networks using custom datasets.</p> <ul> <li>Training Detection Network with Custom Datasets</li> <li>1. Dataset preperation<ul> <li>1.1 Preparing Training Data</li> <li>1.2 Preparing Validation Data</li> </ul> </li> <li>2. Configuration File Preperation<ul> <li>2.1 Configure train/validation datasets</li> <li>2.2 Configure train/validation transform pipelines</li> <li>2.3 Configure the model architecture</li> <li>2.4 Configure training hyperparameters</li> </ul> </li> <li>3. Model Training, Evaluation, and Inference<ul> <li>3.1 Training</li> <li>3.2 Evaluation</li> <li>3.3 Inference</li> <li>3.3.1 Environment Preparation</li> <li>3.3.2 Model Conversion</li> <li>3.3.3 Inference (Python)</li> </ul> </li> </ul>"},{"location":"tutorials/training_detection_custom_dataset/#1-dataset-preperation","title":"1. Dataset preperation","text":"<p>Currently, MindOCR detection network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images, text bounding boxes, and transcriptions. An example of the target file format is: <pre><code>img_1.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>It is read by DetDataset. If your dataset is not in the same format as the example format, see instructions on how convert different datasets' annotations into the supported format.</p> <ul> <li><code>SynthTextDataset</code>: A file format provided by SynthText800k. More details about this dataset can be found here. The annotation file is a <code>.mat</code> file consisting of <code>imnames</code>(image names), <code>wordBB</code>(word-level bounding-boxes), <code>charBB</code>(character-level bounding boxes), and <code>txt</code> (text strings). It is read by SynthTextDataset. Users can take <code>SynthTextDataset</code> as a reference to write their custom dataset class.</li> </ul> <p>We recommend users to prepare text detection datasets in the <code>Common Dataset</code> format, and then use DetDataset to load the data. The following tutorials further explain on the detailed steps.</p>"},{"location":"tutorials/training_detection_custom_dataset/#11-preparing-training-data","title":"1.1 Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file <code>train_det.txt</code> at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # A list of dictionaries\nimg_1.jpg\\t[{\"transcription\": \"Genaxis Theatre\", \"points\": [[377, 117], [463, 117], [465, 130], [378, 130]]}, {\"transcription\": \"[06]\", \"points\": [[493, 115], [519, 115], [519, 131], [493, 131]]}, {...}]\nimg_2.jpg\\t[{\"transcription\": \"guardian\", \"points\": [[642, 250], [769, 230], [775, 255], [648, 275]]}]\n...\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- train_det.txt\n    |- training\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#12-preparing-validation-data","title":"1.2 Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file <code>val_det.txt</code> at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- val_det.txt\n    |- validation\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#2-configuration-file-preperation","title":"2. Configuration File Preperation","text":"<p>To prepare the corresponding configuration file, users should specify the directories for the training and validation datasets.</p>"},{"location":"tutorials/training_detection_custom_dataset/#21-configure-trainvalidation-datasets","title":"2.1 Configure train/validation datasets","text":"<p>Please select <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: train_det.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\ndataset:\ntype: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: val_det.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#22-configure-trainvalidation-transform-pipelines","title":"2.2 Configure train/validation transform pipelines","text":"<p>Take the <code>train.dataset.transform_pipeline</code> field in the <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example. It specifies a set of transformations applied on the image or labels to generate the data as the model inputs or the loss function inputs. These transform functions are defined in <code>mindocr/data/transforms</code>.</p> <pre><code>...\ntrain:\n...\ndataset:\ntransform_pipeline:\n- DecodeImage:\nimg_mode: RGB\nto_float32: False\n- DetLabelEncode:\n- RandomColorAdjust:\nbrightness: 0.1255  # 32.0 / 255\nsaturation: 0.5\n- RandomHorizontalFlip:\np: 0.5\n- RandomRotate:\ndegrees: [ -10, 10 ]\nexpand_canvas: False\np: 1.0\n- RandomScale:\nscale_range: [ 0.5, 3.0 ]\np: 1.0\n- RandomCropWithBBox:\nmax_tries: 10\nmin_crop_ratio: 0.1\ncrop_size: [ 640, 640 ]\np: 1.0\n- ValidatePolygons:\n- ShrinkBinaryMap:\nmin_text_size: 8\nshrink_ratio: 0.4\n- BorderMap:\nshrink_ratio: 0.4\nthresh_min: 0.3\nthresh_max: 0.7\n- NormalizeImage:\nbgr_to_rgb: False\nis_hwc: True\nmean: imagenet\nstd: imagenet\n- ToCHWImage:\n...\n</code></pre> <ul> <li> <p><code>DecodeImage</code> and <code>DetLabelEncode</code>: the two transform functions parse the strings in <code>train_det.txt</code> file, load both the image and the labels, and save them as a dictionary;</p> </li> <li> <p><code>RandomColorAdjust</code>,  <code>RandomHorizontalFlip</code>, <code>RandomRotate</code>, <code>RandomScale</code>, and <code>RandomCropWithBBox</code>: these transform functions perform typical image augmentation operations. Except for <code>RandomColorAdjust</code>, all other functions alter the bounding box labels;</p> </li> <li> <p><code>ValidatePolygons</code>: it filters out the bounding boxes that are outside of the image due to previous augmentations;</p> </li> <li> <p><code>ShrinkBinaryMap</code> and <code>BorderMap</code>: they make the binary map and the border map needed for dbnet training;</p> </li> <li> <p><code>NormalizeImage</code>: it normalizes the image by the mean and variance of the ImageNet dataset;</p> </li> <li> <p><code>ToCHWImage</code>: it changes <code>HWC</code> images to <code>CHW</code> images.</p> </li> </ul> <p>For validation transform pipeline, all image augmentation operations are removed, and replaced by a simple resize function:</p> <p><pre><code>eval:\ndataset\ntransform_pipeline:\n- DecodeImage:\nimg_mode: RGB\nto_float32: False\n- DetLabelEncode:\n- DetResize:\ntarget_size: [ 736, 1280 ]\nkeep_ratio: False\nforce_divisable: True\n- NormalizeImage:\nbgr_to_rgb: False\nis_hwc: True\nmean: imagenet\nstd: imagenet\n- ToCHWImage:\n</code></pre> More tutorials on transform functions can be found in the transform tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#23-configure-the-model-architecture","title":"2.3 Configure the model architecture","text":"<p>Although different models have different architectures, MindOCR formulates them as a general three-stage architecture: <code>[backbone]-&gt;[neck]-&gt;[head]</code>. Take <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example:</p> <p><pre><code>model:\ntype: det\ntransform: null\nbackbone:\nname: det_resnet50  # Only ResNet50 is supported at the moment\npretrained: True    # Whether to use weights pretrained on ImageNet\nneck:\nname: DBFPN         # FPN part of the DBNet\nout_channels: 256\nbias: False\nuse_asf: False      # Adaptive Scale Fusion module from DBNet++ (use it for DBNet++ only)\nhead:\nname: DBHead\nk: 50               # amplifying factor for Differentiable Binarization\nbias: False\nadaptive: True      # True for training, False for inference\n</code></pre>  The backbone, neck, and head modules are all defined under <code>mindocr/models/backbones</code>, <code>mindocr/models/necks</code>, and <code>mindocr/models/heads</code>.</p>"},{"location":"tutorials/training_detection_custom_dataset/#24-configure-training-hyperparameters","title":"2.4 Configure training hyperparameters","text":"<p>Some training hyperparameters in <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> are defined as follows: <pre><code>metric:\nname: DetMetric\nmain_indicator: f-score\n\nloss:\nname: L1BalancedCELoss\neps: 1.0e-6\nl1_scale: 10\nbce_scale: 5\nbce_replace: bceloss\n\nscheduler:\nscheduler: polynomial_decay\nlr: 0.007\nnum_epochs: 1200\ndecay_rate: 0.9\nwarmup_epochs: 3\n\noptimizer:\nopt: SGD\nfilter_bias_and_bn: false\nmomentum: 0.9\nweight_decay: 1.0e-4\n</code></pre> It uses <code>SGD</code> optimizer (in <code>mindocr/optim/optim.factory.py</code>) and <code>polynomial_decay</code> (in <code>mindocr/scheduler/scheduler_factory.py</code>) as the learning scheduler. The loss function is <code>L1BalancedCELoss</code> (in <code>mindocr/losses/det_loss.py</code>) and the evaluation metric is <code>DetMetric</code> ( in <code>mindocr/metrics/det_metrics.py</code>).</p>"},{"location":"tutorials/training_detection_custom_dataset/#3-model-training-evaluation-and-inference","title":"3. Model Training, Evaluation, and Inference","text":"<p>When all configurations have been specified, users can start training their models. MindOCR supports evaluation and inference after the model is trained.</p>"},{"location":"tutorials/training_detection_custom_dataset/#31-training","title":"3.1 Training","text":"<ul> <li>Standalone training</li> </ul> <p>In standalone training, the model is trained on a single device (<code>device:0</code> by default). Users should set <code>system.distribute</code> in yaml config file to be <code>False</code>, and the <code>system.device_id</code> to the target device id if users want to run this model on a device other than <code>device:0</code>.</p> <p>Take <code>configs/det/dbnet/db_r50_icdar15.yaml</code> as an example, the training command is:</p> <pre><code>python tools/train.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <ul> <li>Distributed training</li> </ul> <p>In distributed training, <code>distribute</code> in yaml config file should be True. On both GPU and Ascend devices, users can use <code>mpirun</code> to launch distributed training. For example, using <code>device:0</code> and <code>device:1</code> to train:</p> <pre><code># n is the number of GPUs/NPUs\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Sometimes, users may want to specify the device ids to run distributed training, for example, <code>device:2</code> and <code>device:3</code>.</p> <p>On GPU devices, before running the <code>mpirun</code> command above, users can run the following command:</p> <pre><code>export CUDA_VISIBLE_DEVICES=2,3\n</code></pre> <p>On Ascend devices, users should create a <code>rank_table.json</code> like this: <pre><code>Copy{\n\"version\": \"1.0\",\n\"server_count\": \"1\",\n\"server_list\": [\n{\n\"server_id\": \"10.155.111.140\",\n\"device\": [\n{\"device_id\": \"2\",\"device_ip\": \"192.3.27.6\",\"rank_id\": \"2\"},\n{\"device_id\": \"3\",\"device_ip\": \"192.4.27.6\",\"rank_id\": \"3\"}],\n\"host_nic_ip\": \"reserve\"\n}\n],\n\"status\": \"completed\"\n}\n</code></pre> To get the <code>device_ip</code> of the target device, run <code>cat /etc/hccn.conf</code> and look for the value of <code>address_x</code>, which is the ip address. More details can be found in distributed training tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#32-evaluation","title":"3.2 Evaluation","text":"<p>To evaluate the accuracy of the trained model, users can use <code>tools/eval.py</code>.</p> <p>Take standalone evaluation as an example. In the yaml config file, <code>system.distribute</code> should be <code>False</code>; the <code>eval.ckpt_load_path</code> should be the target ckpt path; <code>eval.dataset_root</code>, <code>eval.data_dir</code>, and <code>eval.label_file</code> should be correctly specified. Then the evaluation can be started by running:</p> <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>MindOCR also supports to specify the arguments in the command line, by running: <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml \\\n--opt eval.ckpt_load_path=\"/path/to/local_ckpt.ckpt\" \\\neval.dataset_root=\"/path/to/val_set/root\" \\\neval.data_dir=\"val_set/dir\"\\\neval.label_file=\"val_set/label\"\n</code></pre></p>"},{"location":"tutorials/training_detection_custom_dataset/#33-inference","title":"3.3 Inference","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend. Inference Tutorial gives detailed steps on how to run inference with MindOCR, which include mainly three steps: environment preparation, model conversion, and inference.</p>"},{"location":"tutorials/training_detection_custom_dataset/#331-environment-preparation","title":"3.3.1 Environment Preparation","text":"<p>Please refer to the environment installation for more information, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"tutorials/training_detection_custom_dataset/#332-model-conversion","title":"3.3.2 Model Conversion","text":"<p>Before runing infernence, users need to export a MindIR file from the trained checkpoint. MindSpore IR (MindIR) is a function-style IR based on graph representation. The MindIR filew stores the model structure and weight parameters needed for inference.</p> <p>Given the trained dbnet checkpoint file, user can use the following commands to export MindIR:</p> <pre><code>python tools/export.py --model_name dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n# or\npython tools/export.py --model_name configs/det/dbnet/db_r50_icdar15.yaml --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n</code></pre> <p>The <code>data_shape</code> is the model input shape of height and width for MindIR file. It may change when the model is changed.</p> <p>Please refer to the Conversion Tutorial for more details about model conversion.</p>"},{"location":"tutorials/training_detection_custom_dataset/#333-inference-python","title":"3.3.3 Inference (Python)","text":"<p>After model conversion, the <code>output.mindir</code> is obtained. Users can go to the <code>deploy/py_infer</code> directory, and use the following command for inference:</p> <pre><code>python infer.py \\\n--input_images_dir=/your_path_to/test_images \\\n--device=Ascend \\\n--device_id=0 \\\n--det_model_path=your_path_to/output.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--backend=lite \\\n--res_save_dir=results_dir\n</code></pre> <p>Please refer to the Inference Tutorials chapter <code>4.1 Command example</code> on more examples of inference commands.</p>"},{"location":"tutorials/training_on_openi/","title":"Training on openi.md","text":""},{"location":"tutorials/training_on_openi/#mindocr-openi-training-guideline","title":"MindOCR OpenI Training Guideline","text":"<p>This tutorial introduces the training method of MindOCR using the OpenI platform.</p>"},{"location":"tutorials/training_on_openi/#clone-the-project","title":"Clone the project","text":"<p>Click on the plus sign and choose to New Migration to clone MindOCR from GitHub to the Openi platform.</p> <p>Enter the MindOCR git url: https://github.com/mindspore-lab/mindocr.git</p>"},{"location":"tutorials/training_on_openi/#prepare-dataset","title":"Prepare Dataset","text":"<p>You can upload your own dataset or associate the project with existing datasets on the platform.</p> <p>Uploading personal datasets requires setting the available clusters to NPU.</p>"},{"location":"tutorials/training_on_openi/#prepare-pretrained-model-optional","title":"Prepare pretrained model (optional)","text":"<p>To upload pre-trained weights, choose the Model tab of your repository.</p> <p>During the import of a local model, set the model's framework to MindSpore.</p>"},{"location":"tutorials/training_on_openi/#new-training-task","title":"New Training Task","text":"<p>Select Training Task -&gt; New Training Task in the Cloudbrain tab.</p> <p>In computing resources choose Ascend NPU.</p> <p>Set the training entry point (Start File) and add run parameters.</p> <ul> <li>To load pre-trained weights, choose the uploaded previously model file in the Select Model field and add <code>ckpt_dir</code> to the run parameters. The <code>ckpt_dir</code> parameter must have the following path: <code>/cache/*.ckpt</code>, where <code>*</code> is the model's file name.</li> <li>In the AI engine, it is necessary to select MindSpore version 1.9 or higher, and set the start file to <code>tools/train.py</code></li> <li>:warning: It is necessary to set <code>enable_modelarts</code> to <code>True</code> in the run parameters.</li> <li>The model's architecture is specified in the <code>config</code> file set in the run parameters. The prefix of the file is always <code>/home/work/user-job-dir/run-version-number</code>, where <code>run-version-number</code> for the newly created training task is usually <code>V0001</code>.</li> </ul>"},{"location":"tutorials/training_on_openi/#modify-existing-training-tasks","title":"Modify existing training tasks","text":"<p>Click the modify button of an existing training task to modify its parameters and run a new training task.</p> <p>Note: <code>run-version-number</code> will change to Parents Version (current run version number) + 1, e.g. <code>V0002</code>.</p>"},{"location":"tutorials/training_on_openi/#view-training-status","title":"View training status","text":"<p>Select a training task to view configuration information, logs, resource occupancy, and download model weights.</p>"},{"location":"tutorials/training_on_openi/#reference","title":"Reference","text":"<p>[1] Modified from https://github.com/mindspore-lab/mindyolo/blob/master/tutorials/cloud/openi.md</p>"},{"location":"tutorials/training_recognition_custom_dataset/","title":"Training Recognition Network with Custom Datasets","text":"<p>This document provides tutorials on how to train recognition networks using custom datasets, including the training of recognition networks in Chinese and English languages.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#dataset-preperation","title":"Dataset preperation","text":"<p>Currently, MindOCR recognition network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images and text files. It is read by RecDataset. - <code>LMDB Dataset</code>: A file format provided by LMDB. It is read by LMDBDataset.</p> <p>The following tutorials take the use of the <code>Common Dataset</code> file format as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-training-data","title":"Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # Corresponding label\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-validation-data","title":"Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#dictionary-preperation","title":"Dictionary Preperation","text":"<p>To train recognition networks for different languages, users need to configure corresponding dictionaries. Only characters that exist in the dictionary will be correctly predicted by the model. MindOCR currently provides three dictionaries, corresponding to Default, Chinese and English respectively. - <code>Default Dictionary</code>\uff1aincludes lowercase English letters and numbers only. If users do not configure the dictionay, this one will be used by default. - <code>English Dictionary</code>\uff1aincludes uppercase and lowercase English letters, numbers and punctuation marks, it is place at <code>mindocr/utils/dict/en_dict.txt</code>. - <code>Chinese Dictionary</code>\uff1aincludes commonly used Chinese characters, uppercase and lowercase English letters, numbers, and punctuation marks, it is placed at <code>mindocr/utils/dict/ch_dict.txt</code>.</p> <p>Currently, MindOCR does not provide a dictionary configuration for other languages. This feature will be released in a upcoming version.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configuration-file-preperation","title":"Configuration File Preperation","text":"<p>To configure the corresponding configuration file for a specific network architecture, users need to provide the necessary settings. As an example, we can take CRNN (with backbone Resnet34) as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-english-model","title":"Configure an English Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n...\n</code></pre> <p>And also modify the corresponding dictionary location to the the English dictionary path.</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>To use the complete English dictionary, users need to modify the <code>common:num_classes</code> attribute in the corresponding configuration file, as the initial configuration file\u2019s dictionary only includes lowercase English and numbers.</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 95                                        # The number is equal to the number of dictionary characters plus 1\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 96                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\nuse_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-english-dictionary","title":"Configuring a custom English dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-chinese-model","title":"Configure an Chinese Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34_ch.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\ndataset:\ntype: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\ndataset_root: dir/to/data/                                        # Root directory of the data\ndata_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\nlabel_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n...\n</code></pre> <p>And also modify the corresponding dictionary location to the the Chinese dictionary path.</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 6625                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\nuse_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-chinese-dictionary","title":"Configuring a custom Chinese dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-training","title":"Model Training","text":"<p>When all datasets and configuration files are ready, users can start training models with their own data. As each model has different training methods, users can refer to the corresponding model introduction documentation for the Model Training and Model Evaluation sections. Here, we will only use CRNN as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-pre-trained-model","title":"Preparing Pre-trained Model","text":"<p>Users can use the pre-trained models that we provide as a starting point for training. Pre-trained models can often improve the convergence speed and even accuracy of the model. Taking the Chinese model as an example, the url for the pre-trained model that we provide is https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt. Users only need to add <code>model.pretrained</code> with the corresponding url in the configuration file as follows:</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\npretrained: https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users encounter network issues, they can try downloading the pre-trained model to their local machine in advance, and then change <code>model.pretrained</code> to the local path as follows:</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\npretrained: /local_path_to_the_ckpt/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users do not need to use the pre-trained model, they can simply delete <code>model.pretrained</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#start-training","title":"Start Training","text":""},{"location":"tutorials/training_recognition_custom_dataset/#distributed-training","title":"Distributed Training","text":"<p>In the case of a large amount of data, we recommend that users use distributed training. For distributed training across multiple Ascend 910 devices or GPU devices, please modify the configuration parameter <code>system.distribute</code> to True, for example:</p> <pre><code># To perform distributed training on 4 GPU/Ascend devices\nmpirun -n 4 python tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#single-device-training","title":"Single Device Training","text":"<p>If you want to train or fine-tune the model on a smaller dataset without distributed training, please modify the configuration parameter <code>system.distribute</code> to <code>False</code> and run:</p> <pre><code># Training on single CPU/GPU/Ascend devices\npython tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>The training results (including checkpoint, performance of each epoch, and curve graph) will be saved in the directory configured by the <code>train.ckpt_save_dir</code> parameter in the YAML configuration file, which is set to <code>./tmp_rec</code> by default.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#resuming-training-from-checkpoint","title":"Resuming Training From Checkpoint","text":"<p>If users expect to load the optimizer, learning rate, and other information of the model while starting or continue training, they can add <code>model.resume</code> to the corresponding local model path in the configuration file as follows, and start training:</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\nresume: /local_path_to_the_ckpt/model.ckpt\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Some models (including CRNN, RARE, SVTR) support mixed precision training to accelerate training speed. Users can try setting the <code>system.amp_level</code> in the configuration file to <code>O2</code> to start mixed precision training, as shown in the following example:</p> <pre><code>system:\nmode: 0\ndistribute: True\namp_level: O2  # Mixed precision training\namp_level_infer: O2\nseed: 42\nlog_interval: 100\nval_while_train: True\ndrop_overflow_update: False\nckpt_max_keep: 5\n...\n</code></pre> <p>To disable mixed precision training, change <code>system.amp_level</code> to <code>O0</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-evaluation","title":"Model Evaluation","text":"<p>To evaluate the accuracy of a trained model, users can use <code>tools/eval.py</code>. Please set the <code>ckpt_load_path</code> parameter in the <code>eval</code> section of the configuration file to the file path of the model checkpoint, and set <code>system.distribute</code> to False, as shown below:</p> <pre><code>system:\ndistribute: False # During evaluation stage, set to False\n...\neval:\nckpt_load_path: /local_path_to_the_ckpt/model.ckpt\n</code></pre> <p>and run</p> <pre><code>python tools/eval.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>You will get a model evaluation result similar to the following:</p> <pre><code>2023-06-16 03:41:20,237:INFO:Performance: {'acc': 0.821939, 'norm_edit_distance': 0.917264}\n</code></pre> <p>The number corresponding to <code>acc</code> is the accuracy of the model.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-inference","title":"Model Inference","text":"<p>Users can quickly obtain the inference results of the model by using the inference script. First, place the images in the same folder, and then execute:</p> <pre><code>python tools/infer/text/predict_rec.py --image_dir {dir_to_your_image_data} --rec_algorithm CRNN_CH --draw_img_save_dir inference_results\n</code></pre> <p>The results will be stored in <code>draw_img_save_dir/rec_results.txt</code>. Here are some examples:</p> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>You will get inference results similar to the following:</p> <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre>"},{"location":"tutorials/transform_tutorial/","title":"Transformation Tutorial","text":""},{"location":"tutorials/transform_tutorial/#mechanism","title":"Mechanism","text":"<ol> <li>Each transformation is a class with a callable function. An example is as follows</li> </ol> <pre><code>class ToCHWImage(object):\n\"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>The input for transformation is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>The transformation api should have clarify the required keys in input and the modified or/and added keys in output the data dict.</p> </li> </ol> <p>Available transformations can be checked in <code>mindocr/data/transforms/*_transform.py</code></p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#text-detection","title":"Text detection","text":""},{"location":"tutorials/transform_tutorial/#1-load-image-and-annotations","title":"1. Load image and annotations","text":""},{"location":"tutorials/transform_tutorial/#preparation","title":"Preparation","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"tutorials/transform_tutorial/#decode-the-image-decodeimage","title":"Decode the image  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"tutorials/transform_tutorial/#detlabelencode","title":"DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#2-image-and-annotation-processingaugmentation","title":"2. Image and annotation processing/augmentation","text":""},{"location":"tutorials/transform_tutorial/#randomcrop-eastrandomcropdata","title":"RandomCrop - EastRandomCropData","text":"<pre><code>from mindocr.data.transforms.general_transforms import RandomCropWithBBox\nimport copy\n\n#crop_data = det_transforms.EastRandomCropData(size=(640, 640))\ncrop_data = RandomCropWithBBox(crop_size=(640, 640))\n\nshow_img(data['image'])\nfor i in range(2):\n    data_cache = copy.deepcopy(data)\n    data_cropped = crop_data(data_cache)\n\n    res_crop = draw_boxes(data_cropped['image'], data_cropped['polys'])\n    show_img(res_crop)\n</code></pre>"},{"location":"tutorials/transform_tutorial/#colorjitter","title":"ColorJitter","text":"<pre><code>random_color_adj = general_transforms.RandomColorAdjust(brightness=0.4, saturation=0.5)\n\ndata_cache = copy.deepcopy(data)\n#data_cache['image'] = data_cache['image'][:,:, ::-1]\ndata_adj = random_color_adj(data_cache)\n#print(data_adj)\nshow_img(data_adj['image'], is_bgr_img=True)\n</code></pre>"},{"location":"tutorials/yaml_configuration/","title":"Configuration parameter description","text":"<ul> <li>system</li> <li>common</li> <li>model</li> <li>postprocess</li> <li>metric</li> <li>loss</li> <li>scheduler, optimizer, loss_scaler</li> <li>scheduler</li> <li>optimizer</li> <li>loss_scaler</li> <li>train, eval</li> <li>train</li> <li>eval</li> </ul> <p>This document takes <code>configs/rec/crnn/crnn_icdar15.yaml</code> as an example to describe the usage of parameters in detail.</p>"},{"location":"tutorials/yaml_configuration/#1-environment-parameters-system","title":"1. Environment parameters (system)","text":"Parameter Description Default Optional Values \u200b\u200b Remarks mode Mindspore running mode (static graph/dynamic graph) 0 0 / 1 0: means running in GRAPH_MODE mode; 1: PYNATIVE_MODE mode distribute Whether to enable parallel training True True / False \\ device_id Specify the device id while standalone training 7 The ids of all devices in the server Only valid when distribute=False (standalone training) and environment variable 'DEVICE_ID' is NOT set. While standalone training, if both this arg and environment variable 'DEVICE_ID' are NOT set, use device 0 by default. amp_level Mixed precision mode O0 O0/O1/O2/O3 'O0' - no change.  'O1' - convert the cells and operations in the whitelist to float16 precision, and keep the rest in float32 precision.  'O2' - Keep the cells and operations in the blacklist with float32 precision, and convert the rest to float16 precision.  'O3' - Convert all networks to float16 precision. seed Random seed 42 Integer \\ ckpt_save_policy The policy for saving model weights top_k \"top_k\" or \"latest_k\" \"top_k\" means to keep the top k checkpoints according to the metric score; \"latest_k\" means to keep the last k checkpoints. The value of <code>k</code> is set via <code>ckpt_max_keep</code> ckpt_max_keep The maximum number of checkpoints to keep during training 5 Integer \\ log_interval The interval of printing logs (unit: epoch) 100 Integer \\ val_while_train Whether to enable the evaluation mode while training True True/False If the value is True, please configure the eval data set synchronously val_start_epoch From which epoch to run the evaluation 1 Interger val_interval Evaluation interval (unit: epoch) 1 Interger drop_overflow_update Whether not updating network parameters when loss/gradient overflows True True/False If value is true, network parameters will not be updated when overflow occurs"},{"location":"tutorials/yaml_configuration/#2-shared-parameters-common","title":"2. Shared parameters (common)","text":"<p>Because the same parameter may need to be reused in different configuration sections, you can customize some common parameters in this section for easy management.</p>"},{"location":"tutorials/yaml_configuration/#3-model-architecture-model","title":"3. Model architecture (model)","text":"<p>In MindOCR, the network architecture of the model is divided into four modules: Transform, Backbone, Neck and Head. For details, please refer to documentation, the following are the configuration instructions and examples of each module.</p> Parameter Description Default Remarks type Network type - Currently supports rec/det; 'rec' means recognition task, 'det' means detection task pretrained Specify pre-trained weight path or url null Supports local checkpoint path or url transform: Transformation method configuration null name Specify transformation method name - Currently supports STN_ON backbone: Backbone network configuration name Specify the backbone network class name or function name - Currently defined classes include rec_resnet34, rec_vgg7, SVTRNet and det_resnet18, det_resnet50, det_resnet152, det_mobilenet_v3. You can also customize new classes, please refer to the documentation for definition. pretrained Whether to load pre-trained backbone weights False Supports bool type or str type to be passed in. If it is True, the default weight will be downloaded and loaded through the url link defined in the backbone py file. If str is passed in, the local checkpoint path or url path can be specified for loading. neck: Network Neck configuration name Neck class name - Currently defined classes include RNNEncoder, DBFPN, EASTFPN and PSEFPN. New classes can also be customized, please refer to the documentation for definition. hidden_size RNN hidden layer unit number - \\ head: Network prediction header configuration name Head class name - Currently supports CTCHead, AttentionHead, DBHead, EASTHead and PSEHead weight_init Set weight initialization 'normal' \\ bias_init Set bias initialization 'zeros' \\ out_channels Set the number of classes - \\ <p>Note: For different networks, the configurable parameters of the backbone/neck/head module will be different. The specific configurable parameters are determined by the init input parameter of the class specified by the <code>name</code> parameter of the module in the above table (For example, assume you specify the neck module is DBFPN. Since the DBFPN class initialization includes adaptive input parameters, parameters such as adaptive can be configured under the model.head in yaml.)</p> <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#4-postprocessing-postprocess","title":"4. Postprocessing (postprocess)","text":"<p>Please see the code in mindocr/postprocess</p> Parameter Description Example Remarks name Post-processing class name - Currently supports DBPostprocess, EASTPostprocess, PSEPostprocess, RecCTCLabelDecode and RecAttnLabelDecode character_dict_path Recognition dictionary path None If None, then use the default dictionary [0-9a-z] use_space_char Set whether to add spaces to the dictionary False True/False <p>Note: For different post-processing methods (specified by name), the configurable parameters are different, and are determined by the input parameters of the initialization method <code>__init__</code> of the post-processing class.</p> <p>Reference example: DBNet, PSENet</p>"},{"location":"tutorials/yaml_configuration/#5-evaluation-metrics-metric","title":"5. Evaluation metrics (metric)","text":"<p>Please see the code in mindocr/metrics</p> Parameter Description Default Remarks name Metric class name - Currently supports RecMetric, DetMetric main_indicator Main indicator, used for comparison of optimal models 'hmean' 'acc' for recognition tasks, 'f-score' for detection tasks character_dict_path Recognition dictionary path None If None, then use the default dictionary \"0123456789abcdefghijklmnopqrstuvwxyz\" ignore_space Whether to filter spaces True True/False print_flag Whether to print log False If set True, then output information such as prediction results and standard answers"},{"location":"tutorials/yaml_configuration/#6-loss-function-loss","title":"6. Loss function (loss)","text":"<p>Please see the code in mindocr/losses</p> Parameter Description Default Remarks name loss function name - Currently supports L1BalancedCELoss, CTCLoss, AttentionLoss, PSEDiceLoss, EASTLoss and CrossEntropySmooth pred_seq_len length of predicted text 26 Determined by network architecture max_label_len The longest label length 25 The value is less than the length of the text predicted by the network batch_size single card batch size 32 \\ <p>Note: For different loss functions (specified by name), the configurable parameters are different and determined by the input parameters of the selected loss function.</p>"},{"location":"tutorials/yaml_configuration/#7-learning-rate-adjustment-strategy-and-optimizer-scheduler-optimizer-loss_scaler","title":"7. Learning rate adjustment strategy and optimizer (scheduler, optimizer, loss_scaler)","text":""},{"location":"tutorials/yaml_configuration/#learning-rate-adjustment-strategy-scheduler","title":"Learning rate adjustment strategy (scheduler)","text":"<p>Please see the code in mindocr/scheduler</p> Parameter Description Default Remarks scheduler Learning rate scheduler name 'constant' Currently supports 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay' min_lr Minimum learning rate 1e-6 Lower lr bound for 'cosine_decay' schedulers. lr Learning rate value 0.01 num_epochs Number of total epochs 200 The number of total epochs for the entire training. warmup_epochs The number of epochs in the training learning rate warmp phase 3 For 'cosine_decay', 'warmup_epochs' indicates the epochs to warmup learning rate from 0 to <code>lr</code>. decay_epochs The number of epochs in the training learning rate decay phase 10 For 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>."},{"location":"tutorials/yaml_configuration/#optimizer","title":"optimizer","text":"<p>Please see the code location: mindocr/optim</p> Parameter Description Default Remarks opt Optimizer name 'adam' Currently supports 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'nadam', 'adan', 'rmsprop', 'adagrad', 'lamb'. filter_bias_and_bn Set whether to exclude the weight decrement of bias and batch norm True If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. momentum momentum 0.9 \\ weight_decay weight decay rate 0 It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. nesterov Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. False True/False"},{"location":"tutorials/yaml_configuration/#loss-scaling-loss_scaler","title":"Loss scaling (loss_scaler)","text":"Parameter Description Default Remarks type Loss scaling method type static Currently supports static, dynamic loss_scale Loss scaling value 1.0 \\ scale_factor When using dynamic loss scaler, the coefficient to dynamically adjust the loss_scale 2.0 At each training step, the loss scaling value is updated to <code>loss_scale</code>/<code>scale_factor</code> when overflow occurs. scale_window When using the dynamic loss scaler, when there is no overflow after the scale_window training step, enlarge the loss_scale by scale_factor times 1000 If the continuous <code>scale_window</code> steps does not overflow, the loss will be increased by <code>loss_scale</code> * <code>scale_factor</code> to update the scaling number"},{"location":"tutorials/yaml_configuration/#8-training-evaluation-and-predict-process-train-eval-predict","title":"8. Training, evaluation and predict process (train, eval, predict)","text":"<p>The configuration of the training process is placed under <code>train</code>, and the configuration of the evaluation phase is placed under <code>eval</code>. Note that during model training, if the training-while-evaluation mode is turned on, that is, when val_while_train=True, an evaluation will be run according to the configuration under <code>eval</code> after each epoch is trained. During the non-training phase, only the <code>eval</code> configuration is read when only running model evaluation.</p>"},{"location":"tutorials/yaml_configuration/#training-process-train","title":"Training process (train)","text":"Parameter Description Default Remarks ckpt_save_dir Set model save path ./tmp_rec \\ resume Resume training after training is interrupted, you can set True/False, or specify the ckpt path that needs to be loaded to resume training False If True, load resume_train.ckpt under the ckpt_save_dir directory to continue training. You can also specify the ckpt file path to load and resume training. dataset_sink_mode Whether the data is directly sinked to the processor for processing - If set to True, the data sinks to the processor, and the data can be returned at least after the end of each epoch gradient_accumulation_steps Number of steps to accumulate the gradients 1 Each step represents a forward calculation, and a reverse correction is performed after the gradient accumulation is completed. clip_grad Whether to clip the gradient False If set to True, gradients are clipped to <code>clip_norm</code> clip_norm The norm of clipping gradient if set clip_grad as True 1 \\ ema Whether to use EMA algorithm False \\ ema_decay EMA decay rate 0.9999 \\ pred_cast_fp32 Whether to cast the data type of logits to fp32 False \\ dataset Dataset configuration For details, please refer to Data document type Dataset class name - Currently supports LMDBDataset, RecDataset and DetDataset dataset_root The root directory of the dataset None Optional data_dir The subdirectory where the dataset is located - If <code>dataset_root</code> is not set, please set this to the full directory label_file The label file path of the dataset - If <code>dataset_root</code> is not set, please set this to the full path, otherwise just set the subpath sample_ratio Data set sampling ratio 1.0 If value &lt; 1.0, random selection shuffle Whether to shuffle the data order True if undering training, otherwise False True/False transform_pipeline Data processing flow None For details, please see transforms output_columns Data loader (data loader) needs to output a list of data attribute names (given to the network/loss calculation/post-processing) (type: list), and the candidate data attribute names are determined by transform_pipeline. None If the value is None, all columns are output. Take crnn as an example, output_columns: ['image', 'text_seq'] net_input_column_index In output_columns, the indices of the input items required by the network construct function [0] \\ label_column_index In output_columns, the indices of the input items required by the loss function [1] \\ loader Data Loading Settings shuffle Whether to shuffle the data order for each epoch True if undering training, otherwise False True/False batch_size Batch size of a single card - \\ drop_remainder Whether to drop the last batch of data when the total data cannot be divided by batch_size True if undering training, otherwise False \\ max_rowsize Specifies the maximum space allocated by shared memory when copying data between multiple processes 64 Default value: 64 num_workers Specifies the number of concurrent processes/threads for batch operations n_cpus / n_devices - 2 This value should be greater than or equal to 2 <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#evaluation-process-eval","title":"Evaluation process (eval)","text":"<p>The parameters of <code>eval</code> are basically the same as <code>train</code>, only a few additional parameters are added, and for the rest, please refer to the parameter description of <code>train</code> above.</p> Parameter Usage Default Remarks ckpt_load_path Set model loading path - \\ num_columns_of_labels Set the number of labels in the dataset output columns None If None, assuming the columns after image (data[1:]) are labels. If not None, the num_columns_of_labels columns after image (data[1:1+num_columns_of_labels]) are labels, and the remaining columns are additional info like image_path. drop_remainder Whether to discard the last batch of data when the total number of data cannot be divided by batch_size True if undering training, otherwise False It is recommended to set it to False when doing model evaluation. If it cannot be divisible, mindocr will automatically select a batch size that is the largest divisible"},{"location":"en/#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li> ICDAR2015 [paper] [download]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k [paper] [download]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"en/index_readme/#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li> ICDAR2015 [paper] [download]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k [paper] [download]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"cn/","title":"\u4e3b\u9875","text":""},{"location":"cn/#_1","title":"\u7b80\u4ecb","text":"<p>MindOCR\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore \u6846\u67b6\u5f00\u53d1\u7684OCR\u5f00\u6e90\u5de5\u5177\u7bb1\uff0c\u96c6\u6210\u7cfb\u5217\u4e3b\u6d41\u6587\u5b57\u68c0\u6d4b\u8bc6\u522b\u7684\u7b97\u6cd5\u3001\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u6613\u7528\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5f00\u53d1\u548c\u5e94\u7528\u4e1a\u754cSoTA\u6587\u672c\u68c0\u6d4b\u3001\u6587\u672c\u8bc6\u522b\u6a21\u578b\uff0c\u5982DBNet/DBNet++\u548cCRNN/SVTR\uff0c\u6ee1\u8db3\u56fe\u50cf\u6587\u6863\u7406\u89e3\u7684\u9700\u6c42\u3002</p>  \u4e3b\u8981\u7279\u6027  <ul> <li>\u6a21\u5757\u5316\u8bbe\u8ba1: MindOCR\u5c06OCR\u4efb\u52a1\u89e3\u8026\u6210\u591a\u4e2a\u53ef\u914d\u7f6e\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u4fee\u6539\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u5b9a\u5236\u5316\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0a\u914d\u7f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u7684\u5168\u6d41\u7a0b\uff1b</li> <li>\u9ad8\u6027\u80fd: MindOCR\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u5176\u8fbe\u5230OCR\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\uff1b</li> <li>\u6613\u7528\u6027: MindOCR\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u5e2e\u52a9\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u8fdb\u884c\u6587\u672c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\u3002</li> </ul>"},{"location":"cn/#_2","title":"\u5b89\u88c5\u6559\u7a0b","text":""},{"location":"cn/#mindspore","title":"MindSpore\u76f8\u5173\u73af\u5883\u51c6\u5907","text":"<p>MindOCR\u57fa\u4e8eMindSpore AI\u6846\u67b6\uff08\u652f\u6301CPU/GPU/NPU\uff09\u5f00\u53d1\uff0c\u5e76\u9002\u914d\u4ee5\u4e0b\u6846\u67b6\u7248\u672c\u3002\u5b89\u88c5\u65b9\u5f0f\u8bf7\u53c2\u89c1\u4e0b\u65b9\u7684\u5b89\u88c5\u94fe\u63a5\u3002</p> <ul> <li>mindspore &gt;= 1.9  [\u5b89\u88c5]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [\u5b89\u88c5]</li> <li>mindspore lite (for inference)  [\u5b89\u88c5]</li> </ul>"},{"location":"cn/#_3","title":"\u5305\u4f9d\u8d56","text":"<p><pre><code>pip install -r requirements.txt\n</code></pre> \u63d0\u793a:</p> <ul> <li> <p>\u5982\u679c\u65e0\u6cd5\u5bfc\u5165sckit_image\uff0c\u8bf7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>$LD_PRELOAD</code>\uff0c\u5982\u4e0b\u6240\u793a(\u76f8\u5173opencv issue)\uff1a</p> <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre> </li> </ul>"},{"location":"cn/#_4","title":"\u901a\u8fc7\u6e90\u6587\u4ef6\u5b89\u88c5\uff08\u63a8\u8350\uff09","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>\u4f7f\u7528 <code>-e</code> \u4ee3\u8868\u53ef\u7f16\u8f91\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5e2e\u52a9\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p>"},{"location":"cn/#pypi","title":"\u901a\u8fc7PyPI\u5b89\u88c5","text":"<pre><code>pip install mindocr\n</code></pre> <p>\u7531\u4e8e\u6b64\u9879\u76ee\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e2d\uff0c\u4ecePyPI\u5b89\u88c5\u7684\u7248\u672c\u76ee\u524d\u5df2\u8fc7\u671f\uff0c\u6211\u4eec\u5c06\u5f88\u5feb\u66f4\u65b0\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"cn/#_5","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"cn/#_6","title":"\u6587\u5b57\u68c0\u6d4b\u548c\u8bc6\u522b\u793a\u4f8b","text":"<p>\u5b89\u88c5\u5b8cMindOCR\u540e\uff0c\u6211\u4eec\u5c31\u5f88\u65b9\u4fbf\u5730\u8fdb\u884c\u4efb\u610f\u56fe\u50cf\u7684\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\uff0c\u5982\u4e0b\u3002</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>\u8fd0\u884c\u7ed3\u675f\u540e\uff0c\u7ed3\u679c\u5c06\u88ab\u9ed8\u8ba4\u4fdd\u5b58\u5728<code>./inference_results</code>\u8def\u5f84\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b\uff1a</p> <p> </p> <p>  \u6587\u672c\u68c0\u6d4b\u3001\u8bc6\u522b\u7ed3\u679c\u53ef\u89c6\u5316  </p> <p>\u53ef\u4ee5\u770b\u5230\u56fe\u50cf\u4e2d\u7684\u6587\u5b57\u5757\u5747\u88ab\u68c0\u6d4b\u51fa\u6765\u5e76\u6b63\u786e\u8bc6\u522b\u3002\u66f4\u8be6\u7ec6\u7684\u7528\u6cd5\u4ecb\u7ecd\uff0c\u8bf7\u53c2\u8003\u63a8\u7406\u6559\u7a0b\u3002</p>"},{"location":"cn/#-","title":"\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30-\u5feb\u901f\u6307\u5357","text":"<p>\u4f7f\u7528<code>tools/train.py</code>\u811a\u672c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u8bad\u7ec3OCR\u6a21\u578b\uff0c\u8be5\u811a\u672c\u53ef\u652f\u6301\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u6a21\u578b\u8bad\u7ec3\u3002 <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <code>--config</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9ayaml\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8be5\u6587\u4ef6\u5b9a\u4e49\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u4f18\u5316\u5668\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7b49\u3002</p> <p>MindOCR\u5728<code>configs</code>\u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u7cfb\u5217SoTA\u7684OCR\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u6237\u53ef\u4ee5\u5feb\u901f\u5c06\u5176\u9002\u914d\u5230\u81ea\u5df1\u7684\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\uff0c\u53c2\u8003\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/db++_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre></p> <p>\u7c7b\u4f3c\u7684\uff0c\u4f7f\u7528<code>tools/eval.py</code> \u811a\u672c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u8bc4\u4f30\u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>python tools/eval.py \\\n--config {path/to/model_config.yaml} \\\n--opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre></p> <p>\u66f4\u591a\u4f7f\u7528\u65b9\u6cd5\uff0c\u8bf7\u53c2\u8003\u4f7f\u7528\u6559\u7a0b\u4e2d\u7684\u6a21\u578b\u8bad\u7ec3\u7ae0\u8282\u3002</p>"},{"location":"cn/#_7","title":"\u4f7f\u7528\u6559\u7a0b","text":"<ul> <li>\u6570\u636e\u96c6<ul> <li>\u6570\u636e\u96c6\u51c6\u5907</li> <li>\u6570\u636e\u589e\u5f3a\u7b56\u7565</li> </ul> </li> <li>\u6a21\u578b\u8bad\u7ec3<ul> <li>Yaml\u914d\u7f6e\u6587\u4ef6</li> <li>\u6587\u672c\u68c0\u6d4b</li> <li>\u6587\u672c\u8bc6\u522b</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u8fdb\u9636\u6280\u5de7\uff1a\u68af\u5ea6\u7d2f\u79ef\uff0cEMA\uff0c\u65ad\u70b9\u7eed\u8bad\u7b49</li> </ul> </li> <li>\u63a8\u7406\u4e0e\u90e8\u7f72<ul> <li>\u57fa\u4e8ePython/C++\u548c\u6607\u817e310\u7684OCR\u63a8\u7406</li> <li>\u57fa\u4e8ePython\u7684OCR\u5728\u7ebf\u63a8\u7406</li> </ul> </li> <li>\u5f00\u53d1\u8005\u6307\u5357<ul> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u96c6</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u65b9\u6cd5</li> <li>\u5982\u4f55\u521b\u5efa\u65b0\u7684OCR\u6a21\u578b</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u540e\u5904\u7406\u65b9\u6cd5</li> </ul> </li> </ul>"},{"location":"cn/#_8","title":"\u6a21\u578b\u5217\u8868","text":"\u6587\u672c\u68c0\u6d4b <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021) [\u656c\u8bf7\u671f\u5f85]</li> </ul> \u6587\u672c\u8bc6\u522b <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> ABINet (CVPR'2021) [coming soon]</li> </ul> <p>\u5173\u4e8e\u4ee5\u4e0a\u6a21\u578b\u7684\u5177\u4f53\u8bad\u7ec3\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u8bf7\u53c2\u89c1configs\u4e0b\u5404\u6a21\u578b\u5b50\u76ee\u5f55\u7684readme\u6587\u6863\u3002</p> <p>\u5173\u4e8eMindSpore Lite\u548cACL\u6a21\u578b\u63a8\u7406\u7684\u652f\u6301\u5217\u8868\uff0c\u8bf7\u53c2\u89c1MindOCR\u652f\u6301\u6a21\u578b\u5217\u8868 and \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868\u3002</p>"},{"location":"cn/#_9","title":"\u6570\u636e\u96c6\u5217\u8868","text":"<p>MindOCR\u63d0\u4f9b\u4e86\u6570\u636e\u683c\u5f0f\u8f6c\u6362\u5de5\u5177 \uff0c\u4ee5\u652f\u6301\u4e0d\u540c\u683c\u5f0f\u7684OCR\u6570\u636e\u96c6\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002 \u5f53\u524d\u5df2\u5728\u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u8fc7\u7684\u516c\u5f00OCR\u6570\u636e\u96c6\u5982\u4e0b\u3002</p> \u901a\u7528OCR\u6570\u636e\u96c6 <ul> <li> ICDAR2015 [paper] [download]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k [paper] [download]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>\u6211\u4eec\u4f1a\u5728\u66f4\u591a\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u8be5\u5217\u8868\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p>"},{"location":"cn/#_10","title":"\u91cd\u8981\u4fe1\u606f","text":""},{"location":"cn/#_11","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li> <p>2023/06/07 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4bPSENet</li> <li>\u6587\u672c\u68c0\u6d4bEAST</li> <li>\u6587\u672c\u8bc6\u522bSVTR 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. \u589e\u52a0\u65ad\u70b9\u7eed\u8bad(resume training)\u529f\u80fd\uff0c\u53ef\u5728\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\u4f7f\u7528\u3002\u5982\u9700\u4f7f\u7528\uff0c\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d<code>model</code>\u5b57\u6bb5\u4e0b\u589e\u52a0<code>resume</code>\u53c2\u6570\uff0c\u5141\u8bb8\u4f20\u5165\u5177\u4f53\u8def\u5f84<code>resume: /path/to/train_resume.ckpt</code>\u6216\u8005\u901a\u8fc7\u8bbe\u7f6e<code>resume: True</code>\u6765\u52a0\u8f7d\u5728ckpt_save_dir\u4e0b\u4fdd\u5b58\u7684trian_resume.ckpt 4. \u6539\u8fdb\u68c0\u6d4b\u6a21\u5757\u7684\u540e\u5904\u7406\u90e8\u5206\uff1a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5c06\u68c0\u6d4b\u5230\u7684\u6587\u672c\u591a\u8fb9\u5f62\u91cd\u65b0\u7f29\u653e\u5230\u539f\u59cb\u56fe\u50cf\u7a7a\u95f4\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728<code>eval.dataset.output_columns</code>\u5217\u8868\u4e2d\u589e\u52a0\"shape_list\"\u5b9e\u73b0\u3002 5. \u91cd\u6784\u5728\u7ebf\u63a8\u7406\u4ee5\u652f\u6301\u66f4\u591a\u6a21\u578b\uff0c\u8be6\u60c5\u8bf7\u53c2\u89c1README.md \u3002</li> </ul> </li> <li> <p>2023/05/15 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4bDBNet++</li> <li>\u6587\u672c\u8bc6\u522bCRNN-Seq2Seq</li> <li>\u5728SynthText\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684DBNet 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>DBNet\u7684\u66f4\u591a\u57fa\u51c6\u7ed3\u679c\u53ef\u4ee5\u5728\u6b64\u627e\u5230. 3. \u6dfb\u52a0\u7528\u4e8e\u4fdd\u5b58\u524dk\u4e2acheckpoint\u7684checkpoint manager\u5e76\u6539\u8fdb\u65e5\u5fd7\u3002 4. Python\u63a8\u7406\u4ee3\u7801\u91cd\u6784\u3002 5. Bug\u4fee\u590d\uff1a\u5bf9\u5927\u578b\u6570\u636e\u96c6\u4f7f\u7528\u5e73\u5747\u635f\u5931meter\uff0c\u5728AMP\u8bad\u7ec3\u4e2d\u5bf9ctcloss\u7981\u7528<code>pred_cast_fp32</code>\uff0c\u4fee\u590d\u5b58\u5728\u65e0\u6548\u591a\u8fb9\u5f62\u7684\u9519\u8bef\u3002</li> </ul> </li> <li> <p>2023/05/04 1. \u652f\u6301\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3checkpoint\uff0c \u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>model-pretrained</code>\u8bbe\u7f6e\u4e3acheckpoint url\u6216\u672c\u5730\u8def\u5f84\u6765\u4f7f\u7528\u3002 2. \u652f\u6301\u8bbe\u7f6e\u6267\u884c\u5305\u62ec\u65cb\u8f6c\u548c\u7ffb\u8f6c\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u6982\u7387\u3002 3. \u4e3a\u6a21\u578b\u8bad\u7ec3\u6dfb\u52a0EMA\u529f\u80fd\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train-ema</code>\uff08\u9ed8\u8ba4\u503c\uff1aFalse\uff09\u548c<code>train-ema_decay</code>\u6765\u542f\u7528\u3002 4. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: \u8f93\u5165\u7f51\u7edc\u7684columns\u6570\u91cf\u6539\u4e3a\u8f93\u5165\u7f51\u7edc\u7684columns\u7d22\u5f15 5. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: \u7528\u7d22\u5f15\u66ff\u6362\u6570\u91cf\uff0c\u4ee5\u8868\u793alebel\u7684\u4f4d\u7f6e\u3002</p> </li> <li> <p>2023/04/21 1. \u6dfb\u52a0\u53c2\u6570\u5206\u7ec4\u4ee5\u652f\u6301\u8bad\u7ec3\u4e2d\u7684\u6b63\u5219\u5316\u3002\u7528\u6cd5\uff1a\u5728yaml config\u4e2d\u6dfb\u52a0<code>grouping_strategy</code>\u53c2\u6570\u4ee5\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6216\u4f7f\u7528<code>no_weight_decay_params</code>\u53c2\u6570\u9009\u62e9\u8981\u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u7684\u5c42\uff08\u4f8b\u5982\uff0cbias\u3001norm\uff09\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 2. \u6dfb\u52a0\u68af\u5ea6\u7d2f\u79ef\uff0c\u652f\u6301\u5927\u6279\u91cf\u8bad\u7ec3\u3002\u7528\u6cd5\uff1a\u5728yaml\u914d\u7f6e\u4e2d\u6dfb\u52a0<code>gradient_accumulation_steps</code>\uff0c\u5168\u5c40\u6279\u91cf\u5927\u5c0f=batch_size * devices * gradient_aaccumulation_steps\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 3. \u6dfb\u52a0\u68af\u5ea6\u88c1\u526a\uff0c\u652f\u6301\u8bad\u7ec3\u7a33\u5b9a\u3002\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>grad_clip</code>\u8bbe\u7f6e\u4e3aTrue\u6765\u542f\u7528\u3002</p> </li> <li> <p>2023/03/23 1. \u589e\u52a0dynamic loss scaler\u652f\u6301, \u4e14\u4e0edrop overflow update\u517c\u5bb9\u3002\u5982\u9700\u4f7f\u7528, \u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0<code>loss_scale</code>\u5b57\u6bb5\u5e76\u5c06<code>type</code>\u53c2\u6570\u8bbe\u4e3a<code>dynamic</code>\uff0c\u53c2\u8003\u4f8b\u5b50\u8bf7\u89c1<code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. \u53c2\u6570\u540d\u4fee\u6539\uff1a<code>output_keys</code> -&gt; <code>output_columns</code>\uff1b<code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code>\uff1b 2. \u66f4\u65b0\u6570\u636e\u6d41\u7a0b\u3002</p> </li> <li> <p>2023/03/13 1. \u589e\u52a0\u7cfb\u7edf\u6d4b\u8bd5\u548cCI\u5de5\u4f5c\u6d41\uff1b 2. \u589e\u52a0modelarts\u5e73\u53f0\u9002\u914d\u5668\uff0c\u4f7f\u5f97\u652f\u6301\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff0c\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a   <pre><code>  i)   \u5728OpenI\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u4efb\u52a1\uff1b\n  ii)  \u5728\u7f51\u9875\u4e0a\u5173\u8054\u6570\u636e\u96c6\uff0c\u5982ic15_mindocr\uff1b\n  iii) \u589e\u52a0 `config` \u53c2\u6570\uff0c\u5728\u7f51\u9875\u7684UI\u754c\u9762\u914d\u7f6eyaml\u6587\u4ef6\u8def\u5f84\uff0c\u5982'/home/work/user-job-dir/V0001/configs/rec/test.yaml'\uff1b\n  iv)  \u5728\u7f51\u9875\u7684UI\u754c\u9762\u589e\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u5c06\u5176\u8bbe\u7f6e\u4e3aTrue\uff1b\n  v)   \u586b\u5199\u5176\u4ed6\u9879\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre></p> </li> </ul>"},{"location":"cn/#_12","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u5305\u62ec\u95ee\u9898\u5355\u548cPR\u5728\u5185\u7684\u6240\u6709\u8d21\u732e\uff0c\u6765\u8ba9MindOCR\u53d8\u5f97\u66f4\u597d\u3002</p> <p>\u8bf7\u53c2\u8003CONTRIBUTING.md\u4f5c\u4e3a\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u6309\u7167Model Template and Guideline\u7684\u6307\u5f15\u8d21\u732e\u4e00\u4e2a\u9002\u914d\u6240\u6709\u63a5\u53e3\u7684\u6a21\u578b\uff0c\u591a\u8c22\u5408\u4f5c\u3002</p>"},{"location":"cn/#_13","title":"\u8bb8\u53ef","text":"<p>\u672c\u9879\u76ee\u9075\u4eceApache License 2.0\u5f00\u6e90\u8bb8\u53ef\u3002</p>"},{"location":"cn/#_14","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u672c\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"cn/index_readme/#mindocr","title":"MindOCR","text":"<p>English | \u4e2d\u6587</p> <p>\ud83d\udcdd\u7b80\u4ecb | \ud83d\udd28\u5b89\u88c5\u6559\u7a0b | \ud83d\ude80\u5feb\u901f\u5f00\u59cb | \ud83d\udcda\u4f7f\u7528\u6559\u7a0b | \ud83c\udf81\u6a21\u578b\u5217\u8868 | \ud83d\udcf0\u6570\u636e\u96c6\u5217\u8868 | \ud83c\udf89\u66f4\u65b0\u65e5\u5fd7</p>"},{"location":"cn/index_readme/#_1","title":"\u7b80\u4ecb","text":"<p>MindOCR\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore \u6846\u67b6\u5f00\u53d1\u7684OCR\u5f00\u6e90\u5de5\u5177\u7bb1\uff0c\u96c6\u6210\u7cfb\u5217\u4e3b\u6d41\u6587\u5b57\u68c0\u6d4b\u8bc6\u522b\u7684\u7b97\u6cd5\u3001\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u6613\u7528\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5f00\u53d1\u548c\u5e94\u7528\u4e1a\u754cSoTA\u6587\u672c\u68c0\u6d4b\u3001\u6587\u672c\u8bc6\u522b\u6a21\u578b\uff0c\u5982DBNet/DBNet++\u548cCRNN/SVTR\uff0c\u6ee1\u8db3\u56fe\u50cf\u6587\u6863\u7406\u89e3\u7684\u9700\u6c42\u3002</p>  \u4e3b\u8981\u7279\u6027  <ul> <li>\u6a21\u5757\u5316\u8bbe\u8ba1: MindOCR\u5c06OCR\u4efb\u52a1\u89e3\u8026\u6210\u591a\u4e2a\u53ef\u914d\u7f6e\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u4fee\u6539\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u5b9a\u5236\u5316\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0a\u914d\u7f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u7684\u5168\u6d41\u7a0b\uff1b</li> <li>\u9ad8\u6027\u80fd: MindOCR\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u5176\u8fbe\u5230OCR\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\uff1b</li> <li>\u6613\u7528\u6027: MindOCR\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u5e2e\u52a9\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u8fdb\u884c\u6587\u672c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\u3002</li> </ul>"},{"location":"cn/index_readme/#_2","title":"\u5b89\u88c5\u6559\u7a0b","text":""},{"location":"cn/index_readme/#mindspore","title":"MindSpore\u76f8\u5173\u73af\u5883\u51c6\u5907","text":"<p>MindOCR\u57fa\u4e8eMindSpore AI\u6846\u67b6\uff08\u652f\u6301CPU/GPU/NPU\uff09\u5f00\u53d1\uff0c\u5e76\u9002\u914d\u4ee5\u4e0b\u6846\u67b6\u7248\u672c\u3002\u5b89\u88c5\u65b9\u5f0f\u8bf7\u53c2\u89c1\u4e0b\u65b9\u7684\u5b89\u88c5\u94fe\u63a5\u3002</p> <ul> <li>mindspore &gt;= 1.9  [\u5b89\u88c5]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [\u5b89\u88c5]</li> <li>mindspore lite (for inference)  [\u5b89\u88c5]</li> </ul>"},{"location":"cn/index_readme/#_3","title":"\u5305\u4f9d\u8d56","text":"<p><pre><code>pip install -r requirements.txt\n</code></pre> \u63d0\u793a:</p> <ul> <li> <p>\u5982\u679c\u65e0\u6cd5\u5bfc\u5165sckit_image\uff0c\u8bf7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>$LD_PRELOAD</code>\uff0c\u5982\u4e0b\u6240\u793a(\u76f8\u5173opencv issue)\uff1a</p> <pre><code>export LD_PRELOAD=path/to/scikit_image.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre> </li> </ul>"},{"location":"cn/index_readme/#_4","title":"\u901a\u8fc7\u6e90\u6587\u4ef6\u5b89\u88c5\uff08\u63a8\u8350\uff09","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>\u4f7f\u7528 <code>-e</code> \u4ee3\u8868\u53ef\u7f16\u8f91\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5e2e\u52a9\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p>"},{"location":"cn/index_readme/#pypi","title":"\u901a\u8fc7PyPI\u5b89\u88c5","text":"<pre><code>pip install mindocr\n</code></pre> <p>\u7531\u4e8e\u6b64\u9879\u76ee\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e2d\uff0c\u4ecePyPI\u5b89\u88c5\u7684\u7248\u672c\u76ee\u524d\u5df2\u8fc7\u671f\uff0c\u6211\u4eec\u5c06\u5f88\u5feb\u66f4\u65b0\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"cn/index_readme/#_5","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"cn/index_readme/#_6","title":"\u6587\u5b57\u68c0\u6d4b\u548c\u8bc6\u522b\u793a\u4f8b","text":"<p>\u5b89\u88c5\u5b8cMindOCR\u540e\uff0c\u6211\u4eec\u5c31\u5f88\u65b9\u4fbf\u5730\u8fdb\u884c\u4efb\u610f\u56fe\u50cf\u7684\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\uff0c\u5982\u4e0b\u3002</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n--det_algorithm DB++  \\\n--rec_algorithm CRNN\n</code></pre> <p>\u8fd0\u884c\u7ed3\u675f\u540e\uff0c\u7ed3\u679c\u5c06\u88ab\u9ed8\u8ba4\u4fdd\u5b58\u5728<code>./inference_results</code>\u8def\u5f84\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b\uff1a</p> <p> </p> <p>  \u6587\u672c\u68c0\u6d4b\u3001\u8bc6\u522b\u7ed3\u679c\u53ef\u89c6\u5316  </p> <p>\u53ef\u4ee5\u770b\u5230\u56fe\u50cf\u4e2d\u7684\u6587\u5b57\u5757\u5747\u88ab\u68c0\u6d4b\u51fa\u6765\u5e76\u6b63\u786e\u8bc6\u522b\u3002\u66f4\u8be6\u7ec6\u7684\u7528\u6cd5\u4ecb\u7ecd\uff0c\u8bf7\u53c2\u8003\u63a8\u7406\u6559\u7a0b\u3002</p>"},{"location":"cn/index_readme/#-","title":"\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30-\u5feb\u901f\u6307\u5357","text":"<p>\u4f7f\u7528<code>tools/train.py</code>\u811a\u672c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u8bad\u7ec3OCR\u6a21\u578b\uff0c\u8be5\u811a\u672c\u53ef\u652f\u6301\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u6a21\u578b\u8bad\u7ec3\u3002 <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <code>--config</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9ayaml\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8be5\u6587\u4ef6\u5b9a\u4e49\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u4f18\u5316\u5668\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7b49\u3002</p> <p>MindOCR\u5728<code>configs</code>\u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u7cfb\u5217SoTA\u7684OCR\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u6237\u53ef\u4ee5\u5feb\u901f\u5c06\u5176\u9002\u914d\u5230\u81ea\u5df1\u7684\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\uff0c\u53c2\u8003\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/db++_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre></p> <p>\u7c7b\u4f3c\u7684\uff0c\u4f7f\u7528<code>tools/eval.py</code> \u811a\u672c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u8bc4\u4f30\u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>python tools/eval.py \\\n--config {path/to/model_config.yaml} \\\n--opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre></p> <p>\u66f4\u591a\u4f7f\u7528\u65b9\u6cd5\uff0c\u8bf7\u53c2\u8003\u4f7f\u7528\u6559\u7a0b\u4e2d\u7684\u6a21\u578b\u8bad\u7ec3\u7ae0\u8282\u3002</p>"},{"location":"cn/index_readme/#_7","title":"\u4f7f\u7528\u6559\u7a0b","text":"<ul> <li>\u6570\u636e\u96c6<ul> <li>\u6570\u636e\u96c6\u51c6\u5907</li> <li>\u6570\u636e\u589e\u5f3a\u7b56\u7565</li> </ul> </li> <li>\u6a21\u578b\u8bad\u7ec3<ul> <li>Yaml\u914d\u7f6e\u6587\u4ef6</li> <li>\u6587\u672c\u68c0\u6d4b</li> <li>\u6587\u672c\u8bc6\u522b</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u8fdb\u9636\u6280\u5de7\uff1a\u68af\u5ea6\u7d2f\u79ef\uff0cEMA\uff0c\u65ad\u70b9\u7eed\u8bad\u7b49</li> </ul> </li> <li>\u63a8\u7406\u4e0e\u90e8\u7f72<ul> <li>\u57fa\u4e8ePython/C++\u548c\u6607\u817e310\u7684OCR\u63a8\u7406</li> <li>\u57fa\u4e8ePython\u7684OCR\u5728\u7ebf\u63a8\u7406</li> </ul> </li> <li>\u5f00\u53d1\u8005\u6307\u5357<ul> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u96c6</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u65b9\u6cd5</li> <li>\u5982\u4f55\u521b\u5efa\u65b0\u7684OCR\u6a21\u578b</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u540e\u5904\u7406\u65b9\u6cd5</li> </ul> </li> </ul>"},{"location":"cn/index_readme/#_8","title":"\u6a21\u578b\u5217\u8868","text":"\u6587\u672c\u68c0\u6d4b <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021) [\u656c\u8bf7\u671f\u5f85]</li> </ul> \u6587\u672c\u8bc6\u522b <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> ABINet (CVPR'2021) [coming soon]</li> </ul> <p>\u5173\u4e8e\u4ee5\u4e0a\u6a21\u578b\u7684\u5177\u4f53\u8bad\u7ec3\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u8bf7\u53c2\u89c1configs\u4e0b\u5404\u6a21\u578b\u5b50\u76ee\u5f55\u7684readme\u6587\u6863\u3002</p> <p>\u5173\u4e8eMindSpore Lite\u548cACL\u6a21\u578b\u63a8\u7406\u7684\u652f\u6301\u5217\u8868\uff0c\u8bf7\u53c2\u89c1MindOCR\u652f\u6301\u6a21\u578b\u5217\u8868 and \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868\u3002</p>"},{"location":"cn/index_readme/#_9","title":"\u6570\u636e\u96c6\u5217\u8868","text":"<p>MindOCR\u63d0\u4f9b\u4e86\u6570\u636e\u683c\u5f0f\u8f6c\u6362\u5de5\u5177 \uff0c\u4ee5\u652f\u6301\u4e0d\u540c\u683c\u5f0f\u7684OCR\u6570\u636e\u96c6\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002 \u5f53\u524d\u5df2\u5728\u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u8fc7\u7684\u516c\u5f00OCR\u6570\u636e\u96c6\u5982\u4e0b\u3002</p> \u901a\u7528OCR\u6570\u636e\u96c6 <ul> <li> ICDAR2015 [paper] [download]</li> <li> Total-Text  [paper]  [download]</li> <li> Syntext150k [paper] [download]</li> <li> MLT2017 [paper]  [download] (multi-language)</li> <li> MSRA-TD500 [paper]  [download]</li> <li> SCUT-CTW1500 [paper]   [download]</li> <li> Chinese-Text-Recognition-Benchmark  [paper]   [download]</li> </ul> <p>\u6211\u4eec\u4f1a\u5728\u66f4\u591a\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u8be5\u5217\u8868\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p>"},{"location":"cn/index_readme/#_10","title":"\u91cd\u8981\u4fe1\u606f","text":""},{"location":"cn/index_readme/#_11","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li> <p>2023/06/07 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4bPSENet</li> <li>\u6587\u672c\u68c0\u6d4bEAST</li> <li>\u6587\u672c\u8bc6\u522bSVTR 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. \u589e\u52a0\u65ad\u70b9\u7eed\u8bad(resume training)\u529f\u80fd\uff0c\u53ef\u5728\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\u4f7f\u7528\u3002\u5982\u9700\u4f7f\u7528\uff0c\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d<code>model</code>\u5b57\u6bb5\u4e0b\u589e\u52a0<code>resume</code>\u53c2\u6570\uff0c\u5141\u8bb8\u4f20\u5165\u5177\u4f53\u8def\u5f84<code>resume: /path/to/train_resume.ckpt</code>\u6216\u8005\u901a\u8fc7\u8bbe\u7f6e<code>resume: True</code>\u6765\u52a0\u8f7d\u5728ckpt_save_dir\u4e0b\u4fdd\u5b58\u7684trian_resume.ckpt 4. \u6539\u8fdb\u68c0\u6d4b\u6a21\u5757\u7684\u540e\u5904\u7406\u90e8\u5206\uff1a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5c06\u68c0\u6d4b\u5230\u7684\u6587\u672c\u591a\u8fb9\u5f62\u91cd\u65b0\u7f29\u653e\u5230\u539f\u59cb\u56fe\u50cf\u7a7a\u95f4\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728<code>eval.dataset.output_columns</code>\u5217\u8868\u4e2d\u589e\u52a0\"shape_list\"\u5b9e\u73b0\u3002 5. \u91cd\u6784\u5728\u7ebf\u63a8\u7406\u4ee5\u652f\u6301\u66f4\u591a\u6a21\u578b\uff0c\u8be6\u60c5\u8bf7\u53c2\u89c1README.md \u3002</li> </ul> </li> <li> <p>2023/05/15 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4bDBNet++</li> <li>\u6587\u672c\u8bc6\u522bCRNN-Seq2Seq</li> <li>\u5728SynthText\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684DBNet 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>DBNet\u7684\u66f4\u591a\u57fa\u51c6\u7ed3\u679c\u53ef\u4ee5\u5728\u6b64\u627e\u5230. 3. \u6dfb\u52a0\u7528\u4e8e\u4fdd\u5b58\u524dk\u4e2acheckpoint\u7684checkpoint manager\u5e76\u6539\u8fdb\u65e5\u5fd7\u3002 4. Python\u63a8\u7406\u4ee3\u7801\u91cd\u6784\u3002 5. Bug\u4fee\u590d\uff1a\u5bf9\u5927\u578b\u6570\u636e\u96c6\u4f7f\u7528\u5e73\u5747\u635f\u5931meter\uff0c\u5728AMP\u8bad\u7ec3\u4e2d\u5bf9ctcloss\u7981\u7528<code>pred_cast_fp32</code>\uff0c\u4fee\u590d\u5b58\u5728\u65e0\u6548\u591a\u8fb9\u5f62\u7684\u9519\u8bef\u3002</li> </ul> </li> <li> <p>2023/05/04 1. \u652f\u6301\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3checkpoint\uff0c \u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>model-pretrained</code>\u8bbe\u7f6e\u4e3acheckpoint url\u6216\u672c\u5730\u8def\u5f84\u6765\u4f7f\u7528\u3002 2. \u652f\u6301\u8bbe\u7f6e\u6267\u884c\u5305\u62ec\u65cb\u8f6c\u548c\u7ffb\u8f6c\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u6982\u7387\u3002 3. \u4e3a\u6a21\u578b\u8bad\u7ec3\u6dfb\u52a0EMA\u529f\u80fd\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train-ema</code>\uff08\u9ed8\u8ba4\u503c\uff1aFalse\uff09\u548c<code>train-ema_decay</code>\u6765\u542f\u7528\u3002 4. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: \u8f93\u5165\u7f51\u7edc\u7684columns\u6570\u91cf\u6539\u4e3a\u8f93\u5165\u7f51\u7edc\u7684columns\u7d22\u5f15 5. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: \u7528\u7d22\u5f15\u66ff\u6362\u6570\u91cf\uff0c\u4ee5\u8868\u793alebel\u7684\u4f4d\u7f6e\u3002</p> </li> <li> <p>2023/04/21 1. \u6dfb\u52a0\u53c2\u6570\u5206\u7ec4\u4ee5\u652f\u6301\u8bad\u7ec3\u4e2d\u7684\u6b63\u5219\u5316\u3002\u7528\u6cd5\uff1a\u5728yaml config\u4e2d\u6dfb\u52a0<code>grouping_strategy</code>\u53c2\u6570\u4ee5\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6216\u4f7f\u7528<code>no_weight_decay_params</code>\u53c2\u6570\u9009\u62e9\u8981\u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u7684\u5c42\uff08\u4f8b\u5982\uff0cbias\u3001norm\uff09\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 2. \u6dfb\u52a0\u68af\u5ea6\u7d2f\u79ef\uff0c\u652f\u6301\u5927\u6279\u91cf\u8bad\u7ec3\u3002\u7528\u6cd5\uff1a\u5728yaml\u914d\u7f6e\u4e2d\u6dfb\u52a0<code>gradient_accumulation_steps</code>\uff0c\u5168\u5c40\u6279\u91cf\u5927\u5c0f=batch_size * devices * gradient_aaccumulation_steps\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 3. \u6dfb\u52a0\u68af\u5ea6\u88c1\u526a\uff0c\u652f\u6301\u8bad\u7ec3\u7a33\u5b9a\u3002\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>grad_clip</code>\u8bbe\u7f6e\u4e3aTrue\u6765\u542f\u7528\u3002</p> </li> <li> <p>2023/03/23 1. \u589e\u52a0dynamic loss scaler\u652f\u6301, \u4e14\u4e0edrop overflow update\u517c\u5bb9\u3002\u5982\u9700\u4f7f\u7528, \u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0<code>loss_scale</code>\u5b57\u6bb5\u5e76\u5c06<code>type</code>\u53c2\u6570\u8bbe\u4e3a<code>dynamic</code>\uff0c\u53c2\u8003\u4f8b\u5b50\u8bf7\u89c1<code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. \u53c2\u6570\u540d\u4fee\u6539\uff1a<code>output_keys</code> -&gt; <code>output_columns</code>\uff1b<code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code>\uff1b 2. \u66f4\u65b0\u6570\u636e\u6d41\u7a0b\u3002</p> </li> <li> <p>2023/03/13 1. \u589e\u52a0\u7cfb\u7edf\u6d4b\u8bd5\u548cCI\u5de5\u4f5c\u6d41\uff1b 2. \u589e\u52a0modelarts\u5e73\u53f0\u9002\u914d\u5668\uff0c\u4f7f\u5f97\u652f\u6301\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff0c\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a   <pre><code>  i)   \u5728OpenI\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u4efb\u52a1\uff1b\n  ii)  \u5728\u7f51\u9875\u4e0a\u5173\u8054\u6570\u636e\u96c6\uff0c\u5982ic15_mindocr\uff1b\n  iii) \u589e\u52a0 `config` \u53c2\u6570\uff0c\u5728\u7f51\u9875\u7684UI\u754c\u9762\u914d\u7f6eyaml\u6587\u4ef6\u8def\u5f84\uff0c\u5982'/home/work/user-job-dir/V0001/configs/rec/test.yaml'\uff1b\n  iv)  \u5728\u7f51\u9875\u7684UI\u754c\u9762\u589e\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u5c06\u5176\u8bbe\u7f6e\u4e3aTrue\uff1b\n  v)   \u586b\u5199\u5176\u4ed6\u9879\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre></p> </li> </ul>"},{"location":"cn/index_readme/#_12","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u5305\u62ec\u95ee\u9898\u5355\u548cPR\u5728\u5185\u7684\u6240\u6709\u8d21\u732e\uff0c\u6765\u8ba9MindOCR\u53d8\u5f97\u66f4\u597d\u3002</p> <p>\u8bf7\u53c2\u8003CONTRIBUTING.md\u4f5c\u4e3a\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u6309\u7167Model Template and Guideline\u7684\u6307\u5f15\u8d21\u732e\u4e00\u4e2a\u9002\u914d\u6240\u6709\u63a5\u53e3\u7684\u6a21\u578b\uff0c\u591a\u8c22\u5408\u4f5c\u3002</p>"},{"location":"cn/index_readme/#_13","title":"\u8bb8\u53ef","text":"<p>\u672c\u9879\u76ee\u9075\u4eceApache License 2.0\u5f00\u6e90\u8bb8\u53ef\u3002</p>"},{"location":"cn/index_readme/#_14","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u672c\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition/","title":"\u4e2d\u6587\u6587\u5b57\u8bc6\u522b\u6570\u636e\u96c6","text":"<p>\u672c\u6587\u6863\u4ecb\u7ecd\u4e2d\u6587\u6587\u672c\u8bc6\u522b\u7684\u6570\u636e\u96c6\u51c6\u5907\u3002</p>"},{"location":"cn/datasets/chinese_text_recognition/#_2","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6309\u7167 Benchmarking-Chinese-Text-Recognition \u4e2d\u7684\u8bbe\u7f6e\uff0c\u6211\u4eec\u4f7f\u7528\u4e0e Datasets \u4e2d\u63cf\u8ff0\u7684\u76f8\u540c\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6570\u636e\u3002</p> <p>\u8bf7\u4e0b\u8f7dDownload\u4e2d\u4ecb\u7ecd\u7684\u4ee5\u4e0bLMDB\u6587\u4ef6\uff1a</p> <ul> <li>\u573a\u666f\u6570\u636e\u96c6\uff1a\u8054\u5408\u6570\u636e\u96c6\u5305\u542b RCTW, ReCTS, LSVT, ArT, CTW</li> <li>\u7f51\u9875\uff1aMTWI</li> <li>\u6587\u6863\uff1a\u4f7f\u7528 Text Render \u751f\u6210</li> <li>\u624b\u5199\u6570\u636e\u96c6\uff1aSCUT-HCCDoc</li> </ul>"},{"location":"cn/datasets/chinese_text_recognition/#_3","title":"\u6570\u636e\u7ed3\u6784\u6574\u7406","text":"<p>\u4e0b\u8f7d\u6587\u4ef6\u540e\uff0c\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u6587\u4ef6\u653e\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939 <code>training</code> \u4e0b\uff0c\u6240\u6709\u9a8c\u8bc1\u6570\u636e\u653e\u5728 <code>validation</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u6240\u6709\u8bc4\u4f30\u6570\u636e\u653e\u5728<code>evaluation</code>\u4e0b\u3002</p> <p>\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition/#_4","title":"\u6570\u636e\u96c6\u914d\u7f6e","text":"<p>\u8981\u4f7f\u7528\u6570\u636e\u96c6\uff0c\u60a8\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u6570\u636e\u96c6\uff0c\u5982\u4e0b\u6240\u793a\u3002</p>"},{"location":"cn/datasets/chinese_text_recognition/#_5","title":"\u6a21\u578b\u8bad\u7ec3","text":"<pre><code>...\ntrain:\n...\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\ndata_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\ndata_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n...\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition/#_6","title":"\u6a21\u578b\u8bc4\u4f30","text":"<pre><code>...\ntrain:\n# \u8bad\u7ec3\u90e8\u5206\u4e0d\u9700\u8981\u4fee\u6539\uff0c\u56e0\u4e0d\u4f1a\u8c03\u7528\n...\neval:\ndataset:\ntype: LMDBDataset\ndataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\ndata_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n...\n</code></pre> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/converters/","title":"Dataset Preparation","text":"<p>\u672c\u6587\u6863\u5c55\u793a\u4e86\u5982\u4f55\u5c06OCR\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\uff08\u4e0d\u5305\u62ecLMDB\uff09\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u4ee5\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <p>\u60a8\u4e5f\u53ef\u4ee5\u53c2\u8003 <code>convert_datasets.sh</code>\u3002\u8fd9\u662f\u5c06\u7ed9\u5b9a\u76ee\u5f55\u4e0b\u6240\u6709\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u7684Shell \u811a\u672c\u3002</p> <p>\u8981\u4e0b\u8f7dOCR\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u683c\u5f0f\u8f6c\u6362\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 Chinese text recognition, CTW1500, ICDAR2015, MLT2017, SVT, Syntext 150k, TD500, Total Text, SynthText \u7684\u8bf4\u660e\u3002</p>"},{"location":"cn/datasets/converters/#_1","title":"\u6587\u672c\u68c0\u6d4b/\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b","text":"<p>\u8f6c\u6362\u540e\u7684\u6807\u6ce8\u6587\u4ef6\u683c\u5f0f\u5e94\u4e3a\uff1a <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>\u4ee5ICDAR2015\uff08ic15\uff09\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u8981\u5c06ic15\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6240\u9700\u7684\u683c\u5f0f\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/train/ch4_training_images \\\n--label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n--output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task det \\\n--image_dir /path/to/ic15/det/test/ch4_test_images \\\n--label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n--output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"cn/datasets/converters/#_2","title":"\u6587\u672c\u8bc6\u522b","text":"<p>\u6587\u672c\u8bc6\u522b\u6570\u636e\u96c6\u7684\u6807\u6ce8\u683c\u5f0f\u5982\u4e0b\uff1a</p> <p><pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> \u8bf7\u6ce8\u610f\uff0c\u56fe\u50cf\u540d\u79f0\u548c\u6587\u672c\u6807\u7b7e\u4ee5<code>\\t</code>\u5206\u9694\u3002</p> <p>\u8981\u8f6c\u6362\u6807\u6ce8\u6587\u4ef6\uff0c\u8bf7\u8fd0\u884c\uff1a <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n--dataset_name  ic15 \\\n--task rec \\\n--label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"cn/datasets/ctw1500/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08SCUT-CTW1500\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/ctw1500/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ctw1500/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/train_images/ \\\n--label_dir path/to/ctw1500/ctw_1500_train_labels \\\n--output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name ctw1500 --task det \\\n--image_dir path/to/ctw1500/test_images/ \\\n--label_dir path/to/ctw1500/gt_ctw_1500 \\\n--output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>ctw1500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/icdar2015/","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>ICDAR 2015 \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u5730\u5740: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> \u4ece\u4f55\u5904\u4e0b\u8f7d ICDAR 2015  ICDAR 2015 \u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u4efb\u52a1\u3002\u4efb\u52a11\u662f\u6587\u672c\u5b9a\u4f4d\u3002\u4efb\u52a13\u662f\u5355\u8bcd\u8bc6\u522b\u3002\u4efb\u52a14\u662f\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u3002\u4efb\u52a12\u6587\u672c\u5206\u5272\u7684\u6570\u636e\u4e0d\u53ef\u7528\u3002  ### Text Localization  \u6709\u56db\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre>  ### Word Recognition  \u6709\u4e09\u4e2a\u4e0e\u4efb\u52a13\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre>  \u8fd9\u4e09\u4e2a\u6587\u4ef6\u4ec5\u7528\u4e8e\u8bad\u7ec3\u5355\u8bcd\u8bc6\u522b\u6a21\u578b\u3002\u8bad\u7ec3\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u4e0d\u9700\u8981\u8fd9\u4e09\u4e2a\u6587\u4ef6\u3002  ### E2E  \u6709\u4e5d\u4e2a\u4e0e\u4efb\u52a14\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=4&amp;com=downloads)\uff09\u3002\u5176\u4e2d\u5305\u62ec\u4efb\u52a11\u4e2d\u7684\u56db\u4e2a\u6587\u4ef6\uff0c \u8fd8\u6709\u4e94\u4e2a\u8bcd\u6c47\u6587\u4ef6\u3002  <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre>  \u5982\u679c\u60a8\u4e0b\u8f7d\u4e86\u4e00\u4e2a\u540d\u4e3a `Challenge4_Test_Task4_GT.zip` \u7684\u6587\u4ef6\uff0c\u8bf7\u6ce8\u610f\u5b83\u4e0e `Challenge4_Test_Task1_GT.zip` \u662f\u76f8\u540c\u7684\u6587\u4ef6\uff0c\u9664\u4e86\u540d\u79f0\u4e0d\u540c\u3002\u5728\u8fd9\u4e2arepo\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 `Challenge4_Test_Task4_GT.zip` \u6765\u4ee3\u66ff ICDAR2015 \u6570\u636e\u96c6\u7684\u6587\u4ef6 `Challenge4_Test_Task1_GT.zip`\u3002    <p>\u5728 icdar2015 \u4e0b\u8f7d\u5b8c\u6210\u4ee5\u540e, \u8bf7\u628a\u6240\u6709\u7684\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/mlt2017/","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>MLT (Multi-Lingual) 2017 \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u5730\u5740: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> \u4ece\u4f55\u5904\u4e0b\u8f7d MLT 2017  MLT 2017 \u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u4efb\u52a1. \u4efb\u52a1 1 \u662f\u6587\u672c\u68c0\u6d4b (\u591a\u8bed\u8a00\u6587\u672c)\u3002 \u4efb\u52a12\u662f\u6587\u672c\u8bc6\u522b\u3002  ### \u6587\u672c\u68c0\u6d4b  \u670911\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a  <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre>  \u6d4b\u8bd5\u96c6\u4e0d\u9700\u8981\u4e0b\u8f7d\u3002  ### \u6587\u672c\u8bc6\u522b  \u67096\u4e2a\u4e0e\u4efb\u52a12\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff08[\u4e0b\u8f7d\u5730\u5740](https://rrc.cvc.uab.es/?ch=8&amp;com=downloads)\uff09\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n&lt;/details&gt;\n\n\n\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e, \u5c06\u6587\u4ef6\u653e\u4e8e `[path-to-data-dir]` \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip   ```  [\u8fd4\u56dedataset converters](converters.md)"},{"location":"cn/datasets/svt/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u8857\u666f\u6587\u672c\u6570\u636e\u96c6\uff08SVT\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"cn/datasets/svt/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/svt/#_3","title":"\u8bc6\u522b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u8bc6\u522b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name  svt --task rec \\\n--image_dir path/to/svt1/ \\\n--label_dir path/to/svt1/train.xml \\\n--output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>svt1/</code> \u4e0b\u6709\u4e00\u4e2a\u6587\u4ef6\u5939 <code>cropped_images/</code> \u548c\u4e00\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>rec_train_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/syntext150k/","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>SynText150k \u6587\u7ae0</p> <p>\u4e0b\u8f7d Syntext-150k (Part1: 54,327 [imgs][annos]. Part2: 94,723 [imgs][annos].)</p> <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/synthtext/","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>SynthText\u662f\u4e00\u4e2a\u5408\u6210\u751f\u6210\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5355\u8bcd\u5b9e\u4f8b\u88ab\u653e\u7f6e\u5728\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\uff0c\u5e76\u8003\u8651\u4e86\u573a\u666f\u5e03\u5c40\u3002</p> <p>\u8bba\u6587 | \u4e0b\u8f7dSynthText</p> <p>\u4e0b\u8f7d<code>SynthText.zip</code>\u6587\u4ef6\u5e76\u89e3\u538b\u7f29\u5230<code>[path-to-data-dir]</code>\u6587\u4ef6\u5939\u4e2d\uff1a <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: \u53e6\u5916, \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u5728\u4f7f\u7528 <code>SynthText</code> \u6570\u636e\u96c6\u4e4b\u524d\u5148\u8fdb\u884c\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e00\u4e9b\u9519\u8bef\u7684\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u4e0b\u5217\u7684\u65b9\u5f0f\u8fdb\u884c\u6821\u6b63: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat\n</code></pre> \u4ee5\u4e0a\u7684\u64cd\u4f5c\u4f1a\u4ea7\u751f\u4e0e<code>SynthText</code>\u539f\u59cb\u6807\u6ce8\u683c\u5f0f\u76f8\u540c\u4f46\u662f\u662f\u7ecf\u8fc7\u8fc7\u6ee4\u540e\u7684\u6807\u6ce8\u6570\u636e.</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/td500/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08MSRA-TD500\uff09\u5b98\u7f51</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/td500/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/td500/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/train/ \\\n--label_dir path/to/MSRA-TD500/train \\\n--output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n--dataset_name td500 --task det \\\n--image_dir path/to/MSRA-TD500/test/ \\\n--label_dir path/to/MSRA-TD500/test \\\n--output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>MSRA-TD500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/totaltext/","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>Total-Text \u6587\u7ae0</p> <p>\u4e0b\u8f7d\u6570\u636e\u96c6\u56fe\u50cf \u5730\u5740  (size = 441Mb).</p> \u56fe\u50cf\u4e0b\u8f7d\u94fe\u63a5     [\u56fe\u50cf\u4e0b\u8f7d\u94fe\u63a5](https://drive.google.com/file/d/1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2/view?usp=sharing) (size = 441Mb).   <p>\u4e0b\u8f7d\u6570\u636e\u96c6\u6807\u6ce8 \u5730\u5740.</p> \u6807\u6ce8\u4e0b\u8f7d\u94fe\u63a5     [\u6807\u6ce8\u4e0b\u8f7d\u94fe\u63a5](https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view?usp=sharing) for text file format('.txt').   <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/inference/convert_dynamic/#-shape","title":"\u63a8\u7406 - \u6a21\u578bShape\u5206\u6863","text":""},{"location":"cn/inference/convert_dynamic/#1","title":"1. \u7b80\u4ecb","text":"<p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff0c\u5982\u68c0\u6d4b\u51fa\u76ee\u6807\u540e\u518d\u8fdb\u884c\u76ee\u6807\u8bc6\u522b\uff0c\u7531\u4e8e\u76ee\u6807\u4e2a\u6570\u548c\u76ee\u6807\u5927\u5c0f\u4e0d\u56fa\u5b9a\uff0c\u5bfc\u81f4\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\u8f93\u5165BatchSize\u548cImageSize\u4e0d\u56fa\u5b9a\u3002\u5982\u679c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u6240\u4ee5\uff0c\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u53ef\u4ee5\u8bbe\u7f6e\u4e00\u4e9b\u5019\u9009\u503c\uff0c\u63a8\u7406\u7684\u65f6\u5019Resize\u5230\u6700\u5339\u914d\u7684\u5019\u9009\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002\u7528\u6237\u53ef\u4ee5\u51ed\u7ecf\u9a8c\u624b\u52a8\u9009\u62e9\u8fd9\u4e9b\u5019\u9009\u503c\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u672c\u5de5\u5177\u96c6\u6210\u4e86\u6570\u636e\u96c6\u7edf\u8ba1\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u7edf\u8ba1\u51fa\u5408\u9002\u7684<code>batch size</code>\u3001<code>height</code>\u548c<code>width</code>\u7ec4\u5408\u4f5c\u4e3a\u5019\u9009\u503c\uff0c\u5e76\u5c01\u88c5\u4e86\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u5206\u6863\u529f\u80fd\u3002</p>"},{"location":"cn/inference/convert_dynamic/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u73af\u5883\u5b89\u88c5\uff0c\u5b89\u88c5ACL\u6216MindSpore Lite\u73af\u5883\u3002</p>"},{"location":"cn/inference/convert_dynamic/#3","title":"3. \u6a21\u578b\u51c6\u5907","text":"<p>\u5f53\u524d\u652f\u6301\u8f93\u5165ONNX\u6a21\u578b\u6587\u4ef6\uff0c\u901a\u8fc7\u9009\u62e9\u4e0d\u540c\u540e\u7aef\uff0c\u81ea\u52a8\u5206\u6863\u5e76\u8f6c\u6362\u4e3aOM\u6216MIndIR\u6a21\u578b\u6587\u4ef6\u3002</p> <p>\u8bf7\u786e\u4fdd\u8f93\u5165\u6a21\u578b\u4e3a\u52a8\u6001Shape\u7248\u7684\u3002\u4f8b\u5982\uff0c\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u5982\u679c\u9700\u8981\u5bf9H\u548cW\u5206\u6863\uff0c\u8981\u786e\u4fdd\u81f3\u5c11H\u548cW\u8f74\u662f\u52a8\u6001\u7684\uff0cShape\u53ef\u4ee5\u4e3a<code>(1,3,-1,-1)</code>\u548c<code>(-1,3,-1,-1)</code>\u7b49\u3002</p>"},{"location":"cn/inference/convert_dynamic/#4","title":"4. \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff1a</p> <ol> <li>\u56fe\u50cf\u6587\u4ef6\u5939</li> </ol> <ul> <li> <p>\u8be5\u5de5\u5177\u4f1a\u8bfb\u53d6\u6587\u4ef6\u5939\u4e0b\u7684\u6240\u6709\u56fe\u50cf\uff0c\u8bb0\u5f55<code>height</code>\u548c<code>width</code>\uff0c\u7edf\u8ba1\u51fa\u5408\u9002\u7684\u5019\u9009\u503c</p> </li> <li> <p>\u9002\u5408\u6587\u672c\u68c0\u6d4b\u548c\u6587\u672c\u8bc6\u522b\u6a21\u578b</p> </li> </ul> <ol> <li>\u6587\u672c\u68c0\u6d4b\u7684\u6807\u6ce8\u6587\u4ef6</li> </ol> <ul> <li> <p>\u53ef\u53c2\u8003converter\uff0c\u5b83\u662f\u53c2\u6570<code>task</code>\u4e3a<code>det</code>\u65f6\u8f93\u51fa\u7684\u6807\u6ce8\u6587\u4ef6</p> </li> <li> <p>\u8be5\u5de5\u5177\u4f1a\u8bfb\u53d6\u6bcf\u5f20\u56fe\u50cf\u4e0b\u6807\u6ce8\u7684\u6587\u672c\u6846\u5750\u6807\uff0c\u8bb0\u5f55<code>height</code>\u548c<code>width</code>\uff0c\u4ee5\u53ca\u6846\u7684\u6570\u91cf\u4f5c\u4e3a<code>batch size</code>\uff0c\u7edf\u8ba1\u51fa\u5408\u9002\u7684\u5019\u9009\u503c</p> </li> <li> <p>\u9002\u5408\u6587\u672c\u8bc6\u522b\u6a21\u578b</p> </li> </ul>"},{"location":"cn/inference/convert_dynamic/#5","title":"5. \u7528\u6cd5","text":"<p><code>cd deploy/models_utils/auto_scaling</code></p>"},{"location":"cn/inference/convert_dynamic/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u5bf9batch size\u8fdb\u884c\u5206\u6863</li> </ul> <p><pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/det_gt.txt\n    --input_shape=-1,3,48,192 \\\n--output_path=output\n</code></pre>   \u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_dynamic_bs.om</code></p> <ul> <li>\u5bf9height\u548cwidth\u8fdb\u884c\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/images \\\n--input_shape=1,3,-1,-1 \\\n--output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_dynamic_hw.om</code></p> <ul> <li>\u5bf9batch szie\u3001height\u548cwidth\u8fdb\u884c\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--dataset_path=/path/to/images \\\n--input_shape=-1,3,-1,-1 \\\n--output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u591a\u4e2aOM\u6a21\u578b\uff0c\u7ec4\u5408\u4e86\u591a\u4e2a\u4e0d\u540cBatch Size\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\uff1a<code>model_dynamic_bs1_hw.om</code>, <code>model_dynamic_bs4_hw.om</code>, ......</p> <ul> <li>\u4e0d\u505a\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n--backend=atc \\\n--model_path=/path/to/model.onnx \\\n--input_shape=4,3,48,192 \\\n--output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_static.om</code></p>"},{"location":"cn/inference/convert_dynamic/#52","title":"5.2 \u8be6\u7ec6\u53c2\u6570","text":"\u540d\u79f0 \u9ed8\u8ba4\u503c \u5fc5\u9700 \u542b\u4e49 model_path \u65e0 \u662f \u6a21\u578b\u6587\u4ef6\u8def\u5f84 input_shape \u65e0 \u662f \u6a21\u578b\u8f93\u5165shape\uff0cNCHW\u683c\u5f0f data_path \u65e0 \u5426 \u6570\u636e\u96c6\u6216\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84 input_name x \u5426 \u6a21\u578b\u7684\u8f93\u5165\u540d backend atc \u5426 \u8f6c\u6362\u5de5\u5177\uff0catc\u6216lite output_path ./output \u5426 \u8f93\u51fa\u6a21\u578b\u4fdd\u5b58\u6587\u4ef6\u5939 soc_version Ascend310P3 \u5426 Ascend\u7684soc\u578b\u53f7\uff0cAscend310P3\u6216Ascend310"},{"location":"cn/inference/convert_dynamic/#53","title":"5.3 \u914d\u7f6e\u6587\u4ef6","text":"<p>\u9664\u4e86\u4e0a\u8ff0\u547d\u4ee4\u884c\u53c2\u6570\u5916\uff0c\u5728auto_scaling.yaml\u4e2d\u8fd8\u6709\u4e00\u4e9b\u53c2\u6570\uff0c\u7528\u4ee5\u63cf\u8ff0\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u65b9\u5f0f\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\uff1a</p> <ul> <li>limit_side_len</li> </ul> <p>\u539f\u59cb\u8f93\u5165\u6570\u636e\u7684<code>height</code>\u548c<code>width</code>\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u8303\u56f4\u6309\u7167\u6bd4\u4f8b\u8fdb\u884c\u538b\u7f29\uff0c\u53ef\u4ee5\u8c03\u6574\u6570\u636e\u7684\u79bb\u6563\u7a0b\u5ea6\u3002</p> <ul> <li>strategy</li> </ul> <p>\u6570\u636e\u7edf\u8ba1\u7b97\u6cd5\u7b56\u7565\uff0c\u652f\u6301<code>mean_std</code>\u548c<code>max_min</code>\u4e24\u79cd\u7b97\u6cd5\uff0c\u9ed8\u8ba4\uff1a<code>mean_std</code>\u3002</p> <ul> <li> <p>mean_std</p> <p><pre><code>mean_std = [mean - n_std * sigma\uff0cmean + n_std * sigma]\n</code></pre>    - max_min <pre><code>max_min = [min - (max - min) * expand_ratio / 2\uff0cmax + (max - min) * expand_ratio / 2]\n</code></pre></p> </li> <li> <p>width_range/height_range</p> </li> </ul> <p>\u5bf9\u79bb\u6563\u7edf\u8ba1\u4e4b\u540e\u7684width/height\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u5c06\u88ab\u8fc7\u6ee4\u3002</p> <ul> <li>interval</li> </ul> <p>\u95f4\u9694\u5927\u5c0f\uff0c\u5982\u67d0\u4e9b\u7f51\u7edc\u53ef\u80fd\u8981\u6c42\u8f93\u5165\u5c3a\u5bf8\u5fc5\u987b\u662f32\u7684\u500d\u6570\u3002</p> <ul> <li>max_scaling_num</li> </ul> <p>\u5206\u6863\u6570\u91cf\u7684\u4e0a\u9650\u3002</p> <ul> <li>batch_choices</li> </ul> <p>\u9ed8\u8ba4\u7684batch size\u503c\uff0c\u5982\u679cdata_path\u4f20\u5165\u7684\u662f\u56fe\u50cf\u6587\u4ef6\u5939\uff0c\u5219\u65e0\u6cd5\u7edf\u8ba1\u51fabatch size\u4fe1\u606f\uff0c\u5c31\u4f1a\u4f7f\u7528\u8be5\u9ed8\u8ba4\u503c\u3002</p> <ul> <li>default_scaling</li> </ul> <p>\u7528\u6237\u4e0d\u4f20\u5165data_path\u6570\u636e\u65f6\uff0c\u63d0\u4f9b\u9ed8\u8ba4\u7684<code>height</code>\u548c<code>width</code>\u5206\u6863\u503c\u3002</p>"},{"location":"cn/inference/convert_tutorial/#-","title":"\u63a8\u7406 - \u6a21\u578b\u8f6c\u6362\u6559\u7a0b","text":""},{"location":"cn/inference/convert_tutorial/#1-mindocr","title":"1. MindOCR\u6a21\u578b","text":"<p>MindOCR\u6a21\u578b\u7684\u63a8\u7406\u4f7f\u7528MindSpore Lite\u540e\u7aef\u3002</p> <pre><code>graph LR;\n    ckpt --&gt; |export| MindIR --&gt; |\"converter_lite(\u79bb\u7ebf\u8f6c\u6362)\"| o[MindIR];</code></pre>"},{"location":"cn/inference/convert_tutorial/#11","title":"1.1 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u5728\u63a8\u7406\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u8bad\u7ec3\u7aef\u7684ckpt\u6587\u4ef6\u5bfc\u51fa\u4e3aMindIR\u6587\u4ef6\uff0c\u5b83\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u6743\u91cd\u53c2\u6570\u3002</p> <p>\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u4e86MIndIR\u5bfc\u51fa\u6587\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5\uff0c\u89c1\u6a21\u578b\u5217\u8868\uff0c\u53ef\u8df3\u8f6c\u5230\u5bf9\u5e94\u6a21\u578b\u7684\u4ecb\u7ecd\u9875\u9762\u8fdb\u884c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/inference/convert_tutorial/#12","title":"1.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u9700\u8981\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\uff0c\u5c06\u4e0a\u8ff0\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\u8fdb\u884c\u79bb\u7ebf\u8f6c\u6362\uff0c\u4ece\u800c\u7528\u4e8eMindSpore Lite\u7684\u63a8\u7406\u3002</p> <p><code>converter_lite</code>\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1\u63a8\u7406\u6a21\u578b\u79bb\u7ebf\u8f6c\u6362\u3002</p> <p>\u5047\u8bbe\u8f93\u5165\u6a21\u578b\u4e3ainput.mindir\uff0c\u7ecf\u8fc7<code>converter_lite</code>\u5de5\u5177\u8f6c\u6362\u540e\u7684\u8f93\u51fa\u6a21\u578b\u4e3aoutput.mindir\uff0c\u5219\u6a21\u578b\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=MINDIR \\\n--device=Ascend \\\n--modelFile=input.mindir \\\n--outputFile=output \\\n--configFile=config.txt\n</code></pre> <p>\u5176\u4e2d\uff0c<code>config.txt</code>\u53ef\u4ee5\u8bbe\u7f6e\u8f6c\u6362\u6a21\u578b\u7684Shape\u548c\u63a8\u7406\u7cbe\u5ea6\u3002</p>"},{"location":"cn/inference/convert_tutorial/#121-shape","title":"1.2.1 \u6a21\u578bShape\u914d\u7f6e","text":"<ul> <li>\u9759\u6001Shape</li> </ul> <p>\u5982\u679c\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u540d\u4e3a<code>x</code>\uff0c\u8f93\u5165Shape\u4e3a<code>(1,3,736,1280)</code>\uff0c\u5219config.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>\u8f6c\u6362\u751f\u6210\u7684output.mindir\u4e3a\u9759\u6001shape\u7248\uff0c\u63a8\u7406\u65f6\u7684\u8f93\u5165\u56fe\u50cf\u9700\u8981Resize\u5230\u8be5input_shape\u4ee5\u6ee1\u8db3\u8f93\u5165\u8981\u6c42\u3002</p> <p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff0c\u5982\u68c0\u6d4b\u51fa\u76ee\u6807\u540e\u518d\u6267\u884c\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\uff0c\u7531\u4e8e\u76ee\u6807\u4e2a\u6570\u548c\u5927\u5c0f\u4e0d\u56fa\u5b9a\uff0c\u5982\u679c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u5047\u8bbe\u5bfc\u51fa\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3, -1, -1)\uff0cNHW\u8fd93\u4e2a\u8f74\u662f\u52a8\u6001\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\u3002</p> <p><code>converter_lite</code>\u901a\u8fc7<code>configFile</code>\u914d\u7f6e<code>[ascend_context]</code>\u4e2d<code>dynamic_dims</code>\u53c2\u6570\u6765\u5b9e\u73b0\uff0c\u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003\u52a8\u6001shape\u914d\u7f6e\uff0c\u4e0b\u6587\u7b80\u79f0\u201d\u5206\u6863\u201c\u3002</p> <p>\u6240\u4ee5\uff0c\u8f6c\u6362\u65f6\u67092\u79cd\u9009\u62e9\uff0c\u901a\u8fc7\u8bbe\u7f6e\u4e0d\u540c\u7684config.txt\u5b9e\u73b0\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u9009\u62e9\u52a8\u6001\u503c\u548c\u6a21\u578b\u8f6c\u6362\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#122","title":"1.2.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>converter_lite</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003Ascend\u8f6c\u6362\u5de5\u5177\u529f\u80fd\u8bf4\u660e\uff0c\u5728\u914d\u7f6e\u6587\u4ef6\u7684\u8868\u683c\u4e2d\u63cf\u8ff0\u4e86<code>precision_mode</code>\u53c2\u6570\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u53ef\u9009\u62e9<code>enforce_fp16</code>\u3001<code>enforce_fp32</code>\u3001<code>preferred_fp32</code>\u548c<code>enforce_origin</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>config.txt</code>\u7684<code>[ascend_context]</code>\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>enforce_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial/#2-paddleocr","title":"2. PaddleOCR\u6a21\u578b","text":"<p>PaddleOCR\u6a21\u578b\u7684\u63a8\u7406\u53ef\u4ee5\u4f7f\u7528ACL\u548cMindSpore Lite\u4e24\u79cd\u540e\u7aef\uff0c\u5206\u522b\u5bf9\u5e94OM\u6a21\u578b\u548cMindIR\u6a21\u578b\u3002</p> <pre><code>graph LR;\n    \u8bad\u7ec3\u6a21\u578b -- export --&gt; \u63a8\u7406\u6a21\u578b -- paddle2onnx --&gt; ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);</code></pre>"},{"location":"cn/inference/convert_tutorial/#21-","title":"2.1 \u8bad\u7ec3\u6a21\u578b -&gt; \u63a8\u7406\u6a21\u578b","text":"<p>\u5728PaddleOCR\u6a21\u578b\u7684\u4e0b\u8f7d\u94fe\u63a5\u4e2d\uff0c\u6709\u8bad\u7ec3\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u4e24\u79cd\u683c\u5f0f\uff0c\u5982\u679c\u63d0\u4f9b\u7684\u662f\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u683c\u5f0f\u3002</p> <p>\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u539fPaddleOCR\u4ecb\u7ecd\u9875\u9762\uff0c\u4e00\u822c\u4f1a\u6709\u8f6c\u6362\u811a\u672c\u6837\u4f8b\uff0c\u53ea\u9700\u8981\u4f20\u5165\u8bad\u7ec3\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u3001\u6a21\u578b\u6587\u4ef6\u548c\u4fdd\u5b58\u8def\u5f84\u5373\u53ef\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n-c configs/det/det_r50_vd_db.yml \\\n-o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\nGlobal.save_inference_dir=./det_db\n</code></pre>"},{"location":"cn/inference/convert_tutorial/#22-onnx","title":"2.2 \u63a8\u7406\u6a21\u578b -&gt; ONNX","text":"<p>\u5b89\u88c5\u6a21\u578b\u8f6c\u6362\u5de5\u5177paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>\u8be6\u7ec6\u4f7f\u7528\u6559\u7a0b\u8bf7\u53c2\u8003Paddle2ONNX\u6a21\u578b\u8f6c\u5316\u4e0e\u9884\u6d4b\u3002</p> <p>\u6267\u884c\u8f6c\u6362\u547d\u4ee4\uff0c\u751f\u6210onnx\u6a21\u578b\uff1a</p> <pre><code>paddle2onnx \\\n--model_dir det_db \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file det_db.onnx \\\n--opset_version 11 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>\u53c2\u6570\u4e2dinput_shape_dict\u7684\u503c\uff0c\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7Netron\u5de5\u5177\u6253\u5f00\u63a8\u7406\u6a21\u578b\u67e5\u770b\uff0c\u6216\u8005\u5728\u4e0a\u8ff0tools/export_model.py\u7684\u4ee3\u7801\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/inference/convert_tutorial/#23-onnx-om","title":"2.3 ONNX -&gt; OM","text":"<p>\u4f7f\u7528ATC\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aOM\u6a21\u578b\u3002</p> <p>\u6607\u817e\u5f20\u91cf\u7f16\u8bd1\u5668\uff08Ascend Tensor Compiler\uff0c\u7b80\u79f0ATC\uff09\u662f\u5f02\u6784\u8ba1\u7b97\u67b6\u6784CANN\u4f53\u7cfb\u4e0b\u7684\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u5b83\u53ef\u4ee5\u5c06\u5f00\u6e90\u6846\u67b6\u7684\u7f51\u7edc\u6a21\u578b\u8f6c\u6362\u4e3a\u6607\u817eAI\u5904\u7406\u5668\u652f\u6301\u7684.om\u683c\u5f0f\u79bb\u7ebf\u6a21\u578b\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1ATC\u6a21\u578b\u8f6c\u6362\u3002</p>"},{"location":"cn/inference/convert_tutorial/#231-shape","title":"2.3.1 \u6a21\u578bShape\u914d\u7f6e","text":"<p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3,-1,-1)\u3002</p> <ul> <li>\u9759\u6001Shape</li> </ul> <p>\u53ef\u4ee5\u8f6c\u6362\u4e3a\u9759\u6001Shape\u7248\u7684\u6a21\u578b\uff0cNHW\u90fd\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,736,1280\" \\\n--input_format=ND \\\n--soc_version=Ascend310P3 \\\n--output=det_db_static \\\n--log=error\n</code></pre> <p>ATC\u5de5\u5177\u901a\u8fc7\u8bbe\u7f6e\u53c2\u6570 dynamic_dims\u6765\u652f\u6301Shape\u7684**\u5206\u6863**\uff0c\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u5982\u4e0b\u4e24\u79cd\u9009\u62e9\uff1a</p> <ul> <li>\u52a8\u6001Image Size</li> </ul> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:1,3,-1,-1\" \\\n--input_format=ND \\\n--dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <ul> <li>\u52a8\u6001Batch Size</li> </ul> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n--framework=5 \\\n--input_shape=\"x:-1,3,736,1280\" \\\n--input_format=ND \\\n--dynamic_dims=\"1;4;8;16;32\" \\\n--soc_version=Ascend310P3 \\\n--output=det_db_dynamic_bs \\\n--log=error\n</code></pre> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4e00\u952e\u5f0f\u5b8c\u6210\u52a8\u6001\u503c\u9009\u62e9\u548c\u6a21\u578b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#232","title":"2.3.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>ATC</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003\u53c2\u6570precision_mode\u7684\u8bf4\u660e\uff0c\u53ef\u9009\u62e9<code>force_fp16</code>\u3001<code>force_fp32</code>\u3001<code>allow_fp32_to_fp16</code>\u3001<code>must_keep_origin_dtype</code>\u548c<code>allow_mix_precision</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>atc</code>\u547d\u4ee4\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>force_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial/#24-onnx-mindir","title":"2.4 ONNX -&gt; MindIR","text":"<p>\u4f7f\u7528converter_lite\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aMindIR\u6a21\u578b\u3002\u5de5\u5177\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1MindSpore Lite\u4e91\u4fa7\u63a8\u7406\u79bb\u7ebf\u6a21\u578b\u8f6c\u6362\u3002</p> <p>\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n--saveType=MINDIR \\\n--NoFusion=false \\\n--fmk=ONNX \\\n--device=Ascend \\\n--modelFile=det_db.onnx \\\n--outputFile=det_db_output \\\n--configFile=config.txt\n</code></pre> <p>\u8f6c\u6362\u6d41\u7a0b\u548cMindOCR\u6a21\u578b\u5b8c\u5168\u76f8\u540c\uff0c\u4ec5\u6709\u533a\u522b\u662f<code>--fmk</code>\u9700\u6307\u5b9a\u8f93\u5165\u662fONNX\u6a21\u578b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002</p>"},{"location":"cn/inference/convert_tutorial/#3-mmocr","title":"3. MMOCR\u6a21\u578b","text":"<p>MMOCR\u4f7f\u7528Pytorch\uff0c\u5176\u6a21\u578b\u6587\u4ef6\u4e00\u822c\u662fpth\u683c\u5f0f\u3002</p> <p>\u9700\u8981\u5148\u628a\u5b83\u5bfc\u51fa\u4e3aONNX\u683c\u5f0f\uff0c\u518d\u8f6c\u6362\u4e3aACL/MindSpore Lite\u652f\u6301\u7684OM/MindIR\u683c\u5f0f\u3002</p> <pre><code>graph LR;\n    pth -- export --&gt;  ONNX;\n    ONNX -- atc --&gt; o1(OM);\n    ONNX -- converter_lite --&gt; o2(MindIR);</code></pre>"},{"location":"cn/inference/convert_tutorial/#31-mmocr-onnx","title":"3.1 MMOCR\u6a21\u578b -&gt; ONNX","text":"<p>MMDeploy\u63d0\u4f9b\u4e86MMOCR\u6a21\u578b\u5bfc\u51faONNX\u7684\u547d\u4ee4\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1\u5982\u4f55\u8f6c\u6362\u6a21\u578b\u3002</p> <p>\u5bf9\u4e8e\u53c2\u6570<code>deploy_cfg</code>\u9700\u9009\u62e9\u76ee\u5f55mmdeploy/configs/mmocr\u4e0b\u7684<code>*_onnxruntime_dynamic.py</code>\u6587\u4ef6\uff0c\u4ece\u800c\u5bfc\u51fa\u4e3a\u52a8\u6001Shape\u7248ONNX\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#32-onnx-om","title":"3.2 ONNX -&gt; OM","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; OM\u3002</p>"},{"location":"cn/inference/convert_tutorial/#33-onnx-mindir","title":"3.3 ONNX -&gt; MindIR","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; MIndIR\u3002</p>"},{"location":"cn/inference/environment/#-","title":"\u63a8\u7406 - \u8fd0\u884c\u73af\u5883\u5b89\u88c5","text":"<p>MindOCR\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\u7684\u63a8\u7406\u3002</p> <p>\u8bf7\u786e\u4fdd\u7cfb\u7edf\u6b63\u786e\u5b89\u88c5\u4e86\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\uff0c\u5982\u679c\u6ca1\u6709\u5b89\u88c5\uff0c\u8bf7\u5148\u53c2\u8003\u5b89\u88c5\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\u5c0f\u8282\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>MindOCR\u540e\u7aef\u652f\u6301ACL\u548cMindSpore Lite\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u4f7f\u7528ACL\u6a21\u5f0f\u63a8\u7406\u524d\u9700\u4f7f\u7528ATC\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210om\u683c\u5f0f\uff0c\u4f7f\u7528MindSpore Lite\u63a8\u7406\u524d\u9700\u4f7f\u7528converter_lite\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210MindIR\u683c\u5f0f\uff0c\u5177\u4f53\u533a\u522b\u5982\u4e0b\uff1a</p> ACL Mindspore Lite \u8f6c\u6362\u5de5\u5177 ATC converter_lite \u63a8\u7406\u6a21\u578b\u683c\u5f0f om MindIR"},{"location":"cn/inference/environment/#1-acl","title":"1. ACL\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684ACL\u65b9\u5f0f\u63a8\u7406\uff0c\u76ee\u524dPython\u4fa7\u4f9d\u8d56\u4e8eMindX\u7684Python API\u63a5\u53e3\uff0c\u8be5\u63a5\u53e3\u6682\u53ea\u652f\u6301Python3.9\u3002</p> \u73af\u5883 \u7248\u672c Python 3.9 MindX 3.0.0 <p>\u5728Python3.9\u73af\u5883\u57fa\u7840\u4e0a\uff0c\u4e0b\u8f7dMindX\u7684mxVision SDK\u5b89\u88c5\u5305\uff0c\u53c2\u8003\u6307\u5bfc\u6559\u7a0b\u8fdb\u884c\u5b89\u88c5\uff0c\u4e3b\u8981\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <p><pre><code># \u589e\u52a0\u53ef\u6267\u884c\u6743\u9650\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# \u6267\u884c\u5b89\u88c5\u547d\u4ee4\uff0c\u5982\u679c\u63d0\u793a\u9700\u6307\u5b9acann\u5305\u8def\u5f84\uff0c\u5219\u589e\u52a0\u53c2\u6570\u5982:--cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nsource mxVision/set_env.sh\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c \u5b89\u88c5\u5b8c\u6bd5\u4e4b\u540e\u6d4b\u8bd5\u4e00\u4e0bmindx\u662f\u5426\u53ef\u4ee5\u6b63\u5e38\u5bfc\u5165\uff1a<code>python -c \"import mindx\"</code></p> <p>\u5982\u679c\u63d0\u793a\u627e\u4e0d\u5230mindx\uff0c\u5219\u8f6c\u5230mxVision/python\u76ee\u5f55\u4e0b\uff0c\u5b89\u88c5\u5bf9\u5e94\u7684whl\u5305\uff1a</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> \u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\u5219\u65e0\u9700\u6267\u884c\u4e0a\u8ff0\u6b65\u9aa4\u3002</p>"},{"location":"cn/inference/environment/#2-mindspore-lite","title":"2. MindSpore Lite\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684MindSpore Lite\u63a8\u7406\uff0c\u9700\u8981\u5b89\u88c52.0.0-rc1\u6216\u4ee5\u4e0a\u7248\u672c\u7684MindSpore Lite\u7684**\u4e91\u4fa7**\u63a8\u7406\u5de5\u5177\u5305\u3002</p> <p>\u5148\u4e0b\u8f7dAscend\u7248\u7684\u4e91\u4fa7\u7248\u672c\u7684\u63a8\u7406\u5de5\u5177\u5305tar.gz\u6587\u4ef6\uff0c\u4ee5\u53caPython\u63a5\u53e3Wheel\u5305\u3002</p> <p>\u4e0b\u8f7d\u5730\u5740\u4e2d\u63d0\u4f9b\u4e863.7\u7248\u672c\u7684Python\u5305\uff0c\u5982\u9700\u5176\u5b83\u7248\u672c\u53ef\u53c2\u8003\u7f16\u8bd1\u6559\u7a0b\u3002</p> <p>\u63a8\u7406\u5de5\u5177\u5305\u5b89\u88c5\u65f6\u76f4\u63a5\u89e3\u538b\u5373\u53ef\uff0c\u5e76\u6ce8\u610f\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff1a</p> <p><pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c\u4f7f\u7528pip\u5b89\u88c5\u6240\u9700\u7684whl\u5305 <pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre></p> <p>\u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\uff0c\u5219\u65e0\u9700\u5b89\u88c5\u3002</p>"},{"location":"cn/inference/inference_tutorial/#-","title":"\u63a8\u7406 - \u4f7f\u7528\u6559\u7a0b","text":""},{"location":"cn/inference/inference_tutorial/#1","title":"1. \u7b80\u4ecb","text":"<p>MindOCR\u7684\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u91c7\u7528MindSpore Lite\u548cACL\u4e24\u79cd\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u89d2\u5ea6\u5206\u7c7b\u548c\u6587\u5b57\u8bc6\u522b\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684OCR\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u5e76\u884c\u5316\u65b9\u5f0f\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002</p>"},{"location":"cn/inference/inference_tutorial/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u8fd0\u884c\u73af\u5883\u51c6\u5907\uff0c\u914d\u7f6eMindOCR\u7684\u63a8\u7406\u8fd0\u884c\u73af\u5883\uff0c\u6ce8\u610f\u7ed3\u5408\u6a21\u578b\u7684\u652f\u6301\u60c5\u51b5\u6765\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/inference/inference_tutorial/#3","title":"3. \u6a21\u578b\u8f6c\u6362","text":"<p>MindOCR\u9664\u4e86\u652f\u6301\u81ea\u8eab\u8bad\u7ec3\u7aef\u5bfc\u51fa\u6a21\u578b\u7684\u63a8\u7406\u5916\uff0c\u8fd8\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5217\u8868\u89c1MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u3002</p> <p>\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u5c06\u5176\u8f6c\u6362\u4e3aMindOCR\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u683c\u5f0f\u3002</p>"},{"location":"cn/inference/inference_tutorial/#4-python","title":"4. \u63a8\u7406 (Python)","text":"<p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u4fa7\u76ee\u5f55\u4e0b\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"cn/inference/inference_tutorial/#41","title":"4.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=det_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--res_save_dir=det\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code># cls_mv3.mindir is converted from ppocr\npython infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n--cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n--res_save_dir=cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>\u8bc6\u522b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u5b57\u8bc6\u522b</p> <pre><code>python infer.py \\\n--input_images_dir=/path/to/images \\\n--backend=lite \\\n--rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n--rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n--res_save_dir=rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728rec/rec_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"cn/inference/inference_tutorial/#42","title":"4.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str lite \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 precision_mode str \u65e0 \u63a8\u7406\u7684\u7cbe\u5ea6\u6a21\u5f0f\uff0c\u6682\u53ea\u652f\u6301\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\uff0c\u6b64\u5904\u4e0d\u751f\u6548 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 vis_det_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_pipeline_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u548c\u6587\u672c\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_font_path str \u65e0 \u7ed8\u5236\u6587\u5b57\u65f6\u7684\u5b57\u4f53\u8def\u5f84 crop_save_dir str \u65e0 \u6587\u672c\u68c0\u6d4b\u540e\u88c1\u526a\u56fe\u7247\u7684\u4fdd\u5b58\u8def\u5f84 show_log bool False \u662f\u5426\u6253\u5370\u65e5\u5fd7 save_log_dir str \u65e0 \u65e5\u5fd7\u4fdd\u5b58\u6587\u4ef6\u5939 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 det_model_name_or_config str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 cls_model_name_or_config str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 rec_model_name_or_config str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199 <p>\u8bf4\u660e\uff1a</p> <p><code>*_model_name_or_config</code>\u53ef\u4ee5\u586b\u6a21\u578b\u540d\u6216YAML\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u53ef\u53c2\u8003MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u3002</p>"},{"location":"cn/inference/inference_tutorial/#5-c","title":"5. \u63a8\u7406 (C++)","text":"<p>\u76ee\u524d\u6682\u65f6\u53ea\u652f\u6301pp-ocr\u7cfb\u5217\u7684\u4e2d\u6587DBNET\u3001CRNN\u3001SVTR\u6a21\u578b\u3002</p> <p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u6d4b\u76ee\u5f55\u4e0b <code>cd deploy/cpp_infer</code>,\u6267\u884c\u7f16\u8bd1\u811a\u672c <code>bash build.sh</code>, \u6784\u5efa\u5b8c\u6210\u4e4b\u540e\u5728\u5f53\u524d\u8def\u5f84dist\u76ee\u5f55\u4e0b\u751f\u6210\u53ef\u6267\u884c\u6587\u4ef6infer\u3002</p>"},{"location":"cn/inference/inference_tutorial/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--cls_model_path /path/to/mindir/crnn \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n--character_dict_path /path/to/ppocr_keys_v1.txt \\\n--res_save_dir det_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n--res_save_dir det\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code>./dist/infer \\\n--input_images_dir /path/to/images \\\n--backend lite \\\n--cls_model_path /path/to/mindir/crnn \\\n--res_save_dir cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"cn/inference/inference_tutorial/#52","title":"5.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str acl \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199"},{"location":"cn/inference/model_evaluation/#_1","title":"\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u8bc4\u4f30","text":""},{"location":"cn/inference/model_evaluation/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u68c0\u6d4b\u7ed3\u679c\uff1a <pre><code>python deploy/eval_utils/eval_det.py \\\n--gt_path=/path/to/det_gt.txt \\\n--pred_path=/path/to/prediction/det_results.txt\n</code></pre></p>"},{"location":"cn/inference/model_evaluation/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u8bc6\u522b\u7ed3\u679c\uff1a</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n--gt_path=/path/to/rec_gt.txt \\\n--pred_path=/path/to/prediction/rec_results.txt \\\n--character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0ccharacter_dict_path\u662f\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u5b57\u5178\u4ec5\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199\u3002</p> <p>\u5728\u8bc4\u4f30PaddleOCR\u6216MMOCR\u7cfb\u5217\u6a21\u578b\u65f6\uff0c\u8bf7\u53c2\u7167\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4f7f\u7528\u5bf9\u5e94\u5b57\u5178\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty/#_1","title":"\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6027\u80fd\u5217\u8868","text":"<p>\u672c\u6587\u6863\u5c06\u7ed9\u51fa\u7b2c\u4e09\u65b9\u63a8\u7406\u6a21\u578b\u5728\u8fdb\u884c\u6a21\u578b\u8f6c\u6362\u540e\uff0c\u4f7f\u7528MindIR\u683c\u5f0f\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6d4b\u8bd5\u6570\u636e recall precision f-score \u6765\u6e90 ch_pp_server_det_v2.0 DB ResNet18_vd MLT17 0.3637 0.6340 0.4622 PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 MLT17 0.2557 0.5021 0.3389 PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 MLT17 0.3258 0.6318 0.4299 PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 MLT17 0.2346 0.4868 0.3166 PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 MLT17 0.2403 0.4597 0.3156 PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 IC15 0.3866 0.4630 0.4214 PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 MLT17 0.5992 0.7348 0.6601 PaddleOCR en_pp_det_dbnet_resnet50vd DBNet ResNet50_vd IC15 0.8281 0.7716 0.7989 PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 0.7463 0.9043 0.8177 PaddleOCR en_pp_det_psenet_resnet50vd PSENet ResNet50_vd IC15 0.7664 0.8463 0.8044 PaddleOCR en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 0.8387 0.7900 0.8136 MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 0.8681 0.8074 0.8367 MMOCR"},{"location":"cn/inference/model_perf_thirdparty/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6d4b\u8bd5\u6570\u636e accuracy norm edit distance \u6765\u6e90 ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (only Chinese) 0.4991 0.7411 PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only Chinese) 0.4991 0.7535 PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (only Chinese) 0.4459 0.7036 PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (only Chinese) 0.2459 0.4878 PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (only English) 0.7964 0.8854 PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 MLT17 (only English) 0.0164 0.0657 PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 MLT17 (only English) 0.4304 0.5944 PaddleOCR en_pp_rec_crnn_resnet34vd CRNN Resnet34_vd IC15 0.6635 0.8392 PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd IC15 0.6428 0.8321 PaddleOCR en_pp_rec_vitstr_vitstr VITSTR vitstr IC15 0.6842 0.8578 PaddleOCR en_mm_rec_nrtr_resnet31 NRTR ResNet31 IC15 0.6726 0.8574 MMOCR en_mm_rec_satrn_shallowcnn SATRN shallowcnn IC15 0.7352 0.8887 MMOCR <p>\u8bf7\u6ce8\u610f\uff0c\u4e0a\u8ff0\u6a21\u578b\u91c7\u7528\u4e86shape\u5206\u6863\uff0c\u56e0\u6b64\u8be5\u6027\u80fd\u4ec5\u8868\u793a\u5728\u67d0\u4e9bshape\u4e0b\u7684\u6027\u80fd\u3002</p>"},{"location":"cn/inference/model_perf_thirdparty/#3","title":"3. \u8bc4\u4f30\u65b9\u6cd5","text":"<p>\u8bf7\u53c2\u8003\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u8bc4\u4f30\u6587\u6863\u3002</p>"},{"location":"cn/inference/models_list/#-mindocr","title":"\u63a8\u7406 - MindOCR\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u63a8\u7406\u652f\u6301\u8bad\u7ec3\u7aefckpt\u5bfc\u51fa\u7684\u6a21\u578b\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u8bf7\u81ea\u884c\u5bfc\u51fa\u6216\u4e0b\u8f7d\u5df2\u9884\u5148\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\uff0c\u5e76\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u518d\u8fdb\u884c\u63a8\u7406\u3002</p>"},{"location":"cn/inference/models_list/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 DBNet MobileNetV3 en db_mobilenetv3_icdar15.yaml ResNet-18 en db_r18_icdar15.yaml ResNet-50 en db_r50_icdar15.yaml DBNet++ ResNet-50 en db++_r50_icdar15.yaml EAST ResNet-50 en east_r50_icdar15.yaml PSENet ResNet-152 en pse_r152_icdar15.yaml ResNet-152 ch pse_r152_ctw1500.yaml"},{"location":"cn/inference/models_list/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u8bed\u8a00 \u914d\u7f6e\u6587\u4ef6 CRNN VGG7 Default en crnn_vgg7.yaml ResNet34_vd Default en crnn_resnet34.yaml ResNet34_vd ch_dict.txt ch crnn_resnet34_ch.yaml"},{"location":"cn/inference/models_list_thirdparty/#-","title":"\u63a8\u7406 - \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u53ef\u4ee5\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002</p> <p>\u5728\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\u540e\uff0c\u9700\u8981\u628a\u5b83\u8f6c\u6362\u4e3aACL/MindSpore Lite\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u6587\u4ef6\uff08MindIR\u6216OM\uff09\uff0c\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\u3002</p> <p>\u5176\u4e2d\uff0c\u6d89\u53ca\u7684\u539f\u59cb\u6a21\u578b\u6587\u4ef6\u5982\u4e0b\u8868\uff1a</p> \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u683c\u5f0f \u7b80\u4ecb pp-train .pdparams\u3001.pdopt\u3001.states PaddlePaddle\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f pp-infer inference.pdmodel\u3001inference.pdiparams PaddlePaddle\u63a8\u7406\u6a21\u578b\uff0c\u53ef\u7531\u5176\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa\u5f97\u5230\uff0c\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u53c2\u6570 pth .pth Pytorch\u6a21\u578b\u6587\u4ef6\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u7ed3\u6784\u3001\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f"},{"location":"cn/inference/models_list_thirdparty/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_det_v2.0 DB ResNet18_vd yaml pp-infer ch_ppocr_server_v2.0_det PaddleOCR ch_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ch_PP-OCRv3_det PaddleOCR ch_pp_det_OCRv2 DB MobileNetV3 yaml pp-infer ch_PP-OCRv2_det PaddleOCR ch_pp_mobile_det_v2.0_slim DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_slim_v2.0_det PaddleOCR ch_pp_mobile_det_v2.0 DB MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_det PaddleOCR en_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer en_PP-OCRv3_det PaddleOCR ml_pp_det_OCRv3 DB MobileNetV3 yaml pp-infer ml_PP-OCRv3_det PaddleOCR en_pp_det_dbnet_resnet50vd DB ResNet50_vd yaml pp-train DBNet PaddleOCR en_pp_det_psenet_resnet50vd PSE ResNet50_vd yaml pp-train PSE PaddleOCR en_pp_det_east_resnet50vd EAST ResNet50_vd yaml pp-train EAST PaddleOCR en_pp_det_sast_resnet50vd SAST ResNet50_vd yaml pp-train SAST PaddleOCR en_mm_det_denetpp_resnet50 DB++ ResNet50 yaml pth DBNetpp MMOCR en_mm_det_fcenet_resnet50 FCENet ResNet50 yaml pth FCENet MMOCR <p>\u6ce8\u610f\uff1a\u5728\u4f7f\u7528en_pp_det_psenet_resnet50vd\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u4fee\u6539onnx\u6587\u4ef6</p> <pre><code>python deploy/models_utils/onnx_optim/insert_pse_postprocess.py \\\n--model_path=./pse_r50vd.onnx \\\n--binary_thresh=0.0 \\\n--scale=1.0\n</code></pre>"},{"location":"cn/inference/models_list_thirdparty/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_server_rec_v2.0 CRNN ResNet34 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_server_v2.0_rec PaddleOCR ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv3_rec PaddleOCR ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance ppocr_keys_v1.txt yaml pp-infer ch_PP-OCRv2_rec PaddleOCR ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 ppocr_keys_v1.txt yaml pp-infer ch_ppocr_mobile_v2.0_rec PaddleOCR en_pp_rec_OCRv3 SVTR MobileNetV1Enhance en_dict.txt yaml pp-infer en_PP-OCRv3_rec PaddleOCR en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_slim_v2.0_rec PaddleOCR en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 en_dict.txt yaml pp-infer en_number_mobile_v2.0_rec PaddleOCR korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance korean_dict.txt yaml pp-infer korean_PP-OCRv3_rec PaddleOCR japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance japan_dict.txt yaml pp-infer japan_PP-OCRv3_rec PaddleOCR chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance chinese_cht_dict.txt yaml pp-infer chinese_cht_PP-OCRv3_rec PaddleOCR te_pp_rec_OCRv3 SVTR MobileNetV1Enhance te_dict.txt yaml pp-infer te_PP-OCRv3_rec PaddleOCR ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance ka_dict.txt yaml pp-infer ka_PP-OCRv3_rec PaddleOCR ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance ta_dict.txt yaml pp-infer ta_PP-OCRv3_rec PaddleOCR latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance latin_dict.txt yaml pp-infer latin_PP-OCRv3_rec PaddleOCR arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance arabic_dict.txt yaml pp-infer arabic_PP-OCRv3_rec PaddleOCR cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance cyrillic_dict.txt yaml pp-infer cyrillic_PP-OCRv3_rec PaddleOCR devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance devanagari_dict.txt yaml pp-infer devanagari_PP-OCRv3_rec PaddleOCR en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd ic15_dict.txt yaml pp-train CRNN PaddleOCR en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd ic15_dict.txt yaml pp-train Rosetta PaddleOCR en_pp_rec_vitstr_vitstr ViTSTR ViTSTR EN_symbol_dict.txt yaml pp-train ViTSTR PaddleOCR en_mm_rec_nrtr_resnet31 NRTR ResNet31 english_digits_symbols.txt yaml pth NRTR MMOCR en_mm_rec_satrn_shallowcnn SATRN ShallowCNN english_digits_symbols.txt yaml pth SATRN MMOCR"},{"location":"cn/inference/models_list_thirdparty/#3","title":"3. \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"\u540d\u79f0 \u6a21\u578b \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 \u6765\u6e90 ch_pp_mobile_cls_v2.0 MobileNetV3 yaml pp-infer ch_ppocr_mobile_v2.0_cls PaddleOCR"},{"location":"cn/inference/models_list_thirdparty/#4","title":"4. \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6027\u80fd","text":"<p>\u8bf7\u53c2\u8003\u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u6d4b\u8bd5\u6027\u80fd\u8868\u683c\u3002</p>"},{"location":"cn/mkdocs/modelzoo/","title":"\u6a21\u578b\u5217\u8868","text":"model type dataset fscore(detection)/accuracy(recognition) mindocr recipe vanilla mindspore dbnet_mobilenetv3 detection icdar2015 77.28 config dbnet_resnet18 detection icdar2015 83.71 config dbnet_resnet50 detection icdar2015 84.99 config link dbnet_resnet50 detection msra-td500 85.03 config dbnet++_resnet50 detection icdar2015 86.60 config psenet_resnet152 detection icdar2015 82.06 config link east_resnet50 detection icdar2015 84.87 config link svtr_tiny recognition IC03,13,15,IIIT,etc 89.02 config crnn_vgg7 recognition IC03,13,15,IIIT,etc 82.03 config link crnn_resnet34_vd recognition IC03,13,15,IIIT,etc 84.45 config rare_resnet34_vd recognition IC03,13,15,IIIT,etc 85.19 config"},{"location":"cn/reference/api_doc/","title":"API doc","text":"<p>\u656c\u8bf7\u671f\u5f85...</p>"},{"location":"cn/tutorials/advanced_train/","title":"\u8fdb\u9636\u8bad\u7ec3\u7b56\u7565","text":""},{"location":"cn/tutorials/advanced_train/#ema","title":"\u7b56\u7565\uff1a\u68af\u5ea6\u7d2f\u79ef\uff0c\u68af\u5ea6\u88c1\u526a\uff0cEMA","text":"<p>\u8bad\u7ec3\u7b56\u7565\u53ef\u5728\u6a21\u578bYAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u914d\u7f6e\u3002\u8bf7\u5728\u8bbe\u7f6e\u540e\u8fd0\u884c<code>tools/train.py</code>\u811a\u672c\u8fdb\u884c\u8bad\u7ec3</p> <p>Yaml\u914d\u7f6e\u6587\u4ef6\u53c2\u8003\u6837\u4f8b</p> <pre><code>train:\ngradient_accumulation_steps: 2\nclip_grad: True\nclip_norm: 5.0\nema: True\nema_decay: 0.9999\n</code></pre>"},{"location":"cn/tutorials/advanced_train/#_2","title":"\u68af\u5ea6\u7d2f\u79ef","text":"<p>\u68af\u5ea6\u7d2f\u79ef\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u663e\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u5728\u540c\u7b49\u663e\u5b58\uff0c\u5141\u8bb8**\u4f7f\u7528\u66f4\u5927\u7684\u5168\u5c40batch size\u8fdb\u884c\u8bad\u7ec3**\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>train.gradient_accumulation_steps</code> \u8bbe\u7f6e\u4e3a\u5927\u4e8e1\u7684\u503c\u6765\u542f\u7528\u68af\u5ea6\u7d2f\u79ef\u529f\u80fd\u3002 \u7b49\u4ef7\u7684\u5168\u5c40batch size\u4e3a\uff1a</p> <p><code>global_batch_size = batch_size * num_devices * gradient_accumulation_steps</code></p>"},{"location":"cn/tutorials/advanced_train/#_3","title":"\u68af\u5ea6\u88c1\u526a","text":"<p>\u68af\u5ea6\u88c1\u526a\u901a\u5e38\u7528\u6765\u7f13\u89e3\u68af\u5ea6\u7206\u70b8/\u6ea2\u51fa\u95ee\u9898\uff0c\u4ee5\u4f7f\u6a21\u578b\u6536\u655b\u66f4\u7a33\u5b9a\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train.clip_grad</code>\u4e3a<code>True</code>\u6765\u542f\u7528\u8be5\u529f\u80fd\uff0c\u8c03\u6574<code>train.clip_norm</code>\u7684\u503c\u53ef\u4ee5\u63a7\u5236\u68af\u5ea6\u88c1\u526a\u8303\u6570\u7684\u5927\u5c0f\u3002</p>"},{"location":"cn/tutorials/advanced_train/#ema_1","title":"EMA","text":"<p>Exponential Moving Average\uff08EMA\uff09\u662f\u4e00\u79cd\u5e73\u6ed1\u6a21\u578b\u6743\u91cd\u7684\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u3002\u5b83\u80fd\u5e2e\u52a9\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u7a33\u5b9a\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u4f1a\u5e26\u6765\u66f4\u597d\u7684\u6a21\u578b\u6027\u80fd\u3002 \u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train.ema</code>\u4e3a<code>True</code>\u6765\u4f7f\u7528\u8be5\u529f\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u8c03\u6574<code>train.ema_decay</code>\u6765\u63a7\u5236\u6743\u91cd\u8870\u51cf\u7387\uff0c\u901a\u5e38\u8bbe\u7f6e\u4e3a\u63a5\u8fd11\u7684\u503c.</p>"},{"location":"cn/tutorials/advanced_train/#_4","title":"\u65ad\u70b9\u7eed\u8bad","text":"<p>\u65ad\u70b9\u7eed\u8bad\u901a\u5e38\u7528\u4e8e\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\uff0c\u6b64\u65f6\u4f7f\u7528\u8be5\u529f\u80fd\u53ef\u4ee5\u7ee7\u7eed\u4ece\u4e2d\u65ad\u5904epoch\u7ee7\u7eed\u8bad\u7ec3\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>model.resume</code>\u4e3a<code>True</code>\u6765\u4f7f\u7528\u8be5\u529f\u80fd\uff0c\u7528\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>model:\nresume: True\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u4ece<code>train.ckpt_save_dir</code>\u76ee\u5f55\u4e2d\u4fdd\u5b58\u7684<code>train_resume.ckpt</code>\u6062\u590d\u3002</p> <p>\u5982\u679c\u8981\u4f7f\u7528\u5176\u4ed6epoch\u7528\u4e8e\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u5728<code>resume</code>\u4e2d\u6307\u5b9aepoch\u8def\u5f84\uff0c\u7528\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>model:\nresume: /some/path/to/train_resume.ckpt\n</code></pre>"},{"location":"cn/tutorials/advanced_train/#openi","title":"OpenI\u4e91\u5e73\u53f0\u8bad\u7ec3","text":"<p>\u8bf7\u53c2\u8003MindOCR\u4e91\u4e0a\u8bad\u7ec3\u5feb\u901f\u5165\u95e8</p>"},{"location":"cn/tutorials/distribute_train/","title":"\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u7684\u6559\u7a0b\uff0c\u5728Ascend\u5904\u7406\u5668\u4e0a\u6709\u4e24\u79cd\u65b9\u5f0f\u53ef\u4ee5\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\uff0c\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u6216\u901a\u8fc7\u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002\u5728GPU\u5904\u7406\u5668\u4e0a\u53ef\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002</p>"},{"location":"cn/tutorials/distribute_train/#openmpi","title":"\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u5f53\u524dMindSpore\u5728Ascend\u4e0a\u5df2\u7ecf\u652f\u6301\u4e86\u901a\u8fc7OpenMPI\u7684mpirun\u547d\u4ee4\u8fd0\u884c\u811a\u672c\uff0c\u7528\u6237\u53ef\u53c2\u8003dbnet readme\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4e0b\u4e3a\u547d\u4ee4\u7528\u4f8b\u3002 \u8fd0\u884c\u547d\u4ee4\u524d\u8bf7\u786e\u4fddyaml\u6587\u4ef6\u4e2d\u7684<code>distribute</code>\u53c2\u6570\u4e3aTrue\u3002</p> <pre><code># n is the number of GPUs/NPUs\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre>"},{"location":"cn/tutorials/distribute_train/#rank_table_file","title":"\u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u6b64\u79cd\u65b9\u6cd5\u5728\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u524d\u9700\u8981\u521b\u5efajson\u683c\u5f0f\u7684HCCL\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u751f\u6210RANK_TABLE_FILE\u6587\u4ef6\uff0c\u4ee5\u4e0b\u4e3a\u751f\u62108\u5361\u76f8\u5e94\u914d\u7f6e\u6587\u4ef6\u547d\u4ee4\uff0c\u66f4\u5177\u4f53\u4fe1\u606f\u53ca\u76f8\u5e94\u811a\u672c\u53c2\u89c1hccl_tools\u4e2d\u7684\u8bf4\u660e\uff0c <pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> \u8f93\u51fa\u4e3a\uff1a <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre> \u5176\u4e2d<code>hccl_8p_10234567_127.0.0.1.json</code>\u4e2d\u5185\u5bb9\u793a\u4f8b\u4e3a\uff1a <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"0\",\n                    \"device_ip\": \"192.168.100.101\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"1\",\n                    \"device_ip\": \"192.168.101.101\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"2\",\n                    \"device_ip\": \"192.168.102.101\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"3\",\n                    \"device_ip\": \"192.168.103.101\",\n                    \"rank_id\": \"3\"\n                },\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"4\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"5\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"6\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"7\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre></p> <p>\u4e4b\u540e\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\uff0c\u8fd0\u884c\u547d\u4ee4\u524d\u8bf7\u786e\u4fddyaml\u6587\u4ef6\u4e2d\u7684<code>distribute</code>\u53c2\u6570\u4e3aTrue\u3002 <pre><code>bash ascend8p.sh\n</code></pre></p> <p>\u4ee5CRNN\u8bad\u7ec3\u4e3a\u4f8b\uff0c\u5176<code>ascend8p.sh</code>\u811a\u672c\u4e3a\uff1a <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${DEVICE_NUM}; i++)); do\nexport DEVICE_ID=$i\nexport RANK_ID=$i\necho \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\nif [ $i -eq 0 ]; then\necho 'i am 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\nelse\necho 'not 0'\npython -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\nfi\ndone\n</code></pre></p> <p>\u5f53\u9700\u8981\u8bad\u7ec3\u5176\u4ed6\u6a21\u578b\u65f6\uff0c\u53ea\u8981\u5c06\u811a\u672c\u4e2d\u7684yaml config\u6587\u4ef6\u8def\u5f84\u66ff\u6362\u5373\u53ef\uff0c\u5373<code>python -u tools/train.py --config path/to/model_config.yaml</code></p> <p>\u6b64\u65f6\u8bad\u7ec3\u5df2\u7ecf\u5f00\u59cb\uff0c\u53ef\u5728<code>train.log</code>\u4e2d\u67e5\u770b\u8bad\u7ec3\u65e5\u5fd7\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u7f51\u7edc","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u6587\u672c\u68c0\u6d4b\u7f51\u7edc\u7684\u6559\u7a0b\u3002</p> <ul> <li>\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u7f51\u7edc</li> <li>1. \u6570\u636e\u96c6\u51c6\u5907<ul> <li>1.1 \u51c6\u5907\u8bad\u7ec3\u6570\u636e</li> <li>1.2 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e</li> </ul> </li> <li>2. \u914d\u7f6e\u6587\u4ef6\u51c6\u5907<ul> <li>2.1 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u6570\u636e\u96c6</li> <li>2.2 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570</li> <li>2.3 \u914d\u7f6e\u6a21\u578b\u67b6\u6784</li> <li>2.4 \u914d\u7f6e\u8bad\u7ec3\u8d85\u53c2\u6570</li> </ul> </li> <li>3. \u6a21\u578b\u8bad\u7ec3, \u6d4b\u8bd5\u548c\u63a8\u7406<ul> <li>3.1 \u8bad\u7ec3</li> <li>3.2 \u8bc4\u4f30</li> <li>3.3 \u63a8\u7406</li> <li>3.3.1 \u73af\u5883\u51c6\u5907</li> <li>3.3.2 \u6a21\u578b\u8f6c\u6362</li> <li>3.3.3 \u63a8\u7406 (Python)</li> </ul> </li> </ul>"},{"location":"cn/tutorials/training_detection_custom_dataset/#1","title":"1. \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u76ee\u524d\uff0cMindOCR\u68c0\u6d4b\u7f51\u7edc\u652f\u6301\u4e24\u79cd\u8f93\u5165\u683c\u5f0f\uff0c\u5206\u522b\u662f:</p> <ul> <li> <p><code>Common Dataset</code>\uff1a\u4e00\u79cd\u6587\u4ef6\u683c\u5f0f\uff0c\u5b58\u50a8\u56fe\u50cf\u3001\u6587\u672c\u8fb9\u754c\u6846\u548c\u6587\u672c\u6807\u6ce8\u3002\u76ee\u6807\u6587\u4ef6\u683c\u5f0f\u7684\u4e00\u4e2a\u793a\u4f8b\u662f\uff1a <pre><code>img_1.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> \u5b83\u7531 DetDataset \u8bfb\u53d6\u3002\u5982\u679c\u60a8\u7684\u6570\u636e\u96c6\u4e0d\u662f\u4e0e\u793a\u4f8b\u683c\u5f0f\u76f8\u540c\u7684\u683c\u5f0f\uff0c\u8bf7\u53c2\u9605 \u8bf4\u660e \uff0c\u4e86\u89e3\u5982\u4f55\u5c06\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6ce8\u91ca\u8f6c\u6362\u4e3a\u652f\u6301\u7684\u683c\u5f0f\u3002</p> </li> <li> <p><code>SynthTextDataset</code>\uff1a\u7531 SynthText800k \u63d0\u4f9b\u7684\u4e00\u79cd\u6587\u4ef6\u683c\u5f0f\u3002 \u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u7ec6\u8282\u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc\u3002\u5b83\u7684\u6807\u6ce8\u6587\u4ef6\u662f\u4e00\u4e2a<code>.mat</code>\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u62ec <code>imnames</code>\uff08\u56fe\u50cf\u540d\u79f0\uff09\u3001<code>wordBB</code>\uff08\u5355\u8bcd\u7ea7\u8fb9\u754c\u6846\uff09\u3001<code>charBB</code>\uff08\u5b57\u7b26\u7ea7\u8fb9\u754c\u6846\uff09\u548c <code>txt</code>\uff08\u6587\u672c\u5b57\u7b26\u4e32\uff09\u3002\u5b83\u7531 SynthTextDataset \u8bfb\u53d6\u3002\u7528\u6237\u53ef\u4ee5\u53c2\u8003 <code>SynthTextDataset</code>\u6765\u7f16\u5199\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\u3002</p> </li> </ul> <p>\u6211\u4eec\u5efa\u8bae\u7528\u6237\u5c06\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\u51c6\u5907\u6210 <code>Common Dataset</code>\u683c\u5f0f\uff0c\u7136\u540e\u4f7f\u7528 <code>DetDataset</code> \u6765\u52a0\u8f7d\u6570\u636e\u3002\u4ee5\u4e0b\u6559\u7a0b\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u8be6\u7ec6\u6b65\u9aa4\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#11","title":"1.1 \u51c6\u5907\u8bad\u7ec3\u6570\u636e","text":"<p>\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u56fe\u50cf\u653e\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u5e76\u5728\u66f4\u9ad8\u7ea7\u522b\u7684\u76ee\u5f55\u4e2d\u6307\u5b9a\u4e00\u4e2a txt \u6587\u4ef6 <code>train_det.txt</code> \uff0c\u6765\u6807\u8bb0\u6240\u6709\u8bad\u7ec3\u56fe\u50cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\u3002txt \u6587\u4ef6\u7684\u4e00\u4e2a\u793a\u4f8b\u5982\u4e0b\uff1a <pre><code># \u6587\u4ef6\u540d   # \u4e00\u4e2a\u5b57\u5178\u5217\u8868\nimg_1.jpg\\t[{\"transcription\": \"Genaxis Theatre\", \"points\": [[377, 117], [463, 117], [465, 130], [378, 130]]}, {\"transcription\": \"[06]\", \"points\": [[493, 115], [519, 115], [519, 131], [493, 131]]}, {...}]\nimg_2.jpg\\t[{\"transcription\": \"guardian\", \"points\": [[642, 250], [769, 230], [775, 255], [648, 275]]}]\n...\n</code></pre></p> <p>\u6ce8\u610f\uff1a\u8bf7\u4f7f\u7528 <code>\\tab</code> \u5206\u9694\u56fe\u50cf\u540d\u79f0\u548c\u6807\u7b7e\uff0c\u907f\u514d\u4f7f\u7528\u7a7a\u683c\u6216\u5176\u4ed6\u5206\u9694\u7b26\u3002</p> <p>\u6700\u7ec8\u7684\u8bad\u7ec3\u96c6\u5c06\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u5b58\u50a8\uff1a</p> <pre><code>|-data\n    |- train_det.txt\n    |- training\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_detection_custom_dataset/#12","title":"1.2 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e","text":"<p>\u7c7b\u4f3c\u5730\uff0c\u8bf7\u5c06\u6240\u6709\u6d4b\u8bd5\u56fe\u50cf\u653e\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u5e76\u5728\u66f4\u9ad8\u7ea7\u522b\u7684\u76ee\u5f55\u4e2d\u6307\u5b9a\u4e00\u4e2a txt \u6587\u4ef6 <code>val_det.txt</code> \uff0c\u6765\u6807\u8bb0\u6240\u6709\u6d4b\u8bd5\u56fe\u50cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u6700\u7ec8\uff0c\u6d4b\u8bd5\u96c6\u7684\u6587\u4ef6\u5939\u5c06\u4f1a\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u5b58\u50a8\uff1a <pre><code>|-data\n    |- val_det.txt\n    |- validation\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre></p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#2","title":"2. \u914d\u7f6e\u6587\u4ef6\u51c6\u5907","text":"<p>\u4e3a\u4e86\u51c6\u5907\u76f8\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u7528\u6237\u5e94\u8be5\u6307\u5b9a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u76ee\u5f55\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#21","title":"2.1 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u6570\u636e\u96c6","text":"<p>\u8bf7\u9009\u62e9 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4f5c\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u4fee\u6539\u5176\u4e2d\u7684<code>train.dataset</code> \u548c <code>eval.dataset</code> \u5b57\u6bb5\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: DetDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u6cd5\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 `Common Dataset` \u683c\u5f0f\ndataset_root: dir/to/data/                                        # \u6570\u636e\u7684\u6839\u76ee\u5f55\ndata_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\nlabel_file: train_det.txt                                       # \u8bad\u7ec3\u6807\u7b7e\u7684\u8def\u5f84\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n...\neval:\ndataset:\ntype: DetDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u6cd5\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 `Common Dataset` \u683c\u5f0f\ndataset_root: dir/to/data/                                        # \u6570\u636e\u7684\u6839\u76ee\u5f55\ndata_dir: validation/                                             # \u6d4b\u8bd5\u6570\u636e\u96c6\u76ee\u5f55\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\nlabel_file: val_det.txt                                     # \u6d4b\u8bd5\u6807\u7b7e\u7684\u8def\u5f84\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n...\n</code></pre>"},{"location":"cn/tutorials/training_detection_custom_dataset/#22","title":"2.2 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570","text":"<p>\u4ee5 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e2d\u7684 <code>train.dataset.transform_pipeline</code> \u4e3a\u4f8b\u3002\u5b83\u6307\u5b9a\u4e86\u4e00\u7ec4\u5e94\u7528\u4e8e\u56fe\u50cf\u6216\u6807\u7b7e\u7684\u8f6c\u6362\u51fd\u6570\uff0c\u7528\u4ee5\u751f\u6210\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\u6216\u635f\u5931\u51fd\u6570\u8f93\u5165\u7684\u6570\u636e\u3002\u8fd9\u4e9b\u8f6c\u6362\u51fd\u6570\u5b9a\u4e49\u5728 <code>mindocr/data/transforms</code> \u4e2d\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntransform_pipeline:\n- DecodeImage:\nimg_mode: RGB\nto_float32: False\n- DetLabelEncode:\n- RandomColorAdjust:\nbrightness: 0.1255  # 32.0 / 255\nsaturation: 0.5\n- RandomHorizontalFlip:\np: 0.5\n- RandomRotate:\ndegrees: [ -10, 10 ]\nexpand_canvas: False\np: 1.0\n- RandomScale:\nscale_range: [ 0.5, 3.0 ]\np: 1.0\n- RandomCropWithBBox:\nmax_tries: 10\nmin_crop_ratio: 0.1\ncrop_size: [ 640, 640 ]\np: 1.0\n- ValidatePolygons:\n- ShrinkBinaryMap:\nmin_text_size: 8\nshrink_ratio: 0.4\n- BorderMap:\nshrink_ratio: 0.4\nthresh_min: 0.3\nthresh_max: 0.7\n- NormalizeImage:\nbgr_to_rgb: False\nis_hwc: True\nmean: imagenet\nstd: imagenet\n- ToCHWImage:\n...\n</code></pre> <ul> <li> <p><code>DecodeImage</code> \u548c <code>DetLabelEncode</code>\uff1a\u8fd9\u4e24\u4e2a\u8f6c\u6362\u51fd\u6570\u89e3\u6790 <code>train_det.txt</code> \u6587\u4ef6\u4e2d\u7684\u5b57\u7b26\u4e32\uff0c\u52a0\u8f7d\u56fe\u50cf\u548c\u6807\u7b7e\uff0c\u5e76\u5c06\u5b83\u4eec\u4fdd\u5b58\u4e3a\u4e00\u4e2a\u5b57\u5178\uff1b</p> </li> <li> <p><code>RandomColorAdjust</code>\uff0c <code>RandomHorizontalFlip</code>\uff0c <code>RandomRotate</code>\uff0c <code>RandomScale</code> \u548c <code>RandomCropWithBBox</code>\uff1a\u8fd9\u4e9b\u8f6c\u6362\u51fd\u6570\u6267\u884c\u5178\u578b\u7684\u56fe\u50cf\u589e\u5f3a\u64cd\u4f5c\u3002\u9664\u4e86 <code>RandomColorAdjust</code>\u4ee5\u5916\uff0c\u5176\u4ed6\u51fd\u6570\u90fd\u4f1a\u6539\u53d8\u8fb9\u754c\u6846\u6807\u7b7e\u3002</p> </li> <li> <p><code>ValidatePolygons</code>\uff1a\u5b83\u8fc7\u6ee4\u6389\u7531\u4e8e\u4e4b\u524d\u7684\u6570\u636e\u589e\u5f3a\u800c\u5728\u51fa\u73b0\u56fe\u50cf\u5916\u90e8\u7684\u8fb9\u754c\u6846\uff1b</p> </li> <li> <p><code>ShrinkBinaryMap</code>\u548c <code>BorderMap</code>\uff1a\u5b83\u4eec\u751f\u6210 <code>dbnet</code> \u8bad\u7ec3\u6240\u9700\u7684\u4e8c\u8fdb\u5236\u56fe\u548c\u8fb9\u754c\u56fe</p> </li> <li> <p><code>NormalizeImage</code>\uff1a\u5b83\u6839\u636e <code>ImageNet</code> \u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\u5bf9\u56fe\u50cf\u8fdb\u884c\u5f52\u4e00\u5316\uff1b</p> </li> <li> <p><code>ToCHWImage</code>\uff1a\u5b83\u5c06 <code>HWC</code> \u56fe\u50cf\u8f6c\u6362\u4e3a <code>CHW</code> \u56fe\u50cf\u3002</p> </li> </ul> <p>\u5bf9\u4e8e\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570\uff0c\u6240\u6709\u7684\u56fe\u50cf\u589e\u5f3a\u64cd\u4f5c\u90fd\u88ab\u79fb\u9664\uff0c\u88ab\u66ff\u6362\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u7f29\u653e\u51fd\u6570\u3002</p> <pre><code>eval:\ndataset\ntransform_pipeline:\n- DecodeImage:\nimg_mode: RGB\nto_float32: False\n- DetLabelEncode:\n- DetResize:\ntarget_size: [ 736, 1280 ]\nkeep_ratio: False\nforce_divisable: True\n- NormalizeImage:\nbgr_to_rgb: False\nis_hwc: True\nmean: imagenet\nstd: imagenet\n- ToCHWImage:\n</code></pre> <p>\u66f4\u591a\u5173\u4e8e\u8f6c\u6362\u51fd\u6570\u7684\u6559\u7a0b\u53ef\u4ee5\u5728 \u8f6c\u6362\u6559\u7a0b \u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#23","title":"2.3 \u914d\u7f6e\u6a21\u578b\u67b6\u6784","text":"<p>\u867d\u7136\u4e0d\u540c\u7684\u6a21\u578b\u6709\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u4f46 <code>MindOCR</code> \u5c06\u5b83\u4eec\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u4e09\u9636\u6bb5\u67b6\u6784\uff1a<code>[backbone]-&gt;[neck]-&gt;[head]</code>\u3002\u4ee5 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e3a\u4f8b\uff1a</p> <pre><code>model:\ntype: det\ntransform: null\nbackbone:\nname: det_resnet50  # \u76ee\u524d\u53ea\u652f\u6301 ResNet50\npretrained: True    # \u662f\u5426\u4f7f\u7528\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u91cd\nneck:\nname: DBFPN         # DBNet \u7684 FPN \u90e8\u5206\nout_channels: 256\nbias: False\nuse_asf: False      # DBNet++ \u4e2d\u7684\u81ea\u9002\u5e94\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\uff08\u4ec5\u7528\u4e8e DBNet++\uff09\nhead:\nname: DBHead\nk: 50               # \u53ef\u5fae\u5206\u4e8c\u503c\u5316\u7684\u653e\u5927\u56e0\u5b50\nbias: False\nadaptive: True      # \u8bad\u7ec3\u65f6\u4e3a True\uff0c\u63a8\u7406\u65f6\u4e3a False\n</code></pre> <p><code>backbone</code>,<code>neck</code>\u548c<code>head</code>\u5b9a\u4e49\u5728 <code>mindocr/models/backbones</code>\u3001<code>mindocr/models/necks</code> \u548c <code>mindocr/models/heads</code> \u4e0b\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#24","title":"2.4 \u914d\u7f6e\u8bad\u7ec3\u8d85\u53c2\u6570","text":"<p><code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e9b\u8bad\u7ec3\u8d85\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>metric:\nname: DetMetric\nmain_indicator: f-score\n\nloss:\nname: L1BalancedCELoss\neps: 1.0e-6\nl1_scale: 10\nbce_scale: 5\nbce_replace: bceloss\n\nscheduler:\nscheduler: polynomial_decay\nlr: 0.007\nnum_epochs: 1200\ndecay_rate: 0.9\nwarmup_epochs: 3\n\noptimizer:\nopt: SGD\nfilter_bias_and_bn: false\nmomentum: 0.9\nweight_decay: 1.0e-4\n</code></pre> \u5b83\u4f7f\u7528 <code>SGD</code> \u4f18\u5316\u5668\uff08\u5728 <code>mindocr/optim/optim.factory.py</code> \u4e2d\uff09\u548c <code>polynomial_decay</code>\uff08\u5728 <code>mindocr/scheduler/scheduler_factory.py</code> \u4e2d\uff09\u4f5c\u4e3a\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002\u635f\u5931\u51fd\u6570\u662f <code>L1BalancedCELoss</code>\uff08\u5728 <code>mindocr/losses/det_loss.py</code> \u4e2d\uff09\uff0c\u8bc4\u4f30\u6307\u6807\u662f <code>DetMetric</code>\uff08\u5728 <code>mindocr/metrics/det_metrics.py</code> \u4e2d\uff09\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#3","title":"3. \u6a21\u578b\u8bad\u7ec3, \u6d4b\u8bd5\u548c\u63a8\u7406","text":"<p>\u5f53\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u90fd\u5df2\u8bbe\u7f6e\u597d\u540e\uff0c\u7528\u6237\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u4ed6\u4eec\u7684\u6a21\u578b\u3002MindOCR\u652f\u6301\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\u8fdb\u884c\u6d4b\u8bd5\u548c\u63a8\u7406\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#31","title":"3.1 \u8bad\u7ec3","text":"<ul> <li>\u5355\u673a\u8bad\u7ec3</li> </ul> <p>\u5728\u5355\u673a\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u662f\u5728\u5355\u4e2a\u8bbe\u5907\u4e0a\u8bad\u7ec3\u7684\uff08\u9ed8\u8ba4\u4e3a<code>device:0</code>\uff09\u3002\u7528\u6237\u5e94\u8be5\u5728<code>yaml</code>\u914d\u7f6e\u6587\u4ef6\u4e2d\u5c06<code>system.distribute</code>\u8bbe\u7f6e\u4e3a<code>False</code>\u3002\u5982\u679c\u7528\u6237\u60f3\u8981\u5728\u9664device:0\u4ee5\u5916\u7684\u8bbe\u5907\u4e0a\u8fd0\u884c\u8fd9\u4e2a\u6a21\u578b\uff0c\u8fd8\u9700\u8981\u5c06<code>system.device_id</code>\u8bbe\u7f6e\u4e3a\u76ee\u6807\u8bbe\u5907id\u3002</p> <p>\u4ee5<code>configs/det/dbnet/db_r50_icdar15.yaml</code>\u4e3a\u4f8b\uff0c\u8bad\u7ec3\u547d\u4ee4\u662f\uff1a <pre><code>python tools/train.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre></p> <ul> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> </ul> <p>\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\uff0cyaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>system.distribute</code>\u5e94\u8be5\u4e3a<code>True</code>\u3002\u5728GPU\u548cAscend\u8bbe\u5907\u4e0a\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>mpirun</code>\u6765\u542f\u52a8\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u4f8b\u5982\uff0c\u4f7f\u7528<code>device:0</code>\u548c<code>device:1</code>\u8fdb\u884c\u8bad\u7ec3\uff1a</p> <p><pre><code># n\u662fGPU/NPU\u7684\u6570\u91cf\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> \u6709\u65f6\uff0c\u7528\u6237\u53ef\u80fd\u60f3\u8981\u6307\u5b9a\u8bbe\u5907id\u6765\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4f8b\u5982\uff0c<code>device:2</code>\u548c<code>device:3</code>\u3002</p> <p>\u5728GPU\u8bbe\u5907\u4e0a\uff0c\u5728\u8fd0\u884c\u4e0a\u9762\u7684<code>mpirun</code>\u547d\u4ee4\u4e4b\u524d\uff0c\u7528\u6237\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>export CUDA_VISIBLE_DEVICES=2,3\n</code></pre> \u5728Ascend\u8bbe\u5907\u4e0a\uff0c\u7528\u6237\u5e94\u8be5\u521b\u5efa\u4e00\u4e2a\u50cf\u8fd9\u6837\u7684<code>rank_table.json</code>\uff1a <pre><code>Copy{\n\"version\": \"1.0\",\n\"server_count\": \"1\",\n\"server_list\": [\n{\n\"server_id\": \"10.155.111.140\",\n\"device\": [\n{\"device_id\": \"2\",\"device_ip\": \"192.3.27.6\",\"rank_id\": \"2\"},\n{\"device_id\": \"3\",\"device_ip\": \"192.4.27.6\",\"rank_id\": \"3\"}],\n\"host_nic_ip\": \"reserve\"\n}\n],\n\"status\": \"completed\"\n}\n</code></pre></p> <p>\u76ee\u6807\u8bbe\u5907\u7684<code>device_ip</code>\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c<code>cat /etc/hccn.conf</code>\u83b7\u53d6\u3002\u8f93\u51fa\u7ed3\u679c\u4e2d\u7684<code>address_x</code>\u5c31\u662f<code>ip</code>\u5730\u5740\u3002\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u6559\u7a0b\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#32","title":"3.2 \u8bc4\u4f30","text":"<p>\u4e3a\u4e86\u8bc4\u4f30\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>tools/eval.py</code>\u3002</p> <p>\u4ee5\u5355\u673a\u8bc4\u4f30\u4e3a\u4f8b\u3002\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c<code>system.distribute</code>\u5e94\u8be5\u4e3a<code>False</code>\uff1b<code>eval.ckpt_load_path</code>\u5e94\u8be5\u662f\u76ee\u6807checkpoint\u8def\u5f84\uff1b<code>eval.dataset_root</code>\uff0c<code>eval.data_dir</code>\u548c<code>eval.label_file</code>\u5e94\u8be5\u6307\u5b9a\u4e3a\u6b63\u786e\u7684\u6d4b\u8bd5\u96c6\u8def\u5f84\u3002\u7136\u540e\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5f00\u59cb\u6d4b\u8bd5\uff1a <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre></p> <p>MindOCR\u8fd8\u652f\u6301\u5728\u547d\u4ee4\u884c\u4e2d\u6307\u5b9a\u53c2\u6570\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u7684\u547d\u4ee4\uff1a <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml \\\n--opt eval.ckpt_load_path=\"/path/to/local_ckpt.ckpt\" \\\neval.dataset_root=\"/path/to/val_set/root\" \\\neval.data_dir=\"val_set/dir\"\\\neval.label_file=\"val_set/label\"\n</code></pre></p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#33","title":"3.3 \u63a8\u7406","text":"<p>MindOCR\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u652f\u6301MindSpore Lite\u548c ACL \u63a8\u7406\u540e\u7aef\u3002\u63a8\u7406\u6559\u7a0b\u7ed9\u51fa\u4e86\u5982\u4f55\u4f7f\u7528MindOCR\u8fdb\u884c\u63a8\u7406\u7684\u8be6\u7ec6\u6b65\u9aa4\uff0c\u4e3b\u8981\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u73af\u5883\u51c6\u5907\u3001\u6a21\u578b\u8f6c\u6362\u548c\u63a8\u7406\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#331","title":"3.3.1 \u73af\u5883\u51c6\u5907","text":"<p>\u8bf7\u53c2\u8003\u73af\u5883\u5b89\u88c5\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u6839\u636e\u6a21\u578b\u6ce8\u610f\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#332","title":"3.3.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u7528\u6237\u9700\u8981\u4ece\u8bad\u7ec3\u5f97\u5230\u7684checkpoint\u6587\u4ef6\u5bfc\u51fa\u4e00\u4e2aMindIR\u6587\u4ef6\u3002MindSpore IR (MindIR)\u662f\u57fa\u4e8e\u56fe\u5f62\u8868\u793a\u7684\u51fd\u6570\u5f0fIR\u3002MindIR\u6587\u4ef6\u5b58\u50a8\u4e86\u63a8\u7406\u6240\u9700\u7684\u6a21\u578b\u7ed3\u6784\u548c\u6743\u91cd\u53c2\u6570\u3002</p> <p>\u6839\u636e\u8bad\u7ec3\u597d\u7684dbnet checkpoint\u6587\u4ef6\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u51faMindIR\uff1a <pre><code>python tools/export.py --model_name dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n# \u6216\u8005\npython tools/export.py --model_name configs/det/dbnet/db_r50_icdar15.yaml --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n</code></pre></p> <p><code>data_shape</code>\u662fMindIR\u6587\u4ef6\u7684\u6a21\u578b\u8f93\u5165\u56fe\u7247\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002\u5f53\u7528\u6237\u4f7f\u7528\u5176\u4ed6\u7684\u6a21\u578b\u65f6\uff0c<code>data_shape</code>\u53ef\u80fd\u4f1a\u6539\u53d8\u3002</p> <p>\u8bf7\u53c2\u8003\u8f6c\u6362\u6559\u7a0b\u83b7\u53d6\u66f4\u591a\u5173\u4e8e\u6a21\u578b\u8f6c\u6362\u7684\u7ec6\u8282\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#333-python","title":"3.3.3 \u63a8\u7406 (Python)","text":"<p>\u7ecf\u8fc7\u6a21\u578b\u8f6c\u6362\u540e\uff0c \u7528\u6237\u80fd\u5f97\u5230<code>output.mindir</code>\u6587\u4ef6\u3002\u7528\u6237\u53ef\u4ee5\u8fdb\u5165\u5230<code>deploy/py_infer</code>\u76ee\u5f55\uff0c\u5e76\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u63a8\u7406\uff1a</p> <pre><code>python infer.py \\\n--input_images_dir=/your_path_to/test_images \\\n--device=Ascend \\\n--device_id=0 \\\n--det_model_path=your_path_to/output.mindir \\\n--det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n--backend=lite \\\n--res_save_dir=results_dir\n</code></pre> <p>\u8bf7\u53c2\u8003\u63a8\u7406\u6559\u7a0b\u7684<code>4.1 \u547d\u4ee4\u793a\u4f8b</code>\u7ae0\u8282\u83b7\u53d6\u66f4\u591a\u4f8b\u5b50\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#mindocr","title":"MindOCR \u4e91\u4e0a\u8bad\u7ec3\u5feb\u901f\u5165\u95e8","text":"<p>\u672c\u6587\u4e3b\u8981\u4ecb\u7ecdMindOCR\u501f\u52a9OPENI\u542f\u667a\u5e73\u53f0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#_1","title":"\u8fc1\u79fb\u5916\u90e8\u9879\u76ee","text":"<p>\u70b9\u51fb\u542f\u667a\u5e73\u53f0\u4e3b\u9875\u9762\u53f3\u4e0a\u89d2\u7684\u52a0\u53f7\uff0c\u4ece\u4e0b\u62c9\u83dc\u5355\u4e2d\u9009\u62e9\u8fc1\u79fb\u5916\u90e8\u9879\u76ee\uff0c\u5c06MindOCR\u4ecegithub\u8fc1\u79fb\u81f3\u542f\u667a\u5e73\u53f0\u3002</p> <p>\u8f93\u5165MindOCR\u7684git url: https://github.com/mindspore-lab/mindocr.git \u5373\u53ef\u8fdb\u884c\u8fc1\u79fb\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#_2","title":"\u51c6\u5907\u6570\u636e\u96c6","text":"<p>\u53ef\u4ee5\u4e0a\u4f20\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u4e5f\u53ef\u4ee5\u5173\u8054\u5e73\u53f0\u5df2\u6709\u7684\u6570\u636e\u96c6\u3002</p> <p>\u4e0a\u4f20\u4e2a\u4eba\u6570\u636e\u96c6\u9700\u5c06\u53ef\u7528\u96c6\u7fa4\u9009\u62e9\u4e3aNPU.</p>"},{"location":"cn/tutorials/training_on_openi/#_3","title":"\u51c6\u5907\u9884\u8bad\u7ec3\u6a21\u578b(\u53ef\u9009)","text":"<p>\u5982\u9700\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u53ef\u5728\u6a21\u578b\u9009\u9879\u5361\u4e2d\u6dfb\u52a0\u3002</p> <p>\u5bfc\u5165\u672c\u5730\u6a21\u578b\u65f6\uff0c\u6a21\u578b\u6846\u67b6\u7eed\u4e3aMindSpore.</p>"},{"location":"cn/tutorials/training_on_openi/#_4","title":"\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1","text":"<p>\u5728\u4e91\u8111\u9009\u9879\u5361\u4e2d\u9009\u62e9\u8bad\u7ec3\u4efb\u52a1-&gt;\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1\u3002</p> <p>\u57fa\u672c\u4fe1\u606f\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u9009\u62e9\u4e3aAscend NPU.</p> <p>\u8bbe\u7f6e\u53c2\u6570\u5e76\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570\u3002</p> <ul> <li>\u5982\u9700\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u53ef\u5728\u9009\u62e9\u6a21\u578b\u4e2d\u9009\u62e9\u5df2\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u5e76\u5728\u8fd0\u884c\u53c2\u6570\u4e2d\u589e\u52a0ckpt_dir\u53c2\u6570\uff0c\u53c2\u6570\u503c\u4e3a/cache/*.ckpt\uff0c*\u4e3a\u5b9e\u9645\u7684\u6587\u4ef6\u540d</li> <li>AI\u5f15\u64ce\u4e2d\u9700\u9009\u62e9mindspore 1.9\u6216\u4ee5\u4e0a\u7684\u7248\u672c\uff0c\u542f\u52a8\u6587\u4ef6\u4e3a<code>tools/train.py</code></li> <li>\u8fd0\u884c\u53c2\u6570\u9700\u6dfb\u52a0<code>enable_modelarts</code>\uff0c\u503c\u4e3aTrue</li> <li>\u8fd0\u884c\u53c2\u6570\u4e2d\u7531<code>config</code>\u53c2\u6570\u6307\u5b9a\u5177\u4f53\u7684\u6a21\u578b\u7b97\u6cd5\uff0c\u53c2\u6570\u503c\u524d\u7f00\u4e3a/home/work/user-job-dir/\u8fd0\u884c\u7248\u672c\u53f7\uff0c\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1\u7684\u8fd0\u884c\u7248\u672c\u53f7\u901a\u5e38\u4e3aV0001</li> </ul>"},{"location":"cn/tutorials/training_on_openi/#_5","title":"\u4fee\u6539\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1","text":"<p>\u70b9\u51fb\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1\u7684\u4fee\u6539\u6309\u94ae\uff0c\u53ef\u4ee5\u57fa\u4e8e\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1\u8fdb\u884c\u53c2\u6570\u4fee\u6539\u5e76\u8fd0\u884c\u65b0\u7684\u8bad\u7ec3\u4efb\u52a1\u3002</p> <p>\u6ce8\u610f\uff1a\u8fd0\u884c\u7248\u672c\u53f7=\u6240\u57fa\u4e8e\u7248\u672c\u53f7+1</p>"},{"location":"cn/tutorials/training_on_openi/#_6","title":"\u72b6\u6001\u67e5\u770b","text":"<p>\u70b9\u51fb\u76f8\u5e94\u7684\u4efb\u52a1\u540d\u79f0\uff0c\u5373\u53ef\u67e5\u770b\u914d\u7f6e\u4fe1\u606f\u3001\u65e5\u5fd7\u3001\u8d44\u6e90\u5360\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u7ed3\u679c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#reference","title":"Reference","text":"<p>[1] Modified from https://github.com/mindspore-lab/mindyolo/blob/master/tutorials/cloud/openi_CN.md</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u8bc6\u522b\u7f51\u7edc","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8fdb\u884c\u8bc6\u522b\u7f51\u7edc\u8bad\u7ec3\u7684\u6559\u5b66\uff0c\u5305\u62ec\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_2","title":"\u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u76ee\u524dMindOCR\u8bc6\u522b\u7f51\u7edc\u652f\u6301\u4e24\u79cd\u8f93\u5165\u5f62\u5f0f\uff0c\u5206\u522b\u4e3a - <code>\u901a\u7528\u6570\u636e</code>\uff1a\u4f7f\u7528\u56fe\u50cf\u548c\u6587\u672c\u6587\u4ef6\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5RecDataset\u7c7b\u578b\u8bfb\u53d6\u3002 - <code>LMDB\u6570\u636e</code>: \u4f7f\u7528LMDB\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5LMDBDataset\u7c7b\u578b\u8bfb\u53d6\u3002</p> <p>\u4ee5\u4e0b\u6559\u5b66\u4ee5\u4f7f\u7528<code>\u901a\u7528\u6570\u636e</code>\u6587\u4ef6\u683c\u5f0f\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_3","title":"\u8bad\u7ec3\u96c6\u51c6\u5907","text":"<p>\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002txt\u6587\u4ef6\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># \u6587\u4ef6\u540d   # \u5bf9\u5e94\u6807\u7b7e\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> \u6ce8\u610f\uff1a\u8bf7\u5c06\u56fe\u7247\u540d\u548c\u6807\u7b7e\u4ee5 \\tab \u4f5c\u4e3a\u5206\u9694\uff0c\u907f\u514d\u4f7f\u7528\u7a7a\u683c\u6216\u5176\u4ed6\u5206\u9694\u7b26\u3002</p> <p>\u6700\u7ec8\u8bad\u7ec3\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_4","title":"\u9a8c\u8bc1\u96c6\u51c6\u5907","text":"<p>\u540c\u6837\uff0c\u8bf7\u5c06\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002\u6700\u7ec8\u9a8c\u8bc1\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_5","title":"\u5b57\u5178\u51c6\u5907","text":"<p>\u4e3a\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\uff0c\u7528\u6237\u9700\u914d\u7f6e\u5bf9\u5e94\u7684\u5b57\u5178\u3002\u53ea\u6709\u5b58\u5728\u4e8e\u5b57\u5178\u4e2d\u7684\u5b57\u7b26\u4f1a\u88ab\u6a21\u578b\u6b63\u786e\u9884\u6d4b\u3002MindOCR\u73b0\u63d0\u4f9b\u9ed8\u8ba4\u3001\u4e2d\u548c\u82f1\u4e09\u79cd\u5b57\u5178\uff0c\u5176\u4e2d - <code>\u9ed8\u8ba4\u5b57\u5178</code>: \u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\u3002\u5982\u7528\u6237\u4e0d\u914d\u7f6e\u5b57\u5178\uff0c\u8be5\u5b57\u5178\u4f1a\u88ab\u9ed8\u8ba4\u4f7f\u7528\u3002 - <code>\u82f1\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/en_dict.txt</code>\u3002 - <code>\u4e2d\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5e38\u7528\u4e2d\u6587\u5b57\u7b26\u3001\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/ch_dict.txt</code>\u3002</p> <p>\u76ee\u524dMindOCR\u6682\u672a\u63d0\u4f9b\u5176\u4ed6\u8bed\u79cd\u7684\u5b57\u5178\u914d\u7f6e\u3002\u8be5\u529f\u80fd\u5c06\u5728\u65b0\u7248\u672c\u4e2d\u63a8\u51fa\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_6","title":"\u914d\u7f6e\u6587\u4ef6\u51c6\u5907","text":"<p>\u9488\u5bf9\u4e0d\u540c\u7f51\u7edc\u7ed3\u6784\uff0c\u7528\u6237\u9700\u914d\u7f6e\u76f8\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u73b0\u5df2CRNN\uff08\u4ee5Resnet34\u4e3a\u9aa8\u5e72\u6a21\u578b\uff09\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_7","title":"\u914d\u7f6e\u82f1\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u82f1\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>\u7531\u4e8e\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\u7684\u5b57\u5178\u9ed8\u8ba4\u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\uff0c\u4e3a\u4f7f\u7528\u5b8c\u6574\u82f1\u6587\u5b57\u5178\uff0c\u7528\u6237\u9700\u8981\u4fee\u6539\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u7684<code>common: num_classes</code>\u5c5e\u6027\uff1a</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 95                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 96                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\nuse_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_8","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u82f1\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\uff08\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_9","title":"\u914d\u7f6e\u4e2d\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34_ch.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u540c\u6837\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n...\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u8bad\u7ec3\u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\ndataset:\ntype: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\ndataset_root: dir/to/data/                                        # \u9a8c\u8bc1\u6570\u636e\u96c6\u6839\u76ee\u5f55\ndata_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\nlabel_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u4e2d\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\ncharacter_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\nnum_classes: &amp;num_classes 6625                                      # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\nuse_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_10","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u4e2d\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1 (\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_11","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u5f53\u6240\u6709\u6570\u636e\u96c6\u548c\u914d\u7f6e\u6587\u4ef6\u51c6\u5907\u5b8c\u6210\uff0c\u7528\u6237\u53ef\u5f00\u59cb\u8bad\u7ec3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u7684\u6a21\u578b\u3002\u7531\u4e8e\u5404\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u4e0d\u540c\uff0c\u7528\u6237\u53ef\u53c2\u8003\u5bf9\u5e94\u6a21\u578b\u4ecb\u7ecd\u6587\u6863\u4e2d\u7684**\u6a21\u578b\u8bad\u7ec3**\u548c**\u6a21\u578b\u8bc4\u4f30**\u7ae0\u8282\u3002 \u8fd9\u91cc\u4ec5\u4ee5CRNN\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_12","title":"\u51c6\u5907\u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u505a\u6a21\u578b\u505a\u4e3a\u8d77\u59cb\u8bad\u7ec3\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5f80\u5f80\u80fd\u63d0\u5347\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u751a\u81f3\u7cbe\u5ea6\u3002\u4ee5\u4e2d\u6587\u6a21\u578b\u4e3a\u4f8b\uff0c\u6211\u4eec\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7f51\u5740\u4e3ahttps://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt, \u7528\u6237\u4ec5\u9700\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u6dfb\u52a0<code>model.pretrained</code>\u6dfb\u52a0\u5bf9\u5e94\u7f51\u5740\u5982\u4e0b</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\npretrained: https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>\u5982\u679c\u9047\u5230\u7f51\u7edc\u95ee\u9898\uff0c\u7528\u6237\u53ef\u5c1d\u8bd5\u9884\u5148\u628a\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u628a<code>model.pretained</code>\u6539\u4e3a\u672c\u5730\u5730\u5740\u5982\u4e0b</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\npretrained: /local_path_to_the_ckpt/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>\u5982\u679c\u7528\u6237\u4e0d\u9700\u8981\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53ea\u9700\u628a<code>model.pretrained</code>\u5220\u9664\u5373\u53ef\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_13","title":"\u542f\u52a8\u8bad\u7ec3","text":""},{"location":"cn/tutorials/training_recognition_custom_dataset/#_14","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u5728\u5927\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u8bae\u7528\u6237\u4f7f\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u5bf9\u4e8e\u5728\u591a\u4e2a\u6607\u817e910\u8bbe\u5907\u6216\u7740GPU\u5361\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u5c06\u914d\u7f6e\u53c2\u6570<code>system.distribute</code>\u4fee\u6539\u4e3aTrue, \u4f8b\u5982\uff1a</p> <pre><code># \u57284\u4e2a GPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmpirun -n 4 python tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_15","title":"\u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u8981\u5728\u6ca1\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\uff0c\u8bf7\u5c06\u914d\u7f6e\u53c2\u6570<code>system.distribute</code>\u4fee\u6539\u4e3aFalse \u5e76\u8fd0\u884c\uff1a</p> <pre><code># CPU/GPU/Ascend \u8bbe\u5907\u4e0a\u7684\u5355\u5361\u8bad\u7ec3\npython tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>\u8bad\u7ec3\u7ed3\u679c\uff08\u5305\u62eccheckpoint\u3001\u6bcf\u4e2aepoch\u7684\u6027\u80fd\u548c\u66f2\u7ebf\u56fe\uff09\u5c06\u88ab\u4fdd\u5b58\u5728yaml\u914d\u7f6e\u6587\u4ef6\u7684<code>train.ckpt_save_dir</code>\u53c2\u6570\u914d\u7f6e\u7684\u76ee\u5f55\u4e0b\uff0c\u9ed8\u8ba4\u4e3a<code>./tmp_rec</code>\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_16","title":"\u65ad\u70b9\u7eed\u8bad","text":"<p>\u5982\u679c\u7528\u6237\u671f\u671b\u5728\u5f00\u59cb\u8bad\u7ec3\u65f6\u540c\u65f6\u52a0\u8f7d\u6a21\u578b\u7684\u4f18\u5316\u5668\uff0c\u5b66\u4e60\u7387\u7b49\u4fe1\u606f\uff0c\u5e76\u7ee7\u7eed\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u9762\u6dfb\u52a0<code>model.resume</code>\u4e3a\u5bf9\u5e94\u7684\u672c\u5730\u6a21\u578b\u5730\u5740\u5982\u4e0b\uff0c\u5e76\u542f\u52a8\u8bad\u7ec3</p> <pre><code>...\nmodel:\ntype: rec\ntransform: null\nbackbone:\nname: rec_resnet34\npretrained: False\nneck:\nname: RNNEncoder\nhidden_size: 64\nhead:\nname: CTCHead\nout_channels: *num_classes\nresume: /local_path_to_the_ckpt/model.ckpt\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_17","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"<p>\u90e8\u5206\u6a21\u578b(\u5305\u62ecCRNN, RARE, SVTR)\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4ee5\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\u3002\u7528\u6237\u53ef\u5c1d\u8bd5\u628a\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>system.amp_level</code>\u8bbe\u4e3a<code>O2</code>\u542f\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u4f8b\u5b50\u5982\u4e0b</p> <pre><code>system:\nmode: 0\ndistribute: True\namp_level: O2  # Mixed precision training\namp_level_infer: O2\nseed: 42\nlog_interval: 100\nval_while_train: True\ndrop_overflow_update: False\nckpt_max_keep: 5\n...\n</code></pre> <p>\u5c06<code>system.amp_level</code>\u6539\u4e3a<code>O0</code>\u5173\u95ed\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_18","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u82e5\u8981\u8bc4\u4f30\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u53ef\u4ee5\u4f7f\u7528<code>tools/eval.py</code>\u3002\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u7684<code>eval</code>\u90e8\u5206\u5c06\u53c2\u6570<code>ckpt_load_path</code>\u8bbe\u7f6e\u4e3a\u6a21\u578bcheckpoint\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u8bbe\u7f6e<code>distribute</code>\u4e3a<code>False</code>\u5982\u4e0b</p> <pre><code>system:\ndistribute: False # During evaluation stage, set to False\n...\neval:\nckpt_load_path: /local_path_to_the_ckpt/model.ckpt\n</code></pre> <p>\u7136\u540e\u8fd0\u884c\uff1a</p> <pre><code>python tools/eval.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>\u4f1a\u5f97\u51fa\u7c7b\u4f3c\u6a21\u578b\u7ed3\u679c\u5982\u4e0b</p> <pre><code>2023-06-16 03:41:20,237:INFO:Performance: {'acc': 0.821939, 'norm_edit_distance': 0.917264}\n</code></pre> <p>\u5176\u4e2d<code>acc</code>\u5bf9\u5e94\u7684\u6570\u5b57\u4e3a\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_19","title":"\u6a21\u578b\u63a8\u7406","text":"<p>\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u63a8\u7406\u811a\u672c\u5feb\u901f\u5f97\u5230\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u679c\u3002\u8bf7\u5148\u5c06\u56fe\u7247\u653e\u81f3\u5728\u540c\u4e00\u6587\u4ef6\u5939\u5185\uff0c\u7136\u540e\u6267\u884c</p> <pre><code>python tools/infer/text/predict_rec.py --image_dir {dir_to_your_image_data} --rec_algorithm CRNN_CH --draw_img_save_dir inference_results\n</code></pre> <p>\u7ed3\u679c\u4f1a\u5b58\u653e\u4e8e<code>draw_img_save_dir/rec_results.txt</code>, \u4ee5\u4e0b\u662f\u90e8\u5206\u4f8b\u5b50</p> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>\u5f97\u51fa\u63a8\u7406\u7ed3\u679c\u5982\u4e0b</p> <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/","title":"Transformation\u6559\u7a0b","text":""},{"location":"cn/tutorials/transform_tutorial/#_1","title":"\u673a\u5236","text":"<ol> <li>\u6bcf\u4e2aTransformation\u90fd\u662f\u4e00\u4e2a\u5177\u6709\u53ef\u8c03\u7528\u51fd\u6570\u7684\u7c7b\u3002\u793a\u4f8b\u5982\u4e0b</li> </ol> <pre><code>class ToCHWImage(object):\n\"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>transformation\u7684\u8f93\u5165\u59cb\u7ec8\u662fdict\uff0c\u5176\u4e2d\u5305\u542bimg_path\u3001raw label\u7b49\u6570\u636e\u4fe1\u606f\u3002</p> </li> <li> <p>transformation api\u5e94\u8be5\u660e\u786e\u8f93\u5165\u4e2d\u6240\u9700\u7684key\u4ee5\u53ca\u8f93\u51fa\u6570\u636e\u4e2d\u4fee\u6539\u6216/\u548c\u6dfb\u52a0\u7684key\u3002</p> </li> </ol> <p>\u53ef\u7528\u7684transformation\u53ef\u4ee5\u5728<code>mindocr/data/transforms/*_transform.py</code>\u4e2d\u53d1\u73b0</p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#_2","title":"\u6587\u672c\u68c0\u6d4b","text":""},{"location":"cn/tutorials/transform_tutorial/#1","title":"1. \u52a0\u8f7d\u56fe\u50cf\u548c\u6ce8\u91ca","text":""},{"location":"cn/tutorials/transform_tutorial/#_3","title":"\u51c6\u5907","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#-decodeimage","title":"\u89e3\u7801\u56fe\u50cf  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#-detlabelencode","title":"\u68c0\u6d4b\u6807\u7b7e\u7f16\u7801 - DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#2","title":"2. \u56fe\u50cf\u548c\u6ce8\u91ca\u5904\u7406/\u589e\u5f3a","text":""},{"location":"cn/tutorials/yaml_configuration/","title":"\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>\u73af\u5883\u53c2\u6570-system</li> <li>\u5171\u7528\u53c2\u6570-common</li> <li>\u6a21\u578b\u5b9a\u4e49\u53c2\u6570-model</li> <li>\u540e\u5904\u7406-postprocess</li> <li>\u8bc4\u4f30\u6307\u6807-metric</li> <li>\u635f\u5931\u51fd\u6570-loss</li> <li>\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u548c\u4f18\u5316\u5668(scheduler, optimizer, loss_scaler)</li> <li>\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565-scheduler</li> <li>\u4f18\u5316\u5668-optimizer</li> <li>\u635f\u5931\u7f29\u653e-loss_scaler</li> <li>\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b(train, eval)</li> <li>\u8bad\u7ec3\u6d41\u7a0b-train</li> <li>\u8bc4\u4f30\u6d41\u7a0b-eval</li> </ul> <p>\u672c\u6587\u6863\u4ee5 <code>configs/rec/crnn/crnn_icdar15.yaml</code> \u4e3a\u4f8b\uff0c\u8be6\u7ec6\u8bf4\u660e\u53c2\u6570\u7684\u7528\u9014\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#1-system","title":"1. \u73af\u5883\u53c2\u6570 (system)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u53ef\u9009\u503c \u5907\u6ce8 mode MindSpore\u8fd0\u884c\u6a21\u5f0f(\u9759\u6001\u56fe/\u52a8\u6001\u56fe) 0 0 / 1 0: \u8868\u793a\u5728GRAPH_MODE\u6a21\u5f0f\u4e2d\u8fd0\u884c; 1: PYNATIVE_MODE\u6a21\u5f0f distribute \u662f\u5426\u5f00\u542f\u5e76\u884c\u8bad\u7ec3 True True / False \\ device_id \u6307\u5b9a\u5355\u5361\u8bad\u7ec3\u65f6\u7684\u5361id 7 \u673a\u5668\u53ef\u7528\u7684\u5361\u7684id \u8be5\u53c2\u6570\u4ec5\u5728distribute=False\uff08\u5355\u5361\u8bad\u7ec3\uff09\u548c\u73af\u5883\u53d8\u91cfDEVICE_ID\u672a\u8bbe\u7f6e\u65f6\u751f\u6548\u3002\u5355\u5361\u8bad\u7ec3\u65f6\uff0c\u5982\u8be5\u53c2\u6570\u548c\u73af\u5883\u53d8\u91cfDEVICE_ID\u5747\u672a\u8bbe\u7f6e\uff0c\u5219\u9ed8\u8ba4\u4f7f\u75280\u5361\u3002 amp_level \u6df7\u5408\u7cbe\u5ea6\u6a21\u5f0f O0 O0/O1/O2/O3 'O0' - \u4e0d\u53d8\u5316\u3002 'O1' - \u5c06\u767d\u540d\u5355\u5185\u7684Cell\u548c\u8fd0\u7b97\u8f6c\u4e3afloat16\u7cbe\u5ea6\uff0c\u5176\u4f59\u90e8\u5206\u4fdd\u6301float32\u7cbe\u5ea6\u3002 'O2' - \u5c06\u9ed1\u540d\u5355\u5185\u7684Cell\u548c\u8fd0\u7b97\u4fdd\u6301float32\u7cbe\u5ea6\uff0c\u5176\u4f59\u90e8\u5206\u8f6c\u4e3afloat16\u7cbe\u5ea6\u3002 'O3' - \u5c06\u7f51\u7edc\u5168\u90e8\u8f6c\u4e3afloat16\u7cbe\u5ea6\u3002 seed \u968f\u673a\u79cd\u5b50 42 Integer \\ ckpt_save_policy \u6a21\u578b\u6743\u91cd\u4fdd\u5b58\u7b56\u7565 top_k \"top_k\" \u6216 \"latest_k\" \"top_k\"\u8868\u793a\u4fdd\u5b58\u524dk\u4e2a\u8bc4\u4f30\u6307\u6807\u5206\u6570\u6700\u9ad8\u7684checkpoint\uff1b\"latest_k\"\u8868\u793a\u4fdd\u5b58\u6700\u65b0\u7684k\u4e2acheckpoint\u3002 <code>k</code>\u7684\u6570\u503c\u901a\u8fc7<code>ckpt_max_keep</code>\u53c2\u6570\u5b9a\u4e49 ckpt_max_keep \u6700\u591a\u4fdd\u5b58\u7684checkpoint\u6570\u91cf 5 Integer \\ log_interval log\u8f93\u51fa\u95f4\u9694(\u5355\u4f4d:step) 100 Integer \\ val_while_train \u662f\u5426\u5f00\u542f\u8fb9\u8bad\u7ec3\u8fb9\u8bc4\u4f30 True True/False \u5982\u679c\u503c\u4e3aTrue\uff0c\u8bf7\u540c\u6b65\u914d\u7f6eeval\u6570\u636e\u96c6 val_start_epoch \u4ece\u7b2c\u51e0\u4e2aepoch\u5f00\u59cb\u8dd1\u8bc4\u4f30 1 Interger val_interval \u8bc4\u4f30\u95f4\u9694(\u5355\u4f4d: epoch) 1 Interger drop_overflow_update \u5f53loss/\u68af\u5ea6\u6ea2\u51fa\u65f6\uff0c\u662f\u5426\u653e\u5f03\u66f4\u65b0\u7f51\u7edc\u53c2\u6570 True True/False \u5982\u679c\u503c\u4e3aTrue\uff0c\u5219\u5f53\u51fa\u73b0\u6ea2\u51fa\u65f6\uff0c\u4e0d\u4f1a\u66f4\u65b0\u7f51\u7edc\u53c2\u6570"},{"location":"cn/tutorials/yaml_configuration/#2-common","title":"2. \u5171\u7528\u53c2\u6570 (common)","text":"<p>\u56e0\u4e3a\u540c\u4e00\u4e2a\u53c2\u6570\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u914d\u7f6e\u90e8\u5206\u90fd\u9700\u8981\u91cd\u590d\u5229\u7528\uff0c\u6240\u4ee5\u60a8\u53ef\u4ee5\u5728\u8fd9\u4e2a\u90e8\u5206\u81ea\u5b9a\u4e49\u4e00\u4e9b\u901a\u7528\u7684\u53c2\u6570\uff0c\u4ee5\u4fbf\u7ba1\u7406\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#3-model","title":"3. \u6a21\u578b\u5b9a\u4e49\u53c2\u6570 (model)","text":"<p>\u5728MindOCR\u4e2d\uff0c\u6a21\u578b\u7684\u7f51\u7edc\u67b6\u6784\u5212\u5206\u4e3a Transform, Backbone, Neck\u548cHead\u56db\u4e2a\u6a21\u5757\u3002\u8be6\u7ec6\u8bf7\u53c2\u9605\u6587\u6863\uff0c\u4ee5\u4e0b\u662f\u5404\u90e8\u5206\u7684\u914d\u7f6e\u8bf4\u660e\u4e0e\u4f8b\u5b50\u3002</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 type \u7f51\u7edc\u7c7b\u578b - \u76ee\u524d\u652f\u6301 rec/det; rec\u8868\u793a\u8bc6\u522b\u4efb\u52a1\uff0cdet\u8868\u793a\u68c0\u6d4b\u4efb\u52a1 pretrained \u6307\u5b9a\u9884\u8bad\u7ec3\u6743\u91cd\u8def\u5f84\u6216url null \u652f\u6301\u672c\u5730checkpoint\u6587\u4ef6\u8def\u5f84\u6216url transform: tranform\u6a21\u5757\u914d\u7f6e name \u6307\u5b9atransform\u7f51\u7edc\u7684\u540d\u5b57 - \u76ee\u524d\u652f\u6301 STN_ON \u53d8\u6362 backbone: \u9aa8\u5e72\u7f51\u7edc\u914d\u7f6e name \u6307\u5b9a\u9aa8\u5e72\u7f51\u7edc\u7c7b\u540d\u6216\u89c4\u683c\u51fd\u6570\u540d - \u76ee\u524d\u5df2\u5b9a\u4e49\u7684\u7c7b\u6709 rec_resnet34, rec_vgg7, SVTRNet and det_resnet18, det_resnet50, det_resnet152, det_mobilenet_v3\u3002\u4ea6\u53ef\u81ea\u5b9a\u4e49\u65b0\u7684\u7c7b\u522b\uff0c\u8bf7\u53c2\u7167\u6587\u6863\u6307\u793a\u5b9a\u4e49\u3002 pretrained \u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u9aa8\u5e72\u6743\u91cd False \u652f\u6301\u4f20\u5165bool\u7c7b\u578b\u6216str\u7c7b\u578b\uff0c\u82e5\u4e3aTrue\uff0c\u5219\u901a\u8fc7backbone py\u4ef6\u4e2d\u5b9a\u4e49\u7684url\u94fe\u63a5\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u9ed8\u8ba4\u6743\u91cd\u3002\u82e5\u4f20\u5165str\uff0c\u53ef\u6307\u5b9a\u672c\u5730checkpoint\u8def\u5f84\u6216url\u8def\u5f84\u8fdb\u884c\u52a0\u8f7d\u3002 neck: \u914d\u7f6e\u7f51\u7edcNeck name Neck\u7c7b\u540d - \u76ee\u524d\u5df2\u5b9a\u4e49\u7684\u7c7b\u6709 RNNEncoder, DBFPN, EASTFPN \u548c PSEFPN. \u4ea6\u53ef\u81ea\u5b9a\u4e49\u65b0\u7684\u7c7b\u522b\uff0c\u8bf7\u53c2\u7167\u6587\u6863\u6307\u793a\u5b9a\u4e49\u3002 hidden_size RNN\u9690\u85cf\u5c42\u5355\u5143\u6570 - \\ head: \u8bbe\u7f6e\u7f51\u7edc\u9884\u6d4b\u5934 name Head\u7c7b\u540d - \u76ee\u524d\u652f\u6301CTCHead, AttentionHead, DBHead, EASTHead \u4ee5\u53ca PSEHead weight_init \u8bbe\u7f6e\u6743\u91cd\u521d\u59cb\u5316 'normal' \\ bias_init \u8bbe\u7f6e\u6743\u504f\u5dee\u521d\u59cb\u5316 'zeros' \\ out_channels \u8bbe\u7f6e\u5206\u7c7b\u6570 - \\ <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u7f51\u7edc\uff0cbackbone/neck/head\u6a21\u5757\u53ef\u914d\u7f6e\u53c2\u6570\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5177\u4f53\u53ef\u914d\u7f6e\u53c2\u6570\u7531\u4e0a\u8868\u6a21\u5757\u7684<code>name</code>\u53c2\u6570\u6307\u5b9a\u7684\u7c7b\u7684__init__\u5165\u53c2\u6240\u51b3\u5b9a \uff08\u5982\u82e5\u6307\u5b9a\u4e0bneck\u6a21\u5757\u7684name\u4e3aDBFPN\uff0c\u7531\u4e8eDBFPN\u7c7b\u521d\u59cb\u5316\u5305\u62ecadaptive\u5165\u53c2\uff0c\u5219\u53ef\u5728yaml\u4e2dmodel.head\u5c42\u7ea7\u4e0b\u914d\u7f6eadaptive\u7b49\u53c2\u6570\u3002</p> <p>\u53c2\u8003\u4f8b\u5b50: DBNet, CRNN</p>"},{"location":"cn/tutorials/yaml_configuration/#4-postprocess","title":"4. \u540e\u5904\u7406 (postprocess)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/postprocess</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u540e\u5904\u7406\u7c7b\u540d - \u76ee\u524d\u652f\u6301 DBPostprocess, EASTPostprocess, PSEPostprocess, RecCTCLabelDecode \u548c RecAttnLabelDecode character_dict_path \u8bc6\u522b\u5b57\u5178\u8def\u5f84 None \u82e5\u503c\u4e3aNone, \u5219\u4f7f\u7528\u9ed8\u8ba4\u5b57\u5178[0-9a-z] use_space_char \u8bbe\u7f6e\u662f\u5426\u6dfb\u52a0\u7a7a\u683c\u5230\u5b57\u5178 False True/False <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u540e\u5904\u7406\u65b9\u6cd5\uff08\u7531name\u6307\u5b9a\uff09\uff0c\u53ef\u914d\u7f6e\u7684\u53c2\u6570\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u7531\u540e\u5904\u7406\u7c7b\u7684\u521d\u59cb\u5316\u65b9\u6cd5__init__\u7684\u5165\u53c2\u6240\u51b3\u5b9a\u3002</p> <p>\u53c2\u8003\u4f8b\u5b50: DBNet, PSENet</p>"},{"location":"cn/tutorials/yaml_configuration/#5-metric","title":"5. \u8bc4\u4f30\u6307\u6807 (metric)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/metrics</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u8bc4\u4f30\u6307\u6807\u7c7b\u540d - \u76ee\u524d\u652f\u6301 RecMetric, DetMetric main_indicator \u4e3b\u8981\u6307\u6807\uff0c\u7528\u4e8e\u6700\u4f18\u6a21\u578b\u7684\u6bd4\u8f83 hmean \u8bc6\u522b\u4efb\u52a1\u4f7f\u7528acc\uff0c\u68c0\u6d4b\u4efb\u52a1\u5efa\u8bae\u4f7f\u7528f-score character_dict_path \u8bc6\u522b\u5b57\u5178\u8def\u5f84 None \u82e5\u503c\u4e3aNone, \u5219\u4f7f\u7528\u9ed8\u8ba4\u5b57\u5178 \"0123456789abcdefghijklmnopqrstuvwxyz\" ignore_space \u662f\u5426\u8fc7\u6ee4\u7a7a\u683c True True/False print_flag \u662f\u5426\u6253\u5370log False \u5982\u8bbe\u7f6eTrue\uff0c\u5219\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c\u548c\u6807\u51c6\u7b54\u6848\u7b49\u4fe1\u606f"},{"location":"cn/tutorials/yaml_configuration/#6-loss","title":"6. \u635f\u5931\u51fd\u6570 (loss)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/losses</p> \u5b57\u6bb5 \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u635f\u5931\u51fd\u6570\u7c7b\u540d - \u76ee\u524d\u652f\u6301 L1BalancedCELoss, CTCLoss, AttentionLoss, PSEDiceLoss, EASTLoss and CrossEntropySmooth pred_seq_len \u9884\u6d4b\u6587\u672c\u7684\u957f\u5ea6 26 \u7531\u7f51\u7edc\u67b6\u6784\u51b3\u5b9a max_label_len \u6700\u957f\u6807\u7b7e\u957f\u5ea6 25 \u6570\u503c\u5e94\u5c0f\u4e8e\u7f51\u7edc\u9884\u6d4b\u6587\u672c\u7684\u957f\u5ea6 batch_size \u5355\u5361\u6279\u91cf\u5927\u5c0f 32 \\ <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u635f\u5931\u51fd\u6570\uff08\u7531name\u6307\u5b9a\uff09\uff0c\u53ef\u914d\u7f6e\u7684\u53c2\u6570\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u7531\u6240\u9009\u7684\u635f\u5931\u51fd\u6570\u7684\u5165\u53c2\u6240\u51b3\u5b9a\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#7-scheduler-optimizer-loss_scaler","title":"7. \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u548c\u4f18\u5316\u5668 (scheduler, optimizer, loss_scaler)","text":""},{"location":"cn/tutorials/yaml_configuration/#scheduler","title":"\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565 (scheduler)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/scheduler</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 scheduler \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u540d\u5b57 'constant' \u76ee\u524d\u652f\u6301 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay' min_lr \u5b66\u4e60\u7387\u6700\u5c0f\u503c 1e-6 'cosine_decay'\u8c03\u6574\u5b66\u4e60\u7387\u7684\u4e0b\u9650 lr \u5b66\u4e60\u7387 0.01 num_epochs \u603b\u8bad\u7ec3epoch\u6570 200 \u6574\u4e2a\u8bad\u7ec3\u7684\u603bepoch\u6570 warmup_epochs \u8bad\u7ec3\u5b66\u4e60\u7387warmp\u9636\u6bb5\u7684epoch\u6570 3 \u5bf9\u4e8e'cosine_decay'\uff0c<code>warmup_epochs</code>\u8868\u793a\u5c06\u5b66\u4e60\u7387\u4ece0\u63d0\u5347\u5230<code>lr</code>\u7684\u65f6\u671f\u3002 decay_epochs \u8bad\u7ec3\u5b66\u4e60\u7387\u8870\u51cf\u9636\u6bb5epoch\u6570 10 \u5bf9\u4e8e'cosine_decay'\uff0c\u8868\u793a\u5728<code>decay_epochs</code>\u5185\u5c06 <code>lr</code> \u8870\u51cf\u5230 <code>min_lr</code>\u3002\u5bf9\u4e8e'step_decay'\uff0c\u8868\u793a\u6bcf\u7ecf\u8fc7<code>decay_epochs</code>\u8f6e\uff0c\u6309<code>decay_rate</code>\u56e0\u5b50\u5c06 <code>lr</code> \u8870\u51cf\u4e00\u6b21\u3002"},{"location":"cn/tutorials/yaml_configuration/#optimizer","title":"\u4f18\u5316\u5668 (optimizer)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/optim</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 opt \u4f18\u5316\u5668\u540d 'adam' \u76ee\u524d\u652f\u6301'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'nadam', 'adan', 'rmsprop', 'adagrad', 'lamb'. filter_bias_and_bn \u8bbe\u7f6e\u662f\u5426\u6392\u9664bias\u548cbatch norm\u7684\u6743\u91cd\u9012\u51cf True \u5982\u679c\u4e3a True\uff0c\u5219\u6743\u91cd\u8870\u51cf\u5c06\u4e0d\u9002\u7528\u4e8e BN \u53c2\u6570\u548c Conv \u6216 Dense \u5c42\u4e2d\u7684\u504f\u5dee\u3002 momentum \u52a8\u91cf 0.9 \\ weight_decay \u6743\u91cd\u9012\u51cf\u7387 0 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cweight decay\u53ef\u4ee5\u662f\u4e00\u4e2a\u5e38\u91cf\u503c\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2aCell\u3002\u4ec5\u5f53\u5e94\u7528\u52a8\u6001\u6743\u91cd\u8870\u51cf\u65f6\uff0c\u5b83\u624d\u662f Cell\u3002\u52a8\u6001\u6743\u91cd\u8870\u51cf\u7c7b\u4f3c\u4e8e\u52a8\u6001\u5b66\u4e60\u7387\uff0c\u7528\u6237\u53ea\u9700\u8981\u4ee5\u5168\u5c40\u6b65\u957f\u4e3a\u8f93\u5165\u81ea\u5b9a\u4e49\u4e00\u4e2a\u6743\u91cd\u8870\u51cf\u65f6\u95f4\u8868\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4f18\u5316\u5668\u8c03\u7528WeightDecaySchedule\u5b9e\u4f8b\u83b7\u53d6\u5f53\u524d\u6b65\u957f\u7684\u6743\u91cd\u8870\u51cf\u503c\u3002 nesterov \u662f\u5426\u4f7f\u7528 Nesterov \u52a0\u901f\u68af\u5ea6 (NAG) \u7b97\u6cd5\u6765\u66f4\u65b0\u68af\u5ea6\u3002 False True/False"},{"location":"cn/tutorials/yaml_configuration/#loss_scaler","title":"\u635f\u5931\u7f29\u653e\u7cfb\u6570 (loss_scaler)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 type loss\u7f29\u653e\u65b9\u6cd5\u7c7b\u578b static \u76ee\u524d\u652f\u6301 static, dynamic\u3002\u5e38\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 loss_scale loss\u7f29\u653e\u7cfb\u6570 1.0 \\ scale_factor \u5f53\u4f7f\u7528dynamic loss scaler\u65f6\uff0c\u52a8\u6001\u8c03\u6574loss_scale\u7684\u7cfb\u6570 2.0 \u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4e2d\uff0c\u5f53\u53d1\u751f\u6ea2\u51fa\u65f6\uff0c\u635f\u5931\u7f29\u653e\u503c\u4f1a\u66f4\u65b0\u4e3a <code>loss_scale</code>/<code>scale_factor</code>\u3002 scale_window \u5f53\u4f7f\u7528dynamic loss scaler\u65f6\uff0c\u7ecf\u8fc7scale_window\u8bad\u7ec3\u6b65\u672a\u51fa\u73b0\u6ea2\u51fa\u65f6\uff0c\u5c06loss_scale\u653e\u5927scale_factor\u500d 1000 \u5982\u679c\u8fde\u7eed\u7684<code>scale_window</code>\u6b65\u6570\u6ca1\u6709\u6ea2\u51fa\uff0c\u635f\u5931\u5c06\u589e\u52a0<code>loss_scale</code>*<code>scale_factor</code>\u7f29\u653e"},{"location":"cn/tutorials/yaml_configuration/#8-train-eval","title":"8. \u8bad\u7ec3\u3001\u8bc4\u4f30\u6d41\u7a0b (train, eval)","text":"<p>\u8bad\u7ec3\u6d41\u7a0b\u7684\u914d\u7f6e\u653e\u5728 <code>train</code> \u5e95\u4e0b\uff0c\u8bc4\u4f30\u9636\u6bb5\u7684\u914d\u7f6e\u653e\u5728 <code>eval</code> \u5e95\u4e0b\u3002\u6ce8\u610f\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u82e5\u6253\u5f00\u8fb9\u8bad\u7ec3\u8fb9\u8bc4\u4f30\u6a21\u5f0f\uff0c\u5373val_while_train=True\u65f6\uff0c\u5219\u5728\u6bcf\u4e2aepoch\u8bad\u7ec3\u5b8c\u6bd5\u540e\u6309\u7167 <code>eval</code> \u5e95\u4e0b\u7684\u914d\u7f6e\u8fd0\u884c\u4e00\u6b21\u8bc4\u4f30\u3002\u5728\u975e\u8bad\u7ec3\u9636\u6bb5\uff0c\u53ea\u8fd0\u884c\u6a21\u578b\u8bc4\u4f30\u7684\u65f6\u5019\uff0c\u53ea\u8bfb\u53d6 <code>eval</code> \u914d\u7f6e\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#train","title":"\u8bad\u7ec3\u6d41\u7a0b (train)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 ckpt_save_dir \u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./tmp_rec \\ resume \u8bad\u7ec3\u4e2d\u65ad\u540e\u6062\u590d\u8bad\u7ec3\uff0c\u53ef\u8bbe\u5b9aTrue/False\uff0c\u6216\u6307\u5b9a\u9700\u8981\u52a0\u8f7d\u6062\u590d\u8bad\u7ec3\u7684ckpt\u8def\u5f84 False \u53ef\u6307\u5b9aTrue/False\u914d\u7f6e\u662f\u5426\u6062\u590d\u8bad\u7ec3\uff0c\u82e5True\uff0c\u5219\u52a0\u8f7dckpt_save_dir\u76ee\u5f55\u4e0b\u7684resume_train.ckpt\u7ee7\u7eed\u8bad\u7ec3\u3002\u4e5f\u53ef\u4ee5\u6307\u5b9ackpt\u6587\u4ef6\u8def\u5f84\u8fdb\u884c\u52a0\u8f7d\u6062\u590d\u8bad\u7ec3\u3002 dataset_sink_mode MindSpore\u6570\u636e\u4e0b\u6c89\u6a21\u5f0f - \u5982\u679c\u8bbe\u7f6eTrue\uff0c\u5219\u6570\u636e\u4e0b\u6c89\u81f3\u5904\u7406\u5668\uff0c\u81f3\u5c11\u5728\u6bcf\u4e2aepoch\u7ed3\u675f\u540e\u624d\u80fd\u8fd4\u56de\u6570\u636e gradient_accumulation_steps \u7d2f\u79ef\u68af\u5ea6\u7684\u6b65\u6570 1 \u6bcf\u4e00\u6b65\u4ee3\u8868\u4e00\u6b21\u6b63\u5411\u8ba1\u7b97\uff0c\u68af\u5ea6\u7d2f\u8ba1\u5b8c\u6210\u518d\u8fdb\u884c\u4e00\u6b21\u53cd\u5411\u4fee\u6b63 clip_grad \u662f\u5426\u88c1\u526a\u68af\u5ea6 False \u5982\u679c\u8bbe\u7f6eTrue\uff0c\u5219\u5c06\u68af\u5ea6\u88c1\u526a\u6210 <code>clip_norm</code> clip_norm \u8bbe\u7f6e\u88c1\u526a\u68af\u5ea6\u7684\u8303\u6570 1 \\ ema \u662f\u5426\u542f\u52a8EMA\u7b97\u6cd5 False \\ ema_decay EMA\u8870\u51cf\u7387 0.9999 \\ pred_cast_fp32 \u662f\u5426\u5c06logits\u7684\u6570\u636e\u7c7b\u578b\u5f3a\u5236\u8f6c\u6362\u4e3afp32 False \\ dataset \u6570\u636e\u96c6\u914d\u7f6e \u8be6\u7ec6\u8bf7\u53c2\u9605Data\u6587\u6863 type \u6570\u636e\u96c6\u7c7b\u578b - \u76ee\u524d\u652f\u6301 LMDBDataset, RecDataset \u548c DetDataset dataset_root \u6570\u636e\u96c6\u6240\u5728\u6839\u76ee\u5f55 None Optional data_dir \u6570\u636e\u96c6\u6240\u5728\u5b50\u76ee\u5f55 - \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e<code>dataset_root</code>\uff0c\u8bf7\u5c06\u6b64\u8bbe\u7f6e\u6210\u5b8c\u6574\u76ee\u5f55 label_file \u6570\u636e\u96c6\u7684\u6807\u7b7e\u6587\u4ef6\u8def\u5f84 - \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e<code>dataset_root</code>\uff0c\u8bf7\u5c06\u6b64\u8bbe\u7f6e\u6210\u5b8c\u6574\u8def\u5f84\uff0c\u5426\u5219\u53ea\u9700\u8bbe\u7f6e\u5b50\u8def\u5f84 sample_ratio \u6570\u636e\u96c6\u62bd\u6837\u6bd4\u7387 1.0 \u82e5\u6570\u503c&lt;1.0\uff0c\u5219\u968f\u673a\u9009\u53d6 shuffle \u662f\u5426\u6253\u4e71\u6570\u636e\u987a\u5e8f \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False transform_pipeline \u6570\u636e\u5904\u7406\u6d41\u7a0b None \u8be6\u60c5\u8bf7\u770b transforms output_columns \u6570\u636e\u52a0\u8f7d\uff08data loader\uff09\u6700\u7ec8\u9700\u8981\u8f93\u51fa\u7684\u6570\u636e\u5c5e\u6027\u540d\u79f0\u5217\u8868\uff08\u7ed9\u5230\u7f51\u7edc/loss\u8ba1\u7b97/\u540e\u5904\u7406) (\u7c7b\u578b\uff1a\u5217\u8868\uff09\uff0c\u5019\u9009\u7684\u6570\u636e\u5c5e\u6027\u540d\u79f0\u7531transform_pipeline\u6240\u51b3\u5b9a\u3002 None \u5982\u679c\u503c\u4e3aNone\uff0c\u5219\u8f93\u51fa\u6240\u6709\u5217\u3002\u4ee5crnn\u4e3a\u4f8b\uff0coutput_columns: ['image', 'text_seq'] net_input_column_index output_columns\u4e2d\uff0c\u5c5e\u4e8e\u7f51\u7edcconstruct\u51fd\u6570\u7684\u8f93\u5165\u9879\u7684\u7d22\u5f15 [0] \\ label_column_index output_columns\u4e2d\uff0c\u5c5e\u4e8eloss\u51fd\u6570\u7684\u8f93\u5165\u9879\u7684\u7d22\u5f15 [1] \\ loader \u6570\u636e\u52a0\u8f7d\u8bbe\u7f6e shuffle \u6bcf\u4e2aepoch\u662f\u5426\u6253\u4e71\u6570\u636e\u987a\u5e8f \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False batch_size \u5355\u5361\u7684\u6279\u91cf\u5927\u5c0f - \\ drop_remainder \u5f53\u6570\u636e\u603b\u6570\u4e0d\u80fd\u9664\u4ee5batch_size\u65f6\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u6279\u6570\u636e \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False max_rowsize \u6307\u5b9a\u5728\u591a\u8fdb\u7a0b\u4e4b\u95f4\u590d\u5236\u6570\u636e\u65f6\uff0c\u5171\u4eab\u5185\u5b58\u5206\u914d\u7684\u6700\u5927\u7a7a\u95f4 64 \\ num_workers \u6307\u5b9a batch \u64cd\u4f5c\u7684\u5e76\u53d1\u8fdb\u7a0b\u6570/\u7ebf\u7a0b\u6570 n_cpus / n_devices - 2 \u8be5\u503c\u5e94\u5927\u4e8e\u6216\u7b49\u4e8e2 <p>\u53c2\u8003\u4f8b\u5b50: DBNet, CRNN</p>"},{"location":"cn/tutorials/yaml_configuration/#eval","title":"\u8bc4\u4f30\u6d41\u7a0b (eval)","text":"<p><code>eval</code> \u7684\u53c2\u6570\u4e0e <code>train</code> \u57fa\u672c\u4e00\u6837\uff0c\u53ea\u8865\u5145\u8bf4\u660e\u51e0\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff0c\u5176\u4f59\u7684\u8bf7\u53c2\u8003\u4e0a\u8ff0<code>train</code>\u7684\u53c2\u6570\u8bf4\u660e\u3002</p> \u5b57\u6bb5 \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 ckpt_load_path \u8bbe\u7f6e\u6a21\u578b\u52a0\u8f7d\u8def\u5f84 - \\ num_columns_of_labels \u8bbe\u7f6e\u6570\u636e\u96c6\u8f93\u51fa\u5217\u4e2d\u7684\u6807\u7b7e\u6570 None \u9ed8\u8ba4\u5047\u8bbe\u56fe\u50cf (data[1:]) \u4e4b\u540e\u7684\u5217\u662f\u6807\u7b7e\u3002\u5982\u679c\u503c\u4e0d\u4e3aNone\uff0c\u5373image(data[1:1+num_columns_of_labels])\u4e4b\u540e\u7684num_columns_of_labels\u5217\u662f\u6807\u7b7e\uff0c\u5176\u4f59\u5217\u662f\u9644\u52a0\u4fe1\u606f\uff0c\u5982image_path\u3002 drop_remainder \u5f53\u6570\u636e\u603b\u6570\u4e0d\u80fd\u9664\u4ee5batch_size\u65f6\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u6279\u6570\u636e \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse \u5728\u505a\u6a21\u578b\u8bc4\u4f30\u65f6\u5efa\u8bae\u8bbe\u7f6e\u6210False\uff0c\u82e5\u4e0d\u80fd\u6574\u9664\uff0cmindocr\u4f1a\u81ea\u52a8\u9009\u62e9\u4e00\u4e2a\u6700\u5927\u53ef\u6574\u9664\u7684batch size"}]}